{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "619cddc0",
   "metadata": {},
   "source": [
    "# Import, load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fea14e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabio.casati/.venvs/3new/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, os, time\n",
    "\n",
    "from llm import LLM, Envs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "from hc import name_clusters_with_topics, get_representative_docs, get_cluster_keywords, hdbscan_kmeans_refine, robust_json_parse\n",
    "from hc import build_cluster_hierarchy, print_clusters, iterative_higher_level_clustering\n",
    "\n",
    "\n",
    "llm = LLM (\n",
    "    # model=\"gpt-4.5-preview\",\n",
    "    model = \"openai/gpt-4.1\",\n",
    "    llm_env = Envs.OPENROUTER.value\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0258ff4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'publish_time', 'abstract', 'keyword', 'tL_DL', 'titlelength',\n",
       "       'paper_decision_time', 'paper_decision', 'paper_decision_comment',\n",
       "       'paper_decision_commentlength', 'review_publish_time', 'reviewer_level',\n",
       "       'paper_score', 'rate', 'review_score_one', 'review_score_two',\n",
       "       'review_score_three', 'review', 'review_contentlength'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tp_2020conference.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec75ccc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'publish_time', 'abstract', 'keyword', 'tL_DL', 'titlelength',\n",
       "       'paper_decision_time', 'paper_decision', 'paper_decision_comment',\n",
       "       'paper_decision_commentlength', 'review_publish_time', 'reviewer_level',\n",
       "       'paper_score', 'rate', 'review_score_one', 'review_score_two',\n",
       "       'review_score_three', 'review', 'review_contentlength'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns #= ['id', 'title', 'abstract', 'keywords', 'authors', 'venue', 'year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d520916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e5b45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paper_decision\n",
       "Decision:###Reject                4638\n",
       "Decision:###Accept (Poster)       1588\n",
       "Decision:###Accept (Spotlight)     320\n",
       "Decision:###Accept (Talk)          145\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['paper_decision'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bcc700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_to_score = {\n",
    "    'Decision:###Reject': 0,\n",
    "    'Decision:###Accept (Poster)': 1,\n",
    "    'Decision:###Accept (Spotlight)': 2,\n",
    "    'Decision:###Accept (Talk)': 3\n",
    "}\n",
    "# Map decisions to scores\n",
    "df['paper_decision_numeric'] = df['paper_decision'].map(decision_to_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b05b0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paper_decision_numeric\n",
       "0.0    4638\n",
       "1.0    1588\n",
       "2.0     320\n",
       "3.0     145\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of distinct titles\n",
    "df['paper_decision_numeric'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3e02dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df with only accpetance and review\n",
    "df_accept = df[df['paper_decision_numeric'] > 0][['title', 'review', 'paper_decision_comment','paper_decision_numeric' ]]\n",
    "\n",
    "# df with only rejection and review\n",
    "df_reject = df[df['paper_decision_numeric'] == 0][['title', 'review', 'paper_decision_comment','paper_decision_numeric' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c859c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paper_decision_numeric\n",
       "0.0    4638\n",
       "1.0    1588\n",
       "2.0     199\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:7500]['paper_decision_numeric'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b9427",
   "metadata": {},
   "source": [
    "# LLM only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cae4ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:28:24,953 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      " Below is a concise summary of the main reasons for acceptance or rejection for each paper, based on the reviews and decision scores:\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Lack of rigorous experimentation: Does not compare to some relevant previous works.\n",
      "  - Limited scope: Evaluated only on image classification; no evidence for applicability to other domains.\n",
      "  - Results are only marginally better than competitors and inconsistently analyzed.\n",
      "  - Missing ablation studies and deeper/theoretical analysis.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 2 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Experiments do not show state-of-the-art results.\n",
      "  - Evaluated only on small standard datasets, lacking larger or more difficult examples.\n",
      "  - Algorithm design and convergence properties are unclear and poorly justified.\n",
      "  - Missing theoretical guarantees and inadequate explanation of key procedures.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 3 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Limited empirical validation (only on simple datasets).\n",
      "  - No theoretical justification or analysis beyond empirical evidence.\n",
      "  - Results are statistically significant but improvements are small.\n",
      "  - Introduces new hyperparameters but lacks evidence of broad applicability.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Highly incremental over existing methods.\n",
      "  - Experimental results not surprising, lacking comparison with better baselines.\n",
      "  - Large-scale experiments on more difficult datasets are missing.\n",
      "  - Contribution is considered minor and not sufficiently novel.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 5 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Method is straightforward with low novelty.\n",
      "  - Cited related work is not adequately discussed, missing context about prior art.\n",
      "  - Contribution is seen as incremental despite competitive empirical results.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Fails to cite or discuss closely related, prior work covering similar ideas with theoretical analysis.\n",
      "  - Contribution is weak as it mainly revisits known concepts without substantial advancement.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 7 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Related theoretical work is not cited, making the empirical focus narrow.\n",
      "  - Lacks thorough empirical analysis and comparisons to alternative methods.\n",
      "  - Baseline comparisons and dataset variety are insufficient.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 8 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Interesting initial results but lacks demonstration that findings transfer or lead to improved perceptual loss functions.\n",
      "  - Missing application to downstream problems and lacks broad generalization impact.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 9 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Findings regarded as unsurprising given the definition of the metric.\n",
      "  - Analysis considered narrow with limited potential impact.\n",
      "  - Presentation and explanation of results need improvement.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 10 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Experiments are incomplete and sometimes anecdotal/confusing.\n",
      "  - No evaluation of proposed feature selection methods for real-world improvement, and no qualitative results shown.\n",
      "  - Some presentation issues; lacks rigorous justification and baseline comparisons.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 11 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Metric definitions and validation are unclear or questionable.\n",
      "  - Results are not presented clearly and require better analysis/explanation.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 12 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Experimental results are unconvincing and limited in dimensionality and function variety.\n",
      "  - Approach may not deliver significant advantages over state-of-the-art optimization methods.\n",
      "  - Questions about scaling and utility remain unanswered.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 13 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Main question about exploration/utility of the approach is not answered.\n",
      "  - Details of the proposed method and results are confusing and lack rigorous explanations.\n",
      "  - Lacks clarity on theoretical or practical improvements.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 14 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Method demonstrates a simple, incremental use case.\n",
      "  - Experiments are limited to easy problems and fail to show strong new capabilities.\n",
      "  - Does not address clearly how method improves over strong ES baselines except with simple tasks.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 15 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Overlaps/duplicates prior work, misses critical citations, and lacks clear novelty.\n",
      "  - Comparisons to known generalization bounds are missing.\n",
      "  - Paper fails to clearly demonstrate new or significant theoretical advances.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 16 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Technical contribution is limited, with much of the analysis already covered in prior work.\n",
      "  - Critically misses key citations and fails to sufficiently situate its findings.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 17 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Results are largely supported by prior work; lacks sufficient theoretical novelty.\n",
      "  - Only limited experimental scope; needs more convincing analysis and comparisons.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 18 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Proofs contain critical mistakes; theoretical contributions are insufficient as currently stated.\n",
      "  - Presentation is incomplete and technical details are unclear; major results in original version are incorrect.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 19 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Technical contribution is scattered/incremental, only basic novelty in application.\n",
      "  - Sister/named baselines missing; lacking explanations of differences to similar models.\n",
      "  - Experiments well-run but advances are minor in technical content.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 20 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Weak novelty just extends existing techniques in a trivial/obvious way.\n",
      "  - Experimental results are questionable (wrong baselines/metrics); lacks significant advancement or convincing evidence.\n",
      "  - Ablation and analysis of design choices is incomplete.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 21 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Closely related to prior work; only minor novelty in combining existing ideas.\n",
      "  - More/better baselines are needed.\n",
      "  - Clear demonstration of significant improvement/generalizability is lacking.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 22 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Lacks comparison to strong/appropriate baselines.\n",
      "  - Unclear/untested benefit of key design features in realistic zero-shot setups.\n",
      "  - Paper is strong empirically but limited in benchmarking/ablation and generality claims.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 23 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - Weak experimental baselines, especially for zero-shot and ablation studies.\n",
      "  - Scope and applicability are limited.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 24 (Score: 0.0, Reject)**\n",
      "- *Main reasons for rejection:*  \n",
      "  - The setting is unclear; methodology and math are difficult to follow for general ML readers.\n",
      "  - Empirical thoroughness, but paper lacks accessible motivation and sufficient exposition to a broad audience.\n",
      "\n",
      "---\n",
      "\n",
      "(For brevity, for the rest, please indicate which numbers if you want detailed summaries for all 100+.)\n",
      "\n",
      "---\n",
      "\n",
      "**In summary:**  \n",
      "Rejections were predominantly due to lack of novelty, insufficient experimentation or ablation, missing baselines, poor theoretical rigor, incremental contributions, unclear writing/exposition, or failure to clearly improve over the state of the art or prior art. Acceptances would be indicated by decision scores > 0.0 (not present above).\n",
      "\n",
      "Let me know if you'd like a summary for a particular paper or for further numbers!\n"
     ]
    }
   ],
   "source": [
    "from jinja2 import Template\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Jinja2 template with your column names\n",
    "template_str = \"\"\"\n",
    "Analyze the following paper reviews and their decision scores (0 = reject, higher numbers = stronger acceptance). Provide the main reasons for acceptance or rejection for each paper in a concise summary.\n",
    "\n",
    "{% for index, row in reviews.iterrows() %}\n",
    "Paper {{ index + 1 }}:\n",
    "Review: {{ row.review }}\n",
    "Decision Score: {{ row.paper_decision_numeric }}\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "template = Template(template_str)\n",
    "\n",
    "\n",
    "# Render the prompt with dataframe data\n",
    "prompt = template.render(reviews=df[:500])\n",
    "\n",
    "\n",
    "response = llm._call(prompt)\n",
    "print(\"LLM Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9f89769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:29:10,994 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      " Certainly! Below is an analytical summary, as requested, of the **top 10 reasons for acceptance** (inferred, since no acceptances are present, these are reverse-engineered from the rejections as their opposites) and the **top 10 reasons for rejection** (based directly on the provided reviews). \n",
      "\n",
      "Reasons are grouped and ranked by assessing their centrality to the decision (i.e., whether papers lacking a feature are almost always rejected), with frequency and explicit reviewer emphasis both taken into account. Synonymous or closely related reasons are combined for clarity and focus.\n",
      "\n",
      "---\n",
      "\n",
      "**Top 10 Reasons for Acceptance:**  \n",
      "*(Inferred from the most common and important grounds for rejection, as acceptances were not given in the summaries)*\n",
      "\n",
      "1. **Strong Technical Novelty and Contribution**\n",
      "   - Importance: A paper presenting genuinely new ideas or techniques, rather than incremental or trivial extensions, is often seen as necessary for publication since it advances the field rather than restating known work.\n",
      "2. **Rigorous and Thorough Experimental Validation**\n",
      "   - Importance: Demonstrating robust results via large-scale, diverse, and challenging datasets, along with well-chosen baselines, shows the effectiveness and reliability of the proposed method.\n",
      "3. **Comprehensive Baseline Comparisons**\n",
      "   - Importance: Carefully benchmarking against strong or state-of-the-art alternatives proves a new approach truly advances over prior work, rather than benefitting from weak or irrelevant comparisons.\n",
      "4. **Solid Theoretical Justification and Analysis**\n",
      "   - Importance: Providing clear mathematical or theoretical grounding to claims ensures that findings are not only empirically useful but also understood at a fundamental level, supporting generalizability.\n",
      "5. **Clear Empirical Advantages Over Prior Work**\n",
      "   - Importance: Evidence that the new method outperforms existing techniques—on relevant measures—demonstrates its value and increases impact.\n",
      "6. **Clarity of Presentation and Exposition**\n",
      "   - Importance: Clear writing, definitions, and structured arguments allow readers and reviewers to easily follow the narrative and verify claims, increasing the paper's accessibility and reliability.\n",
      "7. **Well-motivated and Relevant Problem Setting**\n",
      "   - Importance: Articulating why the problem or approach matters contextualizes the work within the field and highlights its necessity.\n",
      "8. **Comprehensive Ablation and Sensitivity Analyses**\n",
      "   - Importance: Delving into the importance of individual components and hyperparameters strengthens confidence that results stem from the new method itself, not tuning or spurious correlations.\n",
      "9. **Sound and Error-free Proofs or Mathematical Content**\n",
      "   - Importance: Flawless technical details—especially proofs—are critical for works making theoretical contributions; errors here often lead to rejection despite other strong qualities.\n",
      "10. **Evidence of Broad Applicability and Generalization**\n",
      "    - Importance: Papers that show results or methods transfer across tasks, domains, or settings are seen as more impactful, as their findings influence a wider audience.\n",
      "\n",
      "---\n",
      "\n",
      "**Top 10 Reasons for Rejection:**  \n",
      "\n",
      "1. **Lack of Novelty / Incremental Contribution** (Frequency: Very High)\n",
      "   - Importance: When a paper is seen as not sufficiently new or only a trivial extension of past work, it fails to meet the threshold for scholarly contribution. This is often regarded as a hard criterion for top venues.\n",
      "2. **Insufficient Experimental Validation / Limited Scope**\n",
      "   - Importance: Testing only on small, simple, or toy datasets, or in narrow settings, limits the evidence for the method's effectiveness and its potential real-world applicability.\n",
      "3. **Missing or Weak Baseline Comparisons**\n",
      "   - Importance: Without robust comparisons to strong prior methods, it's impossible to determine if the contribution actually represents progress or is just outperforming simplistic alternatives.\n",
      "4. **Poor or Lacking Theoretical Justification**\n",
      "   - Importance: Absence of convincing theory, proofs, or explanation of key procedures undermines the credibility of claims—especially for works that purport to advance understanding.\n",
      "5. **Weak, Marginal, or Unconvincing Results**\n",
      "   - Importance: When results are only marginally better, statistically insignificant, or show little practical gain, the work appears less impactful or even irrelevant.\n",
      "6. **Failure to Properly Discuss/Cite Prior Work**\n",
      "   - Importance: Missing key citations or insufficiently situating contributions in the context of the field implies lack of originality or awareness, and may suggest duplicative or redundant research.\n",
      "7. **Inadequate Ablation / Sensitivity Analysis**\n",
      "   - Importance: Not analyzing the importance of specific components or parameters makes it unclear what drives the observed results—reducing trust in the findings.\n",
      "8. **Unclear Writing, Exposition, or Method Explanation**\n",
      "   - Importance: If methodology, results, or motivation are not clearly spelled out, reviewers may not understand or trust the claims—blocking acceptance regardless of technical merit.\n",
      "9. **Errors in Proofs, Technical Details, or Methodology**\n",
      "   - Importance: Mistakes in critical technical content (e.g., proof errors, incorrect results) fundamentally undermine papers, often leading to automatic rejection.\n",
      "10. **Limited Impact, Applicability, or Generalization**\n",
      "    - Importance: If work is not broadly applicable, fails to address meaningful problems, or is unlikely to influence future research, it may be seen as irrelevant or niche and rejected as such.\n",
      "\n",
      "---\n",
      "\n",
      "**How Rankings Were Determined:**  \n",
      "- **Core Criteria:** Issues like \"lack of novelty\" and \"insufficient experimentation\" appear as grounds for rejection in virtually every summary and are almost always critical for acceptance—placing them at the top.\n",
      "- **Comparative Baselines** and **theoretical justification** are key differentiators between rigorous and weak work, as highlighted in many reviews.\n",
      "- **Writing/presentation clarity** and **impact/generalizability** are less frequently cited than novelty but are decisive when they are negative or missing.\n",
      "- Similar reasons (e.g., \"incremental contribution\" and \"lack of novelty\") are grouped; subtle distinctions (theoretical vs. empirical issues) are kept where the reviews split them.\n",
      "- The order is informed by whether failing on a criterion was a dealbreaker in the review narratives.\n",
      "\n",
      "---\n",
      "\n",
      "**In summary:**  \n",
      "Acceptance essentially requires a clear, novel, theoretically sound, and empirically demonstrated advance over the state of the art, clearly situated, rigorously presented, and broadly valuable. Rejections primarily arise from failure in one or more of these critical dimensions.\n"
     ]
    }
   ],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "# Simulated previous LLM response (replace with your actual response)\n",
    "\n",
    "# Jinja2 template for the new prompt\n",
    "template_str = \"\"\"\n",
    "You are an expert in analyzing academic paper reviews. Below is a list of reviews for multiple papers, each with main reasons for acceptance or rejection. Your task is to:\n",
    "\n",
    "1. Identify the top ten reasons for acceptance and the top ten reasons for rejection across all papers.\n",
    "2. For each reason, provide a brief desription of its importance in the determination of acceptance or rejection.\n",
    "\n",
    "Do not base your analysis solely on frequency. Try to determine how important each reason is in the context of the paper reviews.\n",
    "Base your decision solely on the reviews provided. Do not include any external information or personal opinions or prior knowledge you have on how papers are evaluated.\n",
    "\n",
    "**Input Response:**\n",
    "{{ previous_response }}\n",
    "\n",
    "**Output Format:**\n",
    "Provide your answer in this exact format:\n",
    "\n",
    "**Top 10 Reasons for Acceptance:**\n",
    "1. [Reason] (Frequency: [N])\n",
    "   - Importance: [Brief explanation]\n",
    "2. ...\n",
    "\n",
    "**Top 10 Reasons for Rejection:**\n",
    "1. [Reason] (Frequency: [N])\n",
    "   - Importance: [Brief explanation]\n",
    "2. ...\n",
    "\n",
    "Ensure your analysis is accurate, groups similar reasons (e.g., \"lacks clarity\" and \"poor presentation\" if related), and prioritizes clarity in ranking.\n",
    "Describe how you arrived at determining the reasons and their importance.\n",
    "\"\"\"\n",
    "template = Template(template_str)\n",
    "\n",
    "# Render the prompt\n",
    "prompt = template.render(previous_response=response)\n",
    "\n",
    "\n",
    "\n",
    "# Make the LLM call\n",
    "reasons_oneshot = llm._call(prompt)\n",
    "print(\"LLM Response:\\n\", reasons_oneshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7732d2",
   "metadata": {},
   "source": [
    "### Formatted output\n",
    "See output at : https://docs.google.com/document/d/122jHdVx9C_YkVQx6_Fqm2NNKev2YOgLN1Jb6JeeNa9E/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e9a32",
   "metadata": {},
   "source": [
    "## with a balanced sample of papers with different acceptance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "078a91ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 10:21:01,309 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      " Here is a concise summary of the main reasons for acceptance or rejection for each paper, based on the provided reviews and decision scores:\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1483**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Insufficient justification of technical design choices (function forms, hyperparameters), unconvincing arguments for the “global score” concept, lack of theoretical explanation for fast runtime, and evaluation limited to intrinsic measures; needs extrinsic benchmarks and more principled analysis.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1091**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Marginal improvements on standard benchmarks; motivation tenuous; unclear where gains originate (possible confounds from other modifications); insufficient ablation studies and qualitative analysis; in need of broader application tests and clearer motivation.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 2108**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Assumptions (especially unbiased stochastic subgradients) possibly do not hold, undermining theoretical results; analysis may be too restrictive; unclear system modeling and experimental relevance; lacks practical insight over prior simplistic models.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4640**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Experimental section insufficient: lacks comparison to other CLS-based approaches and results in more complex/sophisticated contexts; evaluation context not on par with literature; baseline might be weak.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 788**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Method is incremental (modification of existing domain adversarial training); improvements mainly in citation formatting, but did not affect substantive aspects of the work; no major novelty.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6242**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Provides useful ablation analysis on MAML, showing that inner loop often is only needed on classifier head (ANIL), but introducing as “new algorithms” is debated—concerns exist about comparison and leaderboard positioning.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 2236**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Lacks parameter stability analysis for regularization; not very interpretable results on real data; idea is interesting but overall impact is limited due to empirical/interpretability concerns.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 444**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Generality and robustness questionable due to limited, clean data; real-world application not demonstrated; baselines and robustness studies missing; incremental.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 7550**  \n",
      "Decision: 2.0 (Accept)  \n",
      "**Main reason:**  \n",
      "- Studies important question (cheaper ML via human rule generalization for labeling); outperforms alternatives; limited to classification/NLP, but overall idea and experiments strong; presentation could be improved.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 123**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Proposed DARTS optimization is a heuristic addressing issues only at a superficial level; empirical performance not clearly improved; speedup source unexplained; not tested on original DARTS space.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 2691**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Notation and presentation unclear; insufficient explanation for likelihood, meta-prior, and motivation for novel LSTM; baseline comparability issues.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1523**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Contribution limited to development of metrics; tested only on two few-shot learning baselines; overall impact and representativeness limited.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1539**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Incremental idea (MCMC-inspired optimization); noise interaction not analyzed; small datasets and lack of competitive baselines in experiments.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6129**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- New application of NAS to object detection; marginal accuracy improvement; limited technical novelty and poor writing, but experimental concerns mostly resolved post-rebuttal.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1661**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Theoretical motivation and depth of experimental analysis lacking; comparisons to SOTA not shown; insufficient real-world validation.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6814**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Theoretical construction shows existence of IBP-certifiable nets; however, nets are exponentially large so not practical, and related work coverage is incomplete.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 7707**  \n",
      "Decision: 3.0 (Accept)  \n",
      "**Main reason:**  \n",
      "- Novel method for pre-training RL agents in unsupervised phase (“VISF”); strong performance on Atari in few-step RL; some clarity and reproducibility issues, but post-rebuttal these were resolved.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 2286**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Hybrid GAN/deep decoder model is clear but yields marginal gains over baseline; motivation is not strong enough; limited out-of-distribution analysis.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 133**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Fails to compare with recent state-of-the-art or on benchmark domain adaptation datasets; connection to other methods not adequately drawn.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4102**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Application is interesting but little novelty in proposed architecture; lack of mathematical or conceptual contribution; comparison to conditional image translation missing.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 2925**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Model details insufficient; results only loosely described and lacking concrete presentation; does not use conference template; too short.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6281**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Introducing a simple aggregation layer atop LSTM for RL, showing improved gradient signal and strong performance; analysis is convincing.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 2757**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- CDEP (contextual decomposition penalization) is a novel idea for penalizing explanation divergence; however, not sufficiently justified or situated among annotator rationale literature, and has mixed empirical comparison to strong baselines.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1558**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Lack of evaluation on harder semantic parsing datasets and direct comparison to prior re-ranking methods; improvements over baseline not justified.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 7422**  \n",
      "Decision: 2.0 (Accept)  \n",
      "**Main reason:**  \n",
      "- Analyzes interaction of learning rate/batch size with Hessian during optimization; new findings about break-even point verified in experiments; questions adequately addressed post-rebuttal.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 678**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Proposed recoder loss for summarization model is clear but does not consistently improve outputs; experimental details and evaluation protocols raise concerns.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 595**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Writing and organization issues; limited novelty in generalization bounds; failure to relate to prior work; insufficient experimental validation.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 7059**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Develops VAEs for product manifolds (non-Euclidean latent spaces), relevant but preliminary; writing needs consolidation; experimental clarity could be improved; recommended “weak accept.”\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 316**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Two separate contributions (new uncertainty metric and new architecture) are disjoint, neither provides a convincing solution; lacks strong comparative or unified empirical validation.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6193**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Proposes to make ProtoNets less dependent on “shot”; novel bound and method lead to competitive generalization, with thoughtful analysis and well-motivated, though improvements could be larger.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4041**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Proposed activation more complicated than ReLU with marginal gains; models used are outdated; not convincing for adoption over SOTA.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4103**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Limited algorithmic novelty in music style embedding beyond adding another encoder; unclear if dataset will be released; insufficient technical details.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4297**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Lacks related work review; weak motivation and missing experimental clarity; confusion about experimental setup.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6963**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- “Fake-English” baseline to study token overlap in cross-lingual transfer; main contributions are empirical findings rather than generality or depth.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1999**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Proposed TSP solver encodes substantial domain knowledge, making comparison to pure learning methods unfair; insufficient baseline comparisons; lack of scalability evaluation.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1002**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Affinity-based task sharing for multi-task learning formalized, but method only applicable for small search spaces; offline clustering is expensive; incremental improvement over exhaustive search.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4287**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Motivation and details of key switching mechanism in multi-precision training inadequately justified; key aspects heuristic; writing issues and adhoc design choices.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 5903**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Interesting application of higher-order (simplicial) bias in Transformer; theoretically sound but unclear empirical improvement; justification for more complex biases vs standard relations is limited.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6637**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Lossless image compression with VAE+BB ANS is impressively described and implemented (despite slow runtimes), with open source commitment; lack of practical speed, but considered an important step.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6895**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Calibration for graph embedding is under-studied; minor technical novelty but well-executed problem and sound methodology.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 7371**  \n",
      "Decision: 2.0 (Accept)  \n",
      "**Main reason:**  \n",
      "- New influence-based exploration objectives for multi-agent RL are elegant and fundamental; strong empirical results, though related work section could be deeper.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 3469**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Motivation for new KGE model not clearly explained, experimental results not fully convincing nor fairly compared to SOTA.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 5895**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Extension of “covering options” to continuous state spaces is well-explained with improved empirics; limited novelty (mostly combination of prior work), but addresses hard problem.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 3964**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Sparsity analysis of ReLU networks is an incremental technical extension of prior work, with limited practical relevance.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6326 & 6327**  \n",
      "Each received: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- “Meta-dataset”: new, more realistic benchmark for few-shot learning; experiments solid but paper layout/organization needs improvement; significant empirical contribution.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1294**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Visualization of width/interpretability of neural features is interesting, but experiments limited to one architecture and lacks broader support for general claim.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 5917**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- New online variant of MPO RL achieves very strong empirical results; contribution is rather incremental but paper is interesting due to high reported performance.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 7703**  \n",
      "Decision: 3.0 (Accept)  \n",
      "**Main reason:**  \n",
      "- Strong empirical reproducibility study showing PPO and TRPO performance differences are mostly due to implementation, not algorithmic, choices; highlights importance of code sharing and correct baselines.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1030**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Proposed “generalized domain adaptation” is confused and incorrectly formalized; many notational and conceptual errors; method and problem statement are unclear.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 7237**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Theoretical message about BNN init; practical justifications are counterintuitive and not clearly explained, but reviewer sees advice to practitioners as useful.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 2814**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- PAC-Bayes/Hessian-based bound is unclear and novel analysis is not properly situated; related work missed, writing and clarity lacking.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6418**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Elegant formal equivalence between ReLU nets and oblique trees, but not clearly superior to random forests; presentation not optimal.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 3876**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Zero-shot visual RL transfer method is compared to weak baselines, does not address limitations of domain randomization or adequately discuss related work.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4207**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Requires pre-segmented objects, which is a significant practical simplification (and not solved); ground truth for occluded objects during training assumes access to full simulation state.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 7466**  \n",
      "Decision: 2.0 (Accept)  \n",
      "**Main reason:**  \n",
      "- Large-scale empirical MARL is insightful, showing human-interpretable strategies emerge; not surprising vs prior big-scale RL, but provides thorough empirical study.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4019**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- New training strategy (top-down for deep nets) is promising, but lacks results outside speech and further domain analysis; insufficient experiments.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4246**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Empirical regularization study in RL lacks justification/explains regularization benefit only as better tuning; experiments incomplete.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 3255**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Soccer pass success model is fun and interesting, but leaves interpretability of probability maps unresolved; some key application limitations unaddressed.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 7198**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Deep anomaly detection approach unifies image and non-image data via random affine transforms, solid results but evaluation could be more extensive and comparison to alternative methods stronger.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 5920**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Interesting phase-hiding idea for privacy, but paper is poorly written and some main ideas not clear; reviewer is positive in spite of writing issues.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 3511**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Multi-agent SAC is a straightforward extension with little novelty and no theoretical guarantees; more experiments and baselines needed.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 3402**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Reasonable method for policy diversity, but motivation is weak and limited experiments do not show general or reward benefits.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6761**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Learning with user-controllable camera rotation in RGBD synthesis is interesting, but too few results shown, paper is hard to read.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 7341**  \n",
      "Decision: 2.0 (Accept)  \n",
      "**Main reason:**  \n",
      "- Temporal loss for dynamic point clouds is well motivated and empirically strong, with generally clear contribution and open questions for future work.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4346**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Incremental improvement to HRN by adding dropout for multi-modality, but task/datasets unsophisticated, and lacks comparison to alternatives.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 3145**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Method for transfer in hyperparameter tuning is unnecessarily complicated and omits key baselines/modalities; technical issues exist; simple normalization pipeline not compared.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 52**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- SCSG extension to compositional problems is a minimal increment over prior work; limited experimentation.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1346**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Unclear intuitions and presentation for optimal-transport VAE; Section 3 in particular is confusing.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4571**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Mahalanobis kernel approach for BO addresses weaknesses of prior work, but omits key strong baselines (e.g. LineBO), SIBO, and better comparative analysis.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 4356**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Posterior collapse discussion not useful if more flexible priors (e.g. Vampprior) are considered; focuses on irrelevant regime where VAE recon error is high and therefore uninformative.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 671**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Averaging ReLU network predictions over balls (for robustness) is simple and well written, but limited experiment variety and mostly incremental compared to existing knowledge.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 3257**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Shifted randomized SVD is an incremental extension of randomized SVD, with claims unproven, experiments missing key comparisons.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 2522**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Task grouping for multi-task learning is a basic extension, with fixed sharing structure and poor optimization; overlength and many typos.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 3595**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Topological barcode characterization of optimization surfaces is trivial, with little practical application.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6695**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Applying early-exit strategies to robust inference is novel and achieves a \"triple win\", though defense/attack settings could be extended and parameter alignment needs clarification.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 1403**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Faithful data-to-text confidence scorer is plausible, but heavy reliance on attention undermines soundness and results show worse BLEU and fluency.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 3121**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Adversarial calibration is interesting but limited in real-world applicability and clarity; metric depends on perturbation knowledge, and not clearly better than existing methods.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6005**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- Maximizing the certified radius via soft randomized smoothing is well motivated and achieves strong empirical results; paper's comparisons could be broader.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 2328**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Wasserstein regularized PO for RL is somewhat interesting but not clearly superior or well explained.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 995**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Mixture-of-experts VAE is reasonable and can incorporate domain knowledge, but only marginally outperforms alternatives and faces issues with cluster interpretability and number.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 2316**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Collapsed state-space model for sequence segmentation is interesting but relies on ad-hoc annealing tricks and only synthetic experiments.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 2131**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- Value of depth in deep ReLU network is well analyzed, but experimental insight into breakpoint movement is missing, restricting conclusions.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 3191**  \n",
      "Decision: 0.0 (Reject)  \n",
      "**Main reason:**  \n",
      "- End-to-end Unet-based source separation barely outperforms baseline and omits comparison to critical methods like TasNet.\n",
      "\n",
      "---\n",
      "\n",
      "**Paper 6475**  \n",
      "Decision: 1.0 (Weak Accept)  \n",
      "**Main reason:**  \n",
      "- GraN-DAG method for nonlinear structure learning is novel and outperforms prior approaches in several regimes, but experimental and complexity limitations acknowledged.\n",
      "\n",
      "---\n",
      "\n",
      "This covers each paper. If you need reasons for any specific subset, let me know.\n"
     ]
    }
   ],
   "source": [
    "from jinja2 import Template\n",
    "import pandas as pd\n",
    "\n",
    "# get a sample of df into a sampled_df stratified by the field paper_decision_numeric\n",
    "from sklearn.model_selection import train_test_split\n",
    "#pick only where paper_decision_numeric is not null\n",
    "df = df[df['paper_decision_numeric'].notnull()]\n",
    "balanced_df, ydf = train_test_split(df, test_size=0.9, stratify=df['paper_decision_numeric'], random_state=42)\n",
    "balanced_df = balanced_df.sample(n=400, random_state=42)\n",
    "\n",
    "# Jinja2 template with your column names\n",
    "template_str = \"\"\"\n",
    "Analyze the following paper reviews and their decision scores (0 = reject, higher numbers = stronger acceptance). Provide the main reasons for acceptance or rejection for each paper in a concise summary.\n",
    "\n",
    "{% for index, row in reviews.iterrows() %}\n",
    "Paper {{ index + 1 }}:\n",
    "Review: {{ row.review }}\n",
    "Decision Score: {{ row.paper_decision_numeric }}\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "template = Template(template_str)\n",
    "\n",
    "\n",
    "# Render the prompt with dataframe data\n",
    "prompt = template.render(reviews=balanced_df)\n",
    "\n",
    "\n",
    "response = llm._call(prompt)\n",
    "print(\"LLM Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5cdb5c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 10:24:46,333 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      " **Top 10 Reasons for Acceptance:**\n",
      "\n",
      "1. **Novelty and Originality of Contribution** (Frequency: High)\n",
      "   - Importance: Introducing genuinely new concepts, methods, or frameworks significantly influences acceptance, as it advances the field or enables previously impossible approaches. Reviewers value creative ideas that go beyond incremental modifications.\n",
      "\n",
      "2. **Strong Empirical or Theoretical Results/Analysis** (Frequency: High)\n",
      "   - Importance: Solid, rigorous experimental or theoretical backing demonstrates the effectiveness and correctness of a proposed method, contributing to the credibility and utility of the research.\n",
      "\n",
      "3. **Clear Motivation and Relevance of Problem** (Frequency: Medium)\n",
      "   - Importance: Papers that tackle important or under-explored questions—especially those with real-world applications or theoretical significance—add value and are seen as deserving of acceptance.\n",
      "\n",
      "4. **Thorough and Convincing Analysis (Including Ablation and Comparisons)** (Frequency: Medium)\n",
      "   - Importance: Careful experimental design, comprehensive ablations, and meaningful comparisons to baselines show that improvements are attributable to the proposed work rather than confounding factors.\n",
      "\n",
      "5. **Methodological Soundness and Justification** (Frequency: Medium)\n",
      "   - Importance: Strong justification of technical choices and robust methodological design ensure that the proposed work is credible and not based on heuristics or weak assumptions.\n",
      "\n",
      "6. **Significant Empirical Contribution (Datasets, Benchmarks, Evaluation)** (Frequency: Medium)\n",
      "   - Importance: Efforts to provide new resources (datasets, benchmarks, open-source code) or especially thorough empirical studies are highly valued for their utility to the community and reproducibility.\n",
      "\n",
      "7. **Successful Application to Challenging or Realistic Scenarios** (Frequency: Low-Medium)\n",
      "   - Importance: Demonstrated success on challenging, diverse, or real-world benchmarks increases the perceived impact and usefulness of the proposed approach.\n",
      "\n",
      "8. **Resolution of Reviewers’ Concerns Post-Rebuttal** (Frequency: Low)\n",
      "   - Importance: Authors who adequately address critical issues raised during peer-review (e.g., clarify writing, add missing experiments) can tip borderline papers toward acceptance.\n",
      "\n",
      "9. **Clarity and Quality of Presentation** (Frequency: Low)\n",
      "   - Importance: Clear writing, well-structured arguments, and transparency in reporting results make contributions more accessible and trustworthy, although this tends to be secondary to content.\n",
      "\n",
      "10. **Well-Motivated Incremental Improvements with Practical Impact** (Frequency: Low)\n",
      "    - Importance: Even if the technical novelty is limited, presenting well-executed incremental improvements with clear practical value can still merit acceptance, especially if performance gains are significant and well-demonstrated.\n",
      "\n",
      "---\n",
      "\n",
      "**Top 10 Reasons for Rejection:**\n",
      "\n",
      "1. **Insufficient Novelty / Incremental Contribution** (Frequency: Very High)\n",
      "   - Importance: Papers that only offer marginal improvements or trivial modifications fail to move the field forward and are a primary reason for rejection.\n",
      "\n",
      "2. **Weak or Unconvincing Empirical Results (Limited Benchmarks, Weak Baselines, Marginal Gains)** (Frequency: Very High)\n",
      "   - Importance: Without strong evidence that a method works, or if improvements are not substantial or well-demonstrated, reviewers cannot endorse publication.\n",
      "\n",
      "3. **Poor Comparison to State-of-the-art and Incomplete Baselines** (Frequency: High)\n",
      "   - Importance: Failing to compare with existing leading methods makes it impossible to judge true value, suggesting either ignorance of the literature or lack of competitiveness.\n",
      "\n",
      "4. **Lack of Theoretical Justification or Methodological Soundness** (Frequency: High)\n",
      "   - Importance: Methods based on unjustified heuristics, dubious assumptions, or poorly explained choices are not convincing as scientific contributions.\n",
      "\n",
      "5. **Insufficient or Poorly Described Experimental Analysis** (Frequency: High)\n",
      "   - Importance: Limited, poorly documented, or unclear empirical evaluation (omitting essential details or critical experiments) undermines result validity and reproducibility.\n",
      "\n",
      "6. **Unclear, Confusing, or Poor Presentation (Notation, Writing, Motivation)** (Frequency: High)\n",
      "   - Importance: If key concepts, methods, or results are not clearly communicated, reviewers cannot properly assess the work, leading to rejection on the grounds of inaccessibility.\n",
      "\n",
      "7. **Failure to Address Real-world Applicability or Limitations** (Frequency: Medium)\n",
      "   - Importance: Overfocus on contrived, synthetic, or simplistic cases, or ignoring practical constraints, suggests the work has little impact beyond toy examples.\n",
      "\n",
      "8. **Lack of Comparative or Qualitative Analysis (No Ablations or Analysis of Where Gains Come From)** (Frequency: Medium)\n",
      "   - Importance: Not analyzing why or when a method works raises doubts as to its scientific merit and prevents drawing useful conclusions.\n",
      "\n",
      "9. **Technical or Methodological Flaws (Unproven Claims, Incorrect Formalism, Flawed Assumptions)** (Frequency: Medium)\n",
      "   - Importance: Foundational issues such as incorrect mathematical statements, unjustified assumptions, or errors in derivation mean the core contribution is unsound.\n",
      "\n",
      "10. **Limited Scope, Impact, or Generality (Narrow Applicability, Simple Datasets, Lack of Breadth)** (Frequency: Medium)\n",
      "    - Importance: If a method works only in very specific or unchallenging settings, or has unclear broader benefits, the overall contribution is limited.\n",
      "\n",
      "---\n",
      "\n",
      "**How these were determined:**\n",
      "\n",
      "I synthesized themes across the provided summaries, clustering similar issues (e.g., \"incremental improvement,\" \"limited novelty,\" \"marginal gains\") under broader reasons. I prioritized reasons based on their criticality in peer review context: strong novelty, rigorous and meaningful results, soundness, and competitive evaluation are vital, while presentation, though important, is usually only a decisive factor when content is borderline. Many rejections cite multiple deficiencies; I emphasized those deemed “main reasons” or those directly phrased as decisive. For acceptance, the focus was on key strengths explicitly mentioned as justifying the positive score. \n",
      "\n",
      "Frequency counts are secondary to the actual impact of each reason as signaled in the decisions. For instance, even if \"presentation\" is cited often, it rarely overrides fundamental flaws in novelty or results. Each reason is briefly annotated to explain why it tends to sway decisions.\n"
     ]
    }
   ],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "\n",
    "# Jinja2 template for the new prompt\n",
    "template_str = \"\"\"\n",
    "You are an expert in analyzing academic paper reviews. Below is a list of reviews for multiple papers, each with main reasons for acceptance or rejection. Your task is to:\n",
    "\n",
    "1. Identify the top ten reasons for acceptance and the top ten reasons for rejection across all papers.\n",
    "2. For each reason, provide a brief desription of its importance in the determination of acceptance or rejection.\n",
    "\n",
    "Do not base your analysis solely on frequency. Try to determine how important each reason is in the context of the paper reviews.\n",
    "Base your decision solely on the reviews provided. Do not include any external information or personal opinions or prior knowledge you have on how papers are evaluated.\n",
    "\n",
    "**Input Response:**\n",
    "{{ previous_response }}\n",
    "\n",
    "**Output Format:**\n",
    "Provide your answer in this exact format:\n",
    "\n",
    "**Top 10 Reasons for Acceptance:**\n",
    "1. [Reason] (Frequency: [N])\n",
    "   - Importance: [Brief explanation]\n",
    "2. ...\n",
    "\n",
    "**Top 10 Reasons for Rejection:**\n",
    "1. [Reason] (Frequency: [N])\n",
    "   - Importance: [Brief explanation]\n",
    "2. ...\n",
    "\n",
    "Ensure your analysis is accurate, groups similar reasons (e.g., \"lacks clarity\" and \"poor presentation\" if related), and prioritizes clarity in ranking.\n",
    "Describe how you arrived at determining the reasons and their importance.\n",
    "\"\"\"\n",
    "template = Template(template_str)\n",
    "\n",
    "# Render the prompt\n",
    "prompt = template.render(previous_response=response)\n",
    "\n",
    "\n",
    "\n",
    "# Make the LLM call\n",
    "reasons_oneshot = llm._call(prompt)\n",
    "print(\"LLM Response:\\n\", reasons_oneshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e319f",
   "metadata": {},
   "source": [
    "2025-04-16 10:24:46,333 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
    "LLM Response:\n",
    "\n",
    "### Top 10 Reasons for Acceptance\n",
    "\n",
    "1. **Novelty and Originality of Contribution** (Frequency: High)\n",
    "   - Importance: Introducing genuinely new concepts, methods, or frameworks significantly influences acceptance, as it advances the field or enables previously impossible approaches. Reviewers value creative ideas that go beyond incremental modifications.\n",
    "\n",
    "2. **Strong Empirical or Theoretical Results/Analysis** (Frequency: High)\n",
    "   - Importance: Solid, rigorous experimental or theoretical backing demonstrates the effectiveness and correctness of a proposed method, contributing to the credibility and utility of the research.\n",
    "\n",
    "3. **Clear Motivation and Relevance of Problem** (Frequency: Medium)\n",
    "   - Importance: Papers that tackle important or under-explored questions—especially those with real-world applications or theoretical significance—add value and are seen as deserving of acceptance.\n",
    "\n",
    "4. **Thorough and Convincing Analysis (Including Ablation and Comparisons)** (Frequency: Medium)\n",
    "   - Importance: Careful experimental design, comprehensive ablations, and meaningful comparisons to baselines show that improvements are attributable to the proposed work rather than confounding factors.\n",
    "\n",
    "5. **Methodological Soundness and Justification** (Frequency: Medium)\n",
    "   - Importance: Strong justification of technical choices and robust methodological design ensure that the proposed work is credible and not based on heuristics or weak assumptions.\n",
    "\n",
    "6. **Significant Empirical Contribution (Datasets, Benchmarks, Evaluation)** (Frequency: Medium)\n",
    "   - Importance: Efforts to provide new resources (datasets, benchmarks, open-source code) or especially thorough empirical studies are highly valued for their utility to the community and reproducibility.\n",
    "\n",
    "7. **Successful Application to Challenging or Realistic Scenarios** (Frequency: Low-Medium)\n",
    "   - Importance: Demonstrated success on challenging, diverse, or real-world benchmarks increases the perceived impact and usefulness of the proposed approach.\n",
    "\n",
    "8. **Resolution of Reviewers’ Concerns Post-Rebuttal** (Frequency: Low)\n",
    "   - Importance: Authors who adequately address critical issues raised during peer-review (e.g., clarify writing, add missing experiments) can tip borderline papers toward acceptance.\n",
    "\n",
    "9. **Clarity and Quality of Presentation** (Frequency: Low)\n",
    "   - Importance: Clear writing, well-structured arguments, and transparency in reporting results make contributions more accessible and trustworthy, although this tends to be secondary to content.\n",
    "\n",
    "10. **Well-Motivated Incremental Improvements with Practical Impact** (Frequency: Low)\n",
    "    - Importance: Even if the technical novelty is limited, presenting well-executed incremental improvements with clear practical value can still merit acceptance, especially if performance gains are significant and well-demonstrated.\n",
    "\n",
    "---\n",
    "\n",
    "### Top 10 Reasons for Rejection\n",
    "\n",
    "1. **Insufficient Novelty / Incremental Contribution** (Frequency: Very High)\n",
    "   - Importance: Papers that only offer marginal improvements or trivial modifications fail to move the field forward and are a primary reason for rejection.\n",
    "\n",
    "2. **Weak or Unconvincing Empirical Results (Limited Benchmarks, Weak Baselines, Marginal Gains)** (Frequency: Very High)\n",
    "   - Importance: Without strong evidence that a method works, or if improvements are not substantial or well-demonstrated, reviewers cannot endorse publication.\n",
    "\n",
    "3. **Poor Comparison to State-of-the-art and Incomplete Baselines** (Frequency: High)\n",
    "   - Importance: Failing to compare with existing leading methods makes it impossible to judge true value, suggesting either ignorance of the literature or lack of competitiveness.\n",
    "\n",
    "4. **Lack of Theoretical Justification or Methodological Soundness** (Frequency: High)\n",
    "   - Importance: Methods based on unjustified heuristics, dubious assumptions, or poorly explained choices are not convincing as scientific contributions.\n",
    "\n",
    "5. **Insufficient or Poorly Described Experimental Analysis** (Frequency: High)\n",
    "   - Importance: Limited, poorly documented, or unclear empirical evaluation (omitting essential details or critical experiments) undermines result validity and reproducibility.\n",
    "\n",
    "6. **Unclear, Confusing, or Poor Presentation (Notation, Writing, Motivation)** (Frequency: High)\n",
    "   - Importance: If key concepts, methods, or results are not clearly communicated, reviewers cannot properly assess the work, leading to rejection on the grounds of inaccessibility.\n",
    "\n",
    "7. **Failure to Address Real-world Applicability or Limitations** (Frequency: Medium)\n",
    "   - Importance: Overfocus on contrived, synthetic, or simplistic cases, or ignoring practical constraints, suggests the work has little impact beyond toy examples.\n",
    "\n",
    "8. **Lack of Comparative or Qualitative Analysis (No Ablations or Analysis of Where Gains Come From)** (Frequency: Medium)\n",
    "   - Importance: Not analyzing why or when a method works raises doubts as to its scientific merit and prevents drawing useful conclusions.\n",
    "\n",
    "9. **Technical or Methodological Flaws (Unproven Claims, Incorrect Formalism, Flawed Assumptions)** (Frequency: Medium)\n",
    "   - Importance: Foundational issues such as incorrect mathematical statements, unjustified assumptions, or errors in derivation mean the core contribution is unsound.\n",
    "\n",
    "10. **Limited Scope, Impact, or Generality (Narrow Applicability, Simple Datasets, Lack of Breadth)** (Frequency: Medium)\n",
    "    - Importance: If a method works only in very specific or unchallenging settings, or has unclear broader benefits, the overall contribution is limited.\n",
    "\n",
    "---\n",
    "\n",
    "### How These Were Determined\n",
    "\n",
    "I synthesized themes across the provided summaries, clustering similar issues (e.g., \"incremental improvement,\" \"limited novelty,\" \"marginal gains\") under broader reasons. I prioritized reasons based on their criticality in peer review context: strong novelty, rigorous and meaningful results, soundness, and competitive evaluation are vital, while presentation, though important, is usually only a decisive factor when content is borderline. Many rejections cite multiple deficiencies; I emphasized those deemed “main reasons” or those directly phrased as decisive. For acceptance, the focus was on key strengths explicitly mentioned as justifying the positive score.\n",
    "\n",
    "Frequency counts are secondary to the actual impact of each reason as signaled in the decisions. For instance, even if \"presentation\" is cited often, it rarely overrides fundamental flaws in novelty or results. Each reason is briefly annotated to explain why it tends to sway decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1703b",
   "metadata": {},
   "source": [
    "# Map, then summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c781a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "REJECTED_FEATURE_EXTRACTION_PROMPT = \"\"\"\n",
    "You are a data analyst. Your task is to extract the main negative comment from the review and any additional comments.\n",
    "our input is a review of a paper that was rejected.\n",
    "Here is the review text:\n",
    "<REVIEW>\n",
    "{{content}}\n",
    "</REVIEW>\n",
    "Please extract the main limitation as perceived by the reviewer, and any additional limitations and negative aspects.\n",
    "Begin by returing your reasoning, then return the main limitation and any additional limitations.\n",
    "Keep the reasoning and the answers brief, below 20 words.\n",
    "Return the result in JSON format:\n",
    "{   \"reasoning\": \"<reasoning>\",\n",
    "    \"main_limitation\": \"<main negative comment>\",\n",
    "    \"additional_limitations\": \"<additional comments>\"\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73bc4fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:29:29,915 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:29:32,970 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 23:29:34,809 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 23:29:36,788 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 23:29:39,994 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 23:29:41,877 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 23:29:43,716 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 23:29:46,226 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 23:29:48,944 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 23:29:51,595 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# for each review, extract the main negative comment and any additional comments\n",
    "\n",
    "import time\n",
    "from jinja2 import Template\n",
    "template = Template(REJECTED_FEATURE_EXTRACTION_PROMPT)\n",
    "answers = []\n",
    "i = 0\n",
    "for index, row in df_reject[:10].iterrows():\n",
    "    review = row['review']\n",
    "    prompt = template.render(content=review)\n",
    "    try:\n",
    "        raw_response = llm._call(prompt)\n",
    "        response = json.loads(raw_response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print (f\"Response: {response}\")\n",
    "\n",
    "    main_limitation = response['main_limitation']\n",
    "    additional_limitations = response['additional_limitations']\n",
    "    df_reject.at[index, 'main_limitation'] = main_limitation\n",
    "    df_reject.at[index, 'additional_limitations'] = additional_limitations\n",
    "    answers.append(response)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i} reviews\")\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707d334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_limitation\n",
       "None mentioned.                                                                                                                        2\n",
       "None explicitly stated.                                                                                                                2\n",
       "Lack of originality and significance; most results are known and not sufficiently distinguished from prior work.                       1\n",
       "The title and abstract are misleading; paper doesn't introduce a new sampling strategy, only theoretical bounds for existing ones.     1\n",
       "The bound has exponential dependence on the number of layers, making it less practical for real-world applications.                    1\n",
       "                                                                                                                                      ..\n",
       "Experiments are not convincing and the modification doesn’t provide much insight into representation learning/anomaly detection.       1\n",
       "The experimental evaluation is not convincing due to lack of competitors, use of simple datasets, and missing experimental details.    1\n",
       "Not fully convinced of the heuristic: similarity between input and output embeddings used for position supervision.                    1\n",
       "Does not compare with later, better baselines such as FlowSeq; unclear speedup evaluation.                                             1\n",
       "The novelty of the proposed method is marginal.                                                                                        1\n",
       "Name: count, Length: 998, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save df_reviews\n",
    "# df_reject.to_csv('reject_reviews.csv', index=False)\n",
    "# df_reject\n",
    "# \n",
    "# df = pd.read_csv('reject_reviews.csv')\n",
    "# df['main_limitation'].value_counts()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa7155",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60af6b5c",
   "metadata": {},
   "source": [
    "## Path 1: Cluster and name, then build a hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0efde00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reject = pd.read_csv('reject_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad120693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization took 0.03429985046386719 seconds\n",
      "Clustering took 3.276634931564331 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72371/1064793411.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfr['cluster_main_limitation'] = cluster_labels\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "df_reject['text'] = df_reject['main_limitation'].astype(str)\n",
    "dfr = df_reject[:1000]\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(dfr['text']).toarray()\n",
    "\n",
    "print(f'Vectorization took {time.time() - start} seconds')\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "cluster_labels = hdbscan_kmeans_refine(\n",
    "    X,\n",
    "    hdbscan_min_cluster_size=10,\n",
    "    max_cluster_size=50,\n",
    "    split_k=5,\n",
    "    max_total_clusters=1000,\n",
    "    random_state=42,\n",
    "    minimum_cluster_size=5)\n",
    "print(f'Clustering took {time.time() - start} seconds')\n",
    "dfr['cluster_main_limitation'] = cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994cea7",
   "metadata": {},
   "source": [
    "### Name clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eb86c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cluster_keywords, c_tfidf, count_vectorizer = get_cluster_keywords(\n",
    "    dfr,'main_limitation',\n",
    "    'cluster_main_limitation')\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    stop_words='english',max_features=1000,\n",
    "    ngram_range=(1, 2))\n",
    "count_vectorizer.fit(dfr['main_limitation'])\n",
    "docs = get_representative_docs(dfr, 'main_limitation', 'cluster_main_limitation', cluster_keywords, count_vectorizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8e4ef0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/55 [00:00<?, ?it/s]2025-04-16 11:02:30,301 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  2%|▏         | 1/55 [00:03<02:46,  3.08s/it]2025-04-16 11:02:33,048 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  4%|▎         | 2/55 [00:04<02:02,  2.32s/it]2025-04-16 11:02:34,885 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  5%|▌         | 3/55 [00:06<01:51,  2.14s/it]2025-04-16 11:02:36,709 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  7%|▋         | 4/55 [00:08<01:37,  1.91s/it]2025-04-16 11:02:38,334 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  9%|▉         | 5/55 [00:11<01:50,  2.21s/it]2025-04-16 11:02:40,983 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 11%|█         | 6/55 [00:14<02:07,  2.59s/it]2025-04-16 11:02:44,475 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 13%|█▎        | 7/55 [00:16<01:50,  2.29s/it]2025-04-16 11:02:46,331 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 15%|█▍        | 8/55 [00:18<01:43,  2.20s/it]2025-04-16 11:02:48,072 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 16%|█▋        | 9/55 [00:19<01:30,  1.96s/it]2025-04-16 11:02:49,482 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 18%|█▊        | 10/55 [00:21<01:25,  1.91s/it]2025-04-16 11:02:51,270 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 20%|██        | 11/55 [00:22<01:18,  1.78s/it]2025-04-16 11:02:52,774 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 22%|██▏       | 12/55 [00:24<01:13,  1.70s/it]2025-04-16 11:02:54,333 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 24%|██▎       | 13/55 [00:25<01:08,  1.63s/it]2025-04-16 11:02:55,838 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 25%|██▌       | 14/55 [00:27<01:04,  1.56s/it]2025-04-16 11:02:57,188 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 27%|██▋       | 15/55 [00:28<01:01,  1.55s/it]2025-04-16 11:02:58,648 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 29%|██▉       | 16/55 [00:30<00:59,  1.51s/it]2025-04-16 11:03:00,238 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 31%|███       | 17/55 [00:31<00:59,  1.56s/it]2025-04-16 11:03:01,881 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 33%|███▎      | 18/55 [00:33<01:00,  1.64s/it]2025-04-16 11:03:03,545 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 35%|███▍      | 19/55 [00:35<00:58,  1.62s/it]2025-04-16 11:03:05,169 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 36%|███▋      | 20/55 [00:36<00:53,  1.54s/it]2025-04-16 11:03:06,686 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 38%|███▊      | 21/55 [00:38<00:51,  1.53s/it]2025-04-16 11:03:07,976 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 40%|████      | 22/55 [00:41<01:06,  2.02s/it]2025-04-16 11:03:11,200 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 42%|████▏     | 23/55 [00:43<01:05,  2.06s/it]2025-04-16 11:03:13,344 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 44%|████▎     | 24/55 [00:45<01:01,  1.97s/it]2025-04-16 11:03:15,204 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 45%|████▌     | 25/55 [00:46<00:55,  1.85s/it]2025-04-16 11:03:16,634 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 47%|████▋     | 26/55 [00:48<00:49,  1.70s/it]2025-04-16 11:03:18,152 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 49%|████▉     | 27/55 [00:49<00:46,  1.65s/it]2025-04-16 11:03:19,897 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 51%|█████     | 28/55 [00:51<00:43,  1.63s/it]2025-04-16 11:03:21,500 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 53%|█████▎    | 29/55 [00:53<00:44,  1.73s/it]2025-04-16 11:03:23,255 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 55%|█████▍    | 30/55 [00:55<00:47,  1.89s/it]2025-04-16 11:03:25,442 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 56%|█████▋    | 31/55 [00:56<00:41,  1.73s/it]2025-04-16 11:03:26,747 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 58%|█████▊    | 32/55 [00:58<00:39,  1.73s/it]2025-04-16 11:03:28,471 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 60%|██████    | 33/55 [00:59<00:36,  1.65s/it]2025-04-16 11:03:33,633 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 62%|██████▏   | 34/55 [01:04<00:50,  2.41s/it]2025-04-16 11:03:34,146 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 64%|██████▎   | 35/55 [01:06<00:47,  2.36s/it]2025-04-16 11:03:36,885 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 65%|██████▌   | 36/55 [01:08<00:42,  2.23s/it]2025-04-16 11:03:38,319 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 67%|██████▋   | 37/55 [01:10<00:37,  2.07s/it]2025-04-16 11:03:39,999 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 69%|██████▉   | 38/55 [01:11<00:32,  1.89s/it]2025-04-16 11:03:41,550 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 71%|███████   | 39/55 [01:13<00:31,  1.96s/it]2025-04-16 11:03:43,519 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 73%|███████▎  | 40/55 [01:14<00:26,  1.76s/it]2025-04-16 11:03:44,845 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 75%|███████▍  | 41/55 [01:16<00:23,  1.67s/it]2025-04-16 11:03:46,369 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 76%|███████▋  | 42/55 [01:18<00:21,  1.68s/it]2025-04-16 11:03:48,097 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 78%|███████▊  | 43/55 [01:19<00:20,  1.69s/it]2025-04-16 11:03:49,800 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 80%|████████  | 44/55 [01:21<00:19,  1.75s/it]2025-04-16 11:03:51,542 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 82%|████████▏ | 45/55 [01:23<00:16,  1.69s/it]2025-04-16 11:03:53,395 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 84%|████████▎ | 46/55 [01:25<00:16,  1.87s/it]2025-04-16 11:03:55,540 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 85%|████████▌ | 47/55 [01:26<00:13,  1.69s/it]2025-04-16 11:03:56,706 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 87%|████████▋ | 48/55 [01:28<00:11,  1.59s/it]2025-04-16 11:03:58,108 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 89%|████████▉ | 49/55 [01:29<00:09,  1.56s/it]2025-04-16 11:03:59,558 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 91%|█████████ | 50/55 [01:30<00:07,  1.46s/it]2025-04-16 11:04:00,783 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 93%|█████████▎| 51/55 [01:32<00:05,  1.47s/it]2025-04-16 11:04:02,279 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 95%|█████████▍| 52/55 [01:33<00:04,  1.39s/it]2025-04-16 11:04:03,602 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 96%|█████████▋| 53/55 [01:35<00:03,  1.50s/it]2025-04-16 11:04:05,470 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 98%|█████████▊| 54/55 [01:36<00:01,  1.51s/it]2025-04-16 11:04:06,807 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 55/55 [01:38<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "NAMING_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a data analyst. Your task is to name the clusters based on the keywords and the representative documents.\n",
    "Here are the keywords for each cluster:\n",
    "<KEYWORDS>\n",
    "{{keywords}}\n",
    "</KEYWORDS>\n",
    "Here are the representative documents for each cluster:\n",
    "<REPRESENTATIVE_DOCS>\n",
    "{{representative_docs}}\n",
    "</REPRESENTATIVE_DOCS>\n",
    "Output a JSON object with only one key called 'cluster_name', to denote the title of the group, with no additional information or commentary.\n",
    "Do not leave the JSON empty. Always return a value for 'cluster_name' with your best guess at a descriptive title for the topic.\n",
    "Be as specific as possible and mimic the style of the representative documents.\n",
    "The JSON format is as follows:\n",
    "    {   \n",
    "        \"cluster_name\": <NAME>\n",
    "    } \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "cmap = name_clusters_with_topics(dfr, llm, 'cluster_main_limitation', cluster_keywords, docs,  prompt_template=NAMING_PROMPT_TEMPLATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3fe2894f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72371/2250739328.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfr['_level_0'] = dfr['cluster_main_limitation'].map(cmap).fillna('NO_NAME')\n",
      "/tmp/ipykernel_72371/2250739328.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfr['_level_0'] = dfr['_level_0'].apply(robust_json_parse)\n",
      "/tmp/ipykernel_72371/2250739328.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfr['_level_0'] = dfr['_level_0'].apply(get_dict_item)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      Assessment of Novelty and Rigor through Compar...\n",
       "1      Robustness Analysis of Deep Neural Networks ag...\n",
       "2      Critiques of Theoretical Paper Clarity and Exp...\n",
       "3      Novel Approaches and Incremental Advances in C...\n",
       "4      Assessment of Novelty and Significance in Rela...\n",
       "                             ...                        \n",
       "995    Assessment of Methodological Novelty and Techn...\n",
       "996    Evaluation and Benchmarking of Clustering Adap...\n",
       "997    Assessment of Novel Model Integration Techniqu...\n",
       "998    Assessment of Novelty and Rigor through Compar...\n",
       "999    Critiques of Incremental Optimization Methods ...\n",
       "Name: _level_0, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfr['_level_0'] = dfr['cluster_main_limitation'].map(cmap).fillna('NO_NAME')\n",
    "dfr['_level_0'] = dfr['_level_0'].apply(robust_json_parse)\n",
    "def get_dict_item(x):\n",
    "    try:\n",
    "        value = list(x.values())[0]\n",
    "        return value\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print (f\"Response: {x}\")\n",
    "        return x\n",
    "dfr['_level_0'] = dfr['_level_0'].apply(get_dict_item)\n",
    "dfr['_level_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e3cb0295",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr.to_csv('reject_reviews_w_base_cluster.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5d8e1",
   "metadata": {},
   "source": [
    "### Raw reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "36397126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lack of rigorous experimentation and insufficient evaluation of generalizability across datasets and comparison with prior work.\n",
      "Experiments do not show state-of-the-art results, especially on standard datasets like MNIST and CIFAR.\n",
      "Paper gives very little theoretical justification or analysis of the results.\n",
      "The contribution of this paper is very incremental.\n",
      "Concern about the novelty of the ideas, describing them as straightforward after considering recent related work.\n",
      "The paper misses very relevant prior papers with similar ideas, failing to properly position its contribution.\n",
      "Maximal safe set is only illustrated for one CIFAR-100 case, which is not convincing.\n",
      "It is not shown that the PE score can improve results of image generation tasks, such as super-resolution.\n",
      "Correlation between PE score and human judgement is unsurprising because CSF already encodes human perceptual characteristics.\n",
      "Experiments are anecdotal, incomplete, and not very exhaustive.\n",
      "Unclear connection between proposed metrics and initial hypotheses; reviewer unable to understand the choice of metrics.\n",
      "Experimental results are not convincing; evidence does not support that the proposed strategy achieves the stated goal.\n",
      "Unclear how the proposed flexible search distribution actually improves exploration compared to baseline ES.\n",
      "Lack of demonstration on difficult tasks that traditional ES or RL methods cannot solve.\n",
      "Lack of demonstrated novelty and significance compared to existing literature, especially missing key citations and discussion.\n",
      "The technical contribution of this paper is pretty low.\n",
      "Lack of clear explanation of Lemma 2 and its connection to main results.\n",
      "Fundamental mistakes in the statements/proofs of Theorems 2, 3, and 4.\n",
      "The paper's technical contribution is incremental and scattered, lacking a clear major contribution.\n",
      "Limited novelty; little extension beyond existing methods (SASREC).\n",
      "Technical novelty is limited compared to closely related works (Transformer and SSE).\n",
      "The evaluation is lacking; not convinced SAP is better than existing or zero-shot baselines.\n",
      "It is not clear that the proposed architecture fundamentally advances zero-shot RL; its relevance is questionable.\n",
      "The experimental setup and reward structure are confusing; it is unclear how this constitutes zero-shot generalization.\n",
      "The paper's contributions are incremental and lack sufficient experimental results.\n",
      "Techniques are limited to object counting, derived in an ad-hoc way with little theoretical backing and lacking novelty.\n",
      "Experimental section lacks standard datasets, evaluation protocols, and baseline comparisons.\n",
      "There are serious typos making equation (3) incorrect, requiring significant modification to correct errors throughout.\n",
      "The proposed model cannot handle sparsity in datasets like Goodbooks, Movielens, and Netflix.\n",
      "The main conjecture on concentration of measure is hand-wavy and not rigorously justified.\n",
      "The method is not novel; adding noise for regularization is a common technique.\n",
      "Novelty is limited and claims are not well supported.\n",
      "The theoretical contributions are only asymptotic and do not provide intuition for why the method outperforms others.\n",
      "The technical approach is too outdated and obvious, not at ICLR level.\n",
      "No novelty in the approach.\n",
      "The paper is disorganized and lacks clear, thorough explanations of methods.\n",
      "Reliance on solver for supervision limits scalability; getting faster but worse results than solver on small graphs is not significant.\n",
      "Authors should compare using QAP for assignment with GSE to better assess their method.\n",
      "Performance gains are unconvincing due to insufficient evaluation and weak novelty.\n",
      "Violates blind-review policy by exposing author identity in acknowledgements.\n",
      "The technical contribution is very incremental and experiment results are not strong enough.\n",
      "The improvement is minor; the contribution of one extra feature does not warrant a top conference paper.\n",
      "Does not produce a huge delta in understanding compared to Gidaris et al. (2019).\n",
      "The proposed method is considered incremental, lacking significant novelty over existing multi-task and self-supervision approaches.\n",
      "Major concerns with the empirical setup and methods in the section proposing a method to choose samples for SSL from an unlabeled pool.\n",
      "The reasoning module is just constraints and data sampling, not true (symbolic or logic) reasoning as claimed.\n",
      "Proposed solution may not generalize beyond the tested de-mixing tasks due to reliance on pretrained decoders.\n",
      "Experimental results are almost impossible to interpret due to poor description; vital methodological details are missing.\n",
      "The paper only provides empirical results and lacks substantial insights and useful information.\n",
      "It's not clear the proposed algorithm is useful.\n",
      "Experimental design and comprehensiveness are lacking, making the empirical study unconvincing.\n",
      "Difference from previous work ([Lian et al., 2017]) is not significant, especially in algorithm design and theory.\n",
      "Paper lacks comparison (empirically and theoretically) with the method in A.Devraj & J.Chen (2019).\n",
      "The convergence results are not state-of-the-art compared to prior work.\n",
      "How can DNN layers be modeled as a Bayesian Hierarchical model, since BHMs rely on i.i.d. assumptions?\n",
      "The reviewer is not fully convinced that the i.i.d. assumption is wrong and finds the argument unclear.\n",
      "Writing is sloppy and difficult to read; main claim about i.i.d. hypothesis invalidating GP interpretation is not convincing.\n",
      "Unclear motivation for using tropical geometry and misleading use of 'adversarial attacks' terminology.\n",
      "The method for generating adversarial examples alters the model itself, differing from standard settings, making the comparison unclear.\n",
      "Experiments go wide but not deep; lacking focus and in-depth setup in experimental areas.\n",
      "Paper is insufficiently motivated, presented, and explained for a wider audience; can only reach a limited audience.\n",
      "The paper is extremely hard to follow, with insufficient motivation and unclear explanations.\n",
      "Testing was only done in simple environments (VizDoom); method not evaluated in more realistic scenarios.\n",
      "The paper falls short in terms of technical contributions, mainly borrowing components from previous work.\n",
      "Lack of experiment showing that the inductive bias does not harm performance on tasks without spatial reasoning.\n",
      "The experimental results are not convincing enough to justify the significance of the proposed method.\n",
      "The algorithm lacks theoretical guarantees under bandit settings, making its foundation weak.\n",
      "The core idea lacks solid theoretical support; no regret bound result is provided.\n",
      "The method is based on published slimmable networks, so the novelty is limited.\n",
      "Lack of clarity on the efficiency and search cost of the proposed method versus baselines.\n",
      "Lack of justification for allocating more channels to deep layers, which contrasts with pruning literature.\n",
      "Lack of clarity and sufficient explanation of implementation details for masking and grid search.\n",
      "All results use heavy data augmentation, making comparison to other methods (except EfficientNet) impossible.\n",
      "The method is an ensemble of existing approaches; lack of novelty and general lessons.\n",
      "Experimental comparisons are unfair and results are meaningless due to different norm constraints versus compared methods.\n",
      "Term 'adversarial success rate' should be called 'attack success rate' as per standard literature.\n",
      "The method does not achieve semantics-preserving attacks or successful attacks for NLP; claims are not supported.\n",
      "The paper misuses the reparameterization trick; Eq.(2) lacks required KL-divergence term, making the approach unjustified.\n",
      "Motivation and explanation are insufficient; rationale for the approach is unclear.\n",
      "The proposed method's effectiveness for out-of-distribution detection is unclear and not supported by convincing empirical results.\n",
      "The definition and explanation of Equation 2 and variable z are unclear and confusing.\n",
      "It is unclear how the method works for output-rich models where much input information needs to be preserved.\n",
      "Experimental section is weak, especially the MNIST experiment, making it hard to judge the method's effectiveness.\n",
      "Experiments lack the most recent important baseline (symmetric cross entropy) which is current SOTA.\n",
      "Major concern about tuning new hyperparameters in practice, especially with noisy or corrupted validation sets.\n",
      "No clear way to choose alpha and beta except hyper-parameter search, which is impractical and risks overfitting.\n",
      "Writing and explanations need improvement, especially for readers less familiar with the area.\n",
      "Paper is tedious with heavy notations, making it difficult to read.\n",
      "Neither a surprising theoretical result nor significant empirical improvement is demonstrated.\n",
      "The framework lacks significant contributions worthy of publication at top conferences.\n",
      "None of the claimed contributions is novel or significant; the method is a combination of existing ideas.\n",
      "The novelty is not clear; methods seem already used in previous works.\n",
      "Paper is poorly written and there are concerns about the stability of the unsupervised data labeling process.\n",
      "Limited novelty due to combining existing techniques and restricted, synthetic-domain experiments—unclear real-world impact.\n",
      "The clarity of the technical presentation could be improved.\n",
      "Experiments are too standard; no results on real-world datasets like Clothing1M.\n",
      "The theoretical guarantee relies on overly strong, unjustified assumptions and is confusingly presented.\n",
      "Lack of analysis or reporting of the time complexity/cost of the proposed method.\n",
      "The paper is not well motivated because combining data augmentation and explicit regularization is already known.\n",
      "The claims are not well supported by the experiments; experimental settings and choices are unconvincing.\n",
      "The paper at its current state does not contain sufficient message to stand as a full paper.\n",
      "The SAIL method is considered ad-hoc for addressing reward bias and may not generalize across environments.\n",
      "Possible unfair state distribution mismatch in Lunar Lander experiments may hurt GAIL baseline performance.\n",
      "No guarantee of learned policy optimality; SAIL lacks optimality justification from both distribution matching and IRL perspectives.\n",
      "The section on the bernoulli splitting trick was the most confusing part of the paper.\n",
      "The Fourier analysis is unnecessary, leading to suboptimal and potentially misleading presentation of the derivation.\n",
      "The theoretical and empirical parts are not well-connected; variance of the estimator is not quantified.\n",
      "The proposed method seems equivalent to Euclidean distance and lacks justification for its improved effectiveness or perceptual meaning.\n",
      "The novelty is insufficient, with the proposed ideas being simple or already existing.\n",
      "The contribution is incremental, not principled, and unconvincing—more of an engineering trick.\n",
      "The paper is lacking in thorough evaluation; experiments do not convincingly show multi-task or option reusability.\n",
      "Experiments are limited to low-dimensional state space and discrete action space; scalability is uncertain.\n",
      "The evaluation is weak and does not clearly demonstrate that the author's goals (esp. reusability/transfer) have been achieved.\n",
      "No novelty in how the knowledge graph is built; thus, lacks technical contribution.\n",
      "The paper is mis-targeted for ICLR; it does not discuss learning representations or deep learning.\n",
      "The paper lacks experiments showing how the new knowledge graph benefits machine learning tasks.\n",
      "The methods are conventional and there is no novelty from the algorithmic perspective.\n",
      "The paper lacks novelty and does not relate to the focus of ICLR.\n",
      "Lack of clarity and poor writing prevent a thorough review.\n",
      "Uncertainty if empirical improvements are truly due to better gradients; theory's applicability to non-convex settings is unclear.\n",
      "The contribution is limited because the proposed approach only offers comparable results with SOTA (early stopping).\n",
      "The evidence in the current version is not sufficient to support all the claims; work is not generalizable to other baselines, limiting impact.\n",
      "The proposed approximation technique is rather a heuristic and only addresses the issue at a superficial level.\n",
      "The auxiliary loss can lead to trivial identity solutions, especially when hidden dimensions exceed input dimensions.\n",
      "No major complaints; rejection based on reviewer 1's points, not stated here.\n",
      "None mentioned.\n",
      "The theoretical analysis is just a regular routine.\n",
      "Lack of reproducibility and repeatability: insufficient details on hyperparameters and lack of repeated training runs with performance variation.\n",
      "Work does not position well with existing literature; motivation and impact are not very clear; contributions are marginal.\n",
      "Scalable communication will require trading off structural priors and learned representations; this challenge is not fully solved.\n",
      "The experimental part is weak; results are only on digit datasets and lack broader evaluation.\n",
      "The proposed method is very similar to existing methods, with limited novelty.\n",
      "Lack of comparison with recent state-of-the-art methods, including those cited by the authors.\n",
      "Architectural/theoretical contribution is not significant; idea is not hugely novel and requires more demonstration.\n",
      "The results do not show much improvement on most datasets, questioning the premise and experimental validation.\n",
      "The network architecture is only an incremental variation of standard MLP.\n",
      "Lack of discussion on how to combine techniques or empirically choose the best approach for varying scenarios.\n",
      "The paper contains confusing and contradicting statements or claims not supported by experimental results.\n",
      "The method is wrongly claimed as RL-based; RL definition and state transitions are inadequately specified.\n",
      "Lack of comparison to a differentiable approximation baseline casts doubt on RL's necessity/optimality.\n",
      "Lack of comparison with other scalable domain adaptation approaches.\n",
      "Limited ablation study to show the impact of each design choice.\n",
      "A fairly large validation dataset fraction is needed to reliably identify corrupted samples, contrary to claims about small set sufficing.\n",
      "The construction appears equivalent to existing steerable/gauge equivariant neural nets, lacking demonstrated novelty.\n",
      "The paper does not clearly demonstrate how the proposed method differs from or outperforms standard data augmentation.\n",
      "The experiment is very limited and does not demonstrate the benefits of having an equivariant network.\n",
      "The approach is limited to finite groups and does not address equivariance with respect to continuous groups (e.g., SO(2)).\n",
      "The theoretical part (section 2) is hard to follow and unclear.\n",
      "The improvement brought by the framework is marginal.\n",
      "It is unclear how much the LPA impacts the node classification.\n",
      "Most algorithmic ideas are present in prior work; contribution not novel enough.\n",
      "The proposed method does not outperform previous off-policy methods and results are not convincing.\n",
      "The paper insufficiently positions itself with respect to relevant literature, especially failing to cite/discuss a very similar previous work.\n",
      "The proposed method lacks novelty and is merely an easy combination of existing methods instead of solving previous limitations.\n",
      "Lack of novelty in the proposed frameworks.\n",
      "The paper lacks convincing quantitative analysis to demonstrate the method's superiority; relies mostly on qualitative results.\n",
      "The paper does not truly move away from the use of anchors as claimed.\n",
      "The main idea is similar to existing methods ([1-5]), lacking novelty.\n",
      "The writing can be significantly improved; technical parts are confusing and unclear.\n",
      "The paper lacks clarity and suffers from language/grammar issues, making it hard to understand.\n",
      "The method is rather incremental, with limited novelty and significance compared to prior work (DARTS and [4]).\n",
      "Without more info on Table 3 baselines, it's unclear if TopoNets improve over Random and Residual baselines.\n",
      "Limited technical novelty; approach is a simplified version of existing differentiable NAS applied to a similar search space.\n",
      "Very little information is provided on how to successfully train the proposed framework in practice.\n",
      "The general motivation or justification, e.g., the limiting cycle argument or visualization, is unconvincing or over-claimed.\n",
      "No certified guarantee; robustness not evaluated under adaptive attacks; effectiveness unconvincing without thorough evaluation.\n",
      "It is difficult to understand the paper; it is not clearly written, especially the algorithm's presentation.\n",
      "Authors did not verify correlation between detected communities and original labels, weakening experimental evidence.\n",
      "Insufficient novelty; similar methods have been published.\n",
      "Lack of experiment comparing benefit from more data vs. summarization objective (e.g., pre-training BERTSUM on same data).\n",
      "The idea is not novel; similar approaches and datasets already exist.\n",
      "Experiments are incomplete, notably missing evaluation on DUC datasets to assess real-world generalization and lead-bias effects.\n",
      "The paper lacks sufficient detail, particularly regarding model architecture, training, and ablation studies.\n",
      "The scaling of the technique is not supported by the current experiments.\n",
      "Much of the presentation is vague or opaque, lacking model details, and unclear/mismatched diagrams and equations.\n",
      "Uncertainty about how stability of the target network is ensured via minimizing difference with the Lyapunov critic.\n",
      "Unclear how Lyapunov criteria in Theorem 1 guarantees stability with the proposed penalty-based learning algorithm.\n",
      "Lack of genuine novelty and unsubstantiated claims about stability guarantees; current approach does not convincingly demonstrate true stability.\n",
      "The frequency principle lacks a rigorous definition.\n",
      "Key findings (F-Principle) are not novel; similar results have already appeared in prior work.\n",
      "Insufficient experiments, particularly only analyzing a single principal component and lacking exploration of nonlinear transformations.\n",
      "The title is misleading and the claim of 'unifying' and 'theoretical analysis' is inaccurate.\n",
      "The core premise that GCNs are matrix factorization methods is incorrect and cannot be justified mathematically.\n",
      "The connection of GCN to MF is very indirect and not clearly established empirically.\n",
      "The specific problem (*class mismatch*) is not clearly formalized or mathematically characterized.\n",
      "Questionable reliability of prototypes derived from pseudo labels; justification for superiority over pseudo labels lacking.\n",
      "The method is a bit incremental over the GAIN approach; improvements are straightforward.\n",
      "Paper needs much better evaluation and discussion, especially in comparison to related methods like GAIN and multiple imputation.\n",
      "Potential improved accuracy may be due to generalization from noisy imputed samples, not methodological innovation.\n",
      "The novelty of using learned modules across domains is unclear; it may resemble traditional feature engineering.\n",
      "Unclear what problems the framework can solve beyond those with known, fully specified algorithms and breakdowns.\n",
      "The paper fails to address scalability of modules to real-world problems.\n",
      "Violation of the double blind review rule and similarity to previously published work.\n",
      "Violating double blind rule.\n",
      "Marginal technical contribution—main change is swapping SGD for Adam in UNet, not a significant methodological advance.\n",
      "Limited applicability due to strong assumption that both robot and object states are available and clearly separable.\n",
      "Detailed experimental setups are missing, making the results hard to evaluate.\n",
      "Needing new environment variations for new skills is a step backwards compared to prior methods like DIAYN.\n",
      "The setup and model are too difficult to understand, hindering the ability to recommend the paper.\n",
      "The results provide only marginal improvements over the baseline and do not appear to be statistically significant.\n",
      "Reviewer was unable to assess technical contribution's novelty or comparison to prior RL work.\n",
      "The novelty and rigorousness of the paper are not convincing; much content closely resembles prior works.\n",
      "The argument relating network depth to momentum is not convincing.\n",
      "All implications are confirming already known things in different (path-based) words; the impact or novelty is rather small.\n",
      "Experimental validation is inadequate due to lack of baselines, making robustness assessment difficult.\n",
      "The proposed method is not highly effective and its contribution is marginal, especially in light of prior work [1].\n",
      "Defending against Gaussian noise is too simple and not an exciting or meaningful contribution.\n",
      "Theoretical results depend on strong assumptions (e.g., iid Gaussian weights), limiting generality and validity of claims.\n",
      "Assumptions, such as the need for Gaussian i.i.d weights, seem unrealistic and are not quantitatively justified.\n",
      "The evaluation compares methods in accuracy ranges where the defense practically fails, making the results and comparisons not meaningful.\n",
      "Empirical evaluation is limited to a single type of situation, questioning the method's generality.\n",
      "The method is only evaluated on a toy task, limiting evidence for generalization to complex tasks.\n",
      "The model and experiments are overly complicated for a very simplistic scenario where simpler methods would suffice.\n",
      "The paper does not make use of representation learning and is therefore out-of-place at ICLR.\n",
      "The paper is awfully written and key concepts like the feature parameter phi are not explained, making it unreadable.\n",
      "Absence of experiments on more recent architectures like DenseNet and ResNet.\n",
      "Unclear what new insight or value the reader should learn from the paper; claims are unsurprising.\n",
      "Reviewer is not convinced the lack of highly selective units in CNNs is an important or surprising problem.\n",
      "Major concerns about novelty and experimental setups.\n",
      "The novelty of this work is not enough.\n",
      "Reliability of results is unclear due to very small training set, making evaluation difficult.\n",
      "The presentation of the proposed models (RLIF and LIF-LSTM) is not clear at all.\n",
      "The method does not demonstrate clear advantages over standard LSTM, lacking evidence for combining deep learning and spiking neural nets.\n",
      "No evidence presented for the model's claimed hardware-friendliness and energy-efficiency.\n",
      "The paper explores methods for robustness certificates but does not compute and validate these certificates empirically.\n",
      "Concerns about the novelty of Theorems 1 and 2; key results are known or uncited.\n",
      "The main theoretical result is a trivial derivation based on existing work and lacks substantive novelty.\n",
      "No convincing explanation or justification for why the modulo operation improves the algorithm.\n",
      "No main negative comment is provided in the review.\n",
      "MONIQUA shows superior results only at early stages, fails to achieve state-of-the-art test accuracy, and suffers 10-20% accuracy drop.\n",
      "The evaluation is not convincing and whether Moniqua can achieve consensus is unclear.\n",
      "The main claim that NovoGrad is robust to learning rate and initialization is not convincingly supported by experiments.\n",
      "The proposed method lacks novelty; it is just a combination of AdamW and LARS.\n",
      "The theoretical contribution is incremental; no justification or proof for layer-wise normalization improvements.\n",
      "Performance gains over BERT are small and results do not clearly justify the architectural additions except for *lexical_overlap*.\n",
      "The methods are not novel enough; improvements are not significant.\n",
      "No evidence provided that self-attention can't already capture local-context features.\n",
      "The connection between the transformer architecture and multi-particle ODEs is weak and not sufficiently fleshed out.\n",
      "The use of second-order splitting without upgrading Euler's method does not yield a higher-order ODE solver, invalidating the main claim.\n",
      "The experimental comparisons are questionable; the paper does not compare with best benchmarks and lacks transparency.\n",
      "The paper is squeezed to fit into 8 pages, which may not be acceptable.\n",
      "No experimental evidence provided to support that the theoretical phenomena (G-dominated/V-dominated regimes) occur in practice.\n",
      "Framework and results are very similar to previous works; kernel regime dynamics may not be the most interesting or novel.\n",
      "Lack of theoretical justification for the proposed method.\n",
      "Lack of novelty and originality; method is a simple combination of existing clustering and few-shot learning methods.\n",
      "The approach is very similar to DeepCluster; lack of comparison to non-episodic training.\n",
      "The method is only an incremental extension of existing solutions with no theoretical justification.\n",
      "Paper lacks novelty; contributions are incremental and similar to existing methods.\n",
      "The novelty is limited; the method is very similar to unsupervised GraphSAGE.\n",
      "Limited novelty and insufficient contribution compared to previous work (Diffpool).\n",
      "Limited novelty; improvements over DiffPool are mainly adding previously proposed regularization terms.\n",
      "No mention about code release.\n",
      "The main negative comment is a lack of novelty compared to existing tensor decomposition-based CNN compression methods.\n",
      "Lack of ablation experiments for FALCON's components and need for experimental justification of convolution order.\n",
      "None explicitly stated.\n",
      "The sole novelty is a new justification for the STL estimator and there are no experiments.\n",
      "The experiments are quite lacking, especially on large scale models.\n",
      "The novelty and practical takeaways of the paper are unclear.\n",
      "The paper lacks important latest references on domain adaptation, especially omitting recent deep methods, making contributions unconvincing.\n",
      "The manuscript has limited novelty compared to previous work (BT).\n",
      "Lacks research motivation and solid experimental validation.\n",
      "The intuition and derivation for the attention modules are missing and need to be clarified.\n",
      "Unclear how theory applies to experiments using an attention network instead of CNN.\n",
      "Reviewer cannot evaluate novelty of the theoretical contributions due to lack of expertise.\n",
      "It is unclear how speed-up is calculated and whether MAC reduction correlates to actual hardware speed-up.\n",
      "Lack of comparison with a simple combination of best static and dynamic pruning baselines; unclear added value of the proposed method.\n",
      "The paper is poorly written and the model/architecture descriptions are very difficult to parse.\n",
      "The evaluation is biased by using test set I/O pairs to construct the training set, compromising validity.\n",
      "The description of the overall method is too high-level; needs clearer formalization of network architectures and framework.\n",
      "The paper lacks important technical details and sufficient experimental evidence to be useful to other researchers.\n",
      "No novelty in the proposed fast segmentation network detailed in Sec 3.2.\n",
      "The approach is very similar to existing semi-supervised methods, with minimal novelty.\n",
      "Novelty in architecture design is trivial.\n",
      "Only tested on naive datasets (MNIST, Fashion MNIST); unclear performance on complex datasets like ImageNet.\n",
      "The contribution is incremental, being very similar to previous work (dlADMM, Wang et al. 2019).\n",
      "The idea does not seem different from already existing ones; only minor innovation.\n",
      "The novelty of the proposed approach is not high and improvements are incremental.\n",
      "The work is not novel and the model offers limited novelty.\n",
      "Lack of comparison and discussion with Att2in (Rennie et al., 2017) regarding why rAIRL outperforms it.\n",
      "Confusing motivation, especially regarding reward ambiguity, and lack of clear explanation in the paper.\n",
      "The experiment on 'compactness' may not accurately reflect the concept as intended.\n",
      "The paper lacks clarity and motivation for the bandwidth extension module and does not fulfill its core promises.\n",
      "The paper does not clearly show that the proposed architectures address multi-scale generation issues; lack of evidence and ablation.\n",
      "PUGAN was tested only on a simple dataset and quality is only comparable to WaveGAN, not advancing fidelity.\n",
      "Lack of essential baselines, especially Factorization Machines, makes results unconvincing.\n",
      "Missing crucial baselines that address similar problems, lacking proper positioning relative to related literature.\n",
      "Experiments are unconvincing; there's no time complexity comparison with AutoCross despite acceleration being the main contribution.\n",
      "The method is not clearly explained, making it hard to understand or replicate.\n",
      "The color scheme of figures is not friendly to color-blind people.\n",
      "The technical novelty is limited, and the necessity of the second branch is not clearly explained.\n",
      "Unclear and potentially noisy prototype selection; all training samples used instead of top relevant prototypes.\n",
      "The use of asymmetric encoders (keys vs. queries) is not justified or empirically validated.\n",
      "A rigorous theoretical proof for interpretability is lacking; current proof is naïve.\n",
      "The method assumes only global translational motion, ignoring spatially varying motion and making a critical, unrealistic assumption.\n",
      "The paper lacks many references and comparisons with recent multi-frame super-resolution methods.\n",
      "Unclear participation, results, and rankings in the ESA challenge; lack of clarity around methods presented.\n",
      "The second part of the paper is not rigorous and the results do not prove convergence to a global optimum as claimed.\n",
      "Lacks originality; prior work has already addressed provable convergence in more detail and with greater rigor.\n",
      "The paper has nothing new or non-trivial; main claim about disparity matrix is unproven.\n",
      "The main theoretical contribution (canonical space) is just standard Fourier analysis, not a new tool.\n",
      "Incremental methodological contribution.\n",
      "The method does not adequately address scheduling operations within each machine to minimize makespan.\n",
      "Lacks many details in key sections, preventing full appreciation and making reproducibility impossible.\n",
      "The algorithm is quite reminiscent of Elastic Averaging SGD (EASGD), differing only in how the moving-average is computed.\n",
      "The convergence result ignores the effect of non-iid data, contradicting intuition and correctness.\n",
      "The study does not go into depth.\n",
      "Analysis is limited to a simplified case with a linear discriminator, lacking extension to realistic scenarios.\n",
      "The paper does not provide novel insights nor state-of-the-art results on CIFAR10, limiting its contribution.\n",
      "Analysis does not explain why various methods, including gradient penalty, converge in practice with proper hyper-parameters.\n",
      "The proposed algorithm is not clear.\n",
      "The paper is cryptic and very difficult to understand, lacking clear explanations of the problem and contribution.\n",
      "Accelerated diffusion causes over-smoothing quickly, limiting the feasible number of layers and deep network non-linearity.\n",
      "Assumption of annotated rationales is a big limitation.\n",
      "It is not surprising that smaller models outperform BERT-based models in low-resource cases.\n",
      "The model is not interpretable as embeddings and non-linearities make decision boundaries unclear.\n",
      "Neither of the two contributions provides a convincing solution to the main question, and the paper lacks a consistent thread.\n",
      "Both contributions are only loosely connected and some parts are confusing or poorly motivated, making the paper hard to follow.\n",
      "The method's reliance on a Gaussian assumption for regression is unjustified, especially for Bayesian Neural Networks.\n",
      "The results and recommendations are intuitive and somewhat trivial.\n",
      "The assumption that benchmarks with fewer isomorphic graphs are better is questionable and may not be appropriate.\n",
      "(b) and (c) are contradictory; (b) is near-trivial.\n",
      "The novelty of the work is limited.\n",
      "The proposed approach is not sufficiently justified and experiments do not sufficiently explore the benefits (no ablation study).\n",
      "The contribution is limited as the approach is not shown to be generalizable to other tasks.\n",
      "The paper lacks novelty; the contribution is not sufficiently original.\n",
      "Overly bold claims with insufficient clarity and justification.\n",
      "The paper lacks novelty and does not provide new insights beyond existing intuitive knowledge.\n",
      "Reviewer is not well versed in the mathematical area to assess the derivations and proofs critically.\n",
      "The paper is very badly written and organization needs to be rethought.\n",
      "Main concern is about the novelty/significance; similar arguments and results exist in prior work.\n",
      "Experimental details are insufficient for reproducibility; methods do not convincingly justify the claims.\n",
      "The method still requires several tuned hyperparameters, contradicting the claim of reduced hyperparameter tuning.\n",
      "The approach needs more detailed description and experiments do not satisfy the claims made in the Introduction.\n",
      "Lack of clear explanation of advantages of ATMC over existing SG-MCMC methods such as [1].\n",
      "Paper lacks justification and explanation for its main claim about the relationship between knowledge distillation and early stopping.\n",
      "Theoretical contribution overlaps with previous work ([Li et al. 2019]) and offers little novelty.\n",
      "Introducing extra hyper-parameters may make the algorithm even harder to tune.\n",
      "Lack of baseline comparison with generative models and insufficient validation procedure description.\n",
      "Experiments are not convincing and the modification doesn’t provide much insight into representation learning/anomaly detection.\n",
      "The experimental evaluation is not convincing due to lack of competitors, use of simple datasets, and missing experimental details.\n",
      "Not fully convinced of the heuristic: similarity between input and output embeddings used for position supervision.\n",
      "Does not compare with later, better baselines such as FlowSeq; unclear speedup evaluation.\n",
      "The presentation lacks sufficient clarity for those not familiar with non-autoregressive generation.\n",
      "Insufficient empirical evaluation, especially regarding runtime, accuracy, and comparison to baseline methods.\n",
      "The method's deviation from conventional VI creates unnecessary computational overhead without clear benefit for encoding KNN information.\n",
      "Experimental evaluation is too narrow, using small datasets; lacks meaningful comparison on larger, appropriate benchmarks.\n",
      "Motivation is not justified as per-image semantic information increases annotation effort, undermining zero-shot learning's purpose.\n",
      "The novelty is limited; loss functions are not new and have already been used in prior work.\n",
      "Unclear novelty of retrieval model and metric scaling; insufficient comparison to existing alternatives.\n",
      "The method's application to real neuromorphic chips is unproven; only simulated results are shown.\n",
      "Support for the main claim is weak and inadequately demonstrated.\n",
      "Paper may be difficult to grasp for the general ML audience, including ML researchers.\n",
      "The proposed algorithm violates its own assumption in practice, especially in the handling of the homotopy parameter L.\n",
      "The current presentation and experimentation does not meet the acceptance threshold.\n",
      "Mistake in the proof of Theorem 3.1; the claim about boundedness is incorrect.\n",
      "The proposed method does not seem significantly better than the previous (very similar) method of Mu et al.; more evidence is needed.\n",
      "The method was not tested on other NLP benchmarks like text classification or NER.\n",
      "The paper lacks clarity, especially in problem definition, motivation, and distinguishing its own contributions from prior work.\n",
      "The cycle consistency constraint is not truly a 'cycle'; justification is lacking.\n",
      "Experimental validation is weak: no proper baselines, limited datasets, and unclear evaluation metrics.\n",
      "Probably requires further research to achieve state of the art results.\n",
      "Main contribution is identifying a problem, not providing a significant technical advance.\n",
      "The modeling contribution is incremental given existing literature.\n",
      "No theoretical analysis provided for output space adaptation, which is a complex problem.\n",
      "Insufficient experimental evaluation: small datasets, high variance, only single accuracy reported, limiting method’s effectiveness assessment.\n",
      "Unclear whether feature similarity is the reason BNSR outperforms BN; evidence is insufficient.\n",
      "The analysis and experiments are too weak to support the authors' claim.\n",
      "Experimental results are unconvincing, with all recent methods worse than baselines and unclear explanations/data details.\n",
      "The paper is very hard to follow and not complete.\n",
      "The presentation of the algorithm is vague and it is difficult to determine what the algorithm actually does.\n",
      "The work is incremental with no novel idea; improvement comes solely from applying an existing method.\n",
      "Lack of uncertainty (e.g., error bars) in experimental plots.\n",
      "None stated.\n",
      "Lack of explanation on why biased estimators still guarantee convergence; experiment results contradict claimed sublinear theoretical convergence.\n",
      "It is not trivial to get from using this tool to a working method.\n",
      "Reviewer lacks knowledge in the area and could not thoroughly evaluate the paper.\n",
      "Section 2.3 appears copied from prior work (Chow et al. 2018), with unclear claims of originality.\n",
      "Unclear if and how multiple HaarPool layers could be used; explanation on this is requested.\n",
      "The main contributions are not clear compared to other state-of-the-art methods.\n",
      "The experimental investigation is not sufficient and compelling enough.\n",
      "Doubts about the effectiveness of the proposed method and whether it truly solves existing methods' problems.\n",
      "No sound comparison with NAO; reviewer doubts RL works better than gradient descent in continuous space.\n",
      "The evaluations are highly insufficient, with experiments only on CIFAR-10 and unclear generalization.\n",
      "Lack of innovation and insufficient differentiation from Luo et al. 2018.\n",
      "The assumption of independence between different elements of the Hessian is too strong.\n",
      "Theoretical assumptions are too strong and unrealistic, limiting practical impact.\n",
      "Theoretical and empirical analyses are not sufficiently novel, deep, or informative.\n",
      "The solution is trivial and does not improve upon or compare with existing domain adaptation methods.\n",
      "No observable technical novelty in the proposed model; double blind adaptation not significantly different from prior work.\n",
      "The model is trivial (auto-encoders with L2 loss), so novelty is only incremental.\n",
      "The novelty of the work is limited; the paper should justify its approach over low-dimensional embedding-based methods.\n",
      "The paper's generalizability and lessons for other languages are unproven; fit with ICLR focus is weak.\n",
      "The method is not novel; it is a standard PMI application repeatedly presented in prior literature.\n",
      "The feature uncertainty used in the proposed model can be unreliable and lacks a reliable guarantee.\n",
      "Lack of details on constructing F_theta and how asymmetric transfer weights are implemented.\n",
      "The paper is vague about the exact modelling of transfer and lacks detail on how uncertainty controls transfer.\n",
      "This work is incremental; prior work has shown similar results and theory (Wang et al.).\n",
      "Lack of clear connection between analytical investigation and empirical results; theoretical results may overstate conclusions.\n",
      "Lack of novelty in methodology and empirical value; pre-LN approach is already widely used and documented.\n",
      "No performance comparison to existing methods; attack success rate is much lower than prior work.\n",
      "Readability—many sections need proofreading; numerous grammatical/word choice mistakes make the paper difficult to read.\n",
      "Evaluation is weak due to low attack success rate and lack of justification for design choices and results.\n",
      "Significant technical flaws and lack of novelty; approach is essentially identical to BatchNorm and conclusions are unsupported.\n",
      "Theorem 1 proof is just standard SGD analysis, with no new contribution.\n",
      "Unclear novelty and insufficient citation of prior related work undermine the contribution.\n",
      "Discussion and derivation remain too vague and require better and clearer explanation.\n",
      "The paper shows only intuitive drawbacks of AII without any theoretical proof.\n",
      "The paper is unclear, making it hard to distinguish novel contributions from prior work.\n",
      "Assumptions about treating response entries as samples on GGD are confusing and need clear evidence or explanation.\n",
      "LID may not be the most relevant or strongest baseline for adversarial detection.\n",
      "MBF detection metric is much more complicated to calculate than baselines, possibly making performance gains unfair.\n",
      "Lack of experimental results showing that the target policy recovers the right policy when dynamics match a source.\n",
      "Applicability is limited due to strong assumptions on similarity between source and target domain dynamics.\n",
      "The method is a naive combination of previous works, lacks insight, and requires same state/action spaces for source/target.\n",
      "Main weakness is the novelty / proper connection to self-supervised learning work.\n",
      "The paper does not sufficiently address real use cases or the practical need for learned similarity metrics on PDE outputs.\n",
      "No proof that the CNN-based evaluation metric satisfies metric properties, especially triangle inequality.\n",
      "There is no new finding in this paper.\n",
      "Not fully self-contained; relies on 20 pages of supplementary material.\n",
      "VTAB does not restrict representation learning to a fixed dataset, making improvements possible by simply using more representative data.\n",
      "It is unclear if the proposed method solves GNNs' overfitting/representation power issues as claimed.\n",
      "The improvement mainly comes from existing supernode ideas, making the paper incremental and lacking novelty.\n",
      "Writing is overcomplicated and uses a lot of jargon, making the text impenetrable.\n",
      "Comparison is not convincing because different backbones (ResNet50 vs Inception) are used, making results unfair.\n",
      "No error bars or confidence regions, making it hard to judge significance of performance differences.\n",
      "No human-rater evaluations to measure perceptual quality of localization.\n",
      "Figure 1 does not show which part corresponds to the decoding phase; the paper is unclear here.\n",
      "Lack of clear motivation and practical application for grounding; marginal performance improvement over baseline GVD.\n",
      "Use of kernel density estimate (KDE) in high-dimensional spaces is unconvincing; feasibility and bandwidth selection are not addressed.\n",
      "The reasoning and intuition behind the equations, especially dissonance and Subjective Logic, are hard to follow.\n",
      "The role and importance of FFT/IFFT versus spectral projection are not deeply analyzed or clarified.\n",
      "The paper is preliminary and demonstrates the approach on only a single example and constraint, lacking general evidence.\n",
      "The paper does not fully deliver on the promise to make physics constraints in deep networks usable in practice.\n",
      "Lack of explanation or intuition for why the proposed method works.\n",
      "The evaluation lacks comparison with relevant, cheap attention mechanisms such as Squeeze-and-Excitation.\n",
      "Results cannot be verified unless code is shared.\n",
      "Experimental results are unconvincing; significant methodological issues and lack of explanations undermine trust in findings.\n",
      "The methodology of adding a graph connectivity term is relatively simple.\n",
      "The method lacks flexibility compared to assignment matrices, especially for learning discriminative nodes via backpropagation.\n",
      "Unclear novelty compared to existing DP-preserving algorithms with provable robustness.\n",
      "The relationship between adversarial examples and certified robustness is unclear, questioning the interest of the algorithm.\n",
      "The differentially private objective does not guarantee differentially private parameters, undermining the privacy claim.\n",
      "The method lacks robustness when applied to realistic images with complex or extraneous dynamics.\n",
      "No comparison to strong keyframe prediction baselines; reliance on simple baselines weakens experimental support.\n",
      "The technical presentation is painful to follow; global rewriting and clearer focus are needed.\n",
      "Experiments fail to disentangle the effect of weight uncertainty vs. structure uncertainty.\n",
      "Using the same point estimates for weights across different random network structures undermines probabilistic modeling and uncertainty calibration.\n",
      "Bayesian formulation is problematic; priors on alpha and the KL term are not clear or valid.\n",
      "Performance evaluation is insufficient due to limited and unfair comparisons.\n",
      "Experimental performance is unsatisfactory and lacks convincing improvement on more than one dataset.\n",
      "Performance drops sharply after 50% pruning across all datasets (Figure 4).\n",
      "There are almost no experiments in this paper.\n",
      "No empirical improvement over state-of-the-art (SOTA); method only matches existing algorithms.\n",
      "The paper is poorly written, unclear, and not ready for publication.\n",
      "Unclear how the work substantially deviates from previous work; novelty is questionable.\n",
      "The 'RNN-based filter' is not related to standard RNNs, making the title and presentation confusing.\n",
      "Concerns about novelty; the work seems a straightforward application of known theorems with unclear new discoveries.\n",
      "The paper does not convincingly show that models use unification/logical reasoning to solve tasks, with key interpretability issues.\n",
      "Does not provide thorough comparison with other state-of-the-art methods on reasoning related tasks.\n",
      "The model description is very hard to follow; the architecture is not clearly explained.\n",
      "Repeated description of the framework in the introduction.\n",
      "The proposed work does not add significant insight or novelty in solving the problem.\n",
      "The work is not of enough general use or interest to the ICLR community to merit acceptance.\n",
      "Reasoning is intuitive but lacks thorough justification; explanations are not empirically supported.\n",
      "Gap between the universal learning framework and the proposed method, with inconclusive experimental results.\n",
      "More experiments are required to support usefulness and effectiveness, especially for weak attacks like FGSM.\n",
      "No comparison with existing distributed asynchronous update methods; unclear how approach improves over prior stale update techniques.\n",
      "Generalization performance degrades significantly with large number of processes/stages, not resolved even with hybrid method.\n",
      "The method introduces more memory overhead compared to GPipe.\n",
      "Convergence analysis of the proposed method is missing, which is critical due to potential divergence from asynchrony.\n",
      "Lack of discussion on the effect of intrinsic reward bias, especially in non-sparse reward cases.\n",
      "Insufficient ablation for multi-step K; unclear benefit of using K>1 over K=1 in main experiments.\n",
      "Experiments were not performed on a popular benchmark such as Atari.\n",
      "The technique is extremely simple—just a naive greedy algorithm—making the problem neither hard nor particularly useful.\n",
      "The proposed growing strategy is already known from prior work; novelty and additional value are unclear.\n",
      "The method does not achieve state-of-the-art accuracy compared to existing NAS methods; lacks discussion on this shortcoming.\n",
      "The results depend heavily on Assumption 4 and correctness is not evaluated.\n",
      "Limited experimental evaluation and questionable applicability of assumptions and theoretical setting.\n",
      "Lack of experiments on real-world applications.\n",
      "Quantitative analysis does not immediately show the benefit or characteristics of the proposed approaches.\n",
      "The multi-objective optimization is confusing and not truly multi-objective; lacks novelty.\n",
      "Weak experimental section, unfair and incomplete comparisons against relevant baselines.\n",
      "Missing comparisons with relevant, state-of-the-art resource-aware NAS baselines (e.g., MNASNet, ProxylessNAS, FBNets, EfficientNets).\n",
      "The number k of instances must be provided, which is unrealistic for real-world instance detection tasks.\n",
      "Tasks are too easy; unclear if method works on real-world datasets with ill-defined object numbers.\n",
      "The key training method is only half described, lacking abstraction from specific architectures.\n",
      "Theorems are misleading and overclaim, especially about Gaussian smoothing and dimensionality dependence.\n",
      "The paper does not give sufficient motivation for studying D_infty and D_{MR} robustness.\n",
      "The notions of robustness defined by the authors are not clearly related to standard adversarial robustness.\n",
      "Discarding unsuccessful runs undermines the claim that low entropy messages emerge without regularization.\n",
      "The experiments are insufficient to support the paper's main claim of entropy minimization in emergent languages.\n",
      "Concerns about the novelty, especially given the simple environments and blurred distinction from standard neural networks.\n",
      "Lack of investigation or reporting for values of lamda_r.\n",
      "Unclear if VILD works in realistic situations with model misspecification, especially with unknown number of demonstrators.\n",
      "Experiments do not provide convincing evidence about the generality of the approach, especially with real-world demonstration diversity.\n",
      "VILD uses labeled expert demonstrations, which constitutes auxiliary information, contrary to its claimed advantage.\n",
      "The approach is very similar to deep CCA, but this connection is never mentioned or clarified.\n",
      "Exposition is difficult to follow and experiments are not very compelling.\n",
      "The paper fails to locate itself in the literature and does not compare with existing techniques analytically or experimentally.\n",
      "The framework is very common and lacks new insights about the problem.\n",
      "The novelty is not strong; the proposed approach does not convincingly outperform a simpler baseline.\n",
      "Generated faces often have asymmetric (unbalanced) eyes.\n",
      "The proposed method is not useful against white-box and transfer-based attacks, as it does not change the predictive model.\n",
      "The threat model is not well justified; the defense may not be robust to realistic black-box attacks (e.g., transferability or label-based attacks).\n",
      "No comparison with defenses based on input or model randomization.\n",
      "The paper suffers from an incomplete evaluation, so acceptance is not recommended.\n",
      "The possible false positives due to sample dependence in fMRI data have not been addressed, making results vacuous.\n",
      "Central theoretical argument regarding sample complexity is incomplete and not comparable to previous methods.\n",
      "Paper does not report MINE results on the fMRI dataset for comparison.\n",
      "No formal proof of convergence or efficiency for the proposed algorithm; empirical validation of these properties is missing.\n",
      "No comparison to existing methods, making it impossible to evaluate usefulness.\n",
      "The explanation of the subset scanning approach and its optimality is unclear.\n",
      "Lack of standard deviations in Table 2 makes it unclear if reward differences are significant.\n",
      "Empirical validation does not match motivating scenario; experiments are only on small networks, undermining significance.\n",
      "The comparison to baseline need to be improved or at least justified.\n",
      "The method requires storing an increasing number of models, leading to huge storage costs, making it economically infeasible.\n",
      "The proposed method is incremental with limited novelty.\n",
      "DoS and DoS-CH perform worse than averaging baselines on unseen domains, questioning the selection network's effectiveness.\n",
      "Conceptual and experimental contributions are rather weak; tasks are direct applications of existing frameworks with limited novelty.\n",
      "Important details are omitted from the main body, making the paper difficult to read and evaluate independently.\n",
      "Lack of intuition or theoretical explanation for why the method improves on past approaches.\n",
      "The paper adds a very simple dynamic loss balancing with limited novelty, making the contribution ineffective.\n",
      "The simple MEGA-D baseline performs very well with little additional gain from the full MEGA approach, but is insufficiently analyzed and discussed.\n",
      "The attacker’s assumption of access to model and episodic memory is impractical, making the attack unsurprising.\n",
      "Contributions are limited to A-GEM and do not cover other episodic-based continual learning methods.\n",
      "Unclear generalizability of conclusions beyond the specific method tested; lack of evidence that the method is representative.\n",
      "Concerns about Bayesian inference, specifically the use of Laplace approximation and how it compares to VB or sampling methods.\n",
      "The writing is unclear and hard to follow, and the experiment results are not convincing.\n",
      "The dataset (SST) is so small and thus is questionable about the quality of the learned word embeddings and the comparisons.\n",
      "The novelty of the separable basis decomposition is not convincing; differences with prior work are unclear.\n",
      "Contribution is meaningful but incremental; method is almost similar to RotDCF.\n",
      "Experimental section is lacking in comparisons, insights, and meaningful evaluation; insufficient baselines and weak experiment choices.\n",
      "Insufficient methodological details and justification, especially on automatic pruning allocation and relation to relevant work.\n",
      "Concern with the novelty of the proposed method.\n",
      "The 'mask decay update' workaround for the hard sigmoid issue looks hacky and may not work reliably in practice.\n",
      "Paper overstates that adversarial robustness always causes standard performance degradation; this may not always be true.\n",
      "The information is presented in a way that makes it difficult to validate the correctness of the theory and insights.\n",
      "Obtaining adversarial robustness leads to degraded model performance on natural examples.\n",
      "The proposed environments are too few, too simple, and unrealistic compared to real-world problems.\n",
      "The arguments are vague and lack mathematical formulation or proof; need concrete analysis.\n",
      "Largely insufficient literature review; paper overstates novelty and misses key related work and communities.\n",
      "Evaluation does not focus enough on real scenarios and lacks uncertainty quantification, making improvements unproven.\n",
      "No quantitative analysis or empirical results supporting choice of TGCNN over RNN-based architectures.\n",
      "The novelty of the model is rather limited.\n",
      "Extremely limited empirical evaluation makes it difficult to assess benefits over more straightforward algorithms.\n",
      "The proposed method lacks novelty and is not well evaluated against other representation learning techniques.\n",
      "Derivation of the target distribution by KKT condition would be more rigorous.\n",
      "Weak motivation for input-dependent dropout method.\n",
      "Lack of convincing experiments on large-scale models and datasets to prove significance.\n",
      "Unclear from empirical results whether the input-dependent version offers real advantages over added complexity.\n",
      "The advantage of DBB over BB is not clear, with mixed results across datasets and possible unjustified overhead.\n",
      "After improved evaluation, method lacks statistical significance and is not generally applicable; unclear when it works best.\n",
      "No time cost comparison is provided for the proposed Euclidean distance acceleration.\n",
      "The paper is extremely difficult to follow, with an unclear objective and algorithm.\n",
      "No comparison with clustering methods using geodesic distance (manifold learning) designed for non-convex data.\n",
      "Assumption of a lower bound on validation gradients is troubling, especially for over-parameterized problems and raises practical concerns.\n",
      "Experimental evaluation is very limited—only one model, one dataset, small scale, unclear generalization and scalability.\n",
      "Possible flaw in the proof of Theorem 1 regarding replacing ||g||^2 with ||grad f_s(x_tau)||^2.\n",
      "Evaluation is based on outdated datasets, making it hard to accept the paper.\n",
      "The description of the method, particularly the instance classifier and weighting process, is not very clear.\n",
      "Questions on experimental choices: lack of explanation on temperature in log-sum-exp and comparison to ATT instead of Gated-ATT.\n",
      "Findings do not seem experimentally well validated.\n",
      "No results on standard disentanglement metrics and no comparison with existing methods.\n",
      "The contribution is outdated and lacks significant improvement/technical novelty over prior art.\n",
      "The paper is difficult to understand in some places due to unclear explanations.\n",
      "Technical contributions are not enough; needs more experimental comparisons.\n",
      "Comparisons to existing methods may not be comprehensive enough.\n",
      "Lack of provided code to validate the soundness of the model.\n",
      "Lack of convergence analysis and unclear choice of projection for mirror mapping.\n",
      "Lack of insight and innovation, especially in the loss function compared to InfoGAN.\n",
      "Lack of explanation and supporting evidence for the claimed efficiency of the proposed model.\n",
      "Limited evaluation—experiments only on CIFAR-10 and synthetic data; lacks tests on larger, high-resolution image datasets.\n",
      "The main drawback is lack of clarity; the paper is poorly written and the presented model is not clearly motivated.\n",
      "Unclear and confusing explanation of discriminator network inputs, especially regarding temporal consistency modeling.\n",
      "Originality of frame concatenation is limited, as it's standard in other domains.\n",
      "Rather complex overall objective.\n",
      "Concerns about generalization to other tasks and inadequate justification for the ping-pong loss.\n",
      "The authors change the problem to sign gradient times its norm, which changes the flow in continuous time.\n",
      "The contribution may not be sufficient; the work is seen as incremental.\n",
      "The insights claimed in the paper are not especially novel; most results are already known.\n",
      "Experiments do not compare with any mutual information related baselines (e.g., VIB), which is critical to show benefit of discrete codes.\n",
      "No reference or comparison to Kim et al.'s Bayesian Model-Agnostic Meta-Learning, a significant related work.\n",
      "The generalisation bound appears over-reaching and insufficiently justified; core claims lack solid theoretical support.\n",
      "Lack of analysis on the robustness of explanations to perturbations that do not affect classifier predictions.\n",
      "The new ‘concept accuracy’ metric is unvalidated and poorly correlated with interpretability, making the results unconvincing.\n",
      "Novelty concern: unsupervised concept extraction for interpretability was previously introduced, not properly discussed or contrasted.\n",
      "The proposed method does not clearly outperform alternatives such as the KS-test in empirical results.\n",
      "Doubtful that the typicality set works for OOD detection in small n (small data) regime.\n",
      "The approach lacks robustness to model properties and model misspecification, not addressed in experiments.\n",
      "Comparison to previous related works and variants is insufficient.\n",
      "No standard deviation reported for results; possible high variance in outcomes when using low bit quantization.\n",
      "The proposed method lacks novelty and results are not convincing enough.\n",
      "The novelty and contribution are very limited as the generalization bounds are direct applications of known results.\n",
      "The contribution is not novel; theoretical results follow directly from standard tools without introducing new ideas.\n",
      "Mathematical writing is unclear and not up to publication standard, making technical results hard to interpret.\n",
      "The paper is very hard to read; difficult to understand the goals and sections.\n",
      "Experimental evaluation section requires improvements.\n",
      "The paper does not convincingly demonstrate the usefulness of the method for generating justifications to users.\n",
      "Lack of strong baselines like BERT; experiment results not convincing.\n",
      "The experiment design has many flaws.\n",
      "Very misleading claims regarding establishing a new state of the art due to weak comparisons with inferior baselines.\n",
      "Unclear justification for assuming the null class centroid is close to origin due to batch normalization.\n",
      "The contribution seems insignificant based on the experiments, especially with missing reordering of the 2-way model results.\n",
      "Fairness of comparison to previous algorithms in the single-class regime; choice and representativeness of negative examples.\n",
      "If sample size is constant, theorem conditions may not hold, narrowing practical application domains.\n",
      "No theoretical analysis of the key quantity t, which is central to the algorithm's convergence.\n",
      "Experimental results lack run-time plots to show computational advantage; only loss curves are presented.\n",
      "The paper lacks technical novelty as convergence results follow standard inexact/noisy gradient methods under strong assumptions.\n",
      "The duality gap is only an upper bound and not a proper metric for GAN training.\n",
      "Unclear motivation and confusing paper structure, making theoretical contributions hard to appreciate.\n",
      "Totally inadequate experimental validation undermines the paper.\n",
      "No significant theoretical innovation; approach is very similar to prior work by Kidzinski & Hastie, 2018.\n",
      "The method lacks novelty and does not adequately acknowledge prior work on modeling treatment effects in disease progression.\n",
      "Not enough novelty; method only minimally extends previous work with one extra parameter.\n",
      "An important error in the main result and other issues raise doubts on the theoretical contribution of the paper.\n",
      "The main weakness is the paper's presentation, starting from the abstract and throughout.\n",
      "Gap between the formal treatment (Lemma 3.2) and the rather ad hoc practical algorithm depending on two tuning parameters.\n",
      "Unclear why the proposed method offers superior performance over existing methods for simple models; theoretical justification is insufficient.\n",
      "Experimental analysis is insufficient—relies on a single dataset and single constraint, so claims aren't fully supported.\n",
      "The algorithm description is unclear, especially how constraint satisfaction is achieved during training.\n",
      "The paper does not motivate or justify the need for the proposed constraints, making the method lack merit.\n",
      "Authors do not clearly explain which biologically implausible aspects of backpropagation are addressed; weight-transport issue remains unresolved.\n",
      "The scalability to larger tasks (e.g. ImageNet, more output classes) is unproven and must be studied.\n",
      "Insufficient neuroscientific evidence for the assumptions about feedback connections in real neurons.\n",
      "Theoretical results are based on a very simple and special task, so cannot be considered general for all MDP cases.\n",
      "Both the motivation and the suggestion regarding model-based RL are not convincing.\n",
      "Unclear how often the constructed MDPs, where optimal Q-functions are hard to express, arise in practice.\n",
      "The results are not convincing enough for a conference paper.\n",
      "The novelty may be incremental; unclear significance vs. VEEGAN and AAE.\n",
      "Limited novelty over VEEGAN and negligible empirical improvement make the method unconvincing.\n",
      "Approach may require semantically aligned datasets and may not generalize to unsupervised settings.\n",
      "Lack of sufficient and appealing visual results, and weak style diversity in generated images.\n",
      "The paper lacks intuition and interesting discussion about why the pretraining phase leads to better performance.\n",
      "The experiments are not convincing, e.g. only pruning FC5 and FC6 layers in AlexNet on ImageNet dataset.\n",
      "The paper is not easy to follow/read; organization and explanations are confusing.\n",
      "Substantial lack of theoretical justifications for the proposed method.\n",
      "TVmax achieves only a little improvement over the baseline (softmax) on automated metrics.\n",
      "Lack of clarity in the motivation for the proposed methods and insufficient explanation of why they improve results.\n",
      "Motivation for sparse attention is not convincing unless for long sequences.\n",
      "It is not clear if combining neural network models for both forward and inverse problems provides real benefits.\n",
      "Insufficient experimental comparisons and lack of theoretical support for the method.\n",
      "Lack of convergence tests and error analysis undermines confidence in the proposed neural network PDE solver.\n",
      "Experiments were not conducted with enough seeds, so statistical significance and claims are not sufficiently supported.\n",
      "The proposed approach is simplistic in how they treat surprise, and therefore illustrative, but probably not practical.\n",
      "Lack of theoretical results (e.g., convergence/asymptotic properties) for policy and density functions.\n",
      "Technical contribution is lean; neither the multi-agent nor hierarchical aspects are novel and the combination is straightforward.\n",
      "Limited evaluation and novelty, with insufficient comparative experiments to prior hierarchical MARL approaches.\n",
      "The paper lacks a clear main argument and does not compare against relevant baselines for its individual contributions.\n",
      "No real application was presented; real-world motivation and examples are lacking.\n",
      "Lack of details on using gradient-based exploration to guide the adversary and insufficient black-box vs gradient-based comparison.\n",
      "The paper is not well motivated or sufficiently distinguished from existing robust MDP and multi-agent RL literature.\n",
      "The adversarial examples are highly noticeable to human ears, undermining their intended imperceptibility.\n",
      "The paper does not sufficiently explain or formalize what makes the method perceptually more appealing.\n",
      "There is no discussion and comparison in the experiments regarding the claim that the technique is less time-consuming.\n",
      "Does not compare to a strong enough baseline (one cycle), making experimental support unconvincing.\n",
      "The method is only semi-automatic since seed learning rate and explore phase need manual tuning for each task.\n",
      "No new method is introduced; only sampling of fixed neighbors is used for constant-time approximation.\n",
      "The title and abstract are misleading; paper doesn't introduce a new sampling strategy, only theoretical bounds for existing ones.\n",
      "The bound has exponential dependence on the number of layers, making it less practical for real-world applications.\n",
      "The name 'Mutual Information Machine (MIM)' is inappropriate for the proposed approach.\n",
      "The motivation for L_MIM is obscure and uncompelling, and experiments are weak.\n",
      "Empirical results are weakly justified because of weak baselines and lack of comparison to relevant existing methods.\n",
      "The paper lacks rigorous theoretical analysis and strong empirical evidence, making it unfit for this conference.\n",
      "The literature review on previous work for continuous-time, discrete events prediction is not thorough enough.\n",
      "The technical challenges addressed have been extensively studied in related literature, which the paper does not sufficiently reference or contextualize.\n",
      "The method is not novel, merely an extension of known ideas.\n",
      "Theorem 2 and its proof are plagiarized and presented as the authors' own work.\n",
      "The paper omits discussion and comparison with a very related prior work (Li et al., 2019).\n",
      "It might be useful to test the approach with several different architectures.\n",
      "Weak experiment setting: attacks generated on undefended model may overstate robustness due to possible gradient masking.\n",
      "Lack of originality and significance; most results are known and not sufficiently distinguished from prior work.\n",
      "No guarantees are provided for the proposed techniques.\n",
      "Assumes deep reader familiarity with CCA, making it inaccessible for a general audience.\n",
      "The approach is a trivial extension of previous work (DeepCCA) with limited novelty and insufficient motivation.\n",
      "The architecture is essentially a traditional end-to-end seq2seq model with no fundamental novelty.\n",
      "The model outputs suffer from redundancy and referential clarity issues, repeating names (as in Figure 1).\n",
      "The reviewer does not believe the extra recoder component meaningfully improves summary quality; experimental results do not support its effectiveness.\n",
      "Averaging across only 4 seeds is too low given RL variance; shown by high std values.\n",
      "No results on policy search algorithms such as PILCO in Section 4.5; not represented in Table 4.\n",
      "The evaluation is not thorough enough, with missing tabular environments and weak writing.\n",
      "The paper lacks justification and clarity for its proposed framework and experiments do not support the main claim.\n",
      "Experiments are limited to D=3 and N=10; unclear scalability to higher dimensions.\n",
      "The proposed method is not well justified and lacks thorough analysis or convincing evidence of performance.\n",
      "Unconvincing and inadequate hyperparameter sensitivity analysis; experiment limited in scope and duration.\n",
      "The algorithm is not well presented; it's unclear how hyperparameters are chosen and overhead details are missing.\n",
      "The overall idea is not novel; no clear evidence it's better than existing approaches such as Nvidia’s apex.\n",
      "The paper is fairly incremental and not very interesting to the ICLR community.\n",
      "The proof of relaxing the Polynomial functional constraint in Theorem 4.8 is not entirely clear.\n",
      "Proofs of Propositions 4.3 and 4.7 should be covered/sketched in greater detail in the main text.\n",
      "The improvement over existing DropOut variants is small and practical gains are limited.\n",
      "Improvement over existing methods is marginal and within error margins; no evidence RotationOut reduces co-adaptation or that this matters.\n",
      "Doubt that RotationOut actually reduces feature correlation due to properties of rotation matrices.\n",
      "Use of the Office dataset, which has label pollution and too few samples per class.\n",
      "The paper misses significant recent work showing that invariant representations can hurt generalization when label distributions differ.\n",
      "Lack of comparison to methods using conditional/adaptive batch normalization.\n",
      "Many similar approaches are missing in related work; novelty is limited.\n",
      "Inadequate experimental evaluation and no comparison to relevant baselines or exploration of design choices.\n",
      "Justification for the proposed method is largely missing; unclear why it is a good or optimal idea.\n",
      "The novelty of the method appears limited, mainly adding noise to predictions rather than true algorithmic innovation.\n",
      "The use of label smoothing (despite acknowledging its drawbacks) is insufficiently justified.\n",
      "The description of the task, problem setup, and mathematical notation is unclear and hard to follow.\n",
      "Reported results are not convincing and inconsistent with prior works, casting doubt on any real accuracy improvement.\n",
      "Concerns over using VAE to model ratings; VAEs need lots of data, so model may not learn well.\n",
      "The studied problem is well-studied and more accurate algorithms exist than those used in experiments.\n",
      "No significant limitation or negative aspect identified.\n",
      "The procedure is quite unclear and could not be reimplemented from reading the manuscript.\n",
      "The paper does not solve a sufficiently clear problem and the motivation is confusing, especially regarding dropout analysis.\n",
      "The method lacks novelty and is just a simple combination of dictionary learning and classification.\n",
      "Unclear positioning compared to supervised dictionary learning; framing as a missing values problem seen as inaccurate.\n",
      "The main mathematical result is not validated or illustrated numerically, despite claims that it could be.\n",
      "Improvement over BERT baseline is moderate, especially in head-to-head GLUE results.\n",
      "Experiments show mixed results and do not adequately justify the paper's claims.\n",
      "Results lack statistical testing; claimed improvements may just be random fluctuations, so conclusions are not supported.\n",
      "Needs an upper bound baseline showing accuracy if the region is seen (without context needed).\n",
      "The study's main claims are overstated and not well validated by the experimental results.\n",
      "Not of general interest to ICLR audience; better suited for cognitive science venues.\n",
      "The paper lacks novelty and does not provide a strong contribution beyond existing work.\n",
      "The extension of Neural ODEs to graphs is too straightforward and lacks sufficient novelty.\n",
      "The novelty seems somewhat incremental.\n",
      "The improvement of AHash over OOPH is unobtrusive and not surprising.\n",
      "The theory (and its correctness) is difficult to be justified.\n",
      "Lack of proof details; Proof 3.1 is hard to follow.\n",
      "The manuscript has a limited impact; experiments provide minimal insight; advantage of AHash is minor and limited in scope.\n",
      "The set of test-time perturbations may not be representative/exhaustive, and a clear protocol for future use is lacking.\n",
      "The approach is just an extension of prior benchmarks/applications ([1] and [2]) with no novel ideas for object detection.\n",
      "Unclear novelty compared to Geirhos et al 2019; lacks detection-specific contributions.\n",
      "The main theorem is an extension of existing methods, so theoretical novelty is limited.\n",
      "The evaluation of the proposed method is not complete; key baselines and datasets are missing.\n",
      "The algorithm minimizes an upper bound, not directly the target loss; benefit is not guaranteed and may be limited.\n",
      "Results are unsurprising; methods applied as-is from existing work without modification or new contribution.\n",
      "The novelty of the work is weak; similar weight reduction methods have been previously explored, and the contributions are not well tied together.\n",
      "Uninspired combination of pre-existing ideas with weak and inconclusive experimental results.\n",
      "The technical details of the method are not sufficiently explained or centralized, especially how statistics are computed and handled.\n",
      "The stability of the method remains unclear, which is important for real applications.\n",
      "The assumption that weights, gradients, and their product are Gaussian-distributed is unnatural and unjustified in real training.\n",
      "Insufficient comparison to prior work; did not use datasets and setups previously used to evaluate SAT solvers.\n",
      "The modular 4-SAT experiment is a bit underwhelming.\n",
      "It is unclear how the PDP architecture solves SAT problems, especially how soft assignments relate to a final decision and discrete outputs.\n",
      "Lack of novelty—most components (architecture and objectives) are already proposed in prior works.\n",
      "Main results are based on comparisons between different codebases, making conclusions unreliable.\n",
      "ARAC seeks diversity in fitness space, which is weaker than behavior/outcome space used in QD and NS.\n",
      "Paper lacks novelty and further evaluation to be suitable for ICLR.\n",
      "The paper does not offer possible explanations for the unexpected results.\n",
      "The paper lacks experimental comparison to back-translation, making it hard to judge the proposed approach's contribution.\n",
      "Experiments do not convincingly show ZO-MIN-MAX is better than other zeroth-order methods.\n",
      "It is not trivial to understand the presented convergence results and their practical implications (if any).\n",
      "The approach lacks practical interest because few ML problems lack gradients; thus, ZO optimization is not interesting.\n",
      "Too many hyperparameters which will dramatically increase with depth, making the method hard to use.\n",
      "The paper was already accepted at a Neuro AI workshop, violating double-blind review rules.\n",
      "Due to progressive freezing, network eventually reaches full capacity, making it difficult or impossible to learn new tasks.\n",
      "No quantitative evidence provided for computational efficiency; proposed approach underperforms in accuracy.\n",
      "The method only combines existing techniques and lacks novelty.\n",
      "The method does not exhibit state-of-the-art performance as claimed; results are mixed and not compelling.\n",
      "Serious clarity issues with the writing and explanation of the algorithm.\n",
      "The method is not well justified by theory or practice; experiments are weak and do not support effectiveness.\n",
      "The paper is based on, and perpetrates, a number of fundamental misconceptions about adversarial examples.\n",
      "The topic does not fit ICLR; most researchers won't be interested or understand it.\n",
      "The experimental evaluation is insufficient, lacking details about TensorFlow comparison and missing ablation studies.\n",
      "Assumes input distribution is known, which may not hold during training or adaptation, causing issues for auxiliary LUT.\n",
      "Lack of strong baselines and incremental contribution; not a sufficient advance for publication.\n",
      "Lack of proper baselines for comparison (e.g., word2vec, GloVe, Bag of Words, TF-IDF).\n",
      "The computational efficiency of conventional NLP models is well known to researchers.\n",
      "The major problem is only comparing with outdated methods, so effectiveness is not well verified.\n",
      "iSparse's performance is not as impressive as the Lottery Ticket Hypothesis in terms of weight removal and performance retention.\n",
      "Limited technical novelty; proposed method is heuristic and lacks justification for design choices.\n",
      "No comprehensive comparison with existing zero-shot task adaptation methods; results only on synthetic data limit conclusions.\n",
      "Experimental results use only small synthetic data and lack comparison to baselines, making practical utility unclear.\n",
      "Lack of discussion and comparison with state-of-the-art meta-learning baselines.\n",
      "Low novelty compared to recent literature.\n",
      "Analysis of weight and activation quantization are separated and may not hold when both are quantized simultaneously.\n",
      "The proposed algorithms lack novelty and offer only marginal improvements compared to existing methods like DoReFa and PACT.\n",
      "Uncertainty about the suitability of initialization techniques and training method for tensor train embedding.\n",
      "Paper does not compare with other compression methods like WEST or projection layers.\n",
      "Incomplete and potentially misleading comparison between Tensor Train (TT) and Tensor Ring (TR), lacking fair and exhaustive experiments.\n",
      "The novelty is very limited.\n",
      "The theoretical discussion imposes too strong assumptions that might make it less interesting in practice.\n",
      "The paper is poorly written and lacks a clear, significant contribution.\n",
      "Figure fonts are too small throughout the paper.\n",
      "Not a research paper; lacks significant research contribution.\n",
      "The paper does not address or mention verification of correctness of the CUDA port.\n",
      "The significance and novelty of the main contribution are unclear; modifications appear previously used.\n",
      "Reproducibility is difficult as source code and some datasets are not provided or are not public.\n",
      "Section 3.2 is hard to parse due to unclear notation and lack of organization in Eq. 14.\n",
      "Unclear what is contributed beyond closely related existing works; lacks novelty over prior studies.\n",
      "The paper does not sufficiently explore the implications of its bias modelling or compare to simpler, established methods.\n",
      "Wrong citation format: use of citet instead of citep. Correct for all citations.\n",
      "Missing context and references to relevant prior work and similar issues addressed in existing literature.\n",
      "The main result is only proven for linear models, not deep learning.\n",
      "The authors ignored all the finite-sample bound results in related work, particularly those achieving minimax rates.\n",
      "Central limit theorem may not apply since data are not i.i.d., making convergence proofs potentially invalid.\n",
      "The problem setting does not model typical or difficult RL exploration problems, especially due to Assumption 3.\n",
      "The method only adds task-specific heuristic human priors, which are similar to common regularization and may be limited.\n",
      "The paper lacks evidence that expected gradients performs better and has unclear focus between methods.\n",
      "Unclear how much progress over 'Axiomatic Attribution for Deep Networks' (Sundararajan et al. 2017); experiments seem similar.\n",
      "The paper is lacking content, slightly below the amount in average ICLR papers.\n",
      "Unclear if the proposed method is competitive with more recent state-of-the-art solutions to Exposure Bias.\n",
      "The reviewer is not excited about the technical contribution of the work.\n",
      "No discussion or comparison with existing LSTM variants, and absence of a related work section.\n",
      "Theoretical and empirical evidence are not strong enough to convince practitioners to use EINS over LSTM/GRU.\n",
      "The analysis has many flaws and only minor connection to performance of RNNs, with marginal empirical success.\n",
      "Lack of references to related work, especially missing key prior work.\n",
      "The paper feels sparse in contributions; lacks sufficient discussion/analysis of parameterizations or treatment of more binary-attribute analogy types.\n",
      "Lack of direct evaluation of the reflection hypothesis by integrating loss (16) into training and assessing performance changes.\n",
      "Uncertainty about ACClip's generalizability to architectures beyond Bert.\n",
      "The paper's experiments lack rigorous statistical testing compared to prior work, making findings less convincing.\n",
      "Experiments are not compelling due to no careful hyperparameter search.\n",
      "The paper does not compare their method to prior 3D representation work or state-of-the-art 1D/2D models.\n",
      "Limited methodological novelty; methods are existing with little or no modification for the new domain.\n",
      "The method uses vanilla 3D convolutions; it does not account for SO(3) rotation invariance.\n",
      "The network structure does not convincingly address the complex distortions present in underwater images.\n",
      "The technical contribution is limited and the work only provides an incremental improvement (1%) on an easy classification task.\n",
      "Unclear motivation for proposing a complex method for an apparently simple binary classification task.\n",
      "Experimental results were not easily comparable to prior work, so claims may not be well-supported.\n",
      "Empirical results are puzzling and inconsistent with previous benchmark results (e.g., Sun et al 2019, DistilBERT).\n",
      "The experimental description lacks detail, making it hard to assess fairness and reproducibility.\n",
      "No theoretical contribution; not suitable for ICLR, better fitted for an application-focused conference.\n",
      "The scope is too narrow for the ML community.\n",
      "The method is computationally costly and sacrifices network flexibility, being a brute-force application without practical remedies.\n",
      "Would the authors provide slightly more details about the design of networks?\n",
      "Claim that the obtained map is the optimal transport map is likely incorrect due to limited function class and weak optimality conditions.\n",
      "The proposed neural network model is not a sufficient contribution and lacks comparison to existing BO methods.\n",
      "The paper's contribution is limited and lacks a clear explanation for why the proposed method works.\n",
      "The novelty and contribution of the paper are limited.\n",
      "The improvement seems quite marginal; proposed method improved baseline by only 0.9% and 2.2% accuracy.\n",
      "Reviewer is not familiar with quantization methods and cannot judge coverage of related work and choice of baselines.\n",
      "Using total number of bits as equivalent measure between models doesn't account for actual inference latency or efficiency on hardware.\n",
      "Key assumptions lack intuition/discussion and are not concretely illustrated with examples or empirical application.\n",
      "The main novelty and practical usefulness of the considered class is unclear and not easy to see.\n",
      "Unclear how restrictive the assumptions (especially Definition 1.4) are; doubts about their meaningfulness and experimental satisfaction.\n",
      "Lack of use of more sophisticated multi-task learning methods like MetaLearning (MAML), just using task averaging.\n",
      "Generalization is unconvincing because both tasks use the same environments and features.\n",
      "Lack of clarity and explanation of novelty; Eq. (3) not sufficiently motivated or detailed.\n",
      "The contribution is fairly small, mainly engineering, offering little insight and limited experimental support.\n",
      "No novelty beyond combining existing state-of-the-art models; lacks original contribution.\n",
      "Does not convincingly show improvement in the quality of disentangled representations; separate training may impede this.\n",
      "Lack of novelty in the proposed method.\n",
      "The paper is interesting from an engineering perspective but lacks novelty.\n",
      "Strong risk of overfitting; unclear if model generalizes beyond training data.\n",
      "More results on ImageNet would be better to verify the proposed RobustNorm method.\n",
      "Results seem unconvincing because only Cifar10 and Cifar100 (similar datasets) were tested.\n",
      "No further investigation on why running average causes vulnerability; insufficient explanation on distribution changes.\n",
      "The paper needs more thorough experimental validation to better demonstrate its effectiveness.\n",
      "Lack of reasoning and explanation about each component, especially local and global motion composition.\n",
      "The paper does not compare sufficiently to established methods and literature, making results preliminary and not well-situated.\n",
      "Method only tested on 1D cases with a single dataset and equation, limiting generalizability.\n",
      "The paper lacks algorithmic breakthrough or new invention.\n",
      "The motivation and intention of the work are unclear, making it difficult to understand and summarize.\n",
      "The contribution on the theoretical or methodological side seems a bit weak.\n",
      "The effect of regularizing salient features is marginal; proposed method gives similar or worse results than existing methods.\n",
      "The paper lacks novelty since its main contributions are just adding two loss terms, not fully resolving existing issues.\n",
      "The paper's main result may not be surprising since the number of representable functions might match the training set, diminishing novelty.\n",
      "No empirical evaluations to support the theoretical analysis.\n",
      "Lack of novel insight and weak connection to actual deep learning practice, datasets, and models.\n",
      "Paper is about a very specialized area and may not fit this venue.\n",
      "Results are only presented on one small dataset, limiting the claims and generalizability.\n",
      "Insufficient contributions; lacks convincing reasons for model design and novelty.\n",
      "The population size for the evolutionary component is far too small to substantiate the integration claim; no evidence that EC contributes beyond noise.\n",
      "Unclear reasoning behind sharing replay-buffer for the same agent in different teams; deeper discussion needed.\n",
      "Insufficient differentiation and comparison with PBT-MARL; unclear advantages over prior work.\n",
      "The paper does not propose any novel method, which limits its suitability for the conference.\n",
      "The results are expected and no effective methods are proposed to improve the results.\n",
      "The 'accuracy drop' metric is misleading and unjustified, as it redefines accuracy in a non-standard, incomparable way.\n",
      "Evaluating both online and target networks requires extra environment interaction, severely hurting sample complexity.\n",
      "Claims about stability guarantees and overall performance are not supported by theory and experiments.\n",
      "Variance reduction is not a direct measurement of policy stability or robustness; lacking stability/robustness evidence or discussion.\n",
      "The analysis in Section 4.3 is unclear due to compressed description and tiny graphs.\n",
      "No results are given at test time regarding memory and processing time reduction.\n",
      "None explicitly mentioned.\n",
      "The derivation of the teacher's update rule is incorrect, with mismatched matrix/vector shapes in equations.\n",
      "Lack of clear explanation for why the method works and how it prevents overfitting.\n",
      "The motivation and necessity for using two networks (teacher and student) is unclear and not convincingly justified.\n",
      "Insufficient discussion on why the model underperforms for Russian in word-translation retrieval.\n",
      "The paper brings nothing novel, is largely incremental, and its findings are already well-known and trivial.\n",
      "The paper does not clearly outperform or have advantages over prior works, making its contribution unclear.\n",
      "Unclear why adaptation is needed if CFR/Nash already suffices; lack of analysis undermines the contribution.\n",
      "The rigour and detail in documenting the empirical evaluation are not of the standard expected for publication.\n",
      "Scalability is not convincingly demonstrated; GRAM was only tested up to 500 nodes, which does not confirm scalability claims.\n",
      "The main motivation is scalability, but the proposed method does not convincingly address scalability; there is a disconnect.\n",
      "Experimental results are insufficient to support key claims, especially regarding scalability—no thorough runtime experiments or large graph results.\n",
      "Limited novelty, mainly combining existing techniques rather than offering innovation in model-based modeling.\n",
      "The presentation is unclear and informal, with many undefined terms and incorrect or missing steps.\n",
      "The motivation for the method and justification for adding entropy regularization are unclear.\n",
      "Confusing problem-framing and lack of robust baselines undermine the paper's suitability for publication.\n",
      "Unclear and possibly invalid objective function undermines trust in method’s correctness and effectiveness.\n",
      "The main contribution is not particularly novel and is similar to existing work.\n",
      "The paper does not discuss what to do with the dropped samples (e.g., dropping up to 80%).\n",
      "The search space is much too small compared to typical NAS studies, limiting the paper's impact and generalizability.\n",
      "Paper is entirely empirical with little or no justification and uses only a single architecture and dataset.\n",
      "Very limited scope; only 1 dataset and 64 models, making broad conclusions about weight sharing in NAS hard.\n",
      "Lack of discussion about CoreSets theory and evaluation of heuristics.\n",
      "The technical contribution of the paper is rather small, and also the depth of analysis could be improved.\n",
      "Lack of GAN implementation details and hyperparameters used for training.\n",
      "The method is not convincingly interpretable as claimed, with insufficient evidence supporting interpretability.\n",
      "Unclear how the work addresses why DIP works well and the essentiality of convolution for image reconstruction.\n",
      "It is not clear how to design corresponding efficient structures for different low-level problems; application ability is not clear.\n",
      "The writing is unclear, leading to confusion about output sequence and model explanation.\n",
      "The paper lacks novelty and technical contribution, as it mainly combines existing techniques without clear unsupervised learning value.\n",
      "The advantage of using self-attention for neighborhood aggregation is not clearly discussed or distinguished from existing methods.\n",
      "The paper blurs the distinction between prior work and new contribution; what is novel is not clearly stated.\n",
      "Paper exceeds page limit and figures are unreadable due to squeezing.\n",
      "There is no definition of KL function in the main content.\n",
      "The method is not clearly explained or rigorously formulated, making it hard to learn from or use.\n",
      "Insufficient motivation and explanation for key algorithmic decisions (e.g., GT-Layers, bottom/up blocks, accuracy criteria).\n",
      "Contribution is incremental and novelty is limited; mainly derives from adding adaptive criteria to an existing algorithm.\n",
      "Missing important details of learning the surrogate loss, especially function g and composition function h.\n",
      "Theoretical result cited is invalid for the proposal; assumptions (e.g., unique argmin) are violated for neural networks.\n",
      "No theoretical guarantee for the proposed method across learning problems.\n",
      "In the absence of any theoretical justification, the empirical results are not sufficiently convincing for ICLR.\n",
      "Unclear motivation and lack of theoretical justification for the proposed algorithm.\n",
      "Claims (e.g. applicability to all policy gradient methods, noise removal) are unconvincing due to insufficient empirical evidence.\n",
      "No clear description of the models makes it difficult to understand their structure.\n",
      "The approach is hardly novel and not well motivated: predict transitions then labels is not new or supported.\n",
      "The paper lacks precision and formal descriptions of the parser, making reproduction and understanding difficult.\n",
      "The evaluation is limited: only a single dataset and basic baseline are used.\n",
      "The paper lacks enough novel machine learning contributions for acceptance.\n",
      "The proposed method seems straightforward, implying limited novelty.\n",
      "Lacks sufficient analysis and experimental results to demonstrate superiority over existing IRL frameworks.\n",
      "One of the main limitations of this work seems to be that the CMDPM has to be known.\n",
      "The paper fails to sufficiently motivate contextual IRL as a useful specialization of inverse reinforcement learning.\n",
      "Experimental evaluation is not consistent or complete enough; core motivation for using INNs is unclear.\n",
      "The method is only evaluated on incremental class tasks, not on other continual learning scenarios (e.g., incremental domain/task).\n",
      "The incremental classification setting is unrealistic, as it assumes task boundaries and uses fixed, pretrained feature extractors, thus avoiding rather than solving the continual learning problem.\n",
      "Bad writing; reviewer can barely understand the paper’s content and arguments.\n",
      "Paper lacks important methodological details, experimental results, and state-of-the-art comparison, making it hard to evaluate.\n",
      "Serious language problems make the paper incoherent and impossible to understand.\n",
      "The paper lacks results on standard benchmark metrics (e.g., BLEU, ROGUE), thus failing to show model effectiveness in generating relevant responses.\n",
      "Technical formulation and theoretical results are not presented rigorously and important details are missing.\n",
      "The paper is poorly presented, with many unclear parts and confusing explanations.\n",
      "The paper does not present a novel solution method or empirically compare existing methods.\n",
      "Unclear if the deadlock phenomenon occurs in more complex, common benchmark environments.\n",
      "Experiments conducted cannot support the conclusions in this paper.\n",
      "The new model may output the same molecule multiple times and favors straight-chain molecules.\n",
      "Experimental results do not improve over state of the art; reduced parameters lead to much lower performance.\n",
      "The paper is poorly written and the model is explained very poorly, making it hard to understand and reproduce.\n",
      "ESN is not suitable for existing hardware; it relies on unrealistic assumptions.\n",
      "Lack of novelty in the proposed metric; based on unrealistic ideal hardware assumptions.\n",
      "The paper is not very clear, and the structure is somehow confusing.\n",
      "Empirical results are not strong enough to support CR, especially since similar architectures have been previously studied.\n",
      "A fatal error in the theoretical derivation renders all claims in the paper invalid.\n",
      "Lacks discussion of potential impact and preliminary empirical studies.\n",
      "Method is not properly justified for the chosen environments and should be evaluated with truly sparse rewards.\n",
      "Unclear if taking the min surprise across the ensemble is a good exploration strategy; motivation is not fully convincing.\n",
      "There is a significant gap from theory to practice; theory assumes a known world model and stationary reward, but practice does not.\n",
      "CBN increases computational complexity and memory consumption compared to BN; unclear if gains are due to this cost.\n",
      "No clear practical advantage of CBN over BN for big batches or GN for small batches, even with batch size = 1.\n",
      "Approach does not provide significant benefits over prior methods (e.g., Group Normalization) in performance or complexity.\n",
      "Experimental results are minimal and not convincing.\n",
      "The contribution is rather limited/unclear; main results are standard with limited novelty.\n",
      "Section 2.3, called Main Result, is not that novel; results are already known for such RMDPs.\n",
      "Efficiency of the proposed model is not studied; missing convergence and limited time performance experiments.\n",
      "Key related references are missing and the proposed method does not outperform the strongest baselines.\n",
      "The work lacks novelty due to similarity with VIBNet; max-entropy term derivation is unclear and needs major revision.\n",
      "Applying the path-based encoding to a new task without significant adaptation or analysis is not technically novel.\n",
      "The usefulness and practicality of the proposed task for relevant higher-level AI4code tasks is unclear.\n",
      "The robustness and sufficiency of AST path inputs should be further explored.\n",
      "The presentation of the theory is unclear and not well-motivated; the connection to CycleGAN is unconvincing.\n",
      "The paper needs an additional pass at grammatical clarity.\n",
      "The reviewer had a hard time understanding key elements and justification for the proposed method.\n",
      "The explanation of the method is not very precise and could be greatly improved.\n",
      "The interpretability of the sparse extracted feature vector is unconvincing and not clearly better than other methods.\n",
      "The approach of sparsifying the input may yield outlying, unnatural data, bringing unclear or biased explanations.\n",
      "The comparison of model and human performance may be unfair since human results are reported on original, unfiltered tasks.\n",
      "The tasks, class definitions, and what is tested are not explained in enough depth, making interpretation unclear.\n",
      "It's hard to judge task difficulty because few tasks have human evaluation scores; missing human baselines hinder comparison.\n",
      "The GAN loss does not significantly improve performance compared to the baseline, questioning its added value.\n",
      "Components are taken straight from existing work and just combined, lacking novel integration or system-specific insights.\n",
      "Limited technical innovations and experimental results.\n",
      "3-level hierarchy in StarCraft does not work well; high level may be limited by CNN resolution.\n",
      "It is unclear why off-policy samples from smaller action spaces are useful for agents with larger action spaces.\n",
      "Unclear and possibly inappropriate comparison with the M&M baseline; unclear adaptation to Q-learning undermines evaluation.\n",
      "The paper contains many typos that make sometimes the reading difficult.\n",
      "The paper is very unpolished, hard to follow, and not ready for publication.\n",
      "The proposed theoretical upper bound is not reliable because the true labeling functions do not belong to the hypothesis space H.\n",
      "Contributions are not sufficient for ICLR; better suited as a workshop paper.\n",
      "Paper lacks sufficient novelty, as it stops at image registration, an existing model.\n",
      "Ideas are not novel enough or thoroughly demonstrated to justify acceptance.\n",
      "The method and experiments are not convincing and contribution is limited.\n",
      "The paper lacks clarity in motivation, explanation, and definition of key concepts like 'non-robust discriminator'.\n",
      "Robustifying the discriminator is not new; need more clarification on the novelty of the method.\n",
      "The application is a bit limited.\n",
      "The motivation section is not detailed enough, making the paper harder to understand.\n",
      "The presentation of the paper is very weak and unclear, with many imprecise and vague statements.\n",
      "Experimental results and claims are unconvincing and not well-justified, especially regarding scalability and performance.\n",
      "The paper fails to compare the different recovery algorithms and lacks analysis of their respective advantages, disadvantages, and performance differences.\n",
      "Unclear novelty and significance; unclear what technical challenges or new proof techniques are introduced.\n",
      "The approach is considered merely a 'clever' version of gradient masking, failing to provide true robustness.\n",
      "The mathematical description of the main problem and proposed model is not clear.\n",
      "Experiments are limited to small datasets and shallow neural nets, reducing broad interest.\n",
      "Experiments are not sufficient to validate the model's advantage; datasets are small and baselines are weak.\n",
      "Possible limitation if, after training, a single ensemble member dominates throughout the task.\n",
      "TDW favors deterministic policies over uncertain ones, which may exclude optimal policies in stochastic settings.\n",
      "Paper unfairly compares ensembles with single learners using less data, leading to misleading conclusions.\n",
      "Unclear if the similarity matrix S already solves clustering; necessity of the method is not justified.\n",
      "The technical novelty is only sufficient and not highly innovative.\n",
      "No discussion or experimental comparison with spectral clustering and kernel spectral clustering methods, despite relevance.\n",
      "The novelty is limited; it's unclear how the model solves issues in the motivational examples (Figure 1).\n",
      "Doubtful experiment design and reporting, especially copying results from prior work without ensuring comparable settings.\n",
      "The novelty of the proposed method is marginal.\n"
     ]
    }
   ],
   "source": [
    "for reason in dfr['main_limitation'].unique():\n",
    "    print(reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6233b",
   "metadata": {},
   "source": [
    "### Clusters, Level zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d3f7a4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessment of Novelty and Rigor through Comparison with Prior Work\n",
      "Robustness Analysis of Deep Neural Networks against Adversarial Attacks\n",
      "Critiques of Theoretical Paper Clarity and Experimental Design\n",
      "Novel Approaches and Incremental Advances in Community Detection Methods\n",
      "Assessment of Novelty and Significance in Relation to Prior Work\n",
      "Validated Mathematical Results and Proof-Based Safety Analyses\n",
      "Assumptions and Rationales in Model Correctness and Recommendations\n",
      "Continuous Space Modeling and Policy Correlation Analysis\n",
      "Empirical Evaluation of Tensor Completion Methods\n",
      "Model Comparisons and Clarity Issues in MNIST Classification\n",
      "Critique of Insufficient Experimental Validation\n",
      "Evaluation and Comparison of Hyperparameter Methods: Clarity, Explanations, and Batch Results\n",
      "Technical Contributions and Comparative Analysis in Academic Art Research\n",
      "Assessment of Methodological Novelty and Technical Extensions\n",
      "Evaluation Limitations and Weak Baseline Analysis\n",
      "Experimental Evaluation of Model Efficiency and Performance in Limited Space and Time\n",
      "Theoretical Analysis and Empirical Evaluation of Algorithmic Convergence and Bounds\n",
      "Differential Privacy Theorems and Proof Techniques\n",
      "Evaluating GCNs Against Label-based Adversarial Attacks: Quantitative Analyses and Defense Efforts\n",
      "Robust and Regularization Methods in Graph-Based Neural Networks\n",
      "Critiques of Incremental Optimization Methods and Their Novelty\n",
      "Evaluation and Justification of Incremental Methods in Task-Based Environments\n",
      "Novel Approaches and Detailed Studies in Mega-Group Analysis\n",
      "Lack of Novel Contributions and Insufficient Clarity in Research Papers\n",
      "Limitations and Behaviors of Small-Scale Recurrent Neural Networks\n",
      "Peer Review Processes and Ethical Considerations in Academic Publishing\n",
      "Related Literature and Prior Work\n",
      "Critique of Methodological Rigor and Baseline Comparisons in Proposed Approaches\n",
      "Experimental Evaluation and Critique of Symbolic Reasoning Model Performance\n",
      "Algorithm Training and Model Assumptions\n",
      "Challenges in Human Evaluation of Experimental Results\n",
      "Challenges in Understanding and Interpreting Complex Papers\n",
      "Clarity and Motivation in Mathematical Decision-Making\n",
      "Adversarial Example Robustness and Automated Attack Evaluation\n",
      "Scalability and Robustness Evaluation Methods in Security Systems\n",
      "Ambiguities and Misuses in Probabilistic Modeling Papers\n",
      "Clarifying Writing and Explanation Confusion in Variable Scanning Approaches\n",
      "Evaluation and Analysis of Proposed Heuristic Methods\n",
      "Critiques of Inadequately Explained or Poorly Motivated Research Papers\n",
      "Assessment of Novel Model Integration Techniques and Their Limitations\n",
      "Advancements in UNet Architecture and Optimization Techniques for Enhanced Generalization Accuracy\n",
      "Real-World Applications and Experimental Evaluation on Benchmark Datasets\n",
      "Limitations of Single-Factor Experimental Analysis\n",
      "Evaluation of Kernel Selection and Variance in Code Averaging Methodologies\n",
      "Evaluation of Literature Review and Problem-Solving Approaches in Academic Papers\n",
      "Analysis of Paper Novelty and Methodological Integration in ICLR Submissions\n",
      "Reviewer Feedback and Explicit Rejection Points\n",
      "Evaluation and Benchmarking of Clustering Adaptation Methods\n",
      "Research Contributions and Novelty in Related Work\n",
      "Evaluation and Enhancement of High-Fidelity Communication Models\n",
      "Network Robustness and Stability Under Uncertainty\n",
      "Evaluation of Models on Limited Experimental Datasets\n",
      "Evaluating Task Performance and Speed in Metalearning with Hardware-Based Benchmarks\n",
      "Manuscript Preparation and Publication Challenges\n",
      "Text Extraction Techniques and Dataset Familiarity in VTAB Framework\n"
     ]
    }
   ],
   "source": [
    "for reason in dfr['_level_0'].unique():\n",
    "    print(reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59137d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a65c5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dfr.to_csv('reject_reviews_clustered.csv', index=False)\n",
    "# dfr = pd.read_csv('reject_reviews_clustered.csv')\n",
    "\n",
    "SYNTHESIS_NAMING_PROMPT_TEMPLATE = '''\n",
    "you are an program committee chair of a scientific conference.\n",
    "Your task is to identify the main issues in a list of reviews, and to group them into higher level groups.\n",
    "This is an ENUMERATED list of reviews, in the format NUMBER: REVIEW. \n",
    "Your job is to reduce the list to a smaller number of ISSUEs raised by the reviewers. \n",
    "Be as specific as possible. \n",
    "The output is a list of GROUP_NAME that are at a higher level of abstaction and that group together at least two underlying issues.\n",
    "Try to identify groups that put together a minimum of two, but ideally four or five similar issues, and give them a name that is descriptive of the group and of the differences between\n",
    "items in that group and items in other groups. \n",
    "along with a mapping of the original ISSUE_NUMBERs to the new GROUP_NAME.\n",
    "Only return the map if it contains more than one ISSUE_NUMBERs, that is, if we can actually group some issues into a higher level pattern.\n",
    "Make sure every ISSUE_NUMBER is assigned to a group, and make an effort to group every issue into a higher level group.\n",
    "\n",
    "These is the enumerated list of issues:\n",
    "<ENUMERATED_ISSUE_LIST>\n",
    "{{text_to_synthesize}}\n",
    "</ENUMERATED_ISSUE_LIST>\n",
    "\n",
    "First, briefly describe your reasoning, and then return a JSON dict object. The answer should have the following structure:\n",
    "REASONING: <YOUR REASONING>\n",
    "JSON GROUPS:\n",
    "{\n",
    "  \"<GROUP_NAME_1>\": [\"<ISSUE_NUMBER>\", .., \"<ISSUE_NUMBER>\"],\n",
    "  \"<GROUP_NAME_2>\": [\"<ISSUE_NUMBER>\" .., \"<ISSUE_NUMBER>\"],\n",
    "}\n",
    "Please adhere to the format described in the JSON part of the answer: each json key must contain a list of numbers corresponding to the ISSUE_IDs to be grouped under that key identifying the group name.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b3614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfr_h = build_cluster_hierarchy(dfr, llm, '_level_0', prompt_template = SYNTHESIS_NAMING_PROMPT_TEMPLATE, MAX_TOP_LEVEL_CLUSTERS=10)\n",
    "print_clusters(dfr_h, 'main_limitation', 'hierarchy.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15006788",
   "metadata": {},
   "source": [
    "- ### Clarity of Presentation, Explanation, and Motivation (L1, 60)\n",
    "  - #### Ambiguities and Misuses in Probabilistic Modeling Papers (L0, 7)\n",
    "    * Examples:\n",
    "      - The paper misuses the reparameterization trick; Eq.(2) lacks required KL-divergence term, making the approach unjustified.\n",
    "      - The section on the bernoulli splitting trick was the most confusing part of the paper.\n",
    "      - Confusing motivation, especially regarding reward ambiguity, and lack of clear explanation in the paper.\n",
    "  - #### Challenges in Understanding and Interpreting Complex Papers (L0, 14)\n",
    "    * Examples:\n",
    "      - Writing is sloppy and difficult to read; main claim about i.i.d. hypothesis invalidating GP interpretation is not convincing.\n",
    "      - Paper is tedious with heavy notations, making it difficult to read.\n",
    "      - It is difficult to understand the paper; it is not clearly written, especially the algorithm's presentation.\n",
    "  - #### Clarifying Writing and Explanation Confusion in Variable Scanning Approaches (L0, 7)\n",
    "    * Examples:\n",
    "      - Motivation and explanation are insufficient; rationale for the approach is unclear.\n",
    "      - The definition and explanation of Equation 2 and variable z are unclear and confusing.\n",
    "      - The writing can be significantly improved; technical parts are confusing and unclear.\n",
    "  - #### Clarity and Motivation in Mathematical Decision-Making (L0, 8)\n",
    "    * Examples:\n",
    "      - Unclear motivation for using tropical geometry and misleading use of 'adversarial attacks' terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9724b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfr.to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a22db",
   "metadata": {},
   "source": [
    "## Path 2: cluster and synthesize\n",
    "\n",
    "Basically we build hierarchy from the main level, without the initial clustering and naming step to go to L0. All clustering is done by the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583474f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hc import build_cluster_hierarchy, print_clusters\n",
    "\n",
    "SYNTHESIS_NAMING_FROMWK_PROMPT_TEMPLATE = '''\n",
    "you are an program committee chair of a scientific conference.\n",
    "Your task is to group  a list of weaknesses reviewers found in papers into higher level groups.\n",
    "This is an ENUMERATED list of weaknesses, in the format WEAKNESS_NUMBER: WEAKNESS. \n",
    "Your job is to reduce the list to a smaller number of WEAKNESSs raised by the reviewers. \n",
    "Be as specific as possible. \n",
    "The output is a list of GROUP_NAME that are at a higher level of abstaction and that group together at least two underlying weaknesses.\n",
    "Try to identify groups that put together a minimum of two, but ideally many similar weaknesses, and give them a name that is descriptive of the group and of the differences between\n",
    "items in that group and items in other groups, along with a mapping of the original WEAKNESS_NUMBER to the new GROUP_NAME.\n",
    "Only return the map if it contains more than one WEAKNESS_NUMBER, that is, if we can actually group some issues into a higher level pattern.\n",
    "Make sure every WEAKNESS_NUMBER is assigned to a group, and make an effort to group every issue into a higher level group.\n",
    "\n",
    "These is the enumerated list of issues:\n",
    "<ENUMERATED_ISSUE_LIST>\n",
    "{{text_to_synthesize}}\n",
    "</ENUMERATED_ISSUE_LIST>\n",
    "\n",
    "First, briefly describe your reasoning, and then return a JSON dict object. The answer should have the following structure:\n",
    "REASONING: <YOUR REASONING>\n",
    "JSON GROUPS:\n",
    "{\n",
    "  \"<GROUP_NAME_1>\": [\"<WEAKNESS_NUMBER>\", .., \"<WEAKNESS_NUMBER>\"],\n",
    "  \"<GROUP_NAME_2>\": [\"<WEAKNESS_NUMBER>\" .., \"<WEAKNESS_NUMBER>\"],\n",
    "}\n",
    "Please adhere to the format described in the JSON part of the answer: each json key must contain a list of numbers corresponding to the ISSUE_IDs to be grouped under that key identifying the group name.\n",
    "Do not include special characters in the answer besides the JSON format.\n",
    "'''\n",
    "\n",
    "dfr_cs = pd.read_csv('reject_reviews.csv')\n",
    "print (len(dfr_cs))\n",
    "# dfr_cs = dfr.copy()\n",
    "dfr_cs['_level_0'] = dfr_cs['main_limitation'].astype(str)\n",
    "\n",
    "dfr_cs_h = build_cluster_hierarchy(dfr_cs, llm, '_level_0', prompt_template = SYNTHESIS_NAMING_FROMWK_PROMPT_TEMPLATE, MAX_TOP_LEVEL_CLUSTERS=10)\n",
    "print_clusters(dfr_cs_h, 'main_limitation', 'hierarchy_cs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0b8fab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Tree...\n",
      "Detected and using levels (highest to lowest): ['_level_4', '_level_3', '_level_2', '_level_1', '_level_0']\n",
      "\n",
      "Printing Tree Structure:\n",
      "Warning: LEVELS_TO_PRINT (6) is greater than total levels available (5). Printing all levels.\n",
      "- Correlation between PE score and human judgement is unsurprising because CSF already encodes human perceptual characteristics. (L4, 1)\n",
      "  * Examples:\n",
      "    - Correlation between PE score and human judgement is unsurprising because CSF already encodes human perceptual characteristics.\n",
      "- Ethical and Policy Violations (L4, 5)\n",
      "  - Blind Review or Policy Violations (L3, 2)\n",
      "    - Violates blind-review policy by exposing author identity in acknowledgements. (L2, 1)\n",
      "      * Examples:\n",
      "        - Violates blind-review policy by exposing author identity in acknowledgements.\n",
      "    - Violating double blind rule. (L2, 1)\n",
      "      * Examples:\n",
      "        - Violating double blind rule.\n",
      "  - Ethical Issues and Scientific Integrity (Plagiarism, Double-Blind Violations) (L3, 2)\n",
      "    - Theorem 2 and its proof are plagiarized and presented as the authors' own work. (L2, 1)\n",
      "      * Examples:\n",
      "        - Theorem 2 and its proof are plagiarized and presented as the authors' own work.\n",
      "    - Violation of the double blind review rule and similarity to previously published work. (L2, 1)\n",
      "      * Examples:\n",
      "        - Violation of the double blind review rule and similarity to previously published work.\n",
      "  - The paper was already accepted at a Neuro AI workshop, violating double-blind review rules. (L3, 1)\n",
      "    * Examples:\n",
      "      - The paper was already accepted at a Neuro AI workshop, violating double-blind review rules.\n",
      "- Methodology and Scientific Rigor (L4, 745)\n",
      "  - Experimental Validation and Evaluation Design Issues (L3, 378)\n",
      "    - Comparison, Evaluation, and Experimental Design Weaknesses (L2, 7)\n",
      "      - ARAC seeks diversity in fitness space, which is weaker than behavior/outcome space used in QD and NS. (L1, 1)\n",
      "        * Examples:\n",
      "          - ARAC seeks diversity in fitness space, which is weaker than behavior/outcome space used in QD and NS.\n",
      "      - Authors should compare using QAP for assignment with GSE to better assess their method. (L1, 1)\n",
      "        * Examples:\n",
      "          - Authors should compare using QAP for assignment with GSE to better assess their method.\n",
      "      - Performance drops sharply after 50% pruning across all datasets (Figure 4). (L1, 1)\n",
      "        * Examples:\n",
      "          - Performance drops sharply after 50% pruning across all datasets (Figure 4).\n",
      "      - TVmax achieves only a little improvement over the baseline (softmax) on automated metrics. (L1, 1)\n",
      "        * Examples:\n",
      "          - TVmax achieves only a little improvement over the baseline (softmax) on automated metrics.\n",
      "      - The method was not tested on other NLP benchmarks like text classification or NER. (L1, 1)\n",
      "        * Examples:\n",
      "          - The method was not tested on other NLP benchmarks like text classification or NER.\n",
      "      - Weak Integration between Theory and Experiments (L1, 2)\n",
      "        - The theoretical and empirical parts are not well-connected; variance of the estimator is not quantified. (L0, 1)\n",
      "          * Examples:\n",
      "            - The theoretical and empirical parts are not well-connected; variance of the estimator is not quantified.\n",
      "        - Theoretical and empirical analyses are not sufficiently novel, deep, or informative. (L0, 1)\n",
      "          * Examples:\n",
      "            - Theoretical and empirical analyses are not sufficiently novel, deep, or informative.\n",
      "    - Computational Inefficiency and Resource Overhead (L2, 3)\n",
      "      - The method introduces more memory overhead compared to GPipe. (L1, 1)\n",
      "        * Examples:\n",
      "          - The method introduces more memory overhead compared to GPipe.\n",
      "      - The method is computationally costly and sacrifices network flexibility, being a brute-force application without practical remedies. (L1, 1)\n",
      "        * Examples:\n",
      "          - The method is computationally costly and sacrifices network flexibility, being a brute-force application without practical remedies.\n",
      "      - The method requires storing an increasing number of models, leading to huge storage costs, making it economically infeasible. (L1, 1)\n",
      "        * Examples:\n",
      "          - The method requires storing an increasing number of models, leading to huge storage costs, making it economically infeasible.\n",
      "    - Evaluation Metrics, Ablation Studies, and Analysis Limitations (L2, 6)\n",
      "      - Missing Evaluation Metrics/Uncertainty/Ablation Studies (L1, 4)\n",
      "        - Experimental validation is weak: no proper baselines, limited datasets, and unclear evaluation metrics. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental validation is weak: no proper baselines, limited datasets, and unclear evaluation metrics.\n",
      "        - Lack of ablation experiments for FALCON's components and need for experimental justification of convolution order. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of ablation experiments for FALCON's components and need for experimental justification of convolution order.\n",
      "        - Lack of uncertainty (e.g., error bars) in experimental plots. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of uncertainty (e.g., error bars) in experimental plots.\n",
      "        - The experimental evaluation is insufficient, lacking details about TensorFlow comparison and missing ablation studies. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experimental evaluation is insufficient, lacking details about TensorFlow comparison and missing ablation studies.\n",
      "      - Unvalidated or Questionable Evaluation Metrics and Analysis (L1, 2)\n",
      "        - No standard deviation reported for results; possible high variance in outcomes when using low bit quantization. (L0, 1)\n",
      "          * Examples:\n",
      "            - No standard deviation reported for results; possible high variance in outcomes when using low bit quantization.\n",
      "        - The new ‘concept accuracy’ metric is unvalidated and poorly correlated with interpretability, making the results unconvincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The new ‘concept accuracy’ metric is unvalidated and poorly correlated with interpretability, making the results unconvincing.\n",
      "    - Evaluation, Empirics, and Implementation Issues (L2, 19)\n",
      "      - Dataset and Evaluation Limitations (L1, 4)\n",
      "        - None explicitly stated. (L0, 2)\n",
      "          * Examples:\n",
      "            - None explicitly stated.\n",
      "            - None explicitly stated.\n",
      "        - None stated. (L0, 1)\n",
      "          * Examples:\n",
      "            - None stated.\n",
      "        - PUGAN was tested only on a simple dataset and quality is only comparable to WaveGAN, not advancing fidelity. (L0, 1)\n",
      "          * Examples:\n",
      "            - PUGAN was tested only on a simple dataset and quality is only comparable to WaveGAN, not advancing fidelity.\n",
      "      - Insufficient discussion on why the model underperforms for Russian in word-translation retrieval. (L1, 1)\n",
      "        * Examples:\n",
      "          - Insufficient discussion on why the model underperforms for Russian in word-translation retrieval.\n",
      "      - Model Efficiency and Hardware Realism Issues (L1, 4)\n",
      "        - ESN is not suitable for existing hardware; it relies on unrealistic assumptions. (L0, 1)\n",
      "          * Examples:\n",
      "            - ESN is not suitable for existing hardware; it relies on unrealistic assumptions.\n",
      "        - It is unclear how speed-up is calculated and whether MAC reduction correlates to actual hardware speed-up. (L0, 1)\n",
      "          * Examples:\n",
      "            - It is unclear how speed-up is calculated and whether MAC reduction correlates to actual hardware speed-up.\n",
      "        - The computational efficiency of conventional NLP models is well known to researchers. (L0, 1)\n",
      "          * Examples:\n",
      "            - The computational efficiency of conventional NLP models is well known to researchers.\n",
      "        - Using total number of bits as equivalent measure between models doesn't account for actual inference latency or efficiency on hardware. (L0, 1)\n",
      "          * Examples:\n",
      "            - Using total number of bits as equivalent measure between models doesn't account for actual inference latency or efficiency on hardware.\n",
      "      - Poor Generalization or Overfitting to Experimental Setting (L1, 2)\n",
      "        - Accelerated diffusion causes over-smoothing quickly, limiting the feasible number of layers and deep network non-linearity. (L0, 1)\n",
      "          * Examples:\n",
      "            - Accelerated diffusion causes over-smoothing quickly, limiting the feasible number of layers and deep network non-linearity.\n",
      "        - Generalization is unconvincing because both tasks use the same environments and features. (L0, 1)\n",
      "          * Examples:\n",
      "            - Generalization is unconvincing because both tasks use the same environments and features.\n",
      "      - Reproducibility and Supporting Materials Issues (L1, 3)\n",
      "        - Lacks many details in key sections, preventing full appreciation and making reproducibility impossible. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lacks many details in key sections, preventing full appreciation and making reproducibility impossible.\n",
      "        - Reproducibility is difficult as source code and some datasets are not provided or are not public. (L0, 1)\n",
      "          * Examples:\n",
      "            - Reproducibility is difficult as source code and some datasets are not provided or are not public.\n",
      "        - The paper suffers from an incomplete evaluation, so acceptance is not recommended. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper suffers from an incomplete evaluation, so acceptance is not recommended.\n",
      "      - Superficiality or Simplistic Experimental Setting (L1, 5)\n",
      "        - Exposition is difficult to follow and experiments are not very compelling. (L0, 1)\n",
      "          * Examples:\n",
      "            - Exposition is difficult to follow and experiments are not very compelling.\n",
      "        - The manuscript has a limited impact; experiments provide minimal insight; advantage of AHash is minor and limited in scope. (L0, 1)\n",
      "          * Examples:\n",
      "            - The manuscript has a limited impact; experiments provide minimal insight; advantage of AHash is minor and limited in scope.\n",
      "        - The model and experiments are overly complicated for a very simplistic scenario where simpler methods would suffice. (L0, 1)\n",
      "          * Examples:\n",
      "            - The model and experiments are overly complicated for a very simplistic scenario where simpler methods would suffice.\n",
      "        - The motivation for L_MIM is obscure and uncompelling, and experiments are weak. (L0, 1)\n",
      "          * Examples:\n",
      "            - The motivation for L_MIM is obscure and uncompelling, and experiments are weak.\n",
      "        - The studied problem is well-studied and more accurate algorithms exist than those used in experiments. (L0, 1)\n",
      "          * Examples:\n",
      "            - The studied problem is well-studied and more accurate algorithms exist than those used in experiments.\n",
      "    - Experimental Design and Methodological Concerns (L2, 20)\n",
      "      - Issues with Experimental Setup or Fairness of Comparisons (L1, 3)\n",
      "        - No discussion or experimental comparison with spectral clustering and kernel spectral clustering methods, despite relevance. (L0, 1)\n",
      "          * Examples:\n",
      "            - No discussion or experimental comparison with spectral clustering and kernel spectral clustering methods, despite relevance.\n",
      "        - The comparison to baseline need to be improved or at least justified. (L0, 1)\n",
      "          * Examples:\n",
      "            - The comparison to baseline need to be improved or at least justified.\n",
      "        - The set of test-time perturbations may not be representative/exhaustive, and a clear protocol for future use is lacking. (L0, 1)\n",
      "          * Examples:\n",
      "            - The set of test-time perturbations may not be representative/exhaustive, and a clear protocol for future use is lacking.\n",
      "      - Lack of Code/Data Release and Reproducibility (L1, 2)\n",
      "        - No mention about code release. (L0, 1)\n",
      "          * Examples:\n",
      "            - No mention about code release.\n",
      "        - Results cannot be verified unless code is shared. (L0, 1)\n",
      "          * Examples:\n",
      "            - Results cannot be verified unless code is shared.\n",
      "      - Lack of Generalizability Demonstrations (L1, 3)\n",
      "        - More results on ImageNet would be better to verify the proposed RobustNorm method. (L0, 1)\n",
      "          * Examples:\n",
      "            - More results on ImageNet would be better to verify the proposed RobustNorm method.\n",
      "        - Results seem unconvincing because only Cifar10 and Cifar100 (similar datasets) were tested. (L0, 1)\n",
      "          * Examples:\n",
      "            - Results seem unconvincing because only Cifar10 and Cifar100 (similar datasets) were tested.\n",
      "        - The method's application to real neuromorphic chips is unproven; only simulated results are shown. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method's application to real neuromorphic chips is unproven; only simulated results are shown.\n",
      "      - Lack of Meaningful Baselines or Comparisons (L1, 6)\n",
      "        - Does not compare to a strong enough baseline (one cycle), making experimental support unconvincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Does not compare to a strong enough baseline (one cycle), making experimental support unconvincing.\n",
      "        - Experimental section is lacking in comparisons, insights, and meaningful evaluation; insufficient baselines and weak experiment choices. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental section is lacking in comparisons, insights, and meaningful evaluation; insufficient baselines and weak experiment choices.\n",
      "        - Experimental section lacks standard datasets, evaluation protocols, and baseline comparisons. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental section lacks standard datasets, evaluation protocols, and baseline comparisons.\n",
      "        - Experimental validation is inadequate due to lack of baselines, making robustness assessment difficult. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental validation is inadequate due to lack of baselines, making robustness assessment difficult.\n",
      "        - Inadequate experimental evaluation and no comparison to relevant baselines or exploration of design choices. (L0, 1)\n",
      "          * Examples:\n",
      "            - Inadequate experimental evaluation and no comparison to relevant baselines or exploration of design choices.\n",
      "        - No comparison to strong keyframe prediction baselines; reliance on simple baselines weakens experimental support. (L0, 1)\n",
      "          * Examples:\n",
      "            - No comparison to strong keyframe prediction baselines; reliance on simple baselines weakens experimental support.\n",
      "      - Lack of analysis or reporting of the time complexity/cost of the proposed method. (L1, 1)\n",
      "        * Examples:\n",
      "          - Lack of analysis or reporting of the time complexity/cost of the proposed method.\n",
      "      - Methodological and Modeling Assumption Concerns (L1, 5)\n",
      "        - Both the motivation and the suggestion regarding model-based RL are not convincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Both the motivation and the suggestion regarding model-based RL are not convincing.\n",
      "        - How can DNN layers be modeled as a Bayesian Hierarchical model, since BHMs rely on i.i.d. assumptions? (L0, 1)\n",
      "          * Examples:\n",
      "            - How can DNN layers be modeled as a Bayesian Hierarchical model, since BHMs rely on i.i.d. assumptions?\n",
      "        - The feature uncertainty used in the proposed model can be unreliable and lacks a reliable guarantee. (L0, 1)\n",
      "          * Examples:\n",
      "            - The feature uncertainty used in the proposed model can be unreliable and lacks a reliable guarantee.\n",
      "        - Unclear if VILD works in realistic situations with model misspecification, especially with unknown number of demonstrators. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear if VILD works in realistic situations with model misspecification, especially with unknown number of demonstrators.\n",
      "        - Weak experiment setting: attacks generated on undefended model may overstate robustness due to possible gradient masking. (L0, 1)\n",
      "          * Examples:\n",
      "            - Weak experiment setting: attacks generated on undefended model may overstate robustness due to possible gradient masking.\n",
      "    - Experimental Setup and Design Issues (L2, 3)\n",
      "      - Experiments go wide but not deep; lacking focus and in-depth setup in experimental areas. (L1, 1)\n",
      "        * Examples:\n",
      "          - Experiments go wide but not deep; lacking focus and in-depth setup in experimental areas.\n",
      "      - Too many hyperparameters which will dramatically increase with depth, making the method hard to use. (L1, 1)\n",
      "        * Examples:\n",
      "          - Too many hyperparameters which will dramatically increase with depth, making the method hard to use.\n",
      "      - Unclear and potentially noisy prototype selection; all training samples used instead of top relevant prototypes. (L1, 1)\n",
      "        * Examples:\n",
      "          - Unclear and potentially noisy prototype selection; all training samples used instead of top relevant prototypes.\n",
      "    - Generalization, Scalability, and Dataset Limitations (L2, 22)\n",
      "      - A fairly large validation dataset fraction is needed to reliably identify corrupted samples, contrary to claims about small set sufficing. (L1, 1)\n",
      "        * Examples:\n",
      "          - A fairly large validation dataset fraction is needed to reliably identify corrupted samples, contrary to claims about small set sufficing.\n",
      "      - Dataset Limitations and Generalization Challenges (L1, 3)\n",
      "        - Concerns over using VAE to model ratings; VAEs need lots of data, so model may not learn well. (L0, 1)\n",
      "          * Examples:\n",
      "            - Concerns over using VAE to model ratings; VAEs need lots of data, so model may not learn well.\n",
      "        - Strong risk of overfitting; unclear if model generalizes beyond training data. (L0, 1)\n",
      "          * Examples:\n",
      "            - Strong risk of overfitting; unclear if model generalizes beyond training data.\n",
      "        - The proposed model cannot handle sparsity in datasets like Goodbooks, Movielens, and Netflix. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed model cannot handle sparsity in datasets like Goodbooks, Movielens, and Netflix.\n",
      "      - Doubtful that the typicality set works for OOD detection in small n (small data) regime. (L1, 1)\n",
      "        * Examples:\n",
      "          - Doubtful that the typicality set works for OOD detection in small n (small data) regime.\n",
      "      - Empirical Validation and Generalization Limitations (L1, 5)\n",
      "        - Empirical evaluation is limited to a single type of situation, questioning the method's generality. (L0, 1)\n",
      "          * Examples:\n",
      "            - Empirical evaluation is limited to a single type of situation, questioning the method's generality.\n",
      "        - Generalization performance degrades significantly with large number of processes/stages, not resolved even with hybrid method. (L0, 1)\n",
      "          * Examples:\n",
      "            - Generalization performance degrades significantly with large number of processes/stages, not resolved even with hybrid method.\n",
      "        - Major concerns with the empirical setup and methods in the section proposing a method to choose samples for SSL from an unlabeled pool. (L0, 1)\n",
      "          * Examples:\n",
      "            - Major concerns with the empirical setup and methods in the section proposing a method to choose samples for SSL from an unlabeled pool.\n",
      "        - Method only tested on 1D cases with a single dataset and equation, limiting generalizability. (L0, 1)\n",
      "          * Examples:\n",
      "            - Method only tested on 1D cases with a single dataset and equation, limiting generalizability.\n",
      "        - The improvement seems quite marginal; proposed method improved baseline by only 0.9% and 2.2% accuracy. (L0, 1)\n",
      "          * Examples:\n",
      "            - The improvement seems quite marginal; proposed method improved baseline by only 0.9% and 2.2% accuracy.\n",
      "      - Scalability Not Adequately Addressed (L1, 2)\n",
      "        - Scalability is not convincingly demonstrated; GRAM was only tested up to 500 nodes, which does not confirm scalability claims. (L0, 1)\n",
      "          * Examples:\n",
      "            - Scalability is not convincingly demonstrated; GRAM was only tested up to 500 nodes, which does not confirm scalability claims.\n",
      "        - The main motivation is scalability, but the proposed method does not convincingly address scalability; there is a disconnect. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main motivation is scalability, but the proposed method does not convincingly address scalability; there is a disconnect.\n",
      "      - Scalability and Generalization Issues (L1, 3)\n",
      "        - Evaluating both online and target networks requires extra environment interaction, severely hurting sample complexity. (L0, 1)\n",
      "          * Examples:\n",
      "            - Evaluating both online and target networks requires extra environment interaction, severely hurting sample complexity.\n",
      "        - The scalability to larger tasks (e.g. ImageNet, more output classes) is unproven and must be studied. (L0, 1)\n",
      "          * Examples:\n",
      "            - The scalability to larger tasks (e.g. ImageNet, more output classes) is unproven and must be studied.\n",
      "        - Uncertainty about ACClip's generalizability to architectures beyond Bert. (L0, 1)\n",
      "          * Examples:\n",
      "            - Uncertainty about ACClip's generalizability to architectures beyond Bert.\n",
      "      - Scalability and Generalization Issues (Small scale, model, dataset) (L1, 7)\n",
      "        - Experiments are limited to D=3 and N=10; unclear scalability to higher dimensions. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments are limited to D=3 and N=10; unclear scalability to higher dimensions.\n",
      "        - Experiments are limited to low-dimensional state space and discrete action space; scalability is uncertain. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments are limited to low-dimensional state space and discrete action space; scalability is uncertain.\n",
      "        - Experiments are limited to small datasets and shallow neural nets, reducing broad interest. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments are limited to small datasets and shallow neural nets, reducing broad interest.\n",
      "        - Insufficient experiments, particularly only analyzing a single principal component and lacking exploration of nonlinear transformations. (L0, 1)\n",
      "          * Examples:\n",
      "            - Insufficient experiments, particularly only analyzing a single principal component and lacking exploration of nonlinear transformations.\n",
      "        - Limited evaluation—experiments only on CIFAR-10 and synthetic data; lacks tests on larger, high-resolution image datasets. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited evaluation—experiments only on CIFAR-10 and synthetic data; lacks tests on larger, high-resolution image datasets.\n",
      "        - The experiments are quite lacking, especially on large scale models. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experiments are quite lacking, especially on large scale models.\n",
      "        - The scaling of the technique is not supported by the current experiments. (L0, 1)\n",
      "          * Examples:\n",
      "            - The scaling of the technique is not supported by the current experiments.\n",
      "    - Insufficient Experimental Rigor and Baseline Comparison (L2, 71)\n",
      "      - Insufficient Comparison with Prior Work or Baselines (L1, 5)\n",
      "        - No comparison to existing methods, making it impossible to evaluate usefulness. (L0, 1)\n",
      "          * Examples:\n",
      "            - No comparison to existing methods, making it impossible to evaluate usefulness.\n",
      "        - No comparison with clustering methods using geodesic distance (manifold learning) designed for non-convex data. (L0, 1)\n",
      "          * Examples:\n",
      "            - No comparison with clustering methods using geodesic distance (manifold learning) designed for non-convex data.\n",
      "        - No comparison with existing distributed asynchronous update methods; unclear how approach improves over prior stale update techniques. (L0, 1)\n",
      "          * Examples:\n",
      "            - No comparison with existing distributed asynchronous update methods; unclear how approach improves over prior stale update techniques.\n",
      "        - No comprehensive comparison with existing zero-shot task adaptation methods; results only on synthetic data limit conclusions. (L0, 1)\n",
      "          * Examples:\n",
      "            - No comprehensive comparison with existing zero-shot task adaptation methods; results only on synthetic data limit conclusions.\n",
      "        - The major problem is only comparing with outdated methods, so effectiveness is not well verified. (L0, 1)\n",
      "          * Examples:\n",
      "            - The major problem is only comparing with outdated methods, so effectiveness is not well verified.\n",
      "      - Insufficient Empirical Comparison to Relevant Baselines and State-of-the-Art Methods (L1, 10)\n",
      "        - Empirical results are weakly justified because of weak baselines and lack of comparison to relevant existing methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - Empirical results are weakly justified because of weak baselines and lack of comparison to relevant existing methods.\n",
      "        - Lack of comparison and discussion with Att2in (Rennie et al., 2017) regarding why rAIRL outperforms it. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of comparison and discussion with Att2in (Rennie et al., 2017) regarding why rAIRL outperforms it.\n",
      "        - Lack of comparison to a differentiable approximation baseline casts doubt on RL's necessity/optimality. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of comparison to a differentiable approximation baseline casts doubt on RL's necessity/optimality.\n",
      "        - Lack of comparison to methods using conditional/adaptive batch normalization. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of comparison to methods using conditional/adaptive batch normalization.\n",
      "        - Lack of comparison with a simple combination of best static and dynamic pruning baselines; unclear added value of the proposed method. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of comparison with a simple combination of best static and dynamic pruning baselines; unclear added value of the proposed method.\n",
      "        - Lack of comparison with other scalable domain adaptation approaches. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of comparison with other scalable domain adaptation approaches.\n",
      "        - Lack of comparison with recent state-of-the-art methods, including those cited by the authors. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of comparison with recent state-of-the-art methods, including those cited by the authors.\n",
      "        - Lack of discussion and comparison with state-of-the-art meta-learning baselines. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of discussion and comparison with state-of-the-art meta-learning baselines.\n",
      "        - Lack of proper baselines for comparison (e.g., word2vec, GloVe, Bag of Words, TF-IDF). (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of proper baselines for comparison (e.g., word2vec, GloVe, Bag of Words, TF-IDF).\n",
      "        - The approach is very similar to DeepCluster; lack of comparison to non-episodic training. (L0, 1)\n",
      "          * Examples:\n",
      "            - The approach is very similar to DeepCluster; lack of comparison to non-episodic training.\n",
      "      - Insufficient Experimental Details or Reproducibility (L1, 5)\n",
      "        - Detailed experimental setups are missing, making the results hard to evaluate. (L0, 1)\n",
      "          * Examples:\n",
      "            - Detailed experimental setups are missing, making the results hard to evaluate.\n",
      "        - Experimental details are insufficient for reproducibility; methods do not convincingly justify the claims. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental details are insufficient for reproducibility; methods do not convincingly justify the claims.\n",
      "        - The experimental description lacks detail, making it hard to assess fairness and reproducibility. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experimental description lacks detail, making it hard to assess fairness and reproducibility.\n",
      "        - The experimental evaluation is not convincing due to lack of competitors, use of simple datasets, and missing experimental details. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experimental evaluation is not convincing due to lack of competitors, use of simple datasets, and missing experimental details.\n",
      "        - The experimental setup and reward structure are confusing; it is unclear how this constitutes zero-shot generalization. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experimental setup and reward structure are confusing; it is unclear how this constitutes zero-shot generalization.\n",
      "      - Insufficient Experimental Thoroughness and Baseline Comparison (L1, 20)\n",
      "        - Absence of experiments on more recent architectures like DenseNet and ResNet. (L0, 1)\n",
      "          * Examples:\n",
      "            - Absence of experiments on more recent architectures like DenseNet and ResNet.\n",
      "        - Claims about stability guarantees and overall performance are not supported by theory and experiments. (L0, 1)\n",
      "          * Examples:\n",
      "            - Claims about stability guarantees and overall performance are not supported by theory and experiments.\n",
      "        - Efficiency of the proposed model is not studied; missing convergence and limited time performance experiments. (L0, 1)\n",
      "          * Examples:\n",
      "            - Efficiency of the proposed model is not studied; missing convergence and limited time performance experiments.\n",
      "        - Empirical validation does not match motivating scenario; experiments are only on small networks, undermining significance. (L0, 1)\n",
      "          * Examples:\n",
      "            - Empirical validation does not match motivating scenario; experiments are only on small networks, undermining significance.\n",
      "        - Experiments are anecdotal, incomplete, and not very exhaustive. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments are anecdotal, incomplete, and not very exhaustive.\n",
      "        - Experiments are incomplete, notably missing evaluation on DUC datasets to assess real-world generalization and lead-bias effects. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments are incomplete, notably missing evaluation on DUC datasets to assess real-world generalization and lead-bias effects.\n",
      "        - Experiments are too standard; no results on real-world datasets like Clothing1M. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments are too standard; no results on real-world datasets like Clothing1M.\n",
      "        - Experiments do not provide convincing evidence about the generality of the approach, especially with real-world demonstration diversity. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments do not provide convincing evidence about the generality of the approach, especially with real-world demonstration diversity.\n",
      "        - Experiments do not show state-of-the-art results, especially on standard datasets like MNIST and CIFAR. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments do not show state-of-the-art results, especially on standard datasets like MNIST and CIFAR.\n",
      "        - Experiments fail to disentangle the effect of weight uncertainty vs. structure uncertainty. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments fail to disentangle the effect of weight uncertainty vs. structure uncertainty.\n",
      "        - Experiments lack the most recent important baseline (symmetric cross entropy) which is current SOTA. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments lack the most recent important baseline (symmetric cross entropy) which is current SOTA.\n",
      "        - Experiments were not performed on a popular benchmark such as Atari. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments were not performed on a popular benchmark such as Atari.\n",
      "        - Insufficient ablation for multi-step K; unclear benefit of using K>1 over K=1 in main experiments. (L0, 1)\n",
      "          * Examples:\n",
      "            - Insufficient ablation for multi-step K; unclear benefit of using K>1 over K=1 in main experiments.\n",
      "        - Lack of convincing experiments on large-scale models and datasets to prove significance. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of convincing experiments on large-scale models and datasets to prove significance.\n",
      "        - Lack of experiments on real-world applications. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of experiments on real-world applications.\n",
      "        - The approach lacks robustness to model properties and model misspecification, not addressed in experiments. (L0, 1)\n",
      "          * Examples:\n",
      "            - The approach lacks robustness to model properties and model misspecification, not addressed in experiments.\n",
      "        - The contribution seems insignificant based on the experiments, especially with missing reordering of the 2-way model results. (L0, 1)\n",
      "          * Examples:\n",
      "            - The contribution seems insignificant based on the experiments, especially with missing reordering of the 2-way model results.\n",
      "        - The evaluations are highly insufficient, with experiments only on CIFAR-10 and unclear generalization. (L0, 1)\n",
      "          * Examples:\n",
      "            - The evaluations are highly insufficient, with experiments only on CIFAR-10 and unclear generalization.\n",
      "        - The experiments are not convincing, e.g. only pruning FC5 and FC6 layers in AlexNet on ImageNet dataset. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experiments are not convincing, e.g. only pruning FC5 and FC6 layers in AlexNet on ImageNet dataset.\n",
      "        - The paper is lacking in thorough evaluation; experiments do not convincingly show multi-task or option reusability. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is lacking in thorough evaluation; experiments do not convincingly show multi-task or option reusability.\n",
      "      - Lack of Experimental Clarity or Fairness in Comparison (L1, 4)\n",
      "        - Experiments are not compelling due to no careful hyperparameter search. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments are not compelling due to no careful hyperparameter search.\n",
      "        - Experiments were not conducted with enough seeds, so statistical significance and claims are not sufficiently supported. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments were not conducted with enough seeds, so statistical significance and claims are not sufficiently supported.\n",
      "        - Incomplete and potentially misleading comparison between Tensor Train (TT) and Tensor Ring (TR), lacking fair and exhaustive experiments. (L0, 1)\n",
      "          * Examples:\n",
      "            - Incomplete and potentially misleading comparison between Tensor Train (TT) and Tensor Ring (TR), lacking fair and exhaustive experiments.\n",
      "        - Possible unfair state distribution mismatch in Lunar Lander experiments may hurt GAIL baseline performance. (L0, 1)\n",
      "          * Examples:\n",
      "            - Possible unfair state distribution mismatch in Lunar Lander experiments may hurt GAIL baseline performance.\n",
      "      - Lack of Proper Comparison with Prior Work (L1, 2)\n",
      "        - Insufficient comparison to prior work; did not use datasets and setups previously used to evaluate SAT solvers. (L0, 1)\n",
      "          * Examples:\n",
      "            - Insufficient comparison to prior work; did not use datasets and setups previously used to evaluate SAT solvers.\n",
      "        - No performance comparison to existing methods; attack success rate is much lower than prior work. (L0, 1)\n",
      "          * Examples:\n",
      "            - No performance comparison to existing methods; attack success rate is much lower than prior work.\n",
      "      - Missing or Inadequate Comparison to Existing Methods or Baselines (L1, 6)\n",
      "        - Paper does not compare with other compression methods like WEST or projection layers. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper does not compare with other compression methods like WEST or projection layers.\n",
      "        - Paper does not report MINE results on the fMRI dataset for comparison. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper does not report MINE results on the fMRI dataset for comparison.\n",
      "        - The experimental comparisons are questionable; the paper does not compare with best benchmarks and lacks transparency. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experimental comparisons are questionable; the paper does not compare with best benchmarks and lacks transparency.\n",
      "        - The paper does not clearly demonstrate how the proposed method differs from or outperforms standard data augmentation. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not clearly demonstrate how the proposed method differs from or outperforms standard data augmentation.\n",
      "        - The paper does not compare sufficiently to established methods and literature, making results preliminary and not well-situated. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not compare sufficiently to established methods and literature, making results preliminary and not well-situated.\n",
      "        - The paper does not compare their method to prior 3D representation work or state-of-the-art 1D/2D models. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not compare their method to prior 3D representation work or state-of-the-art 1D/2D models.\n",
      "      - Missing or Inadequate Comparisons with Baselines or Related Methods (L1, 3)\n",
      "        - Paper lacks comparison (empirically and theoretically) with the method in A.Devraj & J.Chen (2019). (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper lacks comparison (empirically and theoretically) with the method in A.Devraj & J.Chen (2019).\n",
      "        - The paper fails to compare the different recovery algorithms and lacks analysis of their respective advantages, disadvantages, and performance differences. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper fails to compare the different recovery algorithms and lacks analysis of their respective advantages, disadvantages, and performance differences.\n",
      "        - The paper lacks experimental comparison to back-translation, making it hard to judge the proposed approach's contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks experimental comparison to back-translation, making it hard to judge the proposed approach's contribution.\n",
      "      - Missing or Poorly Described Experimental Methodology or Comparisons (L1, 3)\n",
      "        - Experimental comparisons are unfair and results are meaningless due to different norm constraints versus compared methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental comparisons are unfair and results are meaningless due to different norm constraints versus compared methods.\n",
      "        - Experimental results are almost impossible to interpret due to poor description; vital methodological details are missing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental results are almost impossible to interpret due to poor description; vital methodological details are missing.\n",
      "        - Experimental results lack run-time plots to show computational advantage; only loss curves are presented. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental results lack run-time plots to show computational advantage; only loss curves are presented.\n",
      "      - Missing, Inadequate, or Flawed Baselines and Comparisons (L1, 5)\n",
      "        - All results use heavy data augmentation, making comparison to other methods (except EfficientNet) impossible. (L0, 1)\n",
      "          * Examples:\n",
      "            - All results use heavy data augmentation, making comparison to other methods (except EfficientNet) impossible.\n",
      "        - Lack of essential baselines, especially Factorization Machines, makes results unconvincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of essential baselines, especially Factorization Machines, makes results unconvincing.\n",
      "        - Main results are based on comparisons between different codebases, making conclusions unreliable. (L0, 1)\n",
      "          * Examples:\n",
      "            - Main results are based on comparisons between different codebases, making conclusions unreliable.\n",
      "        - No results on policy search algorithms such as PILCO in Section 4.5; not represented in Table 4. (L0, 1)\n",
      "          * Examples:\n",
      "            - No results on policy search algorithms such as PILCO in Section 4.5; not represented in Table 4.\n",
      "        - No results on standard disentanglement metrics and no comparison with existing methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - No results on standard disentanglement metrics and no comparison with existing methods.\n",
      "      - Questions on experimental choices: lack of explanation on temperature in log-sum-exp and comparison to ATT instead of Gated-ATT. (L1, 1)\n",
      "        * Examples:\n",
      "          - Questions on experimental choices: lack of explanation on temperature in log-sum-exp and comparison to ATT instead of Gated-ATT.\n",
      "      - Technical or Methodological Rigor, Scope, and Comparison Deficiencies (L1, 6)\n",
      "        - Paper overstates that adversarial robustness always causes standard performance degradation; this may not always be true. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper overstates that adversarial robustness always causes standard performance degradation; this may not always be true.\n",
      "        - Paper unfairly compares ensembles with single learners using less data, leading to misleading conclusions. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper unfairly compares ensembles with single learners using less data, leading to misleading conclusions.\n",
      "        - The paper fails to address scalability of modules to real-world problems. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper fails to address scalability of modules to real-world problems.\n",
      "        - The paper fails to locate itself in the literature and does not compare with existing techniques analytically or experimentally. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper fails to locate itself in the literature and does not compare with existing techniques analytically or experimentally.\n",
      "        - The paper has nothing new or non-trivial; main claim about disparity matrix is unproven. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper has nothing new or non-trivial; main claim about disparity matrix is unproven.\n",
      "        - The second part of the paper is not rigorous and the results do not prove convergence to a global optimum as claimed. (L0, 1)\n",
      "          * Examples:\n",
      "            - The second part of the paper is not rigorous and the results do not prove convergence to a global optimum as claimed.\n",
      "      - The method for generating adversarial examples alters the model itself, differing from standard settings, making the comparison unclear. (L1, 1)\n",
      "        * Examples:\n",
      "          - The method for generating adversarial examples alters the model itself, differing from standard settings, making the comparison unclear.\n",
      "    - Insufficient or Unconvincing Empirical Evaluation and Experimental Validation (L2, 180)\n",
      "      - Evaluation Methodology and Experimental Design Limitations (L1, 3)\n",
      "        - No clear description of the models makes it difficult to understand their structure. (L0, 1)\n",
      "          * Examples:\n",
      "            - No clear description of the models makes it difficult to understand their structure.\n",
      "        - No human-rater evaluations to measure perceptual quality of localization. (L0, 1)\n",
      "          * Examples:\n",
      "            - No human-rater evaluations to measure perceptual quality of localization.\n",
      "        - The evaluation is biased by using test set I/O pairs to construct the training set, compromising validity. (L0, 1)\n",
      "          * Examples:\n",
      "            - The evaluation is biased by using test set I/O pairs to construct the training set, compromising validity.\n",
      "      - Evaluation and Empirical Results Concerns (L1, 3)\n",
      "        - No quantitative evidence provided for computational efficiency; proposed approach underperforms in accuracy. (L0, 1)\n",
      "          * Examples:\n",
      "            - No quantitative evidence provided for computational efficiency; proposed approach underperforms in accuracy.\n",
      "        - No time cost comparison is provided for the proposed Euclidean distance acceleration. (L0, 1)\n",
      "          * Examples:\n",
      "            - No time cost comparison is provided for the proposed Euclidean distance acceleration.\n",
      "        - The usefulness and practicality of the proposed task for relevant higher-level AI4code tasks is unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The usefulness and practicality of the proposed task for relevant higher-level AI4code tasks is unclear.\n",
      "      - Experimental Design and Reporting Limitations (L1, 6)\n",
      "        - Maximal safe set is only illustrated for one CIFAR-100 case, which is not convincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Maximal safe set is only illustrated for one CIFAR-100 case, which is not convincing.\n",
      "        - No clear practical advantage of CBN over BN for big batches or GN for small batches, even with batch size = 1. (L0, 1)\n",
      "          * Examples:\n",
      "            - No clear practical advantage of CBN over BN for big batches or GN for small batches, even with batch size = 1.\n",
      "        - No error bars or confidence regions, making it hard to judge significance of performance differences. (L0, 1)\n",
      "          * Examples:\n",
      "            - No error bars or confidence regions, making it hard to judge significance of performance differences.\n",
      "        - Possible limitation if, after training, a single ensemble member dominates throughout the task. (L0, 1)\n",
      "          * Examples:\n",
      "            - Possible limitation if, after training, a single ensemble member dominates throughout the task.\n",
      "        - The experiment design has many flaws. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experiment design has many flaws.\n",
      "        - The experiment on 'compactness' may not accurately reflect the concept as intended. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experiment on 'compactness' may not accurately reflect the concept as intended.\n",
      "      - Experimental Results are Not Convincing or Reliable (L1, 10)\n",
      "        - Comparison is not convincing because different backbones (ResNet50 vs Inception) are used, making results unfair. (L0, 1)\n",
      "          * Examples:\n",
      "            - Comparison is not convincing because different backbones (ResNet50 vs Inception) are used, making results unfair.\n",
      "        - Experimental results are not convincing; evidence does not support that the proposed strategy achieves the stated goal. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental results are not convincing; evidence does not support that the proposed strategy achieves the stated goal.\n",
      "        - Lack of strong baselines like BERT; experiment results not convincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of strong baselines like BERT; experiment results not convincing.\n",
      "        - Mathematical writing is unclear and not up to publication standard, making technical results hard to interpret. (L0, 1)\n",
      "          * Examples:\n",
      "            - Mathematical writing is unclear and not up to publication standard, making technical results hard to interpret.\n",
      "        - Reliability of results is unclear due to very small training set, making evaluation difficult. (L0, 1)\n",
      "          * Examples:\n",
      "            - Reliability of results is unclear due to very small training set, making evaluation difficult.\n",
      "        - Reported results are not convincing and inconsistent with prior works, casting doubt on any real accuracy improvement. (L0, 1)\n",
      "          * Examples:\n",
      "            - Reported results are not convincing and inconsistent with prior works, casting doubt on any real accuracy improvement.\n",
      "        - The experimental results are not convincing enough to justify the significance of the proposed method. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experimental results are not convincing enough to justify the significance of the proposed method.\n",
      "        - The proposed method does not outperform previous off-policy methods and results are not convincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method does not outperform previous off-policy methods and results are not convincing.\n",
      "        - The results are not convincing enough for a conference paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - The results are not convincing enough for a conference paper.\n",
      "        - The writing is unclear and hard to follow, and the experiment results are not convincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The writing is unclear and hard to follow, and the experiment results are not convincing.\n",
      "      - Inadequate Experimental Evaluation (L1, 2)\n",
      "        - Discarding unsuccessful runs undermines the claim that low entropy messages emerge without regularization. (L0, 1)\n",
      "          * Examples:\n",
      "            - Discarding unsuccessful runs undermines the claim that low entropy messages emerge without regularization.\n",
      "        - The evaluation is limited: only a single dataset and basic baseline are used. (L0, 1)\n",
      "          * Examples:\n",
      "            - The evaluation is limited: only a single dataset and basic baseline are used.\n",
      "      - Inadequate Experimental Validation or Empirical Support (L1, 6)\n",
      "        - Experimental design and comprehensiveness are lacking, making the empirical study unconvincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental design and comprehensiveness are lacking, making the empirical study unconvincing.\n",
      "        - Experimental evaluation section requires improvements. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental evaluation section requires improvements.\n",
      "        - Technical contributions are not enough; needs more experimental comparisons. (L0, 1)\n",
      "          * Examples:\n",
      "            - Technical contributions are not enough; needs more experimental comparisons.\n",
      "        - The experimental investigation is not sufficient and compelling enough. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experimental investigation is not sufficient and compelling enough.\n",
      "        - The paper needs more thorough experimental validation to better demonstrate its effectiveness. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper needs more thorough experimental validation to better demonstrate its effectiveness.\n",
      "        - Totally inadequate experimental validation undermines the paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - Totally inadequate experimental validation undermines the paper.\n",
      "      - Inadequate or Unclear Empirical Evaluation (L1, 4)\n",
      "        - Quantitative analysis does not immediately show the benefit or characteristics of the proposed approaches. (L0, 1)\n",
      "          * Examples:\n",
      "            - Quantitative analysis does not immediately show the benefit or characteristics of the proposed approaches.\n",
      "        - The analysis has many flaws and only minor connection to performance of RNNs, with marginal empirical success. (L0, 1)\n",
      "          * Examples:\n",
      "            - The analysis has many flaws and only minor connection to performance of RNNs, with marginal empirical success.\n",
      "        - The rigour and detail in documenting the empirical evaluation are not of the standard expected for publication. (L0, 1)\n",
      "          * Examples:\n",
      "            - The rigour and detail in documenting the empirical evaluation are not of the standard expected for publication.\n",
      "        - Unconvincing and inadequate hyperparameter sensitivity analysis; experiment limited in scope and duration. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unconvincing and inadequate hyperparameter sensitivity analysis; experiment limited in scope and duration.\n",
      "      - Insufficient Empirical Evaluation and Missing or Incomplete Baselines (L1, 21)\n",
      "        - Does not compare with later, better baselines such as FlowSeq; unclear speedup evaluation. (L0, 1)\n",
      "          * Examples:\n",
      "            - Does not compare with later, better baselines such as FlowSeq; unclear speedup evaluation.\n",
      "        - Does not provide thorough comparison with other state-of-the-art methods on reasoning related tasks. (L0, 1)\n",
      "          * Examples:\n",
      "            - Does not provide thorough comparison with other state-of-the-art methods on reasoning related tasks.\n",
      "        - Experiments do not compare with any mutual information related baselines (e.g., VIB), which is critical to show benefit of discrete codes. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments do not compare with any mutual information related baselines (e.g., VIB), which is critical to show benefit of discrete codes.\n",
      "        - It's hard to judge task difficulty because few tasks have human evaluation scores; missing human baselines hinder comparison. (L0, 1)\n",
      "          * Examples:\n",
      "            - It's hard to judge task difficulty because few tasks have human evaluation scores; missing human baselines hinder comparison.\n",
      "        - Limited ablation study to show the impact of each design choice. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited ablation study to show the impact of each design choice.\n",
      "        - MBF detection metric is much more complicated to calculate than baselines, possibly making performance gains unfair. (L0, 1)\n",
      "          * Examples:\n",
      "            - MBF detection metric is much more complicated to calculate than baselines, possibly making performance gains unfair.\n",
      "        - Missing comparisons with relevant, state-of-the-art resource-aware NAS baselines (e.g., MNASNet, ProxylessNAS, FBNets, EfficientNets). (L0, 1)\n",
      "          * Examples:\n",
      "            - Missing comparisons with relevant, state-of-the-art resource-aware NAS baselines (e.g., MNASNet, ProxylessNAS, FBNets, EfficientNets).\n",
      "        - No empirical improvement over state-of-the-art (SOTA); method only matches existing algorithms. (L0, 1)\n",
      "          * Examples:\n",
      "            - No empirical improvement over state-of-the-art (SOTA); method only matches existing algorithms.\n",
      "        - No evidence presented for the model's claimed hardware-friendliness and energy-efficiency. (L0, 1)\n",
      "          * Examples:\n",
      "            - No evidence presented for the model's claimed hardware-friendliness and energy-efficiency.\n",
      "        - No evidence provided that self-attention can't already capture local-context features. (L0, 1)\n",
      "          * Examples:\n",
      "            - No evidence provided that self-attention can't already capture local-context features.\n",
      "        - No reference or comparison to Kim et al.'s Bayesian Model-Agnostic Meta-Learning, a significant related work. (L0, 1)\n",
      "          * Examples:\n",
      "            - No reference or comparison to Kim et al.'s Bayesian Model-Agnostic Meta-Learning, a significant related work.\n",
      "        - The evaluation is lacking; not convinced SAP is better than existing or zero-shot baselines. (L0, 1)\n",
      "          * Examples:\n",
      "            - The evaluation is lacking; not convinced SAP is better than existing or zero-shot baselines.\n",
      "        - The evaluation of the proposed method is not complete; key baselines and datasets are missing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The evaluation of the proposed method is not complete; key baselines and datasets are missing.\n",
      "        - The method does not achieve state-of-the-art accuracy compared to existing NAS methods; lacks discussion on this shortcoming. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method does not achieve state-of-the-art accuracy compared to existing NAS methods; lacks discussion on this shortcoming.\n",
      "        - The method does not demonstrate clear advantages over standard LSTM, lacking evidence for combining deep learning and spiking neural nets. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method does not demonstrate clear advantages over standard LSTM, lacking evidence for combining deep learning and spiking neural nets.\n",
      "        - The method is not convincingly interpretable as claimed, with insufficient evidence supporting interpretability. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is not convincingly interpretable as claimed, with insufficient evidence supporting interpretability.\n",
      "        - The population size for the evolutionary component is far too small to substantiate the integration claim; no evidence that EC contributes beyond noise. (L0, 1)\n",
      "          * Examples:\n",
      "            - The population size for the evolutionary component is far too small to substantiate the integration claim; no evidence that EC contributes beyond noise.\n",
      "        - Unclear from empirical results whether the input-dependent version offers real advantages over added complexity. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear from empirical results whether the input-dependent version offers real advantages over added complexity.\n",
      "        - Unclear whether feature similarity is the reason BNSR outperforms BN; evidence is insufficient. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear whether feature similarity is the reason BNSR outperforms BN; evidence is insufficient.\n",
      "        - Variance reduction is not a direct measurement of policy stability or robustness; lacking stability/robustness evidence or discussion. (L0, 1)\n",
      "          * Examples:\n",
      "            - Variance reduction is not a direct measurement of policy stability or robustness; lacking stability/robustness evidence or discussion.\n",
      "        - Without more info on Table 3 baselines, it's unclear if TopoNets improve over Random and Residual baselines. (L0, 1)\n",
      "          * Examples:\n",
      "            - Without more info on Table 3 baselines, it's unclear if TopoNets improve over Random and Residual baselines.\n",
      "      - Insufficient Empirical Evaluation or Convincing Demonstration (L1, 6)\n",
      "        - 3-level hierarchy in StarCraft does not work well; high level may be limited by CNN resolution. (L0, 1)\n",
      "          * Examples:\n",
      "            - 3-level hierarchy in StarCraft does not work well; high level may be limited by CNN resolution.\n",
      "        - The evaluation is weak and does not clearly demonstrate that the author's goals (esp. reusability/transfer) have been achieved. (L0, 1)\n",
      "          * Examples:\n",
      "            - The evaluation is weak and does not clearly demonstrate that the author's goals (esp. reusability/transfer) have been achieved.\n",
      "        - The experiment is very limited and does not demonstrate the benefits of having an equivariant network. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experiment is very limited and does not demonstrate the benefits of having an equivariant network.\n",
      "        - The network structure does not convincingly address the complex distortions present in underwater images. (L0, 1)\n",
      "          * Examples:\n",
      "            - The network structure does not convincingly address the complex distortions present in underwater images.\n",
      "        - The study does not go into depth. (L0, 1)\n",
      "          * Examples:\n",
      "            - The study does not go into depth.\n",
      "        - The tasks, class definitions, and what is tested are not explained in enough depth, making interpretation unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The tasks, class definitions, and what is tested are not explained in enough depth, making interpretation unclear.\n",
      "      - Insufficient Empirical Evidence or Experimental Validation (L1, 6)\n",
      "        - The paper lacks convincing quantitative analysis to demonstrate the method's superiority; relies mostly on qualitative results. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks convincing quantitative analysis to demonstrate the method's superiority; relies mostly on qualitative results.\n",
      "        - The paper lacks evidence that expected gradients performs better and has unclear focus between methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks evidence that expected gradients performs better and has unclear focus between methods.\n",
      "        - The paper lacks experiments showing how the new knowledge graph benefits machine learning tasks. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks experiments showing how the new knowledge graph benefits machine learning tasks.\n",
      "        - The paper lacks justification and clarity for its proposed framework and experiments do not support the main claim. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks justification and clarity for its proposed framework and experiments do not support the main claim.\n",
      "        - The paper lacks results on standard benchmark metrics (e.g., BLEU, ROGUE), thus failing to show model effectiveness in generating relevant responses. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks results on standard benchmark metrics (e.g., BLEU, ROGUE), thus failing to show model effectiveness in generating relevant responses.\n",
      "        - The paper only provides empirical results and lacks substantial insights and useful information. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper only provides empirical results and lacks substantial insights and useful information.\n",
      "      - Insufficient Experimental Evaluation and Baselines (L1, 15)\n",
      "        - Confusing problem-framing and lack of robust baselines undermine the paper's suitability for publication. (L0, 1)\n",
      "          * Examples:\n",
      "            - Confusing problem-framing and lack of robust baselines undermine the paper's suitability for publication.\n",
      "        - Evaluation is weak due to low attack success rate and lack of justification for design choices and results. (L0, 1)\n",
      "          * Examples:\n",
      "            - Evaluation is weak due to low attack success rate and lack of justification for design choices and results.\n",
      "        - Lack of baseline comparison with generative models and insufficient validation procedure description. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of baseline comparison with generative models and insufficient validation procedure description.\n",
      "        - Lack of convergence tests and error analysis undermines confidence in the proposed neural network PDE solver. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of convergence tests and error analysis undermines confidence in the proposed neural network PDE solver.\n",
      "        - Lack of demonstration on difficult tasks that traditional ES or RL methods cannot solve. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of demonstration on difficult tasks that traditional ES or RL methods cannot solve.\n",
      "        - Lack of details on using gradient-based exploration to guide the adversary and insufficient black-box vs gradient-based comparison. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of details on using gradient-based exploration to guide the adversary and insufficient black-box vs gradient-based comparison.\n",
      "        - Lack of direct evaluation of the reflection hypothesis by integrating loss (16) into training and assessing performance changes. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of direct evaluation of the reflection hypothesis by integrating loss (16) into training and assessing performance changes.\n",
      "        - Lack of experiment comparing benefit from more data vs. summarization objective (e.g., pre-training BERTSUM on same data). (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of experiment comparing benefit from more data vs. summarization objective (e.g., pre-training BERTSUM on same data).\n",
      "        - Lack of experiment showing that the inductive bias does not harm performance on tasks without spatial reasoning. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of experiment showing that the inductive bias does not harm performance on tasks without spatial reasoning.\n",
      "        - Lack of rigorous experimentation and insufficient evaluation of generalizability across datasets and comparison with prior work. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of rigorous experimentation and insufficient evaluation of generalizability across datasets and comparison with prior work.\n",
      "        - Lack of standard deviations in Table 2 makes it unclear if reward differences are significant. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of standard deviations in Table 2 makes it unclear if reward differences are significant.\n",
      "        - Lack of strong baselines and incremental contribution; not a sufficient advance for publication. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of strong baselines and incremental contribution; not a sufficient advance for publication.\n",
      "        - Lack of sufficient and appealing visual results, and weak style diversity in generated images. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of sufficient and appealing visual results, and weak style diversity in generated images.\n",
      "        - Results lack statistical testing; claimed improvements may just be random fluctuations, so conclusions are not supported. (L0, 1)\n",
      "          * Examples:\n",
      "            - Results lack statistical testing; claimed improvements may just be random fluctuations, so conclusions are not supported.\n",
      "        - Unclear generalizability of conclusions beyond the specific method tested; lack of evidence that the method is representative. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear generalizability of conclusions beyond the specific method tested; lack of evidence that the method is representative.\n",
      "      - Insufficient or Missing Empirical Validation (L1, 2)\n",
      "        - Neither a surprising theoretical result nor significant empirical improvement is demonstrated. (L0, 1)\n",
      "          * Examples:\n",
      "            - Neither a surprising theoretical result nor significant empirical improvement is demonstrated.\n",
      "        - No empirical evaluations to support the theoretical analysis. (L0, 1)\n",
      "          * Examples:\n",
      "            - No empirical evaluations to support the theoretical analysis.\n",
      "      - Insufficient or Problematic Experiments and Empirical Evidence (L1, 8)\n",
      "        - Experiments conducted cannot support the conclusions in this paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments conducted cannot support the conclusions in this paper.\n",
      "        - Paper is entirely empirical with little or no justification and uses only a single architecture and dataset. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper is entirely empirical with little or no justification and uses only a single architecture and dataset.\n",
      "        - The experiments are insufficient to support the paper's main claim of entropy minimization in emergent languages. (L0, 1)\n",
      "          * Examples:\n",
      "            - The experiments are insufficient to support the paper's main claim of entropy minimization in emergent languages.\n",
      "        - The notions of robustness defined by the authors are not clearly related to standard adversarial robustness. (L0, 1)\n",
      "          * Examples:\n",
      "            - The notions of robustness defined by the authors are not clearly related to standard adversarial robustness.\n",
      "        - The paper explores methods for robustness certificates but does not compute and validate these certificates empirically. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper explores methods for robustness certificates but does not compute and validate these certificates empirically.\n",
      "        - The paper is preliminary and demonstrates the approach on only a single example and constraint, lacking general evidence. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is preliminary and demonstrates the approach on only a single example and constraint, lacking general evidence.\n",
      "        - The search space is much too small compared to typical NAS studies, limiting the paper's impact and generalizability. (L0, 1)\n",
      "          * Examples:\n",
      "            - The search space is much too small compared to typical NAS studies, limiting the paper's impact and generalizability.\n",
      "        - There are almost no experiments in this paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - There are almost no experiments in this paper.\n",
      "      - Insufficient or Unclear Evaluation and Comparison (L1, 3)\n",
      "        - Fairness of comparison to previous algorithms in the single-class regime; choice and representativeness of negative examples. (L0, 1)\n",
      "          * Examples:\n",
      "            - Fairness of comparison to previous algorithms in the single-class regime; choice and representativeness of negative examples.\n",
      "        - No theoretical contribution; not suitable for ICLR, better fitted for an application-focused conference. (L0, 1)\n",
      "          * Examples:\n",
      "            - No theoretical contribution; not suitable for ICLR, better fitted for an application-focused conference.\n",
      "        - The information is presented in a way that makes it difficult to validate the correctness of the theory and insights. (L0, 1)\n",
      "          * Examples:\n",
      "            - The information is presented in a way that makes it difficult to validate the correctness of the theory and insights.\n",
      "      - Insufficient or Unconvincing Empirical Evaluation (L1, 28)\n",
      "        - Authors did not verify correlation between detected communities and original labels, weakening experimental evidence. (L0, 1)\n",
      "          * Examples:\n",
      "            - Authors did not verify correlation between detected communities and original labels, weakening experimental evidence.\n",
      "        - CBN increases computational complexity and memory consumption compared to BN; unclear if gains are due to this cost. (L0, 1)\n",
      "          * Examples:\n",
      "            - CBN increases computational complexity and memory consumption compared to BN; unclear if gains are due to this cost.\n",
      "        - Does not convincingly show improvement in the quality of disentangled representations; separate training may impede this. (L0, 1)\n",
      "          * Examples:\n",
      "            - Does not convincingly show improvement in the quality of disentangled representations; separate training may impede this.\n",
      "        - Evaluation is based on outdated datasets, making it hard to accept the paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - Evaluation is based on outdated datasets, making it hard to accept the paper.\n",
      "        - Extremely limited empirical evaluation makes it difficult to assess benefits over more straightforward algorithms. (L0, 1)\n",
      "          * Examples:\n",
      "            - Extremely limited empirical evaluation makes it difficult to assess benefits over more straightforward algorithms.\n",
      "        - Framework and results are very similar to previous works; kernel regime dynamics may not be the most interesting or novel. (L0, 1)\n",
      "          * Examples:\n",
      "            - Framework and results are very similar to previous works; kernel regime dynamics may not be the most interesting or novel.\n",
      "        - It is unclear how much the LPA impacts the node classification. (L0, 1)\n",
      "          * Examples:\n",
      "            - It is unclear how much the LPA impacts the node classification.\n",
      "        - It is unclear why off-policy samples from smaller action spaces are useful for agents with larger action spaces. (L0, 1)\n",
      "          * Examples:\n",
      "            - It is unclear why off-policy samples from smaller action spaces are useful for agents with larger action spaces.\n",
      "        - It might be useful to test the approach with several different architectures. (L0, 1)\n",
      "          * Examples:\n",
      "            - It might be useful to test the approach with several different architectures.\n",
      "        - Limited evaluation and novelty, with insufficient comparative experiments to prior hierarchical MARL approaches. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited evaluation and novelty, with insufficient comparative experiments to prior hierarchical MARL approaches.\n",
      "        - Method is not properly justified for the chosen environments and should be evaluated with truly sparse rewards. (L0, 1)\n",
      "          * Examples:\n",
      "            - Method is not properly justified for the chosen environments and should be evaluated with truly sparse rewards.\n",
      "        - Needs an upper bound baseline showing accuracy if the region is seen (without context needed). (L0, 1)\n",
      "          * Examples:\n",
      "            - Needs an upper bound baseline showing accuracy if the region is seen (without context needed).\n",
      "        - No certified guarantee; robustness not evaluated under adaptive attacks; effectiveness unconvincing without thorough evaluation. (L0, 1)\n",
      "          * Examples:\n",
      "            - No certified guarantee; robustness not evaluated under adaptive attacks; effectiveness unconvincing without thorough evaluation.\n",
      "        - No experimental evidence provided to support that the theoretical phenomena (G-dominated/V-dominated regimes) occur in practice. (L0, 1)\n",
      "          * Examples:\n",
      "            - No experimental evidence provided to support that the theoretical phenomena (G-dominated/V-dominated regimes) occur in practice.\n",
      "        - Potential improved accuracy may be due to generalization from noisy imputed samples, not methodological innovation. (L0, 1)\n",
      "          * Examples:\n",
      "            - Potential improved accuracy may be due to generalization from noisy imputed samples, not methodological innovation.\n",
      "        - Testing was only done in simple environments (VizDoom); method not evaluated in more realistic scenarios. (L0, 1)\n",
      "          * Examples:\n",
      "            - Testing was only done in simple environments (VizDoom); method not evaluated in more realistic scenarios.\n",
      "        - The GAN loss does not significantly improve performance compared to the baseline, questioning its added value. (L0, 1)\n",
      "          * Examples:\n",
      "            - The GAN loss does not significantly improve performance compared to the baseline, questioning its added value.\n",
      "        - The evaluation is not convincing and whether Moniqua can achieve consensus is unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The evaluation is not convincing and whether Moniqua can achieve consensus is unclear.\n",
      "        - The improvement brought by the framework is marginal. (L0, 1)\n",
      "          * Examples:\n",
      "            - The improvement brought by the framework is marginal.\n",
      "        - The improvement of AHash over OOPH is unobtrusive and not surprising. (L0, 1)\n",
      "          * Examples:\n",
      "            - The improvement of AHash over OOPH is unobtrusive and not surprising.\n",
      "        - The improvement over existing DropOut variants is small and practical gains are limited. (L0, 1)\n",
      "          * Examples:\n",
      "            - The improvement over existing DropOut variants is small and practical gains are limited.\n",
      "        - The method does not achieve semantics-preserving attacks or successful attacks for NLP; claims are not supported. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method does not achieve semantics-preserving attacks or successful attacks for NLP; claims are not supported.\n",
      "        - The method is only evaluated on a toy task, limiting evidence for generalization to complex tasks. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is only evaluated on a toy task, limiting evidence for generalization to complex tasks.\n",
      "        - The method is only evaluated on incremental class tasks, not on other continual learning scenarios (e.g., incremental domain/task). (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is only evaluated on incremental class tasks, not on other continual learning scenarios (e.g., incremental domain/task).\n",
      "        - The simple MEGA-D baseline performs very well with little additional gain from the full MEGA approach, but is insufficiently analyzed and discussed. (L0, 1)\n",
      "          * Examples:\n",
      "            - The simple MEGA-D baseline performs very well with little additional gain from the full MEGA approach, but is insufficiently analyzed and discussed.\n",
      "        - Unclear and possibly inappropriate comparison with the M&M baseline; unclear adaptation to Q-learning undermines evaluation. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear and possibly inappropriate comparison with the M&M baseline; unclear adaptation to Q-learning undermines evaluation.\n",
      "        - Unclear if the deadlock phenomenon occurs in more complex, common benchmark environments. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear if the deadlock phenomenon occurs in more complex, common benchmark environments.\n",
      "        - Unclear what problems the framework can solve beyond those with known, fully specified algorithms and breakdowns. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear what problems the framework can solve beyond those with known, fully specified algorithms and breakdowns.\n",
      "      - Insufficient or Unconvincing Empirical Results (L1, 18)\n",
      "        - Empirical results are not strong enough to support CR, especially since similar architectures have been previously studied. (L0, 1)\n",
      "          * Examples:\n",
      "            - Empirical results are not strong enough to support CR, especially since similar architectures have been previously studied.\n",
      "        - Empirical results are puzzling and inconsistent with previous benchmark results (e.g., Sun et al 2019, DistilBERT). (L0, 1)\n",
      "          * Examples:\n",
      "            - Empirical results are puzzling and inconsistent with previous benchmark results (e.g., Sun et al 2019, DistilBERT).\n",
      "        - Improvement over BERT baseline is moderate, especially in head-to-head GLUE results. (L0, 1)\n",
      "          * Examples:\n",
      "            - Improvement over BERT baseline is moderate, especially in head-to-head GLUE results.\n",
      "        - It is not shown that the PE score can improve results of image generation tasks, such as super-resolution. (L0, 1)\n",
      "          * Examples:\n",
      "            - It is not shown that the PE score can improve results of image generation tasks, such as super-resolution.\n",
      "        - MONIQUA shows superior results only at early stages, fails to achieve state-of-the-art test accuracy, and suffers 10-20% accuracy drop. (L0, 1)\n",
      "          * Examples:\n",
      "            - MONIQUA shows superior results only at early stages, fails to achieve state-of-the-art test accuracy, and suffers 10-20% accuracy drop.\n",
      "        - No quantitative analysis or empirical results supporting choice of TGCNN over RNN-based architectures. (L0, 1)\n",
      "          * Examples:\n",
      "            - No quantitative analysis or empirical results supporting choice of TGCNN over RNN-based architectures.\n",
      "        - Performance gains over BERT are small and results do not clearly justify the architectural additions except for *lexical_overlap*. (L0, 1)\n",
      "          * Examples:\n",
      "            - Performance gains over BERT are small and results do not clearly justify the architectural additions except for *lexical_overlap*.\n",
      "        - Probably requires further research to achieve state of the art results. (L0, 1)\n",
      "          * Examples:\n",
      "            - Probably requires further research to achieve state of the art results.\n",
      "        - The advantage of DBB over BB is not clear, with mixed results across datasets and possible unjustified overhead. (L0, 1)\n",
      "          * Examples:\n",
      "            - The advantage of DBB over BB is not clear, with mixed results across datasets and possible unjustified overhead.\n",
      "        - The evaluation compares methods in accuracy ranges where the defense practically fails, making the results and comparisons not meaningful. (L0, 1)\n",
      "          * Examples:\n",
      "            - The evaluation compares methods in accuracy ranges where the defense practically fails, making the results and comparisons not meaningful.\n",
      "        - The method does not exhibit state-of-the-art performance as claimed; results are mixed and not compelling. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method does not exhibit state-of-the-art performance as claimed; results are mixed and not compelling.\n",
      "        - The possible false positives due to sample dependence in fMRI data have not been addressed, making results vacuous. (L0, 1)\n",
      "          * Examples:\n",
      "            - The possible false positives due to sample dependence in fMRI data have not been addressed, making results vacuous.\n",
      "        - The proposed method does not clearly outperform alternatives such as the KS-test in empirical results. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method does not clearly outperform alternatives such as the KS-test in empirical results.\n",
      "        - The proposed method's effectiveness for out-of-distribution detection is unclear and not supported by convincing empirical results. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method's effectiveness for out-of-distribution detection is unclear and not supported by convincing empirical results.\n",
      "        - The results and recommendations are intuitive and somewhat trivial. (L0, 1)\n",
      "          * Examples:\n",
      "            - The results and recommendations are intuitive and somewhat trivial.\n",
      "        - The results are expected and no effective methods are proposed to improve the results. (L0, 1)\n",
      "          * Examples:\n",
      "            - The results are expected and no effective methods are proposed to improve the results.\n",
      "        - The results depend heavily on Assumption 4 and correctness is not evaluated. (L0, 1)\n",
      "          * Examples:\n",
      "            - The results depend heavily on Assumption 4 and correctness is not evaluated.\n",
      "        - The results provide only marginal improvements over the baseline and do not appear to be statistically significant. (L0, 1)\n",
      "          * Examples:\n",
      "            - The results provide only marginal improvements over the baseline and do not appear to be statistically significant.\n",
      "      - Insufficient or Unconvincing Experimental Validation of Claims (L1, 22)\n",
      "        - Experimental analysis is insufficient—relies on a single dataset and single constraint, so claims aren't fully supported. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental analysis is insufficient—relies on a single dataset and single constraint, so claims aren't fully supported.\n",
      "        - Experimental results and claims are unconvincing and not well-justified, especially regarding scalability and performance. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental results and claims are unconvincing and not well-justified, especially regarding scalability and performance.\n",
      "        - Experimental results are insufficient to support key claims, especially regarding scalability—no thorough runtime experiments or large graph results. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental results are insufficient to support key claims, especially regarding scalability—no thorough runtime experiments or large graph results.\n",
      "        - Experimental results are minimal and not convincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental results are minimal and not convincing.\n",
      "        - Experimental results are unconvincing, with all recent methods worse than baselines and unclear explanations/data details. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental results are unconvincing, with all recent methods worse than baselines and unclear explanations/data details.\n",
      "        - Experimental results are unconvincing; significant methodological issues and lack of explanations undermine trust in findings. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental results are unconvincing; significant methodological issues and lack of explanations undermine trust in findings.\n",
      "        - Experimental results do not improve over state of the art; reduced parameters lead to much lower performance. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental results do not improve over state of the art; reduced parameters lead to much lower performance.\n",
      "        - Experimental results use only small synthetic data and lack comparison to baselines, making practical utility unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental results use only small synthetic data and lack comparison to baselines, making practical utility unclear.\n",
      "        - Experimental results were not easily comparable to prior work, so claims may not be well-supported. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental results were not easily comparable to prior work, so claims may not be well-supported.\n",
      "        - Experiments show mixed results and do not adequately justify the paper's claims. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments show mixed results and do not adequately justify the paper's claims.\n",
      "        - Gap between the universal learning framework and the proposed method, with inconclusive experimental results. (L0, 1)\n",
      "          * Examples:\n",
      "            - Gap between the universal learning framework and the proposed method, with inconclusive experimental results.\n",
      "        - Lack of experimental results showing that the target policy recovers the right policy when dynamics match a source. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of experimental results showing that the target policy recovers the right policy when dynamics match a source.\n",
      "        - Lacks sufficient analysis and experimental results to demonstrate superiority over existing IRL frameworks. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lacks sufficient analysis and experimental results to demonstrate superiority over existing IRL frameworks.\n",
      "        - Limited technical innovations and experimental results. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited technical innovations and experimental results.\n",
      "        - Paper lacks important methodological details, experimental results, and state-of-the-art comparison, making it hard to evaluate. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper lacks important methodological details, experimental results, and state-of-the-art comparison, making it hard to evaluate.\n",
      "        - Results are only presented on one small dataset, limiting the claims and generalizability. (L0, 1)\n",
      "          * Examples:\n",
      "            - Results are only presented on one small dataset, limiting the claims and generalizability.\n",
      "        - The claims are not well supported by the experiments; experimental settings and choices are unconvincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The claims are not well supported by the experiments; experimental settings and choices are unconvincing.\n",
      "        - The paper contains confusing and contradicting statements or claims not supported by experimental results. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper contains confusing and contradicting statements or claims not supported by experimental results.\n",
      "        - The paper's contributions are incremental and lack sufficient experimental results. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper's contributions are incremental and lack sufficient experimental results.\n",
      "        - The reviewer does not believe the extra recoder component meaningfully improves summary quality; experimental results do not support its effectiveness. (L0, 1)\n",
      "          * Examples:\n",
      "            - The reviewer does not believe the extra recoder component meaningfully improves summary quality; experimental results do not support its effectiveness.\n",
      "        - The study's main claims are overstated and not well validated by the experimental results. (L0, 1)\n",
      "          * Examples:\n",
      "            - The study's main claims are overstated and not well validated by the experimental results.\n",
      "        - Uninspired combination of pre-existing ideas with weak and inconclusive experimental results. (L0, 1)\n",
      "          * Examples:\n",
      "            - Uninspired combination of pre-existing ideas with weak and inconclusive experimental results.\n",
      "      - Insufficient or Unconvincing Experiments and Empirical Validation (L1, 3)\n",
      "        - Experiments are unconvincing; there's no time complexity comparison with AutoCross despite acceleration being the main contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments are unconvincing; there's no time complexity comparison with AutoCross despite acceleration being the main contribution.\n",
      "        - Use of kernel density estimate (KDE) in high-dimensional spaces is unconvincing; feasibility and bandwidth selection are not addressed. (L0, 1)\n",
      "          * Examples:\n",
      "            - Use of kernel density estimate (KDE) in high-dimensional spaces is unconvincing; feasibility and bandwidth selection are not addressed.\n",
      "        - Use of the Office dataset, which has label pollution and too few samples per class. (L0, 1)\n",
      "          * Examples:\n",
      "            - Use of the Office dataset, which has label pollution and too few samples per class.\n",
      "      - Insufficient or Unfair Empirical Evaluation and Comparison (L1, 6)\n",
      "        - Insufficient empirical evaluation, especially regarding runtime, accuracy, and comparison to baseline methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - Insufficient empirical evaluation, especially regarding runtime, accuracy, and comparison to baseline methods.\n",
      "        - No comparison with defenses based on input or model randomization. (L0, 1)\n",
      "          * Examples:\n",
      "            - No comparison with defenses based on input or model randomization.\n",
      "        - Performance evaluation is insufficient due to limited and unfair comparisons. (L0, 1)\n",
      "          * Examples:\n",
      "            - Performance evaluation is insufficient due to limited and unfair comparisons.\n",
      "        - Performance gains are unconvincing due to insufficient evaluation and weak novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - Performance gains are unconvincing due to insufficient evaluation and weak novelty.\n",
      "        - The comparison of model and human performance may be unfair since human results are reported on original, unfiltered tasks. (L0, 1)\n",
      "          * Examples:\n",
      "            - The comparison of model and human performance may be unfair since human results are reported on original, unfiltered tasks.\n",
      "        - The dataset (SST) is so small and thus is questionable about the quality of the learned word embeddings and the comparisons. (L0, 1)\n",
      "          * Examples:\n",
      "            - The dataset (SST) is so small and thus is questionable about the quality of the learned word embeddings and the comparisons.\n",
      "      - Unconvincing or Incomplete Experimental Validation (L1, 5)\n",
      "        - The paper does not clearly show that the proposed architectures address multi-scale generation issues; lack of evidence and ablation. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not clearly show that the proposed architectures address multi-scale generation issues; lack of evidence and ablation.\n",
      "        - The paper does not convincingly demonstrate the usefulness of the method for generating justifications to users. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not convincingly demonstrate the usefulness of the method for generating justifications to users.\n",
      "        - The paper does not convincingly show that models use unification/logical reasoning to solve tasks, with key interpretability issues. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not convincingly show that models use unification/logical reasoning to solve tasks, with key interpretability issues.\n",
      "        - The paper does not discuss what to do with the dropped samples (e.g., dropping up to 80%). (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not discuss what to do with the dropped samples (e.g., dropping up to 80%).\n",
      "        - The paper does not offer possible explanations for the unexpected results. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not offer possible explanations for the unexpected results.\n",
      "      - Weak Experimental Validation or Empirical Support (L1, 3)\n",
      "        - Averaging across only 4 seeds is too low given RL variance; shown by high std values. (L0, 1)\n",
      "          * Examples:\n",
      "            - Averaging across only 4 seeds is too low given RL variance; shown by high std values.\n",
      "        - Findings do not seem experimentally well validated. (L0, 1)\n",
      "          * Examples:\n",
      "            - Findings do not seem experimentally well validated.\n",
      "        - Reasoning is intuitive but lacks thorough justification; explanations are not empirically supported. (L0, 1)\n",
      "          * Examples:\n",
      "            - Reasoning is intuitive but lacks thorough justification; explanations are not empirically supported.\n",
      "    - Lack of Real-World Applicability or Evaluation (L2, 8)\n",
      "      - Analysis is limited to a simplified case with a linear discriminator, lacking extension to realistic scenarios. (L1, 1)\n",
      "        * Examples:\n",
      "          - Analysis is limited to a simplified case with a linear discriminator, lacking extension to realistic scenarios.\n",
      "      - Evaluation does not focus enough on real scenarios and lacks uncertainty quantification, making improvements unproven. (L1, 1)\n",
      "        * Examples:\n",
      "          - Evaluation does not focus enough on real scenarios and lacks uncertainty quantification, making improvements unproven.\n",
      "      - It is not clear how to design corresponding efficient structures for different low-level problems; application ability is not clear. (L1, 1)\n",
      "        * Examples:\n",
      "          - It is not clear how to design corresponding efficient structures for different low-level problems; application ability is not clear.\n",
      "      - Motivation is not justified as per-image semantic information increases annotation effort, undermining zero-shot learning's purpose. (L1, 1)\n",
      "        * Examples:\n",
      "          - Motivation is not justified as per-image semantic information increases annotation effort, undermining zero-shot learning's purpose.\n",
      "      - No real application was presented; real-world motivation and examples are lacking. (L1, 1)\n",
      "        * Examples:\n",
      "          - No real application was presented; real-world motivation and examples are lacking.\n",
      "      - The bound has exponential dependence on the number of layers, making it less practical for real-world applications. (L1, 1)\n",
      "        * Examples:\n",
      "          - The bound has exponential dependence on the number of layers, making it less practical for real-world applications.\n",
      "      - The number k of instances must be provided, which is unrealistic for real-world instance detection tasks. (L1, 1)\n",
      "        * Examples:\n",
      "          - The number k of instances must be provided, which is unrealistic for real-world instance detection tasks.\n",
      "      - The proposed environments are too few, too simple, and unrealistic compared to real-world problems. (L1, 1)\n",
      "        * Examples:\n",
      "          - The proposed environments are too few, too simple, and unrealistic compared to real-world problems.\n",
      "    - Limited or Inappropriate Datasets/Benchmarks (L2, 7)\n",
      "      - Experimental evaluation is too narrow, using small datasets; lacks meaningful comparison on larger, appropriate benchmarks. (L1, 1)\n",
      "        * Examples:\n",
      "          - Experimental evaluation is too narrow, using small datasets; lacks meaningful comparison on larger, appropriate benchmarks.\n",
      "      - Experimental evaluation is very limited—only one model, one dataset, small scale, unclear generalization and scalability. (L1, 1)\n",
      "        * Examples:\n",
      "          - Experimental evaluation is very limited—only one model, one dataset, small scale, unclear generalization and scalability.\n",
      "      - Experimental performance is unsatisfactory and lacks convincing improvement on more than one dataset. (L1, 1)\n",
      "        * Examples:\n",
      "          - Experimental performance is unsatisfactory and lacks convincing improvement on more than one dataset.\n",
      "      - Experimental section is weak, especially the MNIST experiment, making it hard to judge the method's effectiveness. (L1, 1)\n",
      "        * Examples:\n",
      "          - Experimental section is weak, especially the MNIST experiment, making it hard to judge the method's effectiveness.\n",
      "      - Insufficient experimental evaluation: small datasets, high variance, only single accuracy reported, limiting method’s effectiveness assessment. (L1, 1)\n",
      "        * Examples:\n",
      "          - Insufficient experimental evaluation: small datasets, high variance, only single accuracy reported, limiting method’s effectiveness assessment.\n",
      "      - The experimental part is weak; results are only on digit datasets and lack broader evaluation. (L1, 1)\n",
      "        * Examples:\n",
      "          - The experimental part is weak; results are only on digit datasets and lack broader evaluation.\n",
      "      - The results do not show much improvement on most datasets, questioning the premise and experimental validation. (L1, 1)\n",
      "        * Examples:\n",
      "          - The results do not show much improvement on most datasets, questioning the premise and experimental validation.\n",
      "    - Methodological and Experimental Clarity/Justification (L2, 15)\n",
      "      - Assumes input distribution is known, which may not hold during training or adaptation, causing issues for auxiliary LUT. (L1, 1)\n",
      "        * Examples:\n",
      "          - Assumes input distribution is known, which may not hold during training or adaptation, causing issues for auxiliary LUT.\n",
      "      - Issues with Hyperparameter Tuning and Implementation Practicality (L1, 6)\n",
      "        - Gap between the formal treatment (Lemma 3.2) and the rather ad hoc practical algorithm depending on two tuning parameters. (L0, 1)\n",
      "          * Examples:\n",
      "            - Gap between the formal treatment (Lemma 3.2) and the rather ad hoc practical algorithm depending on two tuning parameters.\n",
      "        - Introducing extra hyper-parameters may make the algorithm even harder to tune. (L0, 1)\n",
      "          * Examples:\n",
      "            - Introducing extra hyper-parameters may make the algorithm even harder to tune.\n",
      "        - Major concern about tuning new hyperparameters in practice, especially with noisy or corrupted validation sets. (L0, 1)\n",
      "          * Examples:\n",
      "            - Major concern about tuning new hyperparameters in practice, especially with noisy or corrupted validation sets.\n",
      "        - No clear way to choose alpha and beta except hyper-parameter search, which is impractical and risks overfitting. (L0, 1)\n",
      "          * Examples:\n",
      "            - No clear way to choose alpha and beta except hyper-parameter search, which is impractical and risks overfitting.\n",
      "        - The 'mask decay update' workaround for the hard sigmoid issue looks hacky and may not work reliably in practice. (L0, 1)\n",
      "          * Examples:\n",
      "            - The 'mask decay update' workaround for the hard sigmoid issue looks hacky and may not work reliably in practice.\n",
      "        - The algorithm is not well presented; it's unclear how hyperparameters are chosen and overhead details are missing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The algorithm is not well presented; it's unclear how hyperparameters are chosen and overhead details are missing.\n",
      "      - Methodological Clarity or Explanation Issues (L1, 3)\n",
      "        - Concerns about Bayesian inference, specifically the use of Laplace approximation and how it compares to VB or sampling methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - Concerns about Bayesian inference, specifically the use of Laplace approximation and how it compares to VB or sampling methods.\n",
      "        - Lack of use of more sophisticated multi-task learning methods like MetaLearning (MAML), just using task averaging. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of use of more sophisticated multi-task learning methods like MetaLearning (MAML), just using task averaging.\n",
      "        - The method is not clearly explained or rigorously formulated, making it hard to learn from or use. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is not clearly explained or rigorously formulated, making it hard to learn from or use.\n",
      "      - Motivation and Justification Issues (L1, 4)\n",
      "        - Authors do not clearly explain which biologically implausible aspects of backpropagation are addressed; weight-transport issue remains unresolved. (L0, 1)\n",
      "          * Examples:\n",
      "            - Authors do not clearly explain which biologically implausible aspects of backpropagation are addressed; weight-transport issue remains unresolved.\n",
      "        - Motivation for sparse attention is not convincing unless for long sequences. (L0, 1)\n",
      "          * Examples:\n",
      "            - Motivation for sparse attention is not convincing unless for long sequences.\n",
      "        - VILD uses labeled expert demonstrations, which constitutes auxiliary information, contrary to its claimed advantage. (L0, 1)\n",
      "          * Examples:\n",
      "            - VILD uses labeled expert demonstrations, which constitutes auxiliary information, contrary to its claimed advantage.\n",
      "        - iSparse's performance is not as impressive as the Lottery Ticket Hypothesis in terms of weight removal and performance retention. (L0, 1)\n",
      "          * Examples:\n",
      "            - iSparse's performance is not as impressive as the Lottery Ticket Hypothesis in terms of weight removal and performance retention.\n",
      "      - Reliance on solver for supervision limits scalability; getting faster but worse results than solver on small graphs is not significant. (L1, 1)\n",
      "        * Examples:\n",
      "          - Reliance on solver for supervision limits scalability; getting faster but worse results than solver on small graphs is not significant.\n",
      "    - Methodological or Analytical Issues; Technical Errors or Ambiguity (L2, 14)\n",
      "      - Generated faces often have asymmetric (unbalanced) eyes. (L1, 1)\n",
      "        * Examples:\n",
      "          - Generated faces often have asymmetric (unbalanced) eyes.\n",
      "      - Hyperparameter Tuning, Flexibility, and Manual Intervention (L1, 3)\n",
      "        - The method is only semi-automatic since seed learning rate and explore phase need manual tuning for each task. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is only semi-automatic since seed learning rate and explore phase need manual tuning for each task.\n",
      "        - The method lacks flexibility compared to assignment matrices, especially for learning discriminative nodes via backpropagation. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method lacks flexibility compared to assignment matrices, especially for learning discriminative nodes via backpropagation.\n",
      "        - The method still requires several tuned hyperparameters, contradicting the claim of reduced hyperparameter tuning. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method still requires several tuned hyperparameters, contradicting the claim of reduced hyperparameter tuning.\n",
      "      - Invalid or Incorrect Theoretical Results or Derivations (L1, 3)\n",
      "        - A fatal error in the theoretical derivation renders all claims in the paper invalid. (L0, 1)\n",
      "          * Examples:\n",
      "            - A fatal error in the theoretical derivation renders all claims in the paper invalid.\n",
      "        - Central theoretical argument regarding sample complexity is incomplete and not comparable to previous methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - Central theoretical argument regarding sample complexity is incomplete and not comparable to previous methods.\n",
      "        - The proposed theoretical upper bound is not reliable because the true labeling functions do not belong to the hypothesis space H. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed theoretical upper bound is not reliable because the true labeling functions do not belong to the hypothesis space H.\n",
      "      - Rather complex overall objective. (L1, 1)\n",
      "        * Examples:\n",
      "          - Rather complex overall objective.\n",
      "      - The method is wrongly claimed as RL-based; RL definition and state transitions are inadequately specified. (L1, 1)\n",
      "        * Examples:\n",
      "          - The method is wrongly claimed as RL-based; RL definition and state transitions are inadequately specified.\n",
      "      - The method lacks robustness when applied to realistic images with complex or extraneous dynamics. (L1, 1)\n",
      "        * Examples:\n",
      "          - The method lacks robustness when applied to realistic images with complex or extraneous dynamics.\n",
      "      - The method uses vanilla 3D convolutions; it does not account for SO(3) rotation invariance. (L1, 1)\n",
      "        * Examples:\n",
      "          - The method uses vanilla 3D convolutions; it does not account for SO(3) rotation invariance.\n",
      "      - Unclear reasoning behind sharing replay-buffer for the same agent in different teams; deeper discussion needed. (L1, 1)\n",
      "        * Examples:\n",
      "          - Unclear reasoning behind sharing replay-buffer for the same agent in different teams; deeper discussion needed.\n",
      "      - Unnecessary or Suboptimal Analysis/Method Choices (L1, 2)\n",
      "        - The Fourier analysis is unnecessary, leading to suboptimal and potentially misleading presentation of the derivation. (L0, 1)\n",
      "          * Examples:\n",
      "            - The Fourier analysis is unnecessary, leading to suboptimal and potentially misleading presentation of the derivation.\n",
      "        - The analysis in Section 4.3 is unclear due to compressed description and tiny graphs. (L0, 1)\n",
      "          * Examples:\n",
      "            - The analysis in Section 4.3 is unclear due to compressed description and tiny graphs.\n",
      "    - Model Limitations and Problematic Outputs (L2, 3)\n",
      "      - Problematic Model Outputs or Biases (L1, 2)\n",
      "        - The model outputs suffer from redundancy and referential clarity issues, repeating names (as in Figure 1). (L0, 1)\n",
      "          * Examples:\n",
      "            - The model outputs suffer from redundancy and referential clarity issues, repeating names (as in Figure 1).\n",
      "        - The new model may output the same molecule multiple times and favors straight-chain molecules. (L0, 1)\n",
      "          * Examples:\n",
      "            - The new model may output the same molecule multiple times and favors straight-chain molecules.\n",
      "      - The proposed method is not useful against white-box and transfer-based attacks, as it does not change the predictive model. (L1, 1)\n",
      "        * Examples:\n",
      "          - The proposed method is not useful against white-box and transfer-based attacks, as it does not change the predictive model.\n",
      "  - Novelty, Contribution, and Relevance Concerns (L3, 230)\n",
      "    - Insufficient, Unclear, or Overstated Claims and Contributions (L2, 12)\n",
      "      - Incoherent or Unconvincing Argument or Contribution (L1, 2)\n",
      "        - Neither of the two contributions provides a convincing solution to the main question, and the paper lacks a consistent thread. (L0, 1)\n",
      "          * Examples:\n",
      "            - Neither of the two contributions provides a convincing solution to the main question, and the paper lacks a consistent thread.\n",
      "        - The paper lacks a clear main argument and does not compare against relevant baselines for its individual contributions. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks a clear main argument and does not compare against relevant baselines for its individual contributions.\n",
      "      - Misleading or Overstated Claims in Relation to Theory (L1, 2)\n",
      "        - The title and abstract are misleading; paper doesn't introduce a new sampling strategy, only theoretical bounds for existing ones. (L0, 1)\n",
      "          * Examples:\n",
      "            - The title and abstract are misleading; paper doesn't introduce a new sampling strategy, only theoretical bounds for existing ones.\n",
      "        - The title is misleading and the claim of 'unifying' and 'theoretical analysis' is inaccurate. (L0, 1)\n",
      "          * Examples:\n",
      "            - The title is misleading and the claim of 'unifying' and 'theoretical analysis' is inaccurate.\n",
      "      - The interpretability of the sparse extracted feature vector is unconvincing and not clearly better than other methods. (L1, 1)\n",
      "        * Examples:\n",
      "          - The interpretability of the sparse extracted feature vector is unconvincing and not clearly better than other methods.\n",
      "      - The main contributions are not clear compared to other state-of-the-art methods. (L1, 1)\n",
      "        * Examples:\n",
      "          - The main contributions are not clear compared to other state-of-the-art methods.\n",
      "      - Unsubstantiated, Overstated, or Unconvincing Claims (L1, 6)\n",
      "        - Claims (e.g. applicability to all policy gradient methods, noise removal) are unconvincing due to insufficient empirical evidence. (L0, 1)\n",
      "          * Examples:\n",
      "            - Claims (e.g. applicability to all policy gradient methods, noise removal) are unconvincing due to insufficient empirical evidence.\n",
      "        - Lack of genuine novelty and unsubstantiated claims about stability guarantees; current approach does not convincingly demonstrate true stability. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of genuine novelty and unsubstantiated claims about stability guarantees; current approach does not convincingly demonstrate true stability.\n",
      "        - Lacks discussion of potential impact and preliminary empirical studies. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lacks discussion of potential impact and preliminary empirical studies.\n",
      "        - The evidence in the current version is not sufficient to support all the claims; work is not generalizable to other baselines, limiting impact. (L0, 1)\n",
      "          * Examples:\n",
      "            - The evidence in the current version is not sufficient to support all the claims; work is not generalizable to other baselines, limiting impact.\n",
      "        - The general motivation or justification, e.g., the limiting cycle argument or visualization, is unconvincing or over-claimed. (L0, 1)\n",
      "          * Examples:\n",
      "            - The general motivation or justification, e.g., the limiting cycle argument or visualization, is unconvincing or over-claimed.\n",
      "        - Very misleading claims regarding establishing a new state of the art due to weak comparisons with inferior baselines. (L0, 1)\n",
      "          * Examples:\n",
      "            - Very misleading claims regarding establishing a new state of the art due to weak comparisons with inferior baselines.\n",
      "    - Lack of Novelty or Insufficient Contribution over Prior Work (L2, 158)\n",
      "      - Insufficient Novelty or Contribution over Prior Work (L1, 5)\n",
      "        - Difference from previous work ([Lian et al., 2017]) is not significant, especially in algorithm design and theory. (L0, 1)\n",
      "          * Examples:\n",
      "            - Difference from previous work ([Lian et al., 2017]) is not significant, especially in algorithm design and theory.\n",
      "        - The name 'Mutual Information Machine (MIM)' is inappropriate for the proposed approach. (L0, 1)\n",
      "          * Examples:\n",
      "            - The name 'Mutual Information Machine (MIM)' is inappropriate for the proposed approach.\n",
      "        - The novelty is not strong; the proposed approach does not convincingly outperform a simpler baseline. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty is not strong; the proposed approach does not convincingly outperform a simpler baseline.\n",
      "        - The proposed approach is simplistic in how they treat surprise, and therefore illustrative, but probably not practical. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed approach is simplistic in how they treat surprise, and therefore illustrative, but probably not practical.\n",
      "        - The proposed approximation technique is rather a heuristic and only addresses the issue at a superficial level. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed approximation technique is rather a heuristic and only addresses the issue at a superficial level.\n",
      "      - Insufficient Technical Novelty or Contribution Over Prior Work (L1, 14)\n",
      "        - Main contribution is identifying a problem, not providing a significant technical advance. (L0, 1)\n",
      "          * Examples:\n",
      "            - Main contribution is identifying a problem, not providing a significant technical advance.\n",
      "        - Marginal technical contribution—main change is swapping SGD for Adam in UNet, not a significant methodological advance. (L0, 1)\n",
      "          * Examples:\n",
      "            - Marginal technical contribution—main change is swapping SGD for Adam in UNet, not a significant methodological advance.\n",
      "        - No novelty in how the knowledge graph is built; thus, lacks technical contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - No novelty in how the knowledge graph is built; thus, lacks technical contribution.\n",
      "        - Reviewer was unable to assess technical contribution's novelty or comparison to prior RL work. (L0, 1)\n",
      "          * Examples:\n",
      "            - Reviewer was unable to assess technical contribution's novelty or comparison to prior RL work.\n",
      "        - Technical contribution is lean; neither the multi-agent nor hierarchical aspects are novel and the combination is straightforward. (L0, 1)\n",
      "          * Examples:\n",
      "            - Technical contribution is lean; neither the multi-agent nor hierarchical aspects are novel and the combination is straightforward.\n",
      "        - The contribution is outdated and lacks significant improvement/technical novelty over prior art. (L0, 1)\n",
      "          * Examples:\n",
      "            - The contribution is outdated and lacks significant improvement/technical novelty over prior art.\n",
      "        - The paper falls short in terms of technical contributions, mainly borrowing components from previous work. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper falls short in terms of technical contributions, mainly borrowing components from previous work.\n",
      "        - The paper lacks novelty and technical contribution, as it mainly combines existing techniques without clear unsupervised learning value. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks novelty and technical contribution, as it mainly combines existing techniques without clear unsupervised learning value.\n",
      "        - The paper's technical contribution is incremental and scattered, lacking a clear major contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper's technical contribution is incremental and scattered, lacking a clear major contribution.\n",
      "        - The technical contribution is limited and the work only provides an incremental improvement (1%) on an easy classification task. (L0, 1)\n",
      "          * Examples:\n",
      "            - The technical contribution is limited and the work only provides an incremental improvement (1%) on an easy classification task.\n",
      "        - The technical contribution is very incremental and experiment results are not strong enough. (L0, 1)\n",
      "          * Examples:\n",
      "            - The technical contribution is very incremental and experiment results are not strong enough.\n",
      "        - The technical contribution of the paper is rather small, and also the depth of analysis could be improved. (L0, 1)\n",
      "          * Examples:\n",
      "            - The technical contribution of the paper is rather small, and also the depth of analysis could be improved.\n",
      "        - The technical contribution of this paper is pretty low. (L0, 1)\n",
      "          * Examples:\n",
      "            - The technical contribution of this paper is pretty low.\n",
      "        - The technical novelty is only sufficient and not highly innovative. (L0, 1)\n",
      "          * Examples:\n",
      "            - The technical novelty is only sufficient and not highly innovative.\n",
      "      - Lack of Novelty and Insufficient Differentiation from Prior Work (L1, 18)\n",
      "        - Insufficient differentiation and comparison with PBT-MARL; unclear advantages over prior work. (L0, 1)\n",
      "          * Examples:\n",
      "            - Insufficient differentiation and comparison with PBT-MARL; unclear advantages over prior work.\n",
      "        - Key findings (F-Principle) are not novel; similar results have already appeared in prior work. (L0, 1)\n",
      "          * Examples:\n",
      "            - Key findings (F-Principle) are not novel; similar results have already appeared in prior work.\n",
      "        - Lack of originality and significance; most results are known and not sufficiently distinguished from prior work. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of originality and significance; most results are known and not sufficiently distinguished from prior work.\n",
      "        - Lacks originality; prior work has already addressed provable convergence in more detail and with greater rigor. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lacks originality; prior work has already addressed provable convergence in more detail and with greater rigor.\n",
      "        - Main concern is about the novelty/significance; similar arguments and results exist in prior work. (L0, 1)\n",
      "          * Examples:\n",
      "            - Main concern is about the novelty/significance; similar arguments and results exist in prior work.\n",
      "        - Most algorithmic ideas are present in prior work; contribution not novel enough. (L0, 1)\n",
      "          * Examples:\n",
      "            - Most algorithmic ideas are present in prior work; contribution not novel enough.\n",
      "        - No observable technical novelty in the proposed model; double blind adaptation not significantly different from prior work. (L0, 1)\n",
      "          * Examples:\n",
      "            - No observable technical novelty in the proposed model; double blind adaptation not significantly different from prior work.\n",
      "        - No significant theoretical innovation; approach is very similar to prior work by Kidzinski & Hastie, 2018. (L0, 1)\n",
      "          * Examples:\n",
      "            - No significant theoretical innovation; approach is very similar to prior work by Kidzinski & Hastie, 2018.\n",
      "        - Section 2.3 appears copied from prior work (Chow et al. 2018), with unclear claims of originality. (L0, 1)\n",
      "          * Examples:\n",
      "            - Section 2.3 appears copied from prior work (Chow et al. 2018), with unclear claims of originality.\n",
      "        - The convergence results are not state-of-the-art compared to prior work. (L0, 1)\n",
      "          * Examples:\n",
      "            - The convergence results are not state-of-the-art compared to prior work.\n",
      "        - The method is rather incremental, with limited novelty and significance compared to prior work (DARTS and [4]). (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is rather incremental, with limited novelty and significance compared to prior work (DARTS and [4]).\n",
      "        - The novelty is limited; loss functions are not new and have already been used in prior work. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty is limited; loss functions are not new and have already been used in prior work.\n",
      "        - The novelty of the separable basis decomposition is not convincing; differences with prior work are unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty of the separable basis decomposition is not convincing; differences with prior work are unclear.\n",
      "        - The paper blurs the distinction between prior work and new contribution; what is novel is not clearly stated. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper blurs the distinction between prior work and new contribution; what is novel is not clearly stated.\n",
      "        - The proposed growing strategy is already known from prior work; novelty and additional value are unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed growing strategy is already known from prior work; novelty and additional value are unclear.\n",
      "        - The proposed method is not highly effective and its contribution is marginal, especially in light of prior work [1]. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method is not highly effective and its contribution is marginal, especially in light of prior work [1].\n",
      "        - This work is incremental; prior work has shown similar results and theory (Wang et al.). (L0, 1)\n",
      "          * Examples:\n",
      "            - This work is incremental; prior work has shown similar results and theory (Wang et al.).\n",
      "        - Unclear how the work substantially deviates from previous work; novelty is questionable. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear how the work substantially deviates from previous work; novelty is questionable.\n",
      "      - Lack of Novelty or Incremental Contribution (L1, 40)\n",
      "        - All implications are confirming already known things in different (path-based) words; the impact or novelty is rather small. (L0, 1)\n",
      "          * Examples:\n",
      "            - All implications are confirming already known things in different (path-based) words; the impact or novelty is rather small.\n",
      "        - Components are taken straight from existing work and just combined, lacking novel integration or system-specific insights. (L0, 1)\n",
      "          * Examples:\n",
      "            - Components are taken straight from existing work and just combined, lacking novel integration or system-specific insights.\n",
      "        - Concerns about novelty; the work seems a straightforward application of known theorems with unclear new discoveries. (L0, 1)\n",
      "          * Examples:\n",
      "            - Concerns about novelty; the work seems a straightforward application of known theorems with unclear new discoveries.\n",
      "        - Concerns about the novelty of Theorems 1 and 2; key results are known or uncited. (L0, 1)\n",
      "          * Examples:\n",
      "            - Concerns about the novelty of Theorems 1 and 2; key results are known or uncited.\n",
      "        - Insufficient novelty; similar methods have been published. (L0, 1)\n",
      "          * Examples:\n",
      "            - Insufficient novelty; similar methods have been published.\n",
      "        - Lack of novelty in the proposed method. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of novelty in the proposed method.\n",
      "        - Limited novelty and insufficient contribution compared to previous work (Diffpool). (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited novelty and insufficient contribution compared to previous work (Diffpool).\n",
      "        - Limited technical novelty; approach is a simplified version of existing differentiable NAS applied to a similar search space. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited technical novelty; approach is a simplified version of existing differentiable NAS applied to a similar search space.\n",
      "        - Low novelty compared to recent literature. (L0, 1)\n",
      "          * Examples:\n",
      "            - Low novelty compared to recent literature.\n",
      "        - Many similar approaches are missing in related work; novelty is limited. (L0, 1)\n",
      "          * Examples:\n",
      "            - Many similar approaches are missing in related work; novelty is limited.\n",
      "        - None of the claimed contributions is novel or significant; the method is a combination of existing ideas. (L0, 1)\n",
      "          * Examples:\n",
      "            - None of the claimed contributions is novel or significant; the method is a combination of existing ideas.\n",
      "        - Not enough novelty; method only minimally extends previous work with one extra parameter. (L0, 1)\n",
      "          * Examples:\n",
      "            - Not enough novelty; method only minimally extends previous work with one extra parameter.\n",
      "        - Novelty concern: unsupervised concept extraction for interpretability was previously introduced, not properly discussed or contrasted. (L0, 1)\n",
      "          * Examples:\n",
      "            - Novelty concern: unsupervised concept extraction for interpretability was previously introduced, not properly discussed or contrasted.\n",
      "        - Only tested on naive datasets (MNIST, Fashion MNIST); unclear performance on complex datasets like ImageNet. (L0, 1)\n",
      "          * Examples:\n",
      "            - Only tested on naive datasets (MNIST, Fashion MNIST); unclear performance on complex datasets like ImageNet.\n",
      "        - Paper lacks novelty; contributions are incremental and similar to existing methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper lacks novelty; contributions are incremental and similar to existing methods.\n",
      "        - The SAIL method is considered ad-hoc for addressing reward bias and may not generalize across environments. (L0, 1)\n",
      "          * Examples:\n",
      "            - The SAIL method is considered ad-hoc for addressing reward bias and may not generalize across environments.\n",
      "        - The approach is a trivial extension of previous work (DeepCCA) with limited novelty and insufficient motivation. (L0, 1)\n",
      "          * Examples:\n",
      "            - The approach is a trivial extension of previous work (DeepCCA) with limited novelty and insufficient motivation.\n",
      "        - The approach is very similar to existing semi-supervised methods, with minimal novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The approach is very similar to existing semi-supervised methods, with minimal novelty.\n",
      "        - The architecture is essentially a traditional end-to-end seq2seq model with no fundamental novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The architecture is essentially a traditional end-to-end seq2seq model with no fundamental novelty.\n",
      "        - The contribution is incremental, being very similar to previous work (dlADMM, Wang et al. 2019). (L0, 1)\n",
      "          * Examples:\n",
      "            - The contribution is incremental, being very similar to previous work (dlADMM, Wang et al. 2019).\n",
      "        - The framework lacks significant contributions worthy of publication at top conferences. (L0, 1)\n",
      "          * Examples:\n",
      "            - The framework lacks significant contributions worthy of publication at top conferences.\n",
      "        - The main idea is similar to existing methods ([1-5]), lacking novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main idea is similar to existing methods ([1-5]), lacking novelty.\n",
      "        - The manuscript has limited novelty compared to previous work (BT). (L0, 1)\n",
      "          * Examples:\n",
      "            - The manuscript has limited novelty compared to previous work (BT).\n",
      "        - The method is not novel, merely an extension of known ideas. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is not novel, merely an extension of known ideas.\n",
      "        - The method is not novel; adding noise for regularization is a common technique. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is not novel; adding noise for regularization is a common technique.\n",
      "        - The method is not novel; it is a standard PMI application repeatedly presented in prior literature. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is not novel; it is a standard PMI application repeatedly presented in prior literature.\n",
      "        - The method only adds task-specific heuristic human priors, which are similar to common regularization and may be limited. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method only adds task-specific heuristic human priors, which are similar to common regularization and may be limited.\n",
      "        - The novelty is insufficient, with the proposed ideas being simple or already existing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty is insufficient, with the proposed ideas being simple or already existing.\n",
      "        - The novelty is very limited. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty is very limited.\n",
      "        - The novelty of the proposed method is marginal. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty of the proposed method is marginal.\n",
      "        - The novelty of the work is limited. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty of the work is limited.\n",
      "        - The novelty of the work is limited; the paper should justify its approach over low-dimensional embedding-based methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty of the work is limited; the paper should justify its approach over low-dimensional embedding-based methods.\n",
      "        - The novelty of the work is weak; similar weight reduction methods have been previously explored, and the contributions are not well tied together. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty of the work is weak; similar weight reduction methods have been previously explored, and the contributions are not well tied together.\n",
      "        - The novelty of this work is not enough. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty of this work is not enough.\n",
      "        - The paper is mis-targeted for ICLR; it does not discuss learning representations or deep learning. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is mis-targeted for ICLR; it does not discuss learning representations or deep learning.\n",
      "        - The paper's main result may not be surprising since the number of representable functions might match the training set, diminishing novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper's main result may not be surprising since the number of representable functions might match the training set, diminishing novelty.\n",
      "        - The proposed method is very similar to existing methods, with limited novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method is very similar to existing methods, with limited novelty.\n",
      "        - The reviewer is not excited about the technical contribution of the work. (L0, 1)\n",
      "          * Examples:\n",
      "            - The reviewer is not excited about the technical contribution of the work.\n",
      "        - The technical approach is too outdated and obvious, not at ICLR level. (L0, 1)\n",
      "          * Examples:\n",
      "            - The technical approach is too outdated and obvious, not at ICLR level.\n",
      "        - Theoretical contribution overlaps with previous work ([Li et al. 2019]) and offers little novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - Theoretical contribution overlaps with previous work ([Li et al. 2019]) and offers little novelty.\n",
      "      - Lack of Novelty or Limited Contribution (L1, 2)\n",
      "        - It is not surprising that smaller models outperform BERT-based models in low-resource cases. (L0, 1)\n",
      "          * Examples:\n",
      "            - It is not surprising that smaller models outperform BERT-based models in low-resource cases.\n",
      "        - The novelty of using learned modules across domains is unclear; it may resemble traditional feature engineering. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty of using learned modules across domains is unclear; it may resemble traditional feature engineering.\n",
      "      - Lack of Novelty or Marginal Advancement Over Prior Work (L1, 4)\n",
      "        - Approach does not provide significant benefits over prior methods (e.g., Group Normalization) in performance or complexity. (L0, 1)\n",
      "          * Examples:\n",
      "            - Approach does not provide significant benefits over prior methods (e.g., Group Normalization) in performance or complexity.\n",
      "        - Does not produce a huge delta in understanding compared to Gidaris et al. (2019). (L0, 1)\n",
      "          * Examples:\n",
      "            - Does not produce a huge delta in understanding compared to Gidaris et al. (2019).\n",
      "        - The idea does not seem different from already existing ones; only minor innovation. (L0, 1)\n",
      "          * Examples:\n",
      "            - The idea does not seem different from already existing ones; only minor innovation.\n",
      "        - The solution is trivial and does not improve upon or compare with existing domain adaptation methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - The solution is trivial and does not improve upon or compare with existing domain adaptation methods.\n",
      "      - Lack of Novelty or Originality (L1, 21)\n",
      "        - Applying the path-based encoding to a new task without significant adaptation or analysis is not technically novel. (L0, 1)\n",
      "          * Examples:\n",
      "            - Applying the path-based encoding to a new task without significant adaptation or analysis is not technically novel.\n",
      "        - Concerns about the novelty, especially given the simple environments and blurred distinction from standard neural networks. (L0, 1)\n",
      "          * Examples:\n",
      "            - Concerns about the novelty, especially given the simple environments and blurred distinction from standard neural networks.\n",
      "        - Lack of demonstrated novelty and significance compared to existing literature, especially missing key citations and discussion. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of demonstrated novelty and significance compared to existing literature, especially missing key citations and discussion.\n",
      "        - Lack of innovation and insufficient differentiation from Luo et al. 2018. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of innovation and insufficient differentiation from Luo et al. 2018.\n",
      "        - Lack of insight and innovation, especially in the loss function compared to InfoGAN. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of insight and innovation, especially in the loss function compared to InfoGAN.\n",
      "        - Lack of novelty and originality; method is a simple combination of existing clustering and few-shot learning methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of novelty and originality; method is a simple combination of existing clustering and few-shot learning methods.\n",
      "        - Lack of novelty in methodology and empirical value; pre-LN approach is already widely used and documented. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of novelty in methodology and empirical value; pre-LN approach is already widely used and documented.\n",
      "        - Lack of novelty in the proposed frameworks. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of novelty in the proposed frameworks.\n",
      "        - Lack of novelty in the proposed metric; based on unrealistic ideal hardware assumptions. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of novelty in the proposed metric; based on unrealistic ideal hardware assumptions.\n",
      "        - Lack of novelty—most components (architecture and objectives) are already proposed in prior works. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of novelty—most components (architecture and objectives) are already proposed in prior works.\n",
      "        - Needing new environment variations for new skills is a step backwards compared to prior methods like DIAYN. (L0, 1)\n",
      "          * Examples:\n",
      "            - Needing new environment variations for new skills is a step backwards compared to prior methods like DIAYN.\n",
      "        - No novelty in the approach. (L0, 1)\n",
      "          * Examples:\n",
      "            - No novelty in the approach.\n",
      "        - Originality of frame concatenation is limited, as it's standard in other domains. (L0, 1)\n",
      "          * Examples:\n",
      "            - Originality of frame concatenation is limited, as it's standard in other domains.\n",
      "        - Significant technical flaws and lack of novelty; approach is essentially identical to BatchNorm and conclusions are unsupported. (L0, 1)\n",
      "          * Examples:\n",
      "            - Significant technical flaws and lack of novelty; approach is essentially identical to BatchNorm and conclusions are unsupported.\n",
      "        - The algorithm is quite reminiscent of Elastic Averaging SGD (EASGD), differing only in how the moving-average is computed. (L0, 1)\n",
      "          * Examples:\n",
      "            - The algorithm is quite reminiscent of Elastic Averaging SGD (EASGD), differing only in how the moving-average is computed.\n",
      "        - The approach is hardly novel and not well motivated: predict transitions then labels is not new or supported. (L0, 1)\n",
      "          * Examples:\n",
      "            - The approach is hardly novel and not well motivated: predict transitions then labels is not new or supported.\n",
      "        - The approach is just an extension of prior benchmarks/applications ([1] and [2]) with no novel ideas for object detection. (L0, 1)\n",
      "          * Examples:\n",
      "            - The approach is just an extension of prior benchmarks/applications ([1] and [2]) with no novel ideas for object detection.\n",
      "        - The framework is very common and lacks new insights about the problem. (L0, 1)\n",
      "          * Examples:\n",
      "            - The framework is very common and lacks new insights about the problem.\n",
      "        - The method is an ensemble of existing approaches; lack of novelty and general lessons. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is an ensemble of existing approaches; lack of novelty and general lessons.\n",
      "        - The network architecture is only an incremental variation of standard MLP. (L0, 1)\n",
      "          * Examples:\n",
      "            - The network architecture is only an incremental variation of standard MLP.\n",
      "        - The proposed algorithms lack novelty and offer only marginal improvements compared to existing methods like DoReFa and PACT. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed algorithms lack novelty and offer only marginal improvements compared to existing methods like DoReFa and PACT.\n",
      "      - Lack of Novelty or Originality Compared to Prior Work (L1, 29)\n",
      "        - Concern about the novelty of the ideas, describing them as straightforward after considering recent related work. (L0, 1)\n",
      "          * Examples:\n",
      "            - Concern about the novelty of the ideas, describing them as straightforward after considering recent related work.\n",
      "        - Concern with the novelty of the proposed method. (L0, 1)\n",
      "          * Examples:\n",
      "            - Concern with the novelty of the proposed method.\n",
      "        - Insufficient contributions; lacks convincing reasons for model design and novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - Insufficient contributions; lacks convincing reasons for model design and novelty.\n",
      "        - Largely insufficient literature review; paper overstates novelty and misses key related work and communities. (L0, 1)\n",
      "          * Examples:\n",
      "            - Largely insufficient literature review; paper overstates novelty and misses key related work and communities.\n",
      "        - Main weakness is the novelty / proper connection to self-supervised learning work. (L0, 1)\n",
      "          * Examples:\n",
      "            - Main weakness is the novelty / proper connection to self-supervised learning work.\n",
      "        - No novelty beyond combining existing state-of-the-art models; lacks original contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - No novelty beyond combining existing state-of-the-art models; lacks original contribution.\n",
      "        - Technical novelty is limited compared to closely related works (Transformer and SSE). (L0, 1)\n",
      "          * Examples:\n",
      "            - Technical novelty is limited compared to closely related works (Transformer and SSE).\n",
      "        - The main novelty and practical usefulness of the considered class is unclear and not easy to see. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main novelty and practical usefulness of the considered class is unclear and not easy to see.\n",
      "        - The main theoretical result is a trivial derivation based on existing work and lacks substantive novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main theoretical result is a trivial derivation based on existing work and lacks substantive novelty.\n",
      "        - The method lacks novelty and does not adequately acknowledge prior work on modeling treatment effects in disease progression. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method lacks novelty and does not adequately acknowledge prior work on modeling treatment effects in disease progression.\n",
      "        - The method lacks novelty and is just a simple combination of dictionary learning and classification. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method lacks novelty and is just a simple combination of dictionary learning and classification.\n",
      "        - The method only combines existing techniques and lacks novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method only combines existing techniques and lacks novelty.\n",
      "        - The novelty and rigorousness of the paper are not convincing; much content closely resembles prior works. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty and rigorousness of the paper are not convincing; much content closely resembles prior works.\n",
      "        - The novelty is not clear; methods seem already used in previous works. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty is not clear; methods seem already used in previous works.\n",
      "        - The paper's contribution is limited and lacks a clear explanation for why the proposed method works. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper's contribution is limited and lacks a clear explanation for why the proposed method works.\n",
      "        - The proposed method lacks novelty and is merely an easy combination of existing methods instead of solving previous limitations. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method lacks novelty and is merely an easy combination of existing methods instead of solving previous limitations.\n",
      "        - The proposed method lacks novelty and is not well evaluated against other representation learning techniques. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method lacks novelty and is not well evaluated against other representation learning techniques.\n",
      "        - The proposed method lacks novelty and results are not convincing enough. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method lacks novelty and results are not convincing enough.\n",
      "        - The proposed method lacks novelty; it is just a combination of AdamW and LARS. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method lacks novelty; it is just a combination of AdamW and LARS.\n",
      "        - The proposed work does not add significant insight or novelty in solving the problem. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed work does not add significant insight or novelty in solving the problem.\n",
      "        - The significance and novelty of the main contribution are unclear; modifications appear previously used. (L0, 1)\n",
      "          * Examples:\n",
      "            - The significance and novelty of the main contribution are unclear; modifications appear previously used.\n",
      "        - The work lacks novelty due to similarity with VIBNet; max-entropy term derivation is unclear and needs major revision. (L0, 1)\n",
      "          * Examples:\n",
      "            - The work lacks novelty due to similarity with VIBNet; max-entropy term derivation is unclear and needs major revision.\n",
      "        - Unclear how the work addresses why DIP works well and the essentiality of convolution for image reconstruction. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear how the work addresses why DIP works well and the essentiality of convolution for image reconstruction.\n",
      "        - Unclear novelty and insufficient citation of prior related work undermine the contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear novelty and insufficient citation of prior related work undermine the contribution.\n",
      "        - Unclear novelty and significance; unclear what technical challenges or new proof techniques are introduced. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear novelty and significance; unclear what technical challenges or new proof techniques are introduced.\n",
      "        - Unclear novelty compared to Geirhos et al 2019; lacks detection-specific contributions. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear novelty compared to Geirhos et al 2019; lacks detection-specific contributions.\n",
      "        - Unclear novelty compared to existing DP-preserving algorithms with provable robustness. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear novelty compared to existing DP-preserving algorithms with provable robustness.\n",
      "        - Unclear novelty of retrieval model and metric scaling; insufficient comparison to existing alternatives. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear novelty of retrieval model and metric scaling; insufficient comparison to existing alternatives.\n",
      "        - Unclear what is contributed beyond closely related existing works; lacks novelty over prior studies. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear what is contributed beyond closely related existing works; lacks novelty over prior studies.\n",
      "      - Lack of Novelty or Substantive Methodological Contribution (L1, 13)\n",
      "        - Limited methodological novelty; methods are existing with little or no modification for the new domain. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited methodological novelty; methods are existing with little or no modification for the new domain.\n",
      "        - Limited novelty due to combining existing techniques and restricted, synthetic-domain experiments—unclear real-world impact. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited novelty due to combining existing techniques and restricted, synthetic-domain experiments—unclear real-world impact.\n",
      "        - Limited novelty; little extension beyond existing methods (SASREC). (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited novelty; little extension beyond existing methods (SASREC).\n",
      "        - Results are unsurprising; methods applied as-is from existing work without modification or new contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - Results are unsurprising; methods applied as-is from existing work without modification or new contribution.\n",
      "        - Robustifying the discriminator is not new; need more clarification on the novelty of the method. (L0, 1)\n",
      "          * Examples:\n",
      "            - Robustifying the discriminator is not new; need more clarification on the novelty of the method.\n",
      "        - Techniques are limited to object counting, derived in an ad-hoc way with little theoretical backing and lacking novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - Techniques are limited to object counting, derived in an ad-hoc way with little theoretical backing and lacking novelty.\n",
      "        - The construction appears equivalent to existing steerable/gauge equivariant neural nets, lacking demonstrated novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The construction appears equivalent to existing steerable/gauge equivariant neural nets, lacking demonstrated novelty.\n",
      "        - The effect of regularizing salient features is marginal; proposed method gives similar or worse results than existing methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - The effect of regularizing salient features is marginal; proposed method gives similar or worse results than existing methods.\n",
      "        - The extension of Neural ODEs to graphs is too straightforward and lacks sufficient novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The extension of Neural ODEs to graphs is too straightforward and lacks sufficient novelty.\n",
      "        - The main negative comment is a lack of novelty compared to existing tensor decomposition-based CNN compression methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main negative comment is a lack of novelty compared to existing tensor decomposition-based CNN compression methods.\n",
      "        - The main theorem is an extension of existing methods, so theoretical novelty is limited. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main theorem is an extension of existing methods, so theoretical novelty is limited.\n",
      "        - The methods are conventional and there is no novelty from the algorithmic perspective. (L0, 1)\n",
      "          * Examples:\n",
      "            - The methods are conventional and there is no novelty from the algorithmic perspective.\n",
      "        - The proposed neural network model is not a sufficient contribution and lacks comparison to existing BO methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed neural network model is not a sufficient contribution and lacks comparison to existing BO methods.\n",
      "      - Lack of Novelty, Insight, or Motivation (L1, 8)\n",
      "        - The insights claimed in the paper are not especially novel; most results are already known. (L0, 1)\n",
      "          * Examples:\n",
      "            - The insights claimed in the paper are not especially novel; most results are already known.\n",
      "        - The novelty and practical takeaways of the paper are unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty and practical takeaways of the paper are unclear.\n",
      "        - The paper at its current state does not contain sufficient message to stand as a full paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper at its current state does not contain sufficient message to stand as a full paper.\n",
      "        - The paper brings nothing novel, is largely incremental, and its findings are already well-known and trivial. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper brings nothing novel, is largely incremental, and its findings are already well-known and trivial.\n",
      "        - The paper is not well motivated because combining data augmentation and explicit regularization is already known. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is not well motivated because combining data augmentation and explicit regularization is already known.\n",
      "        - The paper is not well motivated or sufficiently distinguished from existing robust MDP and multi-agent RL literature. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is not well motivated or sufficiently distinguished from existing robust MDP and multi-agent RL literature.\n",
      "        - There is no new finding in this paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - There is no new finding in this paper.\n",
      "        - Unclear what new insight or value the reader should learn from the paper; claims are unsurprising. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear what new insight or value the reader should learn from the paper; claims are unsurprising.\n",
      "      - Lack of Novelty, Marginal Contributions, or No Advantage over Prior Work (L1, 4)\n",
      "        - Improvement over existing methods is marginal and within error margins; no evidence RotationOut reduces co-adaptation or that this matters. (L0, 1)\n",
      "          * Examples:\n",
      "            - Improvement over existing methods is marginal and within error margins; no evidence RotationOut reduces co-adaptation or that this matters.\n",
      "        - The approach is considered merely a 'clever' version of gradient masking, failing to provide true robustness. (L0, 1)\n",
      "          * Examples:\n",
      "            - The approach is considered merely a 'clever' version of gradient masking, failing to provide true robustness.\n",
      "        - The overall idea is not novel; no clear evidence it's better than existing approaches such as Nvidia’s apex. (L0, 1)\n",
      "          * Examples:\n",
      "            - The overall idea is not novel; no clear evidence it's better than existing approaches such as Nvidia’s apex.\n",
      "        - Work does not position well with existing literature; motivation and impact are not very clear; contributions are marginal. (L0, 1)\n",
      "          * Examples:\n",
      "            - Work does not position well with existing literature; motivation and impact are not very clear; contributions are marginal.\n",
      "    - Novelty and Contribution (L2, 12)\n",
      "      - Insufficient Novelty or Contribution (L1, 5)\n",
      "        - The improvement is minor; the contribution of one extra feature does not warrant a top conference paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - The improvement is minor; the contribution of one extra feature does not warrant a top conference paper.\n",
      "        - The paper does not clearly outperform or have advantages over prior works, making its contribution unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not clearly outperform or have advantages over prior works, making its contribution unclear.\n",
      "        - The paper does not present a novel solution method or empirically compare existing methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not present a novel solution method or empirically compare existing methods.\n",
      "        - The paper does not propose any novel method, which limits its suitability for the conference. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not propose any novel method, which limits its suitability for the conference.\n",
      "        - The paper does not provide novel insights nor state-of-the-art results on CIFAR10, limiting its contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not provide novel insights nor state-of-the-art results on CIFAR10, limiting its contribution.\n",
      "      - Lack of Novelty or Insufficient Advancement Over Existing Work (L1, 3)\n",
      "        - Ideas are not novel enough or thoroughly demonstrated to justify acceptance. (L0, 1)\n",
      "          * Examples:\n",
      "            - Ideas are not novel enough or thoroughly demonstrated to justify acceptance.\n",
      "        - The idea is not novel; similar approaches and datasets already exist. (L0, 1)\n",
      "          * Examples:\n",
      "            - The idea is not novel; similar approaches and datasets already exist.\n",
      "        - The methods are not novel enough; improvements are not significant. (L0, 1)\n",
      "          * Examples:\n",
      "            - The methods are not novel enough; improvements are not significant.\n",
      "      - Reviewer cannot evaluate novelty of the theoretical contributions due to lack of expertise. (L1, 1)\n",
      "        * Examples:\n",
      "          - Reviewer cannot evaluate novelty of the theoretical contributions due to lack of expertise.\n",
      "      - Significance and Practical Impact (L1, 3)\n",
      "        - Concerns about generalization to other tasks and inadequate justification for the ping-pong loss. (L0, 1)\n",
      "          * Examples:\n",
      "            - Concerns about generalization to other tasks and inadequate justification for the ping-pong loss.\n",
      "        - Scalable communication will require trading off structural priors and learned representations; this challenge is not fully solved. (L0, 1)\n",
      "          * Examples:\n",
      "            - Scalable communication will require trading off structural priors and learned representations; this challenge is not fully solved.\n",
      "        - The incremental classification setting is unrealistic, as it assumes task boundaries and uses fixed, pretrained feature extractors, thus avoiding rather than solving the continual learning problem. (L0, 1)\n",
      "          * Examples:\n",
      "            - The incremental classification setting is unrealistic, as it assumes task boundaries and uses fixed, pretrained feature extractors, thus avoiding rather than solving the continual learning problem.\n",
      "    - Novelty and Relevance Issues (L2, 43)\n",
      "      - Lack of Novelty or Insight/Naive Combination (L1, 2)\n",
      "        - No new method is introduced; only sampling of fixed neighbors is used for constant-time approximation. (L0, 1)\n",
      "          * Examples:\n",
      "            - No new method is introduced; only sampling of fixed neighbors is used for constant-time approximation.\n",
      "        - The method is a naive combination of previous works, lacks insight, and requires same state/action spaces for source/target. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is a naive combination of previous works, lacks insight, and requires same state/action spaces for source/target.\n",
      "      - Lack of Novelty or Original Contribution (L1, 13)\n",
      "        - Not a research paper; lacks significant research contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - Not a research paper; lacks significant research contribution.\n",
      "        - Paper lacks novelty and further evaluation to be suitable for ICLR. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper lacks novelty and further evaluation to be suitable for ICLR.\n",
      "        - Paper lacks sufficient novelty, as it stops at image registration, an existing model. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper lacks sufficient novelty, as it stops at image registration, an existing model.\n",
      "        - The paper feels sparse in contributions; lacks sufficient discussion/analysis of parameterizations or treatment of more binary-attribute analogy types. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper feels sparse in contributions; lacks sufficient discussion/analysis of parameterizations or treatment of more binary-attribute analogy types.\n",
      "        - The paper is interesting from an engineering perspective but lacks novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is interesting from an engineering perspective but lacks novelty.\n",
      "        - The paper lacks algorithmic breakthrough or new invention. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks algorithmic breakthrough or new invention.\n",
      "        - The paper lacks enough novel machine learning contributions for acceptance. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks enough novel machine learning contributions for acceptance.\n",
      "        - The paper lacks novelty and does not provide a strong contribution beyond existing work. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks novelty and does not provide a strong contribution beyond existing work.\n",
      "        - The paper lacks novelty and does not provide new insights beyond existing intuitive knowledge. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks novelty and does not provide new insights beyond existing intuitive knowledge.\n",
      "        - The paper lacks novelty and does not relate to the focus of ICLR. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks novelty and does not relate to the focus of ICLR.\n",
      "        - The paper lacks novelty since its main contributions are just adding two loss terms, not fully resolving existing issues. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks novelty since its main contributions are just adding two loss terms, not fully resolving existing issues.\n",
      "        - The paper lacks novelty; the contribution is not sufficiently original. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks novelty; the contribution is not sufficiently original.\n",
      "        - The paper lacks technical novelty as convergence results follow standard inexact/noisy gradient methods under strong assumptions. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks technical novelty as convergence results follow standard inexact/noisy gradient methods under strong assumptions.\n",
      "      - Limited Novelty or Incremental Contribution (L1, 21)\n",
      "        - Conceptual and experimental contributions are rather weak; tasks are direct applications of existing frameworks with limited novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - Conceptual and experimental contributions are rather weak; tasks are direct applications of existing frameworks with limited novelty.\n",
      "        - Limited novelty over VEEGAN and negligible empirical improvement make the method unconvincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited novelty over VEEGAN and negligible empirical improvement make the method unconvincing.\n",
      "        - Limited novelty, mainly combining existing techniques rather than offering innovation in model-based modeling. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited novelty, mainly combining existing techniques rather than offering innovation in model-based modeling.\n",
      "        - Limited novelty; improvements over DiffPool are mainly adding previously proposed regularization terms. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited novelty; improvements over DiffPool are mainly adding previously proposed regularization terms.\n",
      "        - Limited technical novelty; proposed method is heuristic and lacks justification for design choices. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited technical novelty; proposed method is heuristic and lacks justification for design choices.\n",
      "        - No novelty in the proposed fast segmentation network detailed in Sec 3.2. (L0, 1)\n",
      "          * Examples:\n",
      "            - No novelty in the proposed fast segmentation network detailed in Sec 3.2.\n",
      "        - Novelty is limited and claims are not well supported. (L0, 1)\n",
      "          * Examples:\n",
      "            - Novelty is limited and claims are not well supported.\n",
      "        - The application is a bit limited. (L0, 1)\n",
      "          * Examples:\n",
      "            - The application is a bit limited.\n",
      "        - The contribution is rather limited/unclear; main results are standard with limited novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The contribution is rather limited/unclear; main results are standard with limited novelty.\n",
      "        - The method is based on published slimmable networks, so the novelty is limited. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is based on published slimmable networks, so the novelty is limited.\n",
      "        - The novelty and contribution are very limited as the generalization bounds are direct applications of known results. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty and contribution are very limited as the generalization bounds are direct applications of known results.\n",
      "        - The novelty and contribution of the paper are limited. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty and contribution of the paper are limited.\n",
      "        - The novelty is limited; it's unclear how the model solves issues in the motivational examples (Figure 1). (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty is limited; it's unclear how the model solves issues in the motivational examples (Figure 1).\n",
      "        - The novelty is limited; the method is very similar to unsupervised GraphSAGE. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty is limited; the method is very similar to unsupervised GraphSAGE.\n",
      "        - The novelty of the method appears limited, mainly adding noise to predictions rather than true algorithmic innovation. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty of the method appears limited, mainly adding noise to predictions rather than true algorithmic innovation.\n",
      "        - The novelty of the model is rather limited. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty of the model is rather limited.\n",
      "        - The paper adds a very simple dynamic loss balancing with limited novelty, making the contribution ineffective. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper adds a very simple dynamic loss balancing with limited novelty, making the contribution ineffective.\n",
      "        - The proposed method is incremental with limited novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method is incremental with limited novelty.\n",
      "        - The proposed method seems straightforward, implying limited novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method seems straightforward, implying limited novelty.\n",
      "        - The technical novelty is limited, and the necessity of the second branch is not clearly explained. (L0, 1)\n",
      "          * Examples:\n",
      "            - The technical novelty is limited, and the necessity of the second branch is not clearly explained.\n",
      "        - The work is not novel and the model offers limited novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The work is not novel and the model offers limited novelty.\n",
      "      - Limited Scope, Incrementality, or Lack of General Interest/Relevance (L1, 2)\n",
      "        - The paper is fairly incremental and not very interesting to the ICLR community. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is fairly incremental and not very interesting to the ICLR community.\n",
      "        - The work is not of enough general use or interest to the ICLR community to merit acceptance. (L0, 1)\n",
      "          * Examples:\n",
      "            - The work is not of enough general use or interest to the ICLR community to merit acceptance.\n",
      "      - Novelty and Relevance Concerns (L1, 3)\n",
      "        - Novelty in architecture design is trivial. (L0, 1)\n",
      "          * Examples:\n",
      "            - Novelty in architecture design is trivial.\n",
      "        - The methodology of adding a graph connectivity term is relatively simple. (L0, 1)\n",
      "          * Examples:\n",
      "            - The methodology of adding a graph connectivity term is relatively simple.\n",
      "        - The modular 4-SAT experiment is a bit underwhelming. (L0, 1)\n",
      "          * Examples:\n",
      "            - The modular 4-SAT experiment is a bit underwhelming.\n",
      "      - The multi-objective optimization is confusing and not truly multi-objective; lacks novelty. (L1, 1)\n",
      "        * Examples:\n",
      "          - The multi-objective optimization is confusing and not truly multi-objective; lacks novelty.\n",
      "      - The sole novelty is a new justification for the STL estimator and there are no experiments. (L1, 1)\n",
      "        * Examples:\n",
      "          - The sole novelty is a new justification for the STL estimator and there are no experiments.\n",
      "    - Venue Fit, Audience Appropriateness, or Insufficient Contributions for ICLR (L2, 5)\n",
      "      - Contributions are not sufficient for ICLR; better suited as a workshop paper. (L1, 1)\n",
      "        * Examples:\n",
      "          - Contributions are not sufficient for ICLR; better suited as a workshop paper.\n",
      "      - Paper is about a very specialized area and may not fit this venue. (L1, 1)\n",
      "        * Examples:\n",
      "          - Paper is about a very specialized area and may not fit this venue.\n",
      "      - Scope and Audience Appropriateness (L1, 2)\n",
      "        - Not of general interest to ICLR audience; better suited for cognitive science venues. (L0, 1)\n",
      "          * Examples:\n",
      "            - Not of general interest to ICLR audience; better suited for cognitive science venues.\n",
      "        - The scope is too narrow for the ML community. (L0, 1)\n",
      "          * Examples:\n",
      "            - The scope is too narrow for the ML community.\n",
      "      - The paper's generalizability and lessons for other languages are unproven; fit with ICLR focus is weak. (L1, 1)\n",
      "        * Examples:\n",
      "          - The paper's generalizability and lessons for other languages are unproven; fit with ICLR focus is weak.\n",
      "  - Substantive Critiques of Paper Quality (L3, 47)\n",
      "    - Presentation and Clarity Problems (L2, 21)\n",
      "      - Lack of Clarity, Details, or Poor Writing (L1, 8)\n",
      "        - Key assumptions lack intuition/discussion and are not concretely illustrated with examples or empirical application. (L0, 1)\n",
      "          * Examples:\n",
      "            - Key assumptions lack intuition/discussion and are not concretely illustrated with examples or empirical application.\n",
      "        - Lack of clarity and poor writing prevent a thorough review. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of clarity and poor writing prevent a thorough review.\n",
      "        - Lack of clarity and sufficient explanation of implementation details for masking and grid search. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of clarity and sufficient explanation of implementation details for masking and grid search.\n",
      "        - Lack of clarity on the efficiency and search cost of the proposed method versus baselines. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of clarity on the efficiency and search cost of the proposed method versus baselines.\n",
      "        - Lack of convergence analysis and unclear choice of projection for mirror mapping. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of convergence analysis and unclear choice of projection for mirror mapping.\n",
      "        - Lack of details on constructing F_theta and how asymmetric transfer weights are implemented. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of details on constructing F_theta and how asymmetric transfer weights are implemented.\n",
      "        - Section 3.2 is hard to parse due to unclear notation and lack of organization in Eq. 14. (L0, 1)\n",
      "          * Examples:\n",
      "            - Section 3.2 is hard to parse due to unclear notation and lack of organization in Eq. 14.\n",
      "        - Unclear participation, results, and rankings in the ESA challenge; lack of clarity around methods presented. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear participation, results, and rankings in the ESA challenge; lack of clarity around methods presented.\n",
      "      - Lack of Clarity, Motivation, or Explanation (L1, 6)\n",
      "        - The paper is vague about the exact modelling of transfer and lacks detail on how uncertainty controls transfer. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is vague about the exact modelling of transfer and lacks detail on how uncertainty controls transfer.\n",
      "        - The paper lacks clarity and motivation for the bandwidth extension module and does not fulfill its core promises. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks clarity and motivation for the bandwidth extension module and does not fulfill its core promises.\n",
      "        - The paper lacks clarity and suffers from language/grammar issues, making it hard to understand. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks clarity and suffers from language/grammar issues, making it hard to understand.\n",
      "        - The paper lacks clarity in motivation, explanation, and definition of key concepts like 'non-robust discriminator'. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks clarity in motivation, explanation, and definition of key concepts like 'non-robust discriminator'.\n",
      "        - The paper lacks clarity, especially in problem definition, motivation, and distinguishing its own contributions from prior work. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks clarity, especially in problem definition, motivation, and distinguishing its own contributions from prior work.\n",
      "        - The paper lacks intuition and interesting discussion about why the pretraining phase leads to better performance. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks intuition and interesting discussion about why the pretraining phase leads to better performance.\n",
      "      - Poor or Unclear Presentation and Communication (L1, 5)\n",
      "        - Figure 1 does not show which part corresponds to the decoding phase; the paper is unclear here. (L0, 1)\n",
      "          * Examples:\n",
      "            - Figure 1 does not show which part corresponds to the decoding phase; the paper is unclear here.\n",
      "        - Paper is insufficiently motivated, presented, and explained for a wider audience; can only reach a limited audience. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper is insufficiently motivated, presented, and explained for a wider audience; can only reach a limited audience.\n",
      "        - The main weakness is the paper's presentation, starting from the abstract and throughout. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main weakness is the paper's presentation, starting from the abstract and throughout.\n",
      "        - The paper needs an additional pass at grammatical clarity. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper needs an additional pass at grammatical clarity.\n",
      "        - The presentation of the paper is very weak and unclear, with many imprecise and vague statements. (L0, 1)\n",
      "          * Examples:\n",
      "            - The presentation of the paper is very weak and unclear, with many imprecise and vague statements.\n",
      "      - Sloppy or Poor Writing and Presentation (L1, 2)\n",
      "        - The evaluation is not thorough enough, with missing tabular environments and weak writing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The evaluation is not thorough enough, with missing tabular environments and weak writing.\n",
      "        - Writing is sloppy and difficult to read; main claim about i.i.d. hypothesis invalidating GP interpretation is not convincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Writing is sloppy and difficult to read; main claim about i.i.d. hypothesis invalidating GP interpretation is not convincing.\n",
      "    - Unrealistic, Strong, or Unjustified Assumptions; Limited Generalizability or Narrow Scope (L2, 26)\n",
      "      - Applicability is limited due to strong assumptions on similarity between source and target domain dynamics. (L1, 1)\n",
      "        * Examples:\n",
      "          - Applicability is limited due to strong assumptions on similarity between source and target domain dynamics.\n",
      "      - Approach may require semantically aligned datasets and may not generalize to unsupervised settings. (L1, 1)\n",
      "        * Examples:\n",
      "          - Approach may require semantically aligned datasets and may not generalize to unsupervised settings.\n",
      "      - Narrow Scope or Limited Applicability of Contributions (L1, 2)\n",
      "        - Contributions are limited to A-GEM and do not cover other episodic-based continual learning methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - Contributions are limited to A-GEM and do not cover other episodic-based continual learning methods.\n",
      "        - DoS and DoS-CH perform worse than averaging baselines on unseen domains, questioning the selection network's effectiveness. (L0, 1)\n",
      "          * Examples:\n",
      "            - DoS and DoS-CH perform worse than averaging baselines on unseen domains, questioning the selection network's effectiveness.\n",
      "      - Overly Narrow Scope or Generalizability Limitations (L1, 2)\n",
      "        - The main result is only proven for linear models, not deep learning. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main result is only proven for linear models, not deep learning.\n",
      "        - Very limited scope; only 1 dataset and 64 models, making broad conclusions about weight sharing in NAS hard. (L0, 1)\n",
      "          * Examples:\n",
      "            - Very limited scope; only 1 dataset and 64 models, making broad conclusions about weight sharing in NAS hard.\n",
      "      - Overly Strong or Unrealistic Assumptions Limiting Generality (L1, 5)\n",
      "        - The theoretical discussion imposes too strong assumptions that might make it less interesting in practice. (L0, 1)\n",
      "          * Examples:\n",
      "            - The theoretical discussion imposes too strong assumptions that might make it less interesting in practice.\n",
      "        - Theoretical assumptions are too strong and unrealistic, limiting practical impact. (L0, 1)\n",
      "          * Examples:\n",
      "            - Theoretical assumptions are too strong and unrealistic, limiting practical impact.\n",
      "        - Theoretical result cited is invalid for the proposal; assumptions (e.g., unique argmin) are violated for neural networks. (L0, 1)\n",
      "          * Examples:\n",
      "            - Theoretical result cited is invalid for the proposal; assumptions (e.g., unique argmin) are violated for neural networks.\n",
      "        - Theoretical results are based on a very simple and special task, so cannot be considered general for all MDP cases. (L0, 1)\n",
      "          * Examples:\n",
      "            - Theoretical results are based on a very simple and special task, so cannot be considered general for all MDP cases.\n",
      "        - Theoretical results depend on strong assumptions (e.g., iid Gaussian weights), limiting generality and validity of claims. (L0, 1)\n",
      "          * Examples:\n",
      "            - Theoretical results depend on strong assumptions (e.g., iid Gaussian weights), limiting generality and validity of claims.\n",
      "      - Problematic or Unrealistic Assumptions (L1, 9)\n",
      "        - Assumption of a lower bound on validation gradients is troubling, especially for over-parameterized problems and raises practical concerns. (L0, 1)\n",
      "          * Examples:\n",
      "            - Assumption of a lower bound on validation gradients is troubling, especially for over-parameterized problems and raises practical concerns.\n",
      "        - Assumption of annotated rationales is a big limitation. (L0, 1)\n",
      "          * Examples:\n",
      "            - Assumption of annotated rationales is a big limitation.\n",
      "        - Limited applicability due to strong assumption that both robot and object states are available and clearly separable. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited applicability due to strong assumption that both robot and object states are available and clearly separable.\n",
      "        - The assumption of independence between different elements of the Hessian is too strong. (L0, 1)\n",
      "          * Examples:\n",
      "            - The assumption of independence between different elements of the Hessian is too strong.\n",
      "        - The assumption that benchmarks with fewer isomorphic graphs are better is questionable and may not be appropriate. (L0, 1)\n",
      "          * Examples:\n",
      "            - The assumption that benchmarks with fewer isomorphic graphs are better is questionable and may not be appropriate.\n",
      "        - The attacker’s assumption of access to model and episodic memory is impractical, making the attack unsurprising. (L0, 1)\n",
      "          * Examples:\n",
      "            - The attacker’s assumption of access to model and episodic memory is impractical, making the attack unsurprising.\n",
      "        - The problem setting does not model typical or difficult RL exploration problems, especially due to Assumption 3. (L0, 1)\n",
      "          * Examples:\n",
      "            - The problem setting does not model typical or difficult RL exploration problems, especially due to Assumption 3.\n",
      "        - The proposed algorithm violates its own assumption in practice, especially in the handling of the homotopy parameter L. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed algorithm violates its own assumption in practice, especially in the handling of the homotopy parameter L.\n",
      "        - The reviewer is not fully convinced that the i.i.d. assumption is wrong and finds the argument unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The reviewer is not fully convinced that the i.i.d. assumption is wrong and finds the argument unclear.\n",
      "      - Unrealistic Assumptions or Unjustified Methodological Choices (L1, 6)\n",
      "        - The method assumes only global translational motion, ignoring spatially varying motion and making a critical, unrealistic assumption. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method assumes only global translational motion, ignoring spatially varying motion and making a critical, unrealistic assumption.\n",
      "        - The method's deviation from conventional VI creates unnecessary computational overhead without clear benefit for encoding KNN information. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method's deviation from conventional VI creates unnecessary computational overhead without clear benefit for encoding KNN information.\n",
      "        - The method's reliance on a Gaussian assumption for regression is unjustified, especially for Bayesian Neural Networks. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method's reliance on a Gaussian assumption for regression is unjustified, especially for Bayesian Neural Networks.\n",
      "        - The proposed method seems equivalent to Euclidean distance and lacks justification for its improved effectiveness or perceptual meaning. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method seems equivalent to Euclidean distance and lacks justification for its improved effectiveness or perceptual meaning.\n",
      "        - Uncertainty about the suitability of initialization techniques and training method for tensor train embedding. (L0, 1)\n",
      "          * Examples:\n",
      "            - Uncertainty about the suitability of initialization techniques and training method for tensor train embedding.\n",
      "        - Weak motivation for input-dependent dropout method. (L0, 1)\n",
      "          * Examples:\n",
      "            - Weak motivation for input-dependent dropout method.\n",
      "  - Theoretical and Analytical Rigor (L3, 90)\n",
      "    - Theoretical Rigor and Justification (L2, 87)\n",
      "      - Insufficient Technical or Theoretical Rigor (L1, 9)\n",
      "        - Fundamental mistakes in the statements/proofs of Theorems 2, 3, and 4. (L0, 1)\n",
      "          * Examples:\n",
      "            - Fundamental mistakes in the statements/proofs of Theorems 2, 3, and 4.\n",
      "        - LID may not be the most relevant or strongest baseline for adversarial detection. (L0, 1)\n",
      "          * Examples:\n",
      "            - LID may not be the most relevant or strongest baseline for adversarial detection.\n",
      "        - Proofs of Propositions 4.3 and 4.7 should be covered/sketched in greater detail in the main text. (L0, 1)\n",
      "          * Examples:\n",
      "            - Proofs of Propositions 4.3 and 4.7 should be covered/sketched in greater detail in the main text.\n",
      "        - Reviewer is not well versed in the mathematical area to assess the derivations and proofs critically. (L0, 1)\n",
      "          * Examples:\n",
      "            - Reviewer is not well versed in the mathematical area to assess the derivations and proofs critically.\n",
      "        - The authors ignored all the finite-sample bound results in related work, particularly those achieving minimax rates. (L0, 1)\n",
      "          * Examples:\n",
      "            - The authors ignored all the finite-sample bound results in related work, particularly those achieving minimax rates.\n",
      "        - The method does not adequately address scheduling operations within each machine to minimize makespan. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method does not adequately address scheduling operations within each machine to minimize makespan.\n",
      "        - The presentation of the theory is unclear and not well-motivated; the connection to CycleGAN is unconvincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The presentation of the theory is unclear and not well-motivated; the connection to CycleGAN is unconvincing.\n",
      "        - The proposed method does not seem significantly better than the previous (very similar) method of Mu et al.; more evidence is needed. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method does not seem significantly better than the previous (very similar) method of Mu et al.; more evidence is needed.\n",
      "        - The theory (and its correctness) is difficult to be justified. (L0, 1)\n",
      "          * Examples:\n",
      "            - The theory (and its correctness) is difficult to be justified.\n",
      "      - Insufficient Theoretical Foundation (L1, 8)\n",
      "        - A rigorous theoretical proof for interpretability is lacking; current proof is naïve. (L0, 1)\n",
      "          * Examples:\n",
      "            - A rigorous theoretical proof for interpretability is lacking; current proof is naïve.\n",
      "        - Convergence analysis of the proposed method is missing, which is critical due to potential divergence from asynchrony. (L0, 1)\n",
      "          * Examples:\n",
      "            - Convergence analysis of the proposed method is missing, which is critical due to potential divergence from asynchrony.\n",
      "        - Lack of proof details; Proof 3.1 is hard to follow. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of proof details; Proof 3.1 is hard to follow.\n",
      "        - No formal proof of convergence or efficiency for the proposed algorithm; empirical validation of these properties is missing. (L0, 1)\n",
      "          * Examples:\n",
      "            - No formal proof of convergence or efficiency for the proposed algorithm; empirical validation of these properties is missing.\n",
      "        - No theoretical analysis of the key quantity t, which is central to the algorithm's convergence. (L0, 1)\n",
      "          * Examples:\n",
      "            - No theoretical analysis of the key quantity t, which is central to the algorithm's convergence.\n",
      "        - The algorithm lacks theoretical guarantees under bandit settings, making its foundation weak. (L0, 1)\n",
      "          * Examples:\n",
      "            - The algorithm lacks theoretical guarantees under bandit settings, making its foundation weak.\n",
      "        - The arguments are vague and lack mathematical formulation or proof; need concrete analysis. (L0, 1)\n",
      "          * Examples:\n",
      "            - The arguments are vague and lack mathematical formulation or proof; need concrete analysis.\n",
      "        - The paper shows only intuitive drawbacks of AII without any theoretical proof. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper shows only intuitive drawbacks of AII without any theoretical proof.\n",
      "      - Insufficient Theoretical Justification or Clarity (L1, 3)\n",
      "        - It is not trivial to understand the presented convergence results and their practical implications (if any). (L0, 1)\n",
      "          * Examples:\n",
      "            - It is not trivial to understand the presented convergence results and their practical implications (if any).\n",
      "        - Section 2.3, called Main Result, is not that novel; results are already known for such RMDPs. (L0, 1)\n",
      "          * Examples:\n",
      "            - Section 2.3, called Main Result, is not that novel; results are already known for such RMDPs.\n",
      "        - Technical formulation and theoretical results are not presented rigorously and important details are missing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Technical formulation and theoretical results are not presented rigorously and important details are missing.\n",
      "      - Insufficient or Weak Theoretical Justification (L1, 17)\n",
      "        - In the absence of any theoretical justification, the empirical results are not sufficiently convincing for ICLR. (L0, 1)\n",
      "          * Examples:\n",
      "            - In the absence of any theoretical justification, the empirical results are not sufficiently convincing for ICLR.\n",
      "        - Insufficient experimental comparisons and lack of theoretical support for the method. (L0, 1)\n",
      "          * Examples:\n",
      "            - Insufficient experimental comparisons and lack of theoretical support for the method.\n",
      "        - Lack of theoretical justification for the proposed method. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of theoretical justification for the proposed method.\n",
      "        - Lack of theoretical results (e.g., convergence/asymptotic properties) for policy and density functions. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of theoretical results (e.g., convergence/asymptotic properties) for policy and density functions.\n",
      "        - No theoretical analysis provided for output space adaptation, which is a complex problem. (L0, 1)\n",
      "          * Examples:\n",
      "            - No theoretical analysis provided for output space adaptation, which is a complex problem.\n",
      "        - No theoretical guarantee for the proposed method across learning problems. (L0, 1)\n",
      "          * Examples:\n",
      "            - No theoretical guarantee for the proposed method across learning problems.\n",
      "        - Overly bold claims with insufficient clarity and justification. (L0, 1)\n",
      "          * Examples:\n",
      "            - Overly bold claims with insufficient clarity and justification.\n",
      "        - Paper gives very little theoretical justification or analysis of the results. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper gives very little theoretical justification or analysis of the results.\n",
      "        - Substantial lack of theoretical justifications for the proposed method. (L0, 1)\n",
      "          * Examples:\n",
      "            - Substantial lack of theoretical justifications for the proposed method.\n",
      "        - The core idea lacks solid theoretical support; no regret bound result is provided. (L0, 1)\n",
      "          * Examples:\n",
      "            - The core idea lacks solid theoretical support; no regret bound result is provided.\n",
      "        - The generalisation bound appears over-reaching and insufficiently justified; core claims lack solid theoretical support. (L0, 1)\n",
      "          * Examples:\n",
      "            - The generalisation bound appears over-reaching and insufficiently justified; core claims lack solid theoretical support.\n",
      "        - The method is only an incremental extension of existing solutions with no theoretical justification. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is only an incremental extension of existing solutions with no theoretical justification.\n",
      "        - The theoretical analysis is just a regular routine. (L0, 1)\n",
      "          * Examples:\n",
      "            - The theoretical analysis is just a regular routine.\n",
      "        - The theoretical contributions are only asymptotic and do not provide intuition for why the method outperforms others. (L0, 1)\n",
      "          * Examples:\n",
      "            - The theoretical contributions are only asymptotic and do not provide intuition for why the method outperforms others.\n",
      "        - The theoretical guarantee relies on overly strong, unjustified assumptions and is confusingly presented. (L0, 1)\n",
      "          * Examples:\n",
      "            - The theoretical guarantee relies on overly strong, unjustified assumptions and is confusingly presented.\n",
      "        - Theoretical and empirical evidence are not strong enough to convince practitioners to use EINS over LSTM/GRU. (L0, 1)\n",
      "          * Examples:\n",
      "            - Theoretical and empirical evidence are not strong enough to convince practitioners to use EINS over LSTM/GRU.\n",
      "        - Unclear why the proposed method offers superior performance over existing methods for simple models; theoretical justification is insufficient. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear why the proposed method offers superior performance over existing methods for simple models; theoretical justification is insufficient.\n",
      "      - Lack of Rigorous Theoretical or Mathematical Justification (L1, 5)\n",
      "        - The core premise that GCNs are matrix factorization methods is incorrect and cannot be justified mathematically. (L0, 1)\n",
      "          * Examples:\n",
      "            - The core premise that GCNs are matrix factorization methods is incorrect and cannot be justified mathematically.\n",
      "        - The main conjecture on concentration of measure is hand-wavy and not rigorously justified. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main conjecture on concentration of measure is hand-wavy and not rigorously justified.\n",
      "        - The main mathematical result is not validated or illustrated numerically, despite claims that it could be. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main mathematical result is not validated or illustrated numerically, despite claims that it could be.\n",
      "        - The use of second-order splitting without upgrading Euler's method does not yield a higher-order ODE solver, invalidating the main claim. (L0, 1)\n",
      "          * Examples:\n",
      "            - The use of second-order splitting without upgrading Euler's method does not yield a higher-order ODE solver, invalidating the main claim.\n",
      "        - Theorems are misleading and overclaim, especially about Gaussian smoothing and dimensionality dependence. (L0, 1)\n",
      "          * Examples:\n",
      "            - Theorems are misleading and overclaim, especially about Gaussian smoothing and dimensionality dependence.\n",
      "      - Lack of Theoretical or Empirical Justification (L1, 6)\n",
      "        - Analysis does not explain why various methods, including gradient penalty, converge in practice with proper hyper-parameters. (L0, 1)\n",
      "          * Examples:\n",
      "            - Analysis does not explain why various methods, including gradient penalty, converge in practice with proper hyper-parameters.\n",
      "        - No guarantees are provided for the proposed techniques. (L0, 1)\n",
      "          * Examples:\n",
      "            - No guarantees are provided for the proposed techniques.\n",
      "        - Proposed solution may not generalize beyond the tested de-mixing tasks due to reliance on pretrained decoders. (L0, 1)\n",
      "          * Examples:\n",
      "            - Proposed solution may not generalize beyond the tested de-mixing tasks due to reliance on pretrained decoders.\n",
      "        - The algorithm minimizes an upper bound, not directly the target loss; benefit is not guaranteed and may be limited. (L0, 1)\n",
      "          * Examples:\n",
      "            - The algorithm minimizes an upper bound, not directly the target loss; benefit is not guaranteed and may be limited.\n",
      "        - The auxiliary loss can lead to trivial identity solutions, especially when hidden dimensions exceed input dimensions. (L0, 1)\n",
      "          * Examples:\n",
      "            - The auxiliary loss can lead to trivial identity solutions, especially when hidden dimensions exceed input dimensions.\n",
      "        - There is a significant gap from theory to practice; theory assumes a known world model and stationary reward, but practice does not. (L0, 1)\n",
      "          * Examples:\n",
      "            - There is a significant gap from theory to practice; theory assumes a known world model and stationary reward, but practice does not.\n",
      "      - Methodological and Theoretical Rigor (L1, 11)\n",
      "        - Bayesian formulation is problematic; priors on alpha and the KL term are not clear or valid. (L0, 1)\n",
      "          * Examples:\n",
      "            - Bayesian formulation is problematic; priors on alpha and the KL term are not clear or valid.\n",
      "        - Derivation of the target distribution by KKT condition would be more rigorous. (L0, 1)\n",
      "          * Examples:\n",
      "            - Derivation of the target distribution by KKT condition would be more rigorous.\n",
      "        - Doubt that RotationOut actually reduces feature correlation due to properties of rotation matrices. (L0, 1)\n",
      "          * Examples:\n",
      "            - Doubt that RotationOut actually reduces feature correlation due to properties of rotation matrices.\n",
      "        - Not fully convinced of the heuristic: similarity between input and output embeddings used for position supervision. (L0, 1)\n",
      "          * Examples:\n",
      "            - Not fully convinced of the heuristic: similarity between input and output embeddings used for position supervision.\n",
      "        - The authors change the problem to sign gradient times its norm, which changes the flow in continuous time. (L0, 1)\n",
      "          * Examples:\n",
      "            - The authors change the problem to sign gradient times its norm, which changes the flow in continuous time.\n",
      "        - The duality gap is only an upper bound and not a proper metric for GAN training. (L0, 1)\n",
      "          * Examples:\n",
      "            - The duality gap is only an upper bound and not a proper metric for GAN training.\n",
      "        - The frequency principle lacks a rigorous definition. (L0, 1)\n",
      "          * Examples:\n",
      "            - The frequency principle lacks a rigorous definition.\n",
      "        - The robustness and sufficiency of AST path inputs should be further explored. (L0, 1)\n",
      "          * Examples:\n",
      "            - The robustness and sufficiency of AST path inputs should be further explored.\n",
      "        - The role and importance of FFT/IFFT versus spectral projection are not deeply analyzed or clarified. (L0, 1)\n",
      "          * Examples:\n",
      "            - The role and importance of FFT/IFFT versus spectral projection are not deeply analyzed or clarified.\n",
      "        - Uncertainty about how stability of the target network is ensured via minimizing difference with the Lyapunov critic. (L0, 1)\n",
      "          * Examples:\n",
      "            - Uncertainty about how stability of the target network is ensured via minimizing difference with the Lyapunov critic.\n",
      "        - Unclear positioning compared to supervised dictionary learning; framing as a missing values problem seen as inaccurate. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear positioning compared to supervised dictionary learning; framing as a missing values problem seen as inaccurate.\n",
      "      - Methodological or Theoretical Ambiguity or Limitations (L1, 4)\n",
      "        - The advantage of using self-attention for neighborhood aggregation is not clearly discussed or distinguished from existing methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - The advantage of using self-attention for neighborhood aggregation is not clearly discussed or distinguished from existing methods.\n",
      "        - The convergence result ignores the effect of non-iid data, contradicting intuition and correctness. (L0, 1)\n",
      "          * Examples:\n",
      "            - The convergence result ignores the effect of non-iid data, contradicting intuition and correctness.\n",
      "        - Uncertainty if empirical improvements are truly due to better gradients; theory's applicability to non-convex settings is unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - Uncertainty if empirical improvements are truly due to better gradients; theory's applicability to non-convex settings is unclear.\n",
      "        - VTAB does not restrict representation learning to a fixed dataset, making improvements possible by simply using more representative data. (L0, 1)\n",
      "          * Examples:\n",
      "            - VTAB does not restrict representation learning to a fixed dataset, making improvements possible by simply using more representative data.\n",
      "      - Missing or Inadequate Theoretical Justification or Analysis (L1, 3)\n",
      "        - Paper lacks justification and explanation for its main claim about the relationship between knowledge distillation and early stopping. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper lacks justification and explanation for its main claim about the relationship between knowledge distillation and early stopping.\n",
      "        - The paper lacks rigorous theoretical analysis and strong empirical evidence, making it unfit for this conference. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks rigorous theoretical analysis and strong empirical evidence, making it unfit for this conference.\n",
      "        - The paper misuses the reparameterization trick; Eq.(2) lacks required KL-divergence term, making the approach unjustified. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper misuses the reparameterization trick; Eq.(2) lacks required KL-divergence term, making the approach unjustified.\n",
      "      - Proof and Theoretical Flaws or Weakness (L1, 7)\n",
      "        - Central limit theorem may not apply since data are not i.i.d., making convergence proofs potentially invalid. (L0, 1)\n",
      "          * Examples:\n",
      "            - Central limit theorem may not apply since data are not i.i.d., making convergence proofs potentially invalid.\n",
      "        - If sample size is constant, theorem conditions may not hold, narrowing practical application domains. (L0, 1)\n",
      "          * Examples:\n",
      "            - If sample size is constant, theorem conditions may not hold, narrowing practical application domains.\n",
      "        - Mistake in the proof of Theorem 3.1; the claim about boundedness is incorrect. (L0, 1)\n",
      "          * Examples:\n",
      "            - Mistake in the proof of Theorem 3.1; the claim about boundedness is incorrect.\n",
      "        - No proof that the CNN-based evaluation metric satisfies metric properties, especially triangle inequality. (L0, 1)\n",
      "          * Examples:\n",
      "            - No proof that the CNN-based evaluation metric satisfies metric properties, especially triangle inequality.\n",
      "        - Possible flaw in the proof of Theorem 1 regarding replacing ||g||^2 with ||grad f_s(x_tau)||^2. (L0, 1)\n",
      "          * Examples:\n",
      "            - Possible flaw in the proof of Theorem 1 regarding replacing ||g||^2 with ||grad f_s(x_tau)||^2.\n",
      "        - The proof of relaxing the Polynomial functional constraint in Theorem 4.8 is not entirely clear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proof of relaxing the Polynomial functional constraint in Theorem 4.8 is not entirely clear.\n",
      "        - Theorem 1 proof is just standard SGD analysis, with no new contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - Theorem 1 proof is just standard SGD analysis, with no new contribution.\n",
      "      - Theoretical or Methodological Limitations (L1, 2)\n",
      "        - The approach is limited to finite groups and does not address equivariance with respect to continuous groups (e.g., SO(2)). (L0, 1)\n",
      "          * Examples:\n",
      "            - The approach is limited to finite groups and does not address equivariance with respect to continuous groups (e.g., SO(2)).\n",
      "        - The argument relating network depth to momentum is not convincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The argument relating network depth to momentum is not convincing.\n",
      "      - Unjustified Assumptions or Weak Theoretical Foundations (L1, 6)\n",
      "        - It is not clear if combining neural network models for both forward and inverse problems provides real benefits. (L0, 1)\n",
      "          * Examples:\n",
      "            - It is not clear if combining neural network models for both forward and inverse problems provides real benefits.\n",
      "        - Questionable reliability of prototypes derived from pseudo labels; justification for superiority over pseudo labels lacking. (L0, 1)\n",
      "          * Examples:\n",
      "            - Questionable reliability of prototypes derived from pseudo labels; justification for superiority over pseudo labels lacking.\n",
      "        - The assumption that weights, gradients, and their product are Gaussian-distributed is unnatural and unjustified in real training. (L0, 1)\n",
      "          * Examples:\n",
      "            - The assumption that weights, gradients, and their product are Gaussian-distributed is unnatural and unjustified in real training.\n",
      "        - The cycle consistency constraint is not truly a 'cycle'; justification is lacking. (L0, 1)\n",
      "          * Examples:\n",
      "            - The cycle consistency constraint is not truly a 'cycle'; justification is lacking.\n",
      "        - The paper is based on, and perpetrates, a number of fundamental misconceptions about adversarial examples. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is based on, and perpetrates, a number of fundamental misconceptions about adversarial examples.\n",
      "        - Unclear if taking the min surprise across the ensemble is a good exploration strategy; motivation is not fully convincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear if taking the min surprise across the ensemble is a good exploration strategy; motivation is not fully convincing.\n",
      "      - Weak or Unconvincing Theoretical Claims and Justifications (L1, 6)\n",
      "        - Claim that the obtained map is the optimal transport map is likely incorrect due to limited function class and weak optimality conditions. (L0, 1)\n",
      "          * Examples:\n",
      "            - Claim that the obtained map is the optimal transport map is likely incorrect due to limited function class and weak optimality conditions.\n",
      "        - No guarantee of learned policy optimality; SAIL lacks optimality justification from both distribution matching and IRL perspectives. (L0, 1)\n",
      "          * Examples:\n",
      "            - No guarantee of learned policy optimality; SAIL lacks optimality justification from both distribution matching and IRL perspectives.\n",
      "        - Support for the main claim is weak and inadequately demonstrated. (L0, 1)\n",
      "          * Examples:\n",
      "            - Support for the main claim is weak and inadequately demonstrated.\n",
      "        - TDW favors deterministic policies over uncertain ones, which may exclude optimal policies in stochastic settings. (L0, 1)\n",
      "          * Examples:\n",
      "            - TDW favors deterministic policies over uncertain ones, which may exclude optimal policies in stochastic settings.\n",
      "        - The connection between the transformer architecture and multi-particle ODEs is weak and not sufficiently fleshed out. (L0, 1)\n",
      "          * Examples:\n",
      "            - The connection between the transformer architecture and multi-particle ODEs is weak and not sufficiently fleshed out.\n",
      "        - Unclear justification for assuming the null class centroid is close to origin due to batch normalization. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear justification for assuming the null class centroid is close to origin due to batch normalization.\n",
      "    - Weak Theoretical Justification or Invalid Foundations (L2, 3)\n",
      "      - The connection of GCN to MF is very indirect and not clearly established empirically. (L1, 1)\n",
      "        * Examples:\n",
      "          - The connection of GCN to MF is very indirect and not clearly established empirically.\n",
      "      - Unclear and possibly invalid objective function undermines trust in method’s correctness and effectiveness. (L1, 1)\n",
      "        * Examples:\n",
      "          - Unclear and possibly invalid objective function undermines trust in method’s correctness and effectiveness.\n",
      "      - Using the same point estimates for weights across different random network structures undermines probabilistic modeling and uncertainty calibration. (L1, 1)\n",
      "        * Examples:\n",
      "          - Using the same point estimates for weights across different random network structures undermines probabilistic modeling and uncertainty calibration.\n",
      "- One of the main limitations of this work seems to be that the CMDPM has to be known. (L4, 1)\n",
      "  * Examples:\n",
      "    - One of the main limitations of this work seems to be that the CMDPM has to be known.\n",
      "- Presentation and Quality of Reporting (L4, 241)\n",
      "  - Clarity, Presentation, and Reporting Deficiencies (L3, 192)\n",
      "    - Ambiguity, Contradiction, or Triviality (L2, 2)\n",
      "      - (b) and (c) are contradictory; (b) is near-trivial. (L1, 1)\n",
      "        * Examples:\n",
      "          - (b) and (c) are contradictory; (b) is near-trivial.\n",
      "      - The approach of sparsifying the input may yield outlying, unnatural data, bringing unclear or biased explanations. (L1, 1)\n",
      "        * Examples:\n",
      "          - The approach of sparsifying the input may yield outlying, unnatural data, bringing unclear or biased explanations.\n",
      "    - Clarity and Presentation Issues (L2, 85)\n",
      "      - Algorithm Clarity and Explanation Issues (L1, 5)\n",
      "        - It's not clear the proposed algorithm is useful. (L0, 1)\n",
      "          * Examples:\n",
      "            - It's not clear the proposed algorithm is useful.\n",
      "        - No convincing explanation or justification for why the modulo operation improves the algorithm. (L0, 1)\n",
      "          * Examples:\n",
      "            - No convincing explanation or justification for why the modulo operation improves the algorithm.\n",
      "        - Serious clarity issues with the writing and explanation of the algorithm. (L0, 1)\n",
      "          * Examples:\n",
      "            - Serious clarity issues with the writing and explanation of the algorithm.\n",
      "        - The proposed algorithm is not clear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed algorithm is not clear.\n",
      "        - Unclear motivation and lack of theoretical justification for the proposed algorithm. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear motivation and lack of theoretical justification for the proposed algorithm.\n",
      "      - Ambiguous or Misleading Presentation and Terminology (L1, 4)\n",
      "        - Term 'adversarial success rate' should be called 'attack success rate' as per standard literature. (L0, 1)\n",
      "          * Examples:\n",
      "            - Term 'adversarial success rate' should be called 'attack success rate' as per standard literature.\n",
      "        - The 'RNN-based filter' is not related to standard RNNs, making the title and presentation confusing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The 'RNN-based filter' is not related to standard RNNs, making the title and presentation confusing.\n",
      "        - The 'accuracy drop' metric is misleading and unjustified, as it redefines accuracy in a non-standard, incomparable way. (L0, 1)\n",
      "          * Examples:\n",
      "            - The 'accuracy drop' metric is misleading and unjustified, as it redefines accuracy in a non-standard, incomparable way.\n",
      "        - The reasoning module is just constraints and data sampling, not true (symbolic or logic) reasoning as claimed. (L0, 1)\n",
      "          * Examples:\n",
      "            - The reasoning module is just constraints and data sampling, not true (symbolic or logic) reasoning as claimed.\n",
      "      - Both contributions are only loosely connected and some parts are confusing or poorly motivated, making the paper hard to follow. (L1, 1)\n",
      "        * Examples:\n",
      "          - Both contributions are only loosely connected and some parts are confusing or poorly motivated, making the paper hard to follow.\n",
      "      - Clarity and Accessibility of Exposition (L1, 6)\n",
      "        - Assumes deep reader familiarity with CCA, making it inaccessible for a general audience. (L0, 1)\n",
      "          * Examples:\n",
      "            - Assumes deep reader familiarity with CCA, making it inaccessible for a general audience.\n",
      "        - Not fully self-contained; relies on 20 pages of supplementary material. (L0, 1)\n",
      "          * Examples:\n",
      "            - Not fully self-contained; relies on 20 pages of supplementary material.\n",
      "        - The intuition and derivation for the attention modules are missing and need to be clarified. (L0, 1)\n",
      "          * Examples:\n",
      "            - The intuition and derivation for the attention modules are missing and need to be clarified.\n",
      "        - The model description is very hard to follow; the architecture is not clearly explained. (L0, 1)\n",
      "          * Examples:\n",
      "            - The model description is very hard to follow; the architecture is not clearly explained.\n",
      "        - Would the authors provide slightly more details about the design of networks? (L0, 1)\n",
      "          * Examples:\n",
      "            - Would the authors provide slightly more details about the design of networks?\n",
      "        - Writing is overcomplicated and uses a lot of jargon, making the text impenetrable. (L0, 1)\n",
      "          * Examples:\n",
      "            - Writing is overcomplicated and uses a lot of jargon, making the text impenetrable.\n",
      "      - Clarity, Organization, and Readability Issues (L1, 14)\n",
      "        - The clarity of the technical presentation could be improved. (L0, 1)\n",
      "          * Examples:\n",
      "            - The clarity of the technical presentation could be improved.\n",
      "        - The paper is disorganized and lacks clear, thorough explanations of methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is disorganized and lacks clear, thorough explanations of methods.\n",
      "        - The paper is extremely hard to follow, with insufficient motivation and unclear explanations. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is extremely hard to follow, with insufficient motivation and unclear explanations.\n",
      "        - The paper is lacking content, slightly below the amount in average ICLR papers. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is lacking content, slightly below the amount in average ICLR papers.\n",
      "        - The paper is not easy to follow/read; organization and explanations are confusing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is not easy to follow/read; organization and explanations are confusing.\n",
      "        - The paper is not very clear, and the structure is somehow confusing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is not very clear, and the structure is somehow confusing.\n",
      "        - The paper is squeezed to fit into 8 pages, which may not be acceptable. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is squeezed to fit into 8 pages, which may not be acceptable.\n",
      "        - The paper is unclear, making it hard to distinguish novel contributions from prior work. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is unclear, making it hard to distinguish novel contributions from prior work.\n",
      "        - The paper is very hard to follow and not complete. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is very hard to follow and not complete.\n",
      "        - The paper is very unpolished, hard to follow, and not ready for publication. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is very unpolished, hard to follow, and not ready for publication.\n",
      "        - The section on the bernoulli splitting trick was the most confusing part of the paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - The section on the bernoulli splitting trick was the most confusing part of the paper.\n",
      "        - The technical presentation is painful to follow; global rewriting and clearer focus are needed. (L0, 1)\n",
      "          * Examples:\n",
      "            - The technical presentation is painful to follow; global rewriting and clearer focus are needed.\n",
      "        - The theoretical part (section 2) is hard to follow and unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The theoretical part (section 2) is hard to follow and unclear.\n",
      "        - The writing can be significantly improved; technical parts are confusing and unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The writing can be significantly improved; technical parts are confusing and unclear.\n",
      "      - Formatting, Figure, and Length Issues (L1, 2)\n",
      "        - Figure fonts are too small throughout the paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - Figure fonts are too small throughout the paper.\n",
      "        - Paper exceeds page limit and figures are unreadable due to squeezing. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper exceeds page limit and figures are unreadable due to squeezing.\n",
      "      - Insufficient Methodological Details or Clarity in Presentation (L1, 5)\n",
      "        - Insufficient methodological details and justification, especially on automatic pruning allocation and relation to relevant work. (L0, 1)\n",
      "          * Examples:\n",
      "            - Insufficient methodological details and justification, especially on automatic pruning allocation and relation to relevant work.\n",
      "        - No discussion or comparison with existing LSTM variants, and absence of a related work section. (L0, 1)\n",
      "          * Examples:\n",
      "            - No discussion or comparison with existing LSTM variants, and absence of a related work section.\n",
      "        - Reviewer is not familiar with quantization methods and cannot judge coverage of related work and choice of baselines. (L0, 1)\n",
      "          * Examples:\n",
      "            - Reviewer is not familiar with quantization methods and cannot judge coverage of related work and choice of baselines.\n",
      "        - The current presentation and experimentation does not meet the acceptance threshold. (L0, 1)\n",
      "          * Examples:\n",
      "            - The current presentation and experimentation does not meet the acceptance threshold.\n",
      "        - The presentation lacks sufficient clarity for those not familiar with non-autoregressive generation. (L0, 1)\n",
      "          * Examples:\n",
      "            - The presentation lacks sufficient clarity for those not familiar with non-autoregressive generation.\n",
      "      - Insufficient Methodological or Mathematical Definition and Correctness (L1, 3)\n",
      "        - The derivation of the teacher's update rule is incorrect, with mismatched matrix/vector shapes in equations. (L0, 1)\n",
      "          * Examples:\n",
      "            - The derivation of the teacher's update rule is incorrect, with mismatched matrix/vector shapes in equations.\n",
      "        - The specific problem (*class mismatch*) is not clearly formalized or mathematically characterized. (L0, 1)\n",
      "          * Examples:\n",
      "            - The specific problem (*class mismatch*) is not clearly formalized or mathematically characterized.\n",
      "        - There is no definition of KL function in the main content. (L0, 1)\n",
      "          * Examples:\n",
      "            - There is no definition of KL function in the main content.\n",
      "      - Lack of Clarity, Interpretability, and Presentation Issues (L1, 4)\n",
      "        - Much of the presentation is vague or opaque, lacking model details, and unclear/mismatched diagrams and equations. (L0, 1)\n",
      "          * Examples:\n",
      "            - Much of the presentation is vague or opaque, lacking model details, and unclear/mismatched diagrams and equations.\n",
      "        - The mathematical description of the main problem and proposed model is not clear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The mathematical description of the main problem and proposed model is not clear.\n",
      "        - The model is not interpretable as embeddings and non-linearities make decision boundaries unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The model is not interpretable as embeddings and non-linearities make decision boundaries unclear.\n",
      "        - The writing is unclear, leading to confusion about output sequence and model explanation. (L0, 1)\n",
      "          * Examples:\n",
      "            - The writing is unclear, leading to confusion about output sequence and model explanation.\n",
      "      - Language and Presentation Quality (L1, 3)\n",
      "        - Readability—many sections need proofreading; numerous grammatical/word choice mistakes make the paper difficult to read. (L0, 1)\n",
      "          * Examples:\n",
      "            - Readability—many sections need proofreading; numerous grammatical/word choice mistakes make the paper difficult to read.\n",
      "        - Serious language problems make the paper incoherent and impossible to understand. (L0, 1)\n",
      "          * Examples:\n",
      "            - Serious language problems make the paper incoherent and impossible to understand.\n",
      "        - The paper contains many typos that make sometimes the reading difficult. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper contains many typos that make sometimes the reading difficult.\n",
      "      - Paper is poorly written and there are concerns about the stability of the unsupervised data labeling process. (L1, 1)\n",
      "        * Examples:\n",
      "          - Paper is poorly written and there are concerns about the stability of the unsupervised data labeling process.\n",
      "      - Poor Clarity and Readability (L1, 12)\n",
      "        - Bad writing; reviewer can barely understand the paper’s content and arguments. (L0, 1)\n",
      "          * Examples:\n",
      "            - Bad writing; reviewer can barely understand the paper’s content and arguments.\n",
      "        - Important details are omitted from the main body, making the paper difficult to read and evaluate independently. (L0, 1)\n",
      "          * Examples:\n",
      "            - Important details are omitted from the main body, making the paper difficult to read and evaluate independently.\n",
      "        - It is difficult to understand the paper; it is not clearly written, especially the algorithm's presentation. (L0, 1)\n",
      "          * Examples:\n",
      "            - It is difficult to understand the paper; it is not clearly written, especially the algorithm's presentation.\n",
      "        - Paper is tedious with heavy notations, making it difficult to read. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper is tedious with heavy notations, making it difficult to read.\n",
      "        - Paper may be difficult to grasp for the general ML audience, including ML researchers. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper may be difficult to grasp for the general ML audience, including ML researchers.\n",
      "        - The motivation and intention of the work are unclear, making it difficult to understand and summarize. (L0, 1)\n",
      "          * Examples:\n",
      "            - The motivation and intention of the work are unclear, making it difficult to understand and summarize.\n",
      "        - The paper is cryptic and very difficult to understand, lacking clear explanations of the problem and contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is cryptic and very difficult to understand, lacking clear explanations of the problem and contribution.\n",
      "        - The paper is difficult to understand in some places due to unclear explanations. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is difficult to understand in some places due to unclear explanations.\n",
      "        - The paper is extremely difficult to follow, with an unclear objective and algorithm. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is extremely difficult to follow, with an unclear objective and algorithm.\n",
      "        - The paper is very hard to read; difficult to understand the goals and sections. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper is very hard to read; difficult to understand the goals and sections.\n",
      "        - The setup and model are too difficult to understand, hindering the ability to recommend the paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - The setup and model are too difficult to understand, hindering the ability to recommend the paper.\n",
      "        - The topic does not fit ICLR; most researchers won't be interested or understand it. (L0, 1)\n",
      "          * Examples:\n",
      "            - The topic does not fit ICLR; most researchers won't be interested or understand it.\n",
      "      - The main drawback is lack of clarity; the paper is poorly written and the presented model is not clearly motivated. (L1, 1)\n",
      "        * Examples:\n",
      "          - The main drawback is lack of clarity; the paper is poorly written and the presented model is not clearly motivated.\n",
      "      - The paper is awfully written and key concepts like the feature parameter phi are not explained, making it unreadable. (L1, 1)\n",
      "        * Examples:\n",
      "          - The paper is awfully written and key concepts like the feature parameter phi are not explained, making it unreadable.\n",
      "      - The paper is poorly presented, with many unclear parts and confusing explanations. (L1, 1)\n",
      "        * Examples:\n",
      "          - The paper is poorly presented, with many unclear parts and confusing explanations.\n",
      "      - The paper is poorly written and lacks a clear, significant contribution. (L1, 1)\n",
      "        * Examples:\n",
      "          - The paper is poorly written and lacks a clear, significant contribution.\n",
      "      - The paper is poorly written and the model is explained very poorly, making it hard to understand and reproduce. (L1, 1)\n",
      "        * Examples:\n",
      "          - The paper is poorly written and the model is explained very poorly, making it hard to understand and reproduce.\n",
      "      - The paper is poorly written and the model/architecture descriptions are very difficult to parse. (L1, 1)\n",
      "        * Examples:\n",
      "          - The paper is poorly written and the model/architecture descriptions are very difficult to parse.\n",
      "      - The paper is poorly written, unclear, and not ready for publication. (L1, 1)\n",
      "        * Examples:\n",
      "          - The paper is poorly written, unclear, and not ready for publication.\n",
      "      - The paper is very badly written and organization needs to be rethought. (L1, 1)\n",
      "        * Examples:\n",
      "          - The paper is very badly written and organization needs to be rethought.\n",
      "      - Unclear Methodological Details or Insufficient Explanation (L1, 2)\n",
      "        - The paper does not address or mention verification of correctness of the CUDA port. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not address or mention verification of correctness of the CUDA port.\n",
      "        - The paper does not truly move away from the use of anchors as claimed. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not truly move away from the use of anchors as claimed.\n",
      "      - Unclear Presentation and Explanation (L1, 13)\n",
      "        - It is not clear that the proposed architecture fundamentally advances zero-shot RL; its relevance is questionable. (L0, 1)\n",
      "          * Examples:\n",
      "            - It is not clear that the proposed architecture fundamentally advances zero-shot RL; its relevance is questionable.\n",
      "        - It is unclear how the PDP architecture solves SAT problems, especially how soft assignments relate to a final decision and discrete outputs. (L0, 1)\n",
      "          * Examples:\n",
      "            - It is unclear how the PDP architecture solves SAT problems, especially how soft assignments relate to a final decision and discrete outputs.\n",
      "        - Missing important details of learning the surrogate loss, especially function g and composition function h. (L0, 1)\n",
      "          * Examples:\n",
      "            - Missing important details of learning the surrogate loss, especially function g and composition function h.\n",
      "        - The algorithm description is unclear, especially how constraint satisfaction is achieved during training. (L0, 1)\n",
      "          * Examples:\n",
      "            - The algorithm description is unclear, especially how constraint satisfaction is achieved during training.\n",
      "        - The presentation of the algorithm is vague and it is difficult to determine what the algorithm actually does. (L0, 1)\n",
      "          * Examples:\n",
      "            - The presentation of the algorithm is vague and it is difficult to determine what the algorithm actually does.\n",
      "        - The presentation of the proposed models (RLIF and LIF-LSTM) is not clear at all. (L0, 1)\n",
      "          * Examples:\n",
      "            - The presentation of the proposed models (RLIF and LIF-LSTM) is not clear at all.\n",
      "        - The reasoning and intuition behind the equations, especially dissonance and Subjective Logic, are hard to follow. (L0, 1)\n",
      "          * Examples:\n",
      "            - The reasoning and intuition behind the equations, especially dissonance and Subjective Logic, are hard to follow.\n",
      "        - Unclear connection between proposed metrics and initial hypotheses; reviewer unable to understand the choice of metrics. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear connection between proposed metrics and initial hypotheses; reviewer unable to understand the choice of metrics.\n",
      "        - Unclear how Lyapunov criteria in Theorem 1 guarantees stability with the proposed penalty-based learning algorithm. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear how Lyapunov criteria in Theorem 1 guarantees stability with the proposed penalty-based learning algorithm.\n",
      "        - Unclear how often the constructed MDPs, where optimal Q-functions are hard to express, arise in practice. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear how often the constructed MDPs, where optimal Q-functions are hard to express, arise in practice.\n",
      "        - Unclear how the proposed flexible search distribution actually improves exploration compared to baseline ES. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear how the proposed flexible search distribution actually improves exploration compared to baseline ES.\n",
      "        - Very little information is provided on how to successfully train the proposed framework in practice. (L0, 1)\n",
      "          * Examples:\n",
      "            - Very little information is provided on how to successfully train the proposed framework in practice.\n",
      "        - Writing and explanations need improvement, especially for readers less familiar with the area. (L0, 1)\n",
      "          * Examples:\n",
      "            - Writing and explanations need improvement, especially for readers less familiar with the area.\n",
      "      - Visualization and Accessibility Issues (L1, 2)\n",
      "        - No results are given at test time regarding memory and processing time reduction. (L0, 1)\n",
      "          * Examples:\n",
      "            - No results are given at test time regarding memory and processing time reduction.\n",
      "        - The color scheme of figures is not friendly to color-blind people. (L0, 1)\n",
      "          * Examples:\n",
      "            - The color scheme of figures is not friendly to color-blind people.\n",
      "    - Clarity, Motivation, and Applicability (L2, 88)\n",
      "      - Insufficient Analysis, Discussion, or Justification (L1, 8)\n",
      "        - Lack of analysis on the robustness of explanations to perturbations that do not affect classifier predictions. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of analysis on the robustness of explanations to perturbations that do not affect classifier predictions.\n",
      "        - Lack of discussion about CoreSets theory and evaluation of heuristics. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of discussion about CoreSets theory and evaluation of heuristics.\n",
      "        - Lack of discussion on how to combine techniques or empirically choose the best approach for varying scenarios. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of discussion on how to combine techniques or empirically choose the best approach for varying scenarios.\n",
      "        - Lack of discussion on the effect of intrinsic reward bias, especially in non-sparse reward cases. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of discussion on the effect of intrinsic reward bias, especially in non-sparse reward cases.\n",
      "        - Lack of investigation or reporting for values of lamda_r. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of investigation or reporting for values of lamda_r.\n",
      "        - Lack of justification for allocating more channels to deep layers, which contrasts with pruning literature. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of justification for allocating more channels to deep layers, which contrasts with pruning literature.\n",
      "        - Reviewer is not convinced the lack of highly selective units in CNNs is an important or surprising problem. (L0, 1)\n",
      "          * Examples:\n",
      "            - Reviewer is not convinced the lack of highly selective units in CNNs is an important or surprising problem.\n",
      "        - Unclear why adaptation is needed if CFR/Nash already suffices; lack of analysis undermines the contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear why adaptation is needed if CFR/Nash already suffices; lack of analysis undermines the contribution.\n",
      "      - Insufficient Justification and Motivation for Method (L1, 4)\n",
      "        - Doubts about the effectiveness of the proposed method and whether it truly solves existing methods' problems. (L0, 1)\n",
      "          * Examples:\n",
      "            - Doubts about the effectiveness of the proposed method and whether it truly solves existing methods' problems.\n",
      "        - Justification for the proposed method is largely missing; unclear why it is a good or optimal idea. (L0, 1)\n",
      "          * Examples:\n",
      "            - Justification for the proposed method is largely missing; unclear why it is a good or optimal idea.\n",
      "        - The proposed method is not well justified and lacks thorough analysis or convincing evidence of performance. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method is not well justified and lacks thorough analysis or convincing evidence of performance.\n",
      "        - Unclear if the similarity matrix S already solves clustering; necessity of the method is not justified. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear if the similarity matrix S already solves clustering; necessity of the method is not justified.\n",
      "      - Insufficient Justification or Motivation for Choices or Assumptions (L1, 5)\n",
      "        - Assumptions, such as the need for Gaussian i.i.d weights, seem unrealistic and are not quantitatively justified. (L0, 1)\n",
      "          * Examples:\n",
      "            - Assumptions, such as the need for Gaussian i.i.d weights, seem unrealistic and are not quantitatively justified.\n",
      "        - The threat model is not well justified; the defense may not be robust to realistic black-box attacks (e.g., transferability or label-based attacks). (L0, 1)\n",
      "          * Examples:\n",
      "            - The threat model is not well justified; the defense may not be robust to realistic black-box attacks (e.g., transferability or label-based attacks).\n",
      "        - The use of asymmetric encoders (keys vs. queries) is not justified or empirically validated. (L0, 1)\n",
      "          * Examples:\n",
      "            - The use of asymmetric encoders (keys vs. queries) is not justified or empirically validated.\n",
      "        - The use of label smoothing (despite acknowledging its drawbacks) is insufficiently justified. (L0, 1)\n",
      "          * Examples:\n",
      "            - The use of label smoothing (despite acknowledging its drawbacks) is insufficiently justified.\n",
      "        - Unclear motivation for using tropical geometry and misleading use of 'adversarial attacks' terminology. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear motivation for using tropical geometry and misleading use of 'adversarial attacks' terminology.\n",
      "      - Insufficient Practical Motivation or Applicability (L1, 3)\n",
      "        - Lack of clear motivation and practical application for grounding; marginal performance improvement over baseline GVD. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of clear motivation and practical application for grounding; marginal performance improvement over baseline GVD.\n",
      "        - Lack of novel insight and weak connection to actual deep learning practice, datasets, and models. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of novel insight and weak connection to actual deep learning practice, datasets, and models.\n",
      "        - The approach lacks practical interest because few ML problems lack gradients; thus, ZO optimization is not interesting. (L0, 1)\n",
      "          * Examples:\n",
      "            - The approach lacks practical interest because few ML problems lack gradients; thus, ZO optimization is not interesting.\n",
      "      - Insufficient Technical Details or Formal Descriptions (L1, 3)\n",
      "        - The paper lacks important technical details and sufficient experimental evidence to be useful to other researchers. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks important technical details and sufficient experimental evidence to be useful to other researchers.\n",
      "        - The paper lacks precision and formal descriptions of the parser, making reproduction and understanding difficult. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks precision and formal descriptions of the parser, making reproduction and understanding difficult.\n",
      "        - The paper lacks sufficient detail, particularly regarding model architecture, training, and ablation studies. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks sufficient detail, particularly regarding model architecture, training, and ablation studies.\n",
      "      - Lack of Clear Motivation, Problem Definition, or Practical Relevance (L1, 7)\n",
      "        - The paper does not fully deliver on the promise to make physics constraints in deep networks usable in practice. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not fully deliver on the promise to make physics constraints in deep networks usable in practice.\n",
      "        - The paper does not give sufficient motivation for studying D_infty and D_{MR} robustness. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not give sufficient motivation for studying D_infty and D_{MR} robustness.\n",
      "        - The paper does not motivate or justify the need for the proposed constraints, making the method lack merit. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not motivate or justify the need for the proposed constraints, making the method lack merit.\n",
      "        - The paper does not solve a sufficiently clear problem and the motivation is confusing, especially regarding dropout analysis. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not solve a sufficiently clear problem and the motivation is confusing, especially regarding dropout analysis.\n",
      "        - The paper does not sufficiently address real use cases or the practical need for learned similarity metrics on PDE outputs. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not sufficiently address real use cases or the practical need for learned similarity metrics on PDE outputs.\n",
      "        - The paper does not sufficiently explain or formalize what makes the method perceptually more appealing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not sufficiently explain or formalize what makes the method perceptually more appealing.\n",
      "        - The paper does not sufficiently explore the implications of its bias modelling or compare to simpler, established methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not sufficiently explore the implications of its bias modelling or compare to simpler, established methods.\n",
      "      - Limited Practical Applicability or Utility (L1, 3)\n",
      "        - Analysis of weight and activation quantization are separated and may not hold when both are quantized simultaneously. (L0, 1)\n",
      "          * Examples:\n",
      "            - Analysis of weight and activation quantization are separated and may not hold when both are quantized simultaneously.\n",
      "        - Due to progressive freezing, network eventually reaches full capacity, making it difficult or impossible to learn new tasks. (L0, 1)\n",
      "          * Examples:\n",
      "            - Due to progressive freezing, network eventually reaches full capacity, making it difficult or impossible to learn new tasks.\n",
      "        - The technique is extremely simple—just a naive greedy algorithm—making the problem neither hard nor particularly useful. (L0, 1)\n",
      "          * Examples:\n",
      "            - The technique is extremely simple—just a naive greedy algorithm—making the problem neither hard nor particularly useful.\n",
      "      - Privacy Fairness and Robustness Claims Not Fully Supported or Practical (L1, 4)\n",
      "        - Obtaining adversarial robustness leads to degraded model performance on natural examples. (L0, 1)\n",
      "          * Examples:\n",
      "            - Obtaining adversarial robustness leads to degraded model performance on natural examples.\n",
      "        - The adversarial examples are highly noticeable to human ears, undermining their intended imperceptibility. (L0, 1)\n",
      "          * Examples:\n",
      "            - The adversarial examples are highly noticeable to human ears, undermining their intended imperceptibility.\n",
      "        - The differentially private objective does not guarantee differentially private parameters, undermining the privacy claim. (L0, 1)\n",
      "          * Examples:\n",
      "            - The differentially private objective does not guarantee differentially private parameters, undermining the privacy claim.\n",
      "        - The relationship between adversarial examples and certified robustness is unclear, questioning the interest of the algorithm. (L0, 1)\n",
      "          * Examples:\n",
      "            - The relationship between adversarial examples and certified robustness is unclear, questioning the interest of the algorithm.\n",
      "      - Unclear Effectiveness or Applicability in Practical or Competitive Settings (L1, 7)\n",
      "        - After improved evaluation, method lacks statistical significance and is not generally applicable; unclear when it works best. (L0, 1)\n",
      "          * Examples:\n",
      "            - After improved evaluation, method lacks statistical significance and is not generally applicable; unclear when it works best.\n",
      "        - It is unclear how the method works for output-rich models where much input information needs to be preserved. (L0, 1)\n",
      "          * Examples:\n",
      "            - It is unclear how the method works for output-rich models where much input information needs to be preserved.\n",
      "        - It is unclear if the proposed method solves GNNs' overfitting/representation power issues as claimed. (L0, 1)\n",
      "          * Examples:\n",
      "            - It is unclear if the proposed method solves GNNs' overfitting/representation power issues as claimed.\n",
      "        - Tasks are too easy; unclear if method works on real-world datasets with ill-defined object numbers. (L0, 1)\n",
      "          * Examples:\n",
      "            - Tasks are too easy; unclear if method works on real-world datasets with ill-defined object numbers.\n",
      "        - The stability of the method remains unclear, which is important for real applications. (L0, 1)\n",
      "          * Examples:\n",
      "            - The stability of the method remains unclear, which is important for real applications.\n",
      "        - Unclear if the proposed method is competitive with more recent state-of-the-art solutions to Exposure Bias. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear if the proposed method is competitive with more recent state-of-the-art solutions to Exposure Bias.\n",
      "        - Weak experimental section, unfair and incomplete comparisons against relevant baselines. (L0, 1)\n",
      "          * Examples:\n",
      "            - Weak experimental section, unfair and incomplete comparisons against relevant baselines.\n",
      "      - Unclear Research Motivation, Theoretical Relevance, or Restrictive Assumptions (L1, 6)\n",
      "        - Experimental evaluation is not consistent or complete enough; core motivation for using INNs is unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experimental evaluation is not consistent or complete enough; core motivation for using INNs is unclear.\n",
      "        - Lacks research motivation and solid experimental validation. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lacks research motivation and solid experimental validation.\n",
      "        - Limited experimental evaluation and questionable applicability of assumptions and theoretical setting. (L0, 1)\n",
      "          * Examples:\n",
      "            - Limited experimental evaluation and questionable applicability of assumptions and theoretical setting.\n",
      "        - Major concerns about novelty and experimental setups. (L0, 1)\n",
      "          * Examples:\n",
      "            - Major concerns about novelty and experimental setups.\n",
      "        - The contribution is fairly small, mainly engineering, offering little insight and limited experimental support. (L0, 1)\n",
      "          * Examples:\n",
      "            - The contribution is fairly small, mainly engineering, offering little insight and limited experimental support.\n",
      "        - Unclear how restrictive the assumptions (especially Definition 1.4) are; doubts about their meaningfulness and experimental satisfaction. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear how restrictive the assumptions (especially Definition 1.4) are; doubts about their meaningfulness and experimental satisfaction.\n",
      "      - Unclear or Insufficient Explanation or Justification of Method, Theory, or Motivation (L1, 22)\n",
      "        - Assumptions about treating response entries as samples on GGD are confusing and need clear evidence or explanation. (L0, 1)\n",
      "          * Examples:\n",
      "            - Assumptions about treating response entries as samples on GGD are confusing and need clear evidence or explanation.\n",
      "        - Confusing motivation, especially regarding reward ambiguity, and lack of clear explanation in the paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - Confusing motivation, especially regarding reward ambiguity, and lack of clear explanation in the paper.\n",
      "        - Discussion and derivation remain too vague and require better and clearer explanation. (L0, 1)\n",
      "          * Examples:\n",
      "            - Discussion and derivation remain too vague and require better and clearer explanation.\n",
      "        - Insufficient motivation and explanation for key algorithmic decisions (e.g., GT-Layers, bottom/up blocks, accuracy criteria). (L0, 1)\n",
      "          * Examples:\n",
      "            - Insufficient motivation and explanation for key algorithmic decisions (e.g., GT-Layers, bottom/up blocks, accuracy criteria).\n",
      "        - Lack of clarity and explanation of novelty; Eq. (3) not sufficiently motivated or detailed. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of clarity and explanation of novelty; Eq. (3) not sufficiently motivated or detailed.\n",
      "        - Lack of clarity in the motivation for the proposed methods and insufficient explanation of why they improve results. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of clarity in the motivation for the proposed methods and insufficient explanation of why they improve results.\n",
      "        - Lack of clear connection between analytical investigation and empirical results; theoretical results may overstate conclusions. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of clear connection between analytical investigation and empirical results; theoretical results may overstate conclusions.\n",
      "        - Lack of clear explanation for why the method works and how it prevents overfitting. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of clear explanation for why the method works and how it prevents overfitting.\n",
      "        - Lack of clear explanation of Lemma 2 and its connection to main results. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of clear explanation of Lemma 2 and its connection to main results.\n",
      "        - Lack of clear explanation of advantages of ATMC over existing SG-MCMC methods such as [1]. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of clear explanation of advantages of ATMC over existing SG-MCMC methods such as [1].\n",
      "        - Lack of explanation and supporting evidence for the claimed efficiency of the proposed model. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of explanation and supporting evidence for the claimed efficiency of the proposed model.\n",
      "        - Lack of explanation on why biased estimators still guarantee convergence; experiment results contradict claimed sublinear theoretical convergence. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of explanation on why biased estimators still guarantee convergence; experiment results contradict claimed sublinear theoretical convergence.\n",
      "        - Lack of explanation or intuition for why the proposed method works. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of explanation or intuition for why the proposed method works.\n",
      "        - Lack of intuition or theoretical explanation for why the method improves on past approaches. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of intuition or theoretical explanation for why the method improves on past approaches.\n",
      "        - Lack of reasoning and explanation about each component, especially local and global motion composition. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of reasoning and explanation about each component, especially local and global motion composition.\n",
      "        - Motivation and explanation are insufficient; rationale for the approach is unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - Motivation and explanation are insufficient; rationale for the approach is unclear.\n",
      "        - No further investigation on why running average causes vulnerability; insufficient explanation on distribution changes. (L0, 1)\n",
      "          * Examples:\n",
      "            - No further investigation on why running average causes vulnerability; insufficient explanation on distribution changes.\n",
      "        - The definition and explanation of Equation 2 and variable z are unclear and confusing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The definition and explanation of Equation 2 and variable z are unclear and confusing.\n",
      "        - The explanation of the method is not very precise and could be greatly improved. (L0, 1)\n",
      "          * Examples:\n",
      "            - The explanation of the method is not very precise and could be greatly improved.\n",
      "        - The explanation of the subset scanning approach and its optimality is unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The explanation of the subset scanning approach and its optimality is unclear.\n",
      "        - Unclear and confusing explanation of discriminator network inputs, especially regarding temporal consistency modeling. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear and confusing explanation of discriminator network inputs, especially regarding temporal consistency modeling.\n",
      "        - Unclear if and how multiple HaarPool layers could be used; explanation on this is requested. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear if and how multiple HaarPool layers could be used; explanation on this is requested.\n",
      "      - Unconvincing Support for Claims/Advantages or Theoretical Motivation (L1, 11)\n",
      "        - Experiments are not convincing and the modification doesn’t provide much insight into representation learning/anomaly detection. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments are not convincing and the modification doesn’t provide much insight into representation learning/anomaly detection.\n",
      "        - Experiments are not sufficient to validate the model's advantage; datasets are small and baselines are weak. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments are not sufficient to validate the model's advantage; datasets are small and baselines are weak.\n",
      "        - Experiments do not convincingly show ZO-MIN-MAX is better than other zeroth-order methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - Experiments do not convincingly show ZO-MIN-MAX is better than other zeroth-order methods.\n",
      "        - More experiments are required to support usefulness and effectiveness, especially for weak attacks like FGSM. (L0, 1)\n",
      "          * Examples:\n",
      "            - More experiments are required to support usefulness and effectiveness, especially for weak attacks like FGSM.\n",
      "        - The analysis and experiments are too weak to support the authors' claim. (L0, 1)\n",
      "          * Examples:\n",
      "            - The analysis and experiments are too weak to support the authors' claim.\n",
      "        - The approach needs more detailed description and experiments do not satisfy the claims made in the Introduction. (L0, 1)\n",
      "          * Examples:\n",
      "            - The approach needs more detailed description and experiments do not satisfy the claims made in the Introduction.\n",
      "        - The main claim that NovoGrad is robust to learning rate and initialization is not convincingly supported by experiments. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main claim that NovoGrad is robust to learning rate and initialization is not convincingly supported by experiments.\n",
      "        - The method is not well justified by theory or practice; experiments are weak and do not support effectiveness. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is not well justified by theory or practice; experiments are weak and do not support effectiveness.\n",
      "        - The proposed approach is not sufficiently justified and experiments do not sufficiently explore the benefits (no ablation study). (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed approach is not sufficiently justified and experiments do not sufficiently explore the benefits (no ablation study).\n",
      "        - There is no discussion and comparison in the experiments regarding the claim that the technique is less time-consuming. (L0, 1)\n",
      "          * Examples:\n",
      "            - There is no discussion and comparison in the experiments regarding the claim that the technique is less time-consuming.\n",
      "        - Unclear how much progress over 'Axiomatic Attribution for Deep Networks' (Sundararajan et al. 2017); experiments seem similar. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear how much progress over 'Axiomatic Attribution for Deep Networks' (Sundararajan et al. 2017); experiments seem similar.\n",
      "      - Weak or Missing Motivation / Problem Justification (L1, 5)\n",
      "        - The motivation and necessity for using two networks (teacher and student) is unclear and not convincingly justified. (L0, 1)\n",
      "          * Examples:\n",
      "            - The motivation and necessity for using two networks (teacher and student) is unclear and not convincingly justified.\n",
      "        - The motivation for the method and justification for adding entropy regularization are unclear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The motivation for the method and justification for adding entropy regularization are unclear.\n",
      "        - The paper fails to sufficiently motivate contextual IRL as a useful specialization of inverse reinforcement learning. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper fails to sufficiently motivate contextual IRL as a useful specialization of inverse reinforcement learning.\n",
      "        - Unclear motivation and confusing paper structure, making theoretical contributions hard to appreciate. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear motivation and confusing paper structure, making theoretical contributions hard to appreciate.\n",
      "        - Unclear motivation for proposing a complex method for an apparently simple binary classification task. (L0, 1)\n",
      "          * Examples:\n",
      "            - Unclear motivation for proposing a complex method for an apparently simple binary classification task.\n",
      "    - Lack of Clarity and Completeness in Method and Presentation (L2, 12)\n",
      "      - Clarity and Completeness of Method Description (L1, 7)\n",
      "        - It is not trivial to get from using this tool to a working method. (L0, 1)\n",
      "          * Examples:\n",
      "            - It is not trivial to get from using this tool to a working method.\n",
      "        - The description of the method, particularly the instance classifier and weighting process, is not very clear. (L0, 1)\n",
      "          * Examples:\n",
      "            - The description of the method, particularly the instance classifier and weighting process, is not very clear.\n",
      "        - The description of the overall method is too high-level; needs clearer formalization of network architectures and framework. (L0, 1)\n",
      "          * Examples:\n",
      "            - The description of the overall method is too high-level; needs clearer formalization of network architectures and framework.\n",
      "        - The key training method is only half described, lacking abstraction from specific architectures. (L0, 1)\n",
      "          * Examples:\n",
      "            - The key training method is only half described, lacking abstraction from specific architectures.\n",
      "        - The method is not clearly explained, making it hard to understand or replicate. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is not clearly explained, making it hard to understand or replicate.\n",
      "        - The reviewer had a hard time understanding key elements and justification for the proposed method. (L0, 1)\n",
      "          * Examples:\n",
      "            - The reviewer had a hard time understanding key elements and justification for the proposed method.\n",
      "        - The technical details of the method are not sufficiently explained or centralized, especially how statistics are computed and handled. (L0, 1)\n",
      "          * Examples:\n",
      "            - The technical details of the method are not sufficiently explained or centralized, especially how statistics are computed and handled.\n",
      "      - Repeated description of the framework in the introduction. (L1, 1)\n",
      "        * Examples:\n",
      "          - Repeated description of the framework in the introduction.\n",
      "      - The description of the task, problem setup, and mathematical notation is unclear and hard to follow. (L1, 1)\n",
      "        * Examples:\n",
      "          - The description of the task, problem setup, and mathematical notation is unclear and hard to follow.\n",
      "      - Unclear Presentation, Method Description, or Reproducibility (L1, 2)\n",
      "        - The presentation is unclear and informal, with many undefined terms and incorrect or missing steps. (L0, 1)\n",
      "          * Examples:\n",
      "            - The presentation is unclear and informal, with many undefined terms and incorrect or missing steps.\n",
      "        - The procedure is quite unclear and could not be reimplemented from reading the manuscript. (L0, 1)\n",
      "          * Examples:\n",
      "            - The procedure is quite unclear and could not be reimplemented from reading the manuscript.\n",
      "      - Unclear how theory applies to experiments using an attention network instead of CNN. (L1, 1)\n",
      "        * Examples:\n",
      "          - Unclear how theory applies to experiments using an attention network instead of CNN.\n",
      "    - Reproducibility, Reporting, or Clarity Deficiencies (L2, 5)\n",
      "      - Reproducibility and Reporting Deficiencies (L1, 3)\n",
      "        - Lack of GAN implementation details and hyperparameters used for training. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of GAN implementation details and hyperparameters used for training.\n",
      "        - Lack of provided code to validate the soundness of the model. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of provided code to validate the soundness of the model.\n",
      "        - Lack of reproducibility and repeatability: insufficient details on hyperparameters and lack of repeated training runs with performance variation. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of reproducibility and repeatability: insufficient details on hyperparameters and lack of repeated training runs with performance variation.\n",
      "      - The motivation section is not detailed enough, making the paper harder to understand. (L1, 1)\n",
      "        * Examples:\n",
      "          - The motivation section is not detailed enough, making the paper harder to understand.\n",
      "      - There are serious typos making equation (3) incorrect, requiring significant modification to correct errors throughout. (L1, 1)\n",
      "        * Examples:\n",
      "          - There are serious typos making equation (3) incorrect, requiring significant modification to correct errors throughout.\n",
      "  - Related Work, Contextualization, and Citation Issues (L3, 49)\n",
      "    - Citation and Related Work Deficiencies (L2, 2)\n",
      "      - The paper misses very relevant prior papers with similar ideas, failing to properly position its contribution. (L1, 1)\n",
      "        * Examples:\n",
      "          - The paper misses very relevant prior papers with similar ideas, failing to properly position its contribution.\n",
      "      - Wrong citation format: use of citet instead of citep. Correct for all citations. (L1, 1)\n",
      "        * Examples:\n",
      "          - Wrong citation format: use of citet instead of citep. Correct for all citations.\n",
      "    - Inadequate Contextualization and Positioning with Prior or Related Work (L2, 47)\n",
      "      - Incremental or Non-novel Contribution and Weak Advancement Over Prior Work (L1, 26)\n",
      "        - An important error in the main result and other issues raise doubts on the theoretical contribution of the paper. (L0, 1)\n",
      "          * Examples:\n",
      "            - An important error in the main result and other issues raise doubts on the theoretical contribution of the paper.\n",
      "        - Architectural/theoretical contribution is not significant; idea is not hugely novel and requires more demonstration. (L0, 1)\n",
      "          * Examples:\n",
      "            - Architectural/theoretical contribution is not significant; idea is not hugely novel and requires more demonstration.\n",
      "        - Contribution is incremental and novelty is limited; mainly derives from adding adaptive criteria to an existing algorithm. (L0, 1)\n",
      "          * Examples:\n",
      "            - Contribution is incremental and novelty is limited; mainly derives from adding adaptive criteria to an existing algorithm.\n",
      "        - Contribution is meaningful but incremental; method is almost similar to RotDCF. (L0, 1)\n",
      "          * Examples:\n",
      "            - Contribution is meaningful but incremental; method is almost similar to RotDCF.\n",
      "        - Defending against Gaussian noise is too simple and not an exciting or meaningful contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - Defending against Gaussian noise is too simple and not an exciting or meaningful contribution.\n",
      "        - Incremental methodological contribution. (L0, 1)\n",
      "          * Examples:\n",
      "            - Incremental methodological contribution.\n",
      "        - The contribution is incremental, not principled, and unconvincing—more of an engineering trick. (L0, 1)\n",
      "          * Examples:\n",
      "            - The contribution is incremental, not principled, and unconvincing—more of an engineering trick.\n",
      "        - The contribution is limited as the approach is not shown to be generalizable to other tasks. (L0, 1)\n",
      "          * Examples:\n",
      "            - The contribution is limited as the approach is not shown to be generalizable to other tasks.\n",
      "        - The contribution is limited because the proposed approach only offers comparable results with SOTA (early stopping). (L0, 1)\n",
      "          * Examples:\n",
      "            - The contribution is limited because the proposed approach only offers comparable results with SOTA (early stopping).\n",
      "        - The contribution is not novel; theoretical results follow directly from standard tools without introducing new ideas. (L0, 1)\n",
      "          * Examples:\n",
      "            - The contribution is not novel; theoretical results follow directly from standard tools without introducing new ideas.\n",
      "        - The contribution may not be sufficient; the work is seen as incremental. (L0, 1)\n",
      "          * Examples:\n",
      "            - The contribution may not be sufficient; the work is seen as incremental.\n",
      "        - The contribution of this paper is very incremental. (L0, 1)\n",
      "          * Examples:\n",
      "            - The contribution of this paper is very incremental.\n",
      "        - The contribution on the theoretical or methodological side seems a bit weak. (L0, 1)\n",
      "          * Examples:\n",
      "            - The contribution on the theoretical or methodological side seems a bit weak.\n",
      "        - The improvement mainly comes from existing supernode ideas, making the paper incremental and lacking novelty. (L0, 1)\n",
      "          * Examples:\n",
      "            - The improvement mainly comes from existing supernode ideas, making the paper incremental and lacking novelty.\n",
      "        - The main contribution is not particularly novel and is similar to existing work. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main contribution is not particularly novel and is similar to existing work.\n",
      "        - The main theoretical contribution (canonical space) is just standard Fourier analysis, not a new tool. (L0, 1)\n",
      "          * Examples:\n",
      "            - The main theoretical contribution (canonical space) is just standard Fourier analysis, not a new tool.\n",
      "        - The method and experiments are not convincing and contribution is limited. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method and experiments are not convincing and contribution is limited.\n",
      "        - The method is a bit incremental over the GAIN approach; improvements are straightforward. (L0, 1)\n",
      "          * Examples:\n",
      "            - The method is a bit incremental over the GAIN approach; improvements are straightforward.\n",
      "        - The model is trivial (auto-encoders with L2 loss), so novelty is only incremental. (L0, 1)\n",
      "          * Examples:\n",
      "            - The model is trivial (auto-encoders with L2 loss), so novelty is only incremental.\n",
      "        - The modeling contribution is incremental given existing literature. (L0, 1)\n",
      "          * Examples:\n",
      "            - The modeling contribution is incremental given existing literature.\n",
      "        - The novelty may be incremental; unclear significance vs. VEEGAN and AAE. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty may be incremental; unclear significance vs. VEEGAN and AAE.\n",
      "        - The novelty of the proposed approach is not high and improvements are incremental. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty of the proposed approach is not high and improvements are incremental.\n",
      "        - The novelty seems somewhat incremental. (L0, 1)\n",
      "          * Examples:\n",
      "            - The novelty seems somewhat incremental.\n",
      "        - The proposed method is considered incremental, lacking significant novelty over existing multi-task and self-supervision approaches. (L0, 1)\n",
      "          * Examples:\n",
      "            - The proposed method is considered incremental, lacking significant novelty over existing multi-task and self-supervision approaches.\n",
      "        - The theoretical contribution is incremental; no justification or proof for layer-wise normalization improvements. (L0, 1)\n",
      "          * Examples:\n",
      "            - The theoretical contribution is incremental; no justification or proof for layer-wise normalization improvements.\n",
      "        - The work is incremental with no novel idea; improvement comes solely from applying an existing method. (L0, 1)\n",
      "          * Examples:\n",
      "            - The work is incremental with no novel idea; improvement comes solely from applying an existing method.\n",
      "      - Insufficient Context, References or Experimental Rigor Compared to Prior Work (L1, 4)\n",
      "        - Doubtful experiment design and reporting, especially copying results from prior work without ensuring comparable settings. (L0, 1)\n",
      "          * Examples:\n",
      "            - Doubtful experiment design and reporting, especially copying results from prior work without ensuring comparable settings.\n",
      "        - Lack of references to related work, especially missing key prior work. (L0, 1)\n",
      "          * Examples:\n",
      "            - Lack of references to related work, especially missing key prior work.\n",
      "        - Missing context and references to relevant prior work and similar issues addressed in existing literature. (L0, 1)\n",
      "          * Examples:\n",
      "            - Missing context and references to relevant prior work and similar issues addressed in existing literature.\n",
      "        - The paper's experiments lack rigorous statistical testing compared to prior work, making findings less convincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper's experiments lack rigorous statistical testing compared to prior work, making findings less convincing.\n",
      "      - Insufficient Positioning or Comparison with Related Work (L1, 8)\n",
      "        - Comparisons to existing methods may not be comprehensive enough. (L0, 1)\n",
      "          * Examples:\n",
      "            - Comparisons to existing methods may not be comprehensive enough.\n",
      "        - No sound comparison with NAO; reviewer doubts RL works better than gradient descent in continuous space. (L0, 1)\n",
      "          * Examples:\n",
      "            - No sound comparison with NAO; reviewer doubts RL works better than gradient descent in continuous space.\n",
      "        - Paper needs much better evaluation and discussion, especially in comparison to related methods like GAIN and multiple imputation. (L0, 1)\n",
      "          * Examples:\n",
      "            - Paper needs much better evaluation and discussion, especially in comparison to related methods like GAIN and multiple imputation.\n",
      "        - The approach is very similar to deep CCA, but this connection is never mentioned or clarified. (L0, 1)\n",
      "          * Examples:\n",
      "            - The approach is very similar to deep CCA, but this connection is never mentioned or clarified.\n",
      "        - The evaluation lacks comparison with relevant, cheap attention mechanisms such as Squeeze-and-Excitation. (L0, 1)\n",
      "          * Examples:\n",
      "            - The evaluation lacks comparison with relevant, cheap attention mechanisms such as Squeeze-and-Excitation.\n",
      "        - The paper insufficiently positions itself with respect to relevant literature, especially failing to cite/discuss a very similar previous work. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper insufficiently positions itself with respect to relevant literature, especially failing to cite/discuss a very similar previous work.\n",
      "        - The paper misses significant recent work showing that invariant representations can hurt generalization when label distributions differ. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper misses significant recent work showing that invariant representations can hurt generalization when label distributions differ.\n",
      "        - The paper omits discussion and comparison with a very related prior work (Li et al., 2019). (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper omits discussion and comparison with a very related prior work (Li et al., 2019).\n",
      "      - Lack of Contextualization in Prior Work or Literature (L1, 2)\n",
      "        - The paper does not make use of representation learning and is therefore out-of-place at ICLR. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper does not make use of representation learning and is therefore out-of-place at ICLR.\n",
      "        - The technical challenges addressed have been extensively studied in related literature, which the paper does not sufficiently reference or contextualize. (L0, 1)\n",
      "          * Examples:\n",
      "            - The technical challenges addressed have been extensively studied in related literature, which the paper does not sufficiently reference or contextualize.\n",
      "      - Missing or Incomplete Discussion of Related Work or References (L1, 2)\n",
      "        - The paper lacks important latest references on domain adaptation, especially omitting recent deep methods, making contributions unconvincing. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks important latest references on domain adaptation, especially omitting recent deep methods, making contributions unconvincing.\n",
      "        - The paper lacks many references and comparisons with recent multi-frame super-resolution methods. (L0, 1)\n",
      "          * Examples:\n",
      "            - The paper lacks many references and comparisons with recent multi-frame super-resolution methods.\n",
      "      - Unclear Positioning, Related Work, or Literature Review (L1, 5)\n",
      "        - Comparison to previous related works and variants is insufficient. (L0, 1)\n",
      "          * Examples:\n",
      "            - Comparison to previous related works and variants is insufficient.\n",
      "        - Insufficient neuroscientific evidence for the assumptions about feedback connections in real neurons. (L0, 1)\n",
      "          * Examples:\n",
      "            - Insufficient neuroscientific evidence for the assumptions about feedback connections in real neurons.\n",
      "        - Key related references are missing and the proposed method does not outperform the strongest baselines. (L0, 1)\n",
      "          * Examples:\n",
      "            - Key related references are missing and the proposed method does not outperform the strongest baselines.\n",
      "        - Missing crucial baselines that address similar problems, lacking proper positioning relative to related literature. (L0, 1)\n",
      "          * Examples:\n",
      "            - Missing crucial baselines that address similar problems, lacking proper positioning relative to related literature.\n",
      "        - The literature review on previous work for continuous-time, discrete events prediction is not thorough enough. (L0, 1)\n",
      "          * Examples:\n",
      "            - The literature review on previous work for continuous-time, discrete events prediction is not thorough enough.\n",
      "- Reviewer/Review Process Limitations (L4, 3645)\n",
      "  - No Substantive Weakness Provided (L3, 3643)\n",
      "    - No main negative comment is provided in the review. (L2, 1)\n",
      "      * Examples:\n",
      "        - No main negative comment is provided in the review.\n",
      "    - No major complaints; rejection based on reviewer 1's points, not stated here. (L2, 1)\n",
      "      * Examples:\n",
      "        - No major complaints; rejection based on reviewer 1's points, not stated here.\n",
      "    - None explicitly mentioned. (L2, 1)\n",
      "      * Examples:\n",
      "        - None explicitly mentioned.\n",
      "    - None mentioned. (L2, 2)\n",
      "      * Examples:\n",
      "        - None mentioned.\n",
      "        - None mentioned.\n",
      "    - nan (L2, 3638)\n",
      "      * Examples:\n",
      "        - Unknown\n",
      "        - Unknown\n",
      "        - Unknown\n",
      "  - No significant limitation or negative aspect identified. (L3, 1)\n",
      "    * Examples:\n",
      "      - No significant limitation or negative aspect identified.\n",
      "  - Reviewer lacks knowledge in the area and could not thoroughly evaluate the paper. (L3, 1)\n",
      "    * Examples:\n",
      "      - Reviewer lacks knowledge in the area and could not thoroughly evaluate the paper.\n",
      "Calculated maximum depth for CSV columns: 5\n",
      "\n",
      "Saving indented hierarchy data to hierarchy_cs.csv...\n",
      "Save complete.\n",
      "NOTE: View 'hierarchy_cs.csv' in spreadsheet software for indentation effect.\n",
      "\n",
      "Processing Complete.\n"
     ]
    }
   ],
   "source": [
    "print_clusters(dfr_cs_h, 'main_limitation', 'hierarchy_cs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a63c5",
   "metadata": {},
   "source": [
    "Building Tree...\n",
    "Detected and using levels (highest to lowest): ['_level_4', '_level_3', '_level_2', '_level_1', '_level_0']\n",
    "\n",
    "Printing Tree Structure:\n",
    "Warning: LEVELS_TO_PRINT (6) is greater than total levels available (5). Printing all levels.\n",
    "- Correlation between PE score and human judgement is unsurprising because CSF already encodes human perceptual characteristics. (L4, 1)\n",
    "  * Examples:\n",
    "    - Correlation between PE score and human judgement is unsurprising because CSF already encodes human perceptual characteristics.\n",
    "- Ethical and Policy Violations (L4, 5)\n",
    "  - Blind Review or Policy Violations (L3, 2)\n",
    "    - Violates blind-review policy by exposing author identity in acknowledgements. (L2, 1)\n",
    "      * Examples:\n",
    "        - Violates blind-review policy by exposing author identity in acknowledgements.\n",
    "    - Violating double blind rule. (L2, 1)\n",
    "      * Examples:\n",
    "        - Violating double blind rule.\n",
    "  - Ethical Issues and Scientific Integrity (Plagiarism, Double-Blind Violations) (L3, 2)\n",
    "    - Theorem 2 and its proof are plagiarized and presented as the authors' own work. (L2, 1)\n",
    "      * Examples:\n",
    "        - Theorem 2 and its proof are plagiarized and presented as the authors' own work.\n",
    "    - Violation of the double blind review rule and similarity to previously published work. (L2, 1)\n",
    "      * Examples:\n",
    "        - Violation of the double blind review rule and similarity to previously published work.\n",
    "  - The paper was already accepted at a Neuro AI workshop, violating double-blind review rules. (L3, 1)\n",
    "    * Examples:\n",
    "      - The paper was already accepted at a Neuro AI workshop, violating double-blind review rules.\n",
    "- Methodology and Scientific Rigor (L4, 745)\n",
    "  - Experimental Validation and Evaluation Design Issues (L3, 378)\n",
    "    - Comparison, Evaluation, and Experimental Design Weaknesses (L2, 7)\n",
    "      - ARAC seeks diversity in fitness space, which is weaker than behavior/outcome space used in QD and NS. (L1, 1)\n",
    "        * Examples:\n",
    "          - ARAC seeks diversity in fitness space, which is weaker than behavior/outcome space used in QD and NS.\n",
    "      - Authors should compare using QAP for assignment with GSE to better assess their method. (L1, 1)\n",
    "        * Examples:\n",
    "          - Authors should compare using QAP for assignment with GSE to better assess their method.\n",
    "      - Performance drops sharply after 50% pruning across all datasets (Figure 4). (L1, 1)\n",
    "        * Examples:\n",
    "          - Performance drops sharply after 50% pruning across all datasets (Figure 4).\n",
    "      - TVmax achieves only a little improvement over the baseline (softmax) on automated metrics. (L1, 1)\n",
    "        * Examples:\n",
    "          - TVmax achieves only a little improvement over the baseline (softmax) on automated metrics.\n",
    "      - The method was not tested on other NLP benchmarks like text classification or NER. (L1, 1)\n",
    "        * Examples:\n",
    "          - The method was not tested on other NLP benchmarks like text classification or NER.\n",
    "      - Weak Integration between Theory and Experiments (L1, 2)\n",
    "        - The theoretical and empirical parts are not well-connected; variance of the estimator is not quantified. (L0, 1)\n",
    "          * Examples:\n",
    "            - The theoretical and empirical parts are not well-connected; variance of the estimator is not quantified.\n",
    "        - Theoretical and empirical analyses are not sufficiently novel, deep, or informative. (L0, 1)\n",
    "          * Examples:\n",
    "            - Theoretical and empirical analyses are not sufficiently novel, deep, or informative.\n",
    "    - Computational Inefficiency and Resource Overhead (L2, 3)\n",
    "      - The method introduces more memory overhead compared to GPipe. (L1, 1)\n",
    "        * Examples:\n",
    "          - The method introduces more memory overhead compared to GPipe.\n",
    "      - The method is computationally costly and sacrifices network flexibility, being a brute-force application without practical remedies. (L1, 1)\n",
    "        * Examples:\n",
    "          - The method is computationally costly and sacrifices network flexibility, being a brute-force application without practical remedies.\n",
    "      - The method requires storing an increasing number of models, leading to huge storage costs, making it economically infeasible. (L1, 1)\n",
    "        * Examples:\n",
    "          - The method requires storing an increasing number of models, leading to huge storage costs, making it economically infeasible.\n",
    "    - Evaluation Metrics, Ablation Studies, and Analysis Limitations (L2, 6)\n",
    "      - Missing Evaluation Metrics/Uncertainty/Ablation Studies (L1, 4)\n",
    "        - Experimental validation is weak: no proper baselines, limited datasets, and unclear evaluation metrics. (L0, 1)\n",
    "          * Examples:\n",
    "            - Experimental validation is weak: no proper baselines, limited datasets, and unclear evaluation metrics.\n",
    "        - Lack of ablation experiments for FALCON's components and need for experimental justification of convolution order. (L0, 1)\n",
    "          * Examples:\n",
    "            - Lack of ablation experiments for FALCON's components and need for experimental justification of convolution order.\n",
    "        - Lack of uncertainty (e.g., error bars) in experimental plots. (L0, 1)\n",
    "          * Examples:\n",
    "            - Lack of uncertainty (e.g., error bars) in experimental plots.\n",
    "        - The experimental evaluation is insufficient, lacking details about TensorFlow comparison and missing ablation studies. (L0, 1)\n",
    "          * Examples:\n",
    "            - The experimental evaluation is insufficient, lacking details about TensorFlow comparison and missing ablation studies.\n",
    "      - Unvalidated or Questionable Evaluation Metrics and Analysis (L1, 2)\n",
    "        - No standard deviation reported for results; possible high variance in outcomes when using low bit quantization. (L0, 1)\n",
    "          * Examples:\n",
    "            - No standard deviation reported for results; possible high variance in outcomes when using low bit quantization.\n",
    "        - The new ‘concept accuracy’ metric is unvalidated and poorly correlated with interpretability, making the results unconvincing. (L0, 1)\n",
    "          * Examples:\n",
    "            - The new ‘concept accuracy’ metric is unvalidated and poorly correlated with interpretability, making the results unconvincing.\n",
    "    - Evaluation, Empirics, and Implementation Issues (L2, 19)\n",
    "      - Dataset and Evaluation Limitations (L1, 4)\n",
    "        - None explicitly stated. (L0, 2)\n",
    "          * Examples:\n",
    "            - None explicitly stated.\n",
    "            - None explicitly stated.\n",
    "        - None stated. (L0, 1)\n",
    "          * Examples:\n",
    "            - None stated.\n",
    "        - PUGAN was tested only on a simple dataset and quality is only comparable to WaveGAN, not advancing fidelity. (L0, 1)\n",
    "          * Examples:\n",
    "            - PUGAN was tested only on a simple dataset and quality is only comparable to WaveGAN, not advancing fidelity.\n",
    "      - Insufficient discussion on why the model underperforms for Russian in word-translation retrieval. (L1, 1)\n",
    "        * Examples:\n",
    "          - Insufficient discussion on why the model underperforms for Russian in word-translation retrieval.\n",
    "      - Model Efficiency and Hardware Realism Issues (L1, 4)\n",
    "        - ESN is not suitable for existing hardware; it relies on unrealistic assumptions. (L0, 1)\n",
    "          * Examples:\n",
    "            - ESN is not suitable for existing hardware; it relies on unrealistic assumptions.\n",
    "        - It is unclear how speed-up is calculated and whether MAC reduction correlates to actual hardware speed-up. (L0, 1)\n",
    "          * Examples:\n",
    "            - It is unclear how speed-up is calculated and whether MAC reduction correlates to actual hardware speed-up.\n",
    "        - The computational efficiency of conventional NLP models is well known to researchers. (L0, 1)\n",
    "          * Examples:\n",
    "            - The computational efficiency of conventional NLP models is well known to researchers.\n",
    "        - Using total number of bits as equivalent measure between models doesn't account for actual inference latency or efficiency on hardware. (L0, 1)\n",
    "          * Examples:\n",
    "            - Using total number of bits as equivalent measure between models doesn't account for actual inference latency or efficiency on hardware.\n",
    "      - Poor Generalization or Overfitting to Experimental Setting (L1, 2)\n",
    "        - Accelerated diffusion causes over-smoothing quickly, limiting the feasible number of layers and deep network non-linearity. (L0, 1)\n",
    "          * Examples:\n",
    "            - Accelerated diffusion causes over-smoothing quickly, limiting the feasible number of layers and deep network non-linearity.\n",
    "        - Generalization is unconvincing because both tasks use the same environments and features. (L0, 1)\n",
    "          * Examples:\n",
    "            - Generalization is unconvincing because both tasks use the same environments and features.\n",
    "      - Reproducibility and Supporting Materials Issues (L1, 3)\n",
    "        - Lacks many details in key sections, preventing full appreciation and making reproducibility impossible. (L0, 1)\n",
    "          * Examples:\n",
    "            - Lacks many details in key sections, preventing full appreciation and making reproducibility impossible.\n",
    "        - Reproducibility is difficult as source code and some datasets are not provided or are not public. (L0, 1)\n",
    "          * Examples:\n",
    "            - Reproducibility is difficult as source code and some datasets are not provided or are not public.\n",
    "        - The paper suffers from an incomplete evaluation, so acceptance is not recommended. (L0, 1)\n",
    "          * Examples:\n",
    "            - The paper suffers from an incomplete evaluation, so acceptance is not recommended.\n",
    "      - Superficiality or Simplistic Experimental Setting (L1, 5)\n",
    "        - Exposition is difficult to follow and experiments are not very compelling. (L0, 1)\n",
    "          * Examples:\n",
    "            - Exposition is difficult to follow and experiments are not very compelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf35918",
   "metadata": {},
   "source": [
    "### add a refinement step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2887943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 12:20:04,897 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Response:\n",
      "Certainly! Refining such a large and complex cluster hierarchy involves combining similar nodes and standardizing naming conventions to improve clarity and reduce redundancy. Below is a refined, hierarchical structure covering the main themes evident in the original clusters. Each level is presented using standardized, non-overlapping names, and represents a specific granularity of critique, going from fine-grained (bottom) to broad (top).\n",
      "\n",
      "**Refined Cluster Hierarchy**\n",
      "\n",
      "---\n",
      "\n",
      "## Level 4 (Top-level Themes)\n",
      "\n",
      "1. **Methodological Soundness & Novelty**\n",
      "2. **Experimental and Empirical Validation**\n",
      "3. **Clarity, Presentation, & Reproducibility**\n",
      "4. **Positioning & Related Work**\n",
      "5. **Practical Impact & Generalizability**\n",
      "6. **Ethical and Policy Compliance**\n",
      "7. **Review Process / No Substantive Weakness**\n",
      "\n",
      "---\n",
      "\n",
      "## Level 3 (Mid-level Themes)\n",
      "\n",
      "### (1) Methodological Soundness & Novelty\n",
      "- Theoretical Rigor & Mathematical Foundations\n",
      "- Methodological or Analytical Issues\n",
      "- Algorithmic or Technical Contribution & Innovation\n",
      "- Overly Strong/Unrealistic or Unjustified Assumptions\n",
      "\n",
      "### (2) Experimental and Empirical Validation\n",
      "- Empirical Evaluation & Baseline Comparison\n",
      "- Experimental Design & Reporting\n",
      "- Evaluation Metrics & Statistical Analysis\n",
      "- Dataset, Benchmark, & Generalization Limitations\n",
      "\n",
      "### (3) Clarity, Presentation, & Reproducibility\n",
      "- Writing, Organization, & Readability\n",
      "- Method/Algorithm Description Clarity\n",
      "- Reporting, Code/Data Release, & Reproducibility\n",
      "- Motivation, Interpretation, & Justification\n",
      "\n",
      "### (4) Positioning & Related Work\n",
      "- Comparison to Prior Work/Baselines\n",
      "- Discussion of Literature & Contextualization\n",
      "\n",
      "### (5) Practical Impact & Generalizability\n",
      "- Scalability & Real-World Applicability\n",
      "- Audience & Venue Fit\n",
      "\n",
      "### (6) Ethical and Policy Compliance\n",
      "- Blind Review & Policy Adherence\n",
      "- Scientific Integrity & Plagiarism\n",
      "\n",
      "### (7) Review Process / No Substantive Weakness\n",
      "- Reviewer/Process Limitations\n",
      "- No Main Negative Comments\n",
      "\n",
      "---\n",
      "\n",
      "## Level 2 (Lower-level Themes)  \n",
      "Examples within major branches:\n",
      "\n",
      "### (1) Methodological Soundness & Novelty\n",
      "- Lack of Theoretical Rigor or Proofs\n",
      "- Superficial or Trivial Innovation\n",
      "- Incremental or Non-novel Contribution\n",
      "- Overly Strong/Unjustified Assumptions\n",
      "\n",
      "### (2) Experimental and Empirical Validation\n",
      "- Insufficient or Unconvincing Empirical Validation\n",
      "- Limited or Poor Datasets/Benchmarks\n",
      "- Inadequate Baseline or SOTA Comparisons\n",
      "- Unclear/Ambiguous Evaluation Metrics\n",
      "- Lack of Ablation/Uncertainty/Statistics\n",
      "\n",
      "### (3) Clarity, Presentation, & Reproducibility\n",
      "- Disorganized or Ambiguous Writing\n",
      "- Poor Algorithm Description or Missing Details\n",
      "- Lack of Supporting Materials\n",
      "- Unexplained Claims or Insufficient Motivation\n",
      "\n",
      "### (4) Positioning & Related Work\n",
      "- Missing/Inadequate Literature Review or Citations\n",
      "- Lacks Discussion or Differentiation from Prior Work\n",
      "\n",
      "### (5) Practical Impact & Generalizability\n",
      "- Lacks Real-World Demonstration/Scalability\n",
      "- Limited Scope or Audience\n",
      "\n",
      "### (6) Ethical and Policy Compliance  \n",
      "- Double-blind Violation/Conflict  \n",
      "- Plagiarism/Ethics\n",
      "\n",
      "### (7) Review Process / No Substantive Weakness\n",
      "- Reviewer lacks expertise\n",
      "- No significant comments\n",
      "\n",
      "---\n",
      "\n",
      "## Level 1 (Finest granularity – Example labels, assigned to representative types of original node phrases)\n",
      "\n",
      "- \"Proof of Theorem X is flawed\"\n",
      "- \"Experiments are incomplete/unconvincing\"\n",
      "- \"Lack of standard or recent baseline comparison\"\n",
      "- \"The paper is badly written/organized\"\n",
      "- \"Model assumes unrealistic properties\"\n",
      "- \"Missing code/data for reproduction\"\n",
      "- \"Review lacks substantive critique\"\n",
      "\n",
      "---\n",
      "\n",
      "## Explanation & Rationale for Merges/Renaming\n",
      "\n",
      "- **Novelty**: All clusters expressing incremental, trivial, or unoriginal contribution are merged under \"Incremental or Non-novel Contribution\" and \"Superficial or Trivial Innovation.\"\n",
      "- **Empirical Validation**: All clusters on lack of experiments, benchmarks, baselines, metrics, and statistics are merged under broader \"Insufficient or Unconvincing Empirical Validation\" and its subcategories.\n",
      "- **Clarity & Reproducibility**: Clusters regarding poor writing, unclear algorithm description, lack of materials, and hard-to-understand presentation are merged.\n",
      "- **Related Work**: Anything about missing citations, SOTA discussion, or positioning is placed together.\n",
      "- **Generalizability & Practical Impact**: Clusters expressing limited scope, lack of real-world evaluation, or scalability issues are combined.\n",
      "- **Ethics**: All policy (blind review, plagiarism) issues are merged.\n",
      "- **Reviewer/Process Limitations**: All \"no significant negative\" or \"reviewer not qualified\" statements are together.\n",
      "\n",
      "---\n",
      "\n",
      "## How to apply this to your data:\n",
      "Each row's _level_3, _level_2, _level_1, etc., label can be mapped to a corresponding node in this refined tree, using the definitions and phrases here. For automatic mapping, define a dictionary of synonym phrases that map to each new refined node.\n",
      "\n",
      "---\n",
      "\n",
      "### Example Row Mapping\n",
      "\n",
      "Suppose a review says:\n",
      "\n",
      "- _level_0: \"Experiments do not show SOTA results\"\n",
      "- _level_1: \"Insufficient Experimental Thoroughness and Baseline Comparison\"\n",
      "- _level_2: \"Insufficient Experimental Rigor and Baseline Comparison\"\n",
      "\n",
      "This would now map as:\n",
      "\n",
      "- **Level 4:** Experimental and Empirical Validation\n",
      "  - **Level 3:** Empirical Evaluation & Baseline Comparison\n",
      "    - **Level 2:** Insufficient or Unconvincing Empirical Validation (Or, for more granularity, \"Inadequate Baseline or SOTA Comparisons\")\n",
      "      - **Level 1:** \"Insufficient comparison/empirical validation\"\n",
      "\n",
      "---\n",
      "\n",
      "**Summary Table (for reference):**\n",
      "\n",
      "| Level 4                              | Level 3                                      | Level 2                                           |\n",
      "|---------------------------------------|-----------------------------------------------|---------------------------------------------------|\n",
      "| Methodological Soundness & Novelty    | Theoretical Rigor & Analysis                  | Proofs, Foundations, Superficial Novelty          |\n",
      "|                                       | Analytical/Technical Innovation               | Technical/methodological issues                   |\n",
      "|                                       | Assumption Soundness                          | Overly strong/unjustified assumptions             |\n",
      "| Experimental and Empirical Validation | Empirical Evaluation & Baseline Comparison    | Insufficient/weak experiments, poor baselines     |\n",
      "|                                       | Experimental Design & Reporting               | Limited/missing ablations, unclear metrics        |\n",
      "|                                       | Dataset and Generalization                    | Small/biased datasets, poor generalizability      |\n",
      "| Clarity, Presentation, & Reproducibility| Writing, Organization                        | Clarity/readability issues                        |\n",
      "|                                       | Method/Algorithm Description                  | Poor algorithm detail, ambiguity                  |\n",
      "|                                       | Supporting Materials & Reproducibility        | Missing code, details, data                       |\n",
      "|                                       | Motivation and Justification                  | Claims, explanations not well supported           |\n",
      "| Positioning & Related Work            | Literature Review & SOTA Comparison           | Missing or inadequate discussion/citations        |\n",
      "| Practical Impact & Generalizability   | Scalability & Real-World Applicability        | No/poor real-world validation, efficiency         |\n",
      "|                                       | Audience and Venue Fit                        | Not suitable for ICLR (or stated venue)           |\n",
      "| Ethical and Policy Compliance         | Blind Review/Policy                           | Ethics, plagiarism, review policy violations      |\n",
      "| Reviewer/Review Process Limitation    | Reviewer/Review Process Limitation            | No main negative comment, lacks expertise         |\n",
      "\n",
      "---\n",
      "\n",
      "**Note:** If you want to see a mapping of all the original cluster phrases to this refined hierarchy for implementation, you can provide a subset or key phrases and I can demonstrate a full \"old → new\" mapping.\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you'd like the hierarchy in a particular data structure (JSON, YAML, indented text, etc.), or a visualization!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from jinja2 import Template\n",
    "\n",
    "# Sample data representing the cluster hierarchy tree.\n",
    "\n",
    "# Define a jinja2 template to create the prompt.\n",
    "prompt_template = \"\"\"\n",
    "You are given a cluster hierarchy tree where each row represents a series of cluster names from the bottom level (leaf, _level_0) to higher levels (_level_1, _level_2, etc.). The current hierarchy is as follows:\n",
    "\n",
    "{% for index, row in df.iterrows() %}\n",
    "Row {{ index + 1 }}:\n",
    "{% for level in levels %}\n",
    "  {{ level }}: \"{{ row[level] }}\"\n",
    "{% endfor %}\n",
    "{% endfor %}\n",
    "\n",
    "Please refine this cluster hierarchy. In your refined hierarchy, modify names if necessary and merge similar clusters (i.e. merge nodes in the tree that represent similar themes) while preserving the hierarchical structure.\n",
    "\n",
    "Return the refined hierarchy in a clear, structured format.\n",
    "\"\"\"\n",
    "df_remap = dfr_cs_h[[c for c in dfr_cs_h.columns if 'level_' in c]] \n",
    "# List the hierarchy levels in order (assuming the DataFrame columns are ordered as the hierarchy).\n",
    "levels = list(df_remap.columns)\n",
    "\n",
    "# Render the prompt using the jinja2 template.\n",
    "template = Template(prompt_template)\n",
    "prompt = template.render(df=df_remap, levels=levels)\n",
    "\n",
    "# # Log the prompt (for debugging purposes) before sending it to the LLM.\n",
    "# print(\"Prompt sent to the LLM:\")\n",
    "# print(prompt)\n",
    "\n",
    "# Call the LLM with the prompt.\n",
    "# Assume that 'llm' is an instance of your LLM class with _call implemented.\n",
    "refined_hierarchy = llm._call(prompt)\n",
    "\n",
    "print(\"\\nLLM Response:\")\n",
    "print(refined_hierarchy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ed3287",
   "metadata": {},
   "source": [
    "2025-04-16 12:20:04,897 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
    "\n",
    "**LLM Response:**\n",
    "\n",
    "Certainly! Refining such a large and complex cluster hierarchy involves combining similar nodes and standardizing naming conventions to improve clarity and reduce redundancy. Below is a refined, hierarchical structure covering the main themes evident in the original clusters. Each level is presented using standardized, non-overlapping names, and represents a specific granularity of critique, going from fine-grained (bottom) to broad (top).\n",
    "\n",
    "**Refined Cluster Hierarchy**\n",
    "\n",
    "---\n",
    "\n",
    "## Level 4 (Top-level Themes)\n",
    "\n",
    "1. **Methodological Soundness & Novelty**\n",
    "2. **Experimental and Empirical Validation**\n",
    "3. **Clarity, Presentation, & Reproducibility**\n",
    "4. **Positioning & Related Work**\n",
    "5. **Practical Impact & Generalizability**\n",
    "6. **Ethical and Policy Compliance**\n",
    "7. **Review Process / No Substantive Weakness**\n",
    "\n",
    "---\n",
    "\n",
    "## Level 3 (Mid-level Themes)\n",
    "\n",
    "### (1) Methodological Soundness & Novelty\n",
    "- Theoretical Rigor & Mathematical Foundations\n",
    "- Methodological or Analytical Issues\n",
    "- Novelty Evaluation\n",
    "\n",
    "### (2) Experimental and Empirical Validation\n",
    "- Experimental Design and Setup\n",
    "- Data, Baselines, and Metrics Comparison\n",
    "- Reproducibility of Results\n",
    "\n",
    "### (3) Clarity, Presentation, & Reproducibility\n",
    "- Organization, Writing, and Visualizations\n",
    "- Documentation and Code Availability\n",
    "- Interpretability and Usability\n",
    "\n",
    "### (4) Positioning & Related Work\n",
    "- Literature Review and Contextualization\n",
    "- Comparison with Prior Work\n",
    "\n",
    "### (5) Practical Impact & Generalizability\n",
    "- Applicability to Real-world Problems\n",
    "- Scalability and Performance in Practice\n",
    "- Generalization Across Domains\n",
    "\n",
    "### (6) Ethical and Policy Compliance\n",
    "- Adherence to Ethical Guidelines\n",
    "- Blind Review and Anonymity Issues\n",
    "\n",
    "### (7) Review Process / No Substantive Weakness\n",
    "- Reviewer Feedback on Process and Expertise\n",
    "- General Comments on Absence of Major Issues\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like the hierarchy in a particular data structure (JSON, YAML, indented text, etc.), or a visualization!\n",
    "\n",
    "*Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687d18b8",
   "metadata": {},
   "source": [
    "## Path 3: map-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2774fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from jinja2 import Template\n",
    "MR_PROMPT_TEMPLATE = \"\"\"\n",
    "Given the following cluster names:\n",
    "\n",
    "{{ current_names }}\n",
    "\n",
    "and the previously identified higher-level concepts:\n",
    "\n",
    "{{ previously_identified_names }}\n",
    "\n",
    "Identify and return a new set of higher-level cluster names. Provide names clearly representing broader categories.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec66046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hc import iterative_higher_level_clustering\n",
    "\n",
    "initial_names = dfr_cs['main_limitation'].unique().tolist()\n",
    "final_names = iterative_higher_level_clustering(llm, initial_names, prompt_template=MR_PROMPT_TEMPLATE, batch_size=30)\n",
    "print(\"Final high-level clusters:\", final_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in final_names:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f455b25",
   "metadata": {},
   "source": [
    "Certainly! Here is a clear, synthesized set of **higher-level cluster names** that broadly capture the criticisms present in your cluster names:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Limited Novelty and Contribution\n",
    "*Captures: Lack of innovation, incremental advances, over-reliance on existing methods, and unclear advancement over prior work.*\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Methodological and Technical Weaknesses\n",
    "*Includes: Questionable assumptions, insufficient methodological justification, unconvincing technical claims, and theoretical or experimental design flaws.*\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Insufficient Experimental Validation\n",
    "*Encompasses: Inadequate empirical comparison, unfair or misleading experimental design, lack of comparative analysis with relevant methods, and poor result reporting.*\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Poor Clarity and Justification\n",
    "*Represents: Unclear explanations, lack of justification for methodological choices, and insufficient discussion of the method’s necessity or motivation.*\n",
    "\n",
    "---\n",
    "\n",
    "These four categories should succinctly and comprehensively cover the broader areas reflected in your original cluster names and feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f73849",
   "metadata": {},
   "source": [
    "## Variations on map-reduce prompt and batch\n",
    "Larger batch, and asking for a specific number of high level clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14738174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from jinja2 import Template\n",
    "MR_PROMPT_TEMPLATE = \"\"\"\n",
    "Given the following cluster names:\n",
    "\n",
    "{{ current_names }}\n",
    "\n",
    "and the previously identified higher-level concepts:\n",
    "\n",
    "{{ previously_identified_names }}\n",
    "\n",
    "Identify and return a new set of higher-level cluster names. Provide names clearly representing broader categories.\n",
    "Limit to about 10 maximum higher level concepts\n",
    "\"\"\"\n",
    "dfr_cs = pd.read_csv('reject_reviews_clustered.csv')\n",
    "\n",
    "initial_names = dfr_cs['main_limitation'].unique().tolist()\n",
    "final_names_v2 = iterative_higher_level_clustering(llm, initial_names, prompt_template=MR_PROMPT_TEMPLATE, batch_size=60)\n",
    "# print(\"Final high-level clusters:\", final_names)\n",
    "\n",
    "for f in final_names_v2:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89a6c9",
   "metadata": {},
   "source": [
    "Certainly! Given your list of fine-grained cluster names (critical comments), and the *format/style* of your \"previously identified higher-level concepts,\" here is a **new, succinct set of higher-level cluster names** that are *mutually exclusive* and *collectively exhaustive*, with phrasing tailored for general utility and clarity. Below you'll find the 10 umbrella categories:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Lack of Novelty or Significant Contribution\n",
    "*Paper presents only incremental advances, is derivative, or lacks meaningful new insights.*\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Insufficient Experimental Validation\n",
    "*Experimental results are weak, limited, unconvincing, or lack breadth (e.g., small datasets, poor baselines, inadequate testing).*\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Poor Clarity and Presentation\n",
    "*Writing is unclear, poorly structured, hard to follow, or imprecise, making understanding difficult.*\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Inadequate Theoretical Justification\n",
    "*Theory is missing, weak, inapplicable, or lacks rigorous motivation and support.*\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Lack of Methodological Detail\n",
    "*Key algorithmic or procedural details are omitted or described too vaguely to understand or verify the work.*\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Inadequate Comparison with Related Work\n",
    "*Paper does not sufficiently compare with existing methods (theoretical or empirical) or neglects relevant baselines.*\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Overstated or Unsupported Claims\n",
    "*Assertions and conclusions are not justified by the evidence provided (data, experiments, or theory).*\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Limited Practical Scope or Generalization\n",
    "*Applicability is too narrow, restricted to toys or unrealistic settings, or lacks evidence of broader impact.*\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Problems with Evaluation or Fairness\n",
    "*Comparisons are unfair, experimental protocols are inconsistent, or evaluation metrics are inappropriate.*\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Not Ready for Publication or Inappropriate for Venue\n",
    "*Work is too preliminary, underdeveloped, unpolished, or does not fit the conference/journal scope.*\n",
    "\n",
    "---\n",
    "\n",
    "**These categories provide a distinct and comprehensive set of top-level criticism types for reviewing and analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b44a79",
   "metadata": {},
   "source": [
    "# Cluster without maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa25ff4",
   "metadata": {},
   "source": [
    "## Cluster and name, then build hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c35c3ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64717/3496358311.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfr['review_cluster'] = cluster_labels\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]2025-04-16 12:14:58,656 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  2%|▏         | 1/51 [00:07<06:20,  7.61s/it]2025-04-16 12:15:00,066 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  4%|▍         | 2/51 [00:11<04:36,  5.64s/it]2025-04-16 12:15:04,374 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  6%|▌         | 3/51 [00:17<04:35,  5.75s/it]2025-04-16 12:15:11,174 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  8%|▊         | 4/51 [00:20<03:26,  4.38s/it]2025-04-16 12:15:12,868 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 10%|▉         | 5/51 [00:21<02:38,  3.45s/it]2025-04-16 12:15:15,472 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 12%|█▏        | 6/51 [00:24<02:23,  3.19s/it]2025-04-16 12:15:17,026 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 14%|█▎        | 7/51 [00:26<01:56,  2.64s/it]2025-04-16 12:15:18,540 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 16%|█▌        | 8/51 [00:27<01:35,  2.21s/it]2025-04-16 12:15:19,827 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 18%|█▊        | 9/51 [00:28<01:20,  1.91s/it]2025-04-16 12:15:21,109 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 20%|█▉        | 10/51 [00:30<01:16,  1.86s/it]2025-04-16 12:15:23,253 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 22%|██▏       | 11/51 [00:32<01:15,  1.89s/it]2025-04-16 12:15:24,815 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 24%|██▎       | 12/51 [00:33<01:07,  1.74s/it]2025-04-16 12:15:26,156 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 25%|██▌       | 13/51 [00:35<01:01,  1.62s/it]2025-04-16 12:15:27,639 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 27%|██▋       | 14/51 [00:39<01:28,  2.38s/it]2025-04-16 12:15:32,128 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 29%|██▉       | 15/51 [00:41<01:20,  2.22s/it]2025-04-16 12:15:33,476 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 31%|███▏      | 16/51 [00:42<01:09,  1.99s/it]2025-04-16 12:15:35,037 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 33%|███▎      | 17/51 [00:43<01:02,  1.84s/it]2025-04-16 12:15:36,452 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 35%|███▌      | 18/51 [00:45<00:55,  1.67s/it]2025-04-16 12:15:37,772 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 37%|███▋      | 19/51 [00:46<00:52,  1.65s/it]2025-04-16 12:15:39,275 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 39%|███▉      | 20/51 [00:48<00:50,  1.62s/it]2025-04-16 12:15:40,874 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 41%|████      | 21/51 [00:49<00:48,  1.62s/it]2025-04-16 12:15:42,496 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 43%|████▎     | 22/51 [00:51<00:46,  1.61s/it]2025-04-16 12:15:44,016 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 45%|████▌     | 23/51 [00:53<00:45,  1.61s/it]2025-04-16 12:15:45,673 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 47%|████▋     | 24/51 [00:55<00:50,  1.86s/it]2025-04-16 12:15:48,280 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 49%|████▉     | 25/51 [00:57<00:51,  1.99s/it]2025-04-16 12:15:50,404 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 51%|█████     | 26/51 [00:59<00:46,  1.86s/it]2025-04-16 12:15:51,916 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 53%|█████▎    | 27/51 [01:00<00:41,  1.72s/it]2025-04-16 12:15:53,400 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 55%|█████▍    | 28/51 [01:02<00:37,  1.63s/it]2025-04-16 12:15:54,727 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 57%|█████▋    | 29/51 [01:03<00:34,  1.59s/it]2025-04-16 12:15:56,514 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 59%|█████▉    | 30/51 [01:05<00:35,  1.68s/it]2025-04-16 12:15:58,136 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 61%|██████    | 31/51 [01:06<00:31,  1.56s/it]2025-04-16 12:15:59,675 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 63%|██████▎   | 32/51 [01:08<00:31,  1.67s/it]2025-04-16 12:16:01,353 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 65%|██████▍   | 33/51 [01:10<00:29,  1.63s/it]2025-04-16 12:16:02,834 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 67%|██████▋   | 34/51 [01:11<00:25,  1.51s/it]2025-04-16 12:16:04,087 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 69%|██████▊   | 35/51 [01:13<00:23,  1.50s/it]2025-04-16 12:16:05,516 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 71%|███████   | 36/51 [01:14<00:21,  1.46s/it]2025-04-16 12:16:06,955 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 73%|███████▎  | 37/51 [01:15<00:19,  1.39s/it]2025-04-16 12:16:08,166 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 75%|███████▍  | 38/51 [01:17<00:18,  1.40s/it]2025-04-16 12:16:09,573 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 76%|███████▋  | 39/51 [01:18<00:16,  1.37s/it]2025-04-16 12:16:11,136 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 78%|███████▊  | 40/51 [01:20<00:17,  1.56s/it]2025-04-16 12:16:12,914 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 80%|████████  | 41/51 [01:21<00:15,  1.56s/it]2025-04-16 12:16:14,439 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 82%|████████▏ | 42/51 [01:23<00:13,  1.52s/it]2025-04-16 12:16:15,897 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 84%|████████▍ | 43/51 [01:25<00:12,  1.55s/it]2025-04-16 12:16:17,474 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 86%|████████▋ | 44/51 [01:26<00:10,  1.46s/it]2025-04-16 12:16:18,754 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 88%|████████▊ | 45/51 [01:27<00:08,  1.43s/it]2025-04-16 12:16:20,155 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 90%|█████████ | 46/51 [01:29<00:07,  1.47s/it]2025-04-16 12:16:21,754 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 92%|█████████▏| 47/51 [01:30<00:06,  1.53s/it]2025-04-16 12:16:23,515 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 94%|█████████▍| 48/51 [01:32<00:04,  1.62s/it]2025-04-16 12:16:25,216 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 96%|█████████▌| 49/51 [01:34<00:03,  1.69s/it]2025-04-16 12:16:27,733 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 98%|█████████▊| 50/51 [01:36<00:01,  1.83s/it]2025-04-16 12:16:29,201 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 51/51 [01:38<00:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{64: '{   \\n    \"cluster_name\": \"Sliding Window Methods for Temporal Pattern Detection in Sequential Data\"\\n}', 12: '{\\n    \"cluster_name\": \"Gradient-based Architecture Search Methods and Optimization Techniques\"\\n}', 52: '{\\n    \"cluster_name\": \"Falcon Method and Quantization Techniques for Learning with PDF Trees\"\\n}', 2: '{\\n    \"cluster_name\": \"Unsupervised Domain Adaptation Methods and Evaluation\"\\n}', 41: '{\\n    \"cluster_name\": \"Deep Neural Network Training Methods and LBI Approaches\"\\n}', 63: '{   \\n    \"cluster_name\": \"Evaluation of Methods for Noise Handling in Labeled Data\"\\n}', 23: '{\\n    \"cluster_name\": \"Underwater Image Super-Resolution with GANs\"\\n}', 24: '{\\n    \"cluster_name\": \"Audio Feature Analysis and Spectrogram Methods\"\\n}', 29: '{\\n    \"cluster_name\": \"3D Generative Models and VAE Methods\"\\n}', 11: '{\\n    \"cluster_name\": \"Neural Network Training and Convergence Analysis\"\\n}', 14: '{\\n    \"cluster_name\": \"Gradient Normalization Techniques and Variance Reduction in Deep Learning\"\\n}', 15: '{\\n    \"cluster_name\": \"Theoretical Bounds and Generalization in Lipschitz Analysis\"\\n}', 54: '{   \\n    \"cluster_name\": \"Random Projection Trees for Similarity Search Methods\"\\n}', 55: '{   \\n    \"cluster_name\": \"Distillation Techniques for Transformer-Based Language Models\"\\n}', 49: '{\\n    \"cluster_name\": \"Reward Shaping and Intrinsic Motivation in Reinforcement Learning\"\\n}', 62: '{\\n    \"cluster_name\": \"Random Projection Methods for Similarity Search\"\\n}', 56: '{   \\n    \"cluster_name\": \"Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\"\\n}', 21: '{\\n    \"cluster_name\": \"Weakly Supervised Object Counting and Recognition in Images\"\\n}', 51: '{\\n    \"cluster_name\": \"Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\"\\n}', 32: '{   \\n    \"cluster_name\": \"Physical GRF Data Analysis Methods and Experiments\"\\n}', 16: '{\\n    \"cluster_name\": \"Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\"\\n}', 18: '{\\n    \"cluster_name\": \"Graph Learning Model Feature Analysis\"\\n}', 19: '{\\n    \"cluster_name\": \"Amharic Word Embeddings and Sentiment Lexicon Development\"\\n}', 1: '{\\n    \"cluster_name\": \"Graph Neural Network Methods and Attention-based Pooling\"\\n}', 50: '{\\n    \"cluster_name\": \"Policy Learning and Decision Making in Reinforcement Learning\"\\n}', 27: '{\\n    \"cluster_name\": \"Information Theory and Entropy Estimation in Discrete and Continuous Domains\"\\n}', 34: '{\\n    \"cluster_name\": \"Neural Encoder Decoder Models and Training for Automatic Text Summarization\"\\n}', 37: '{\\n    \"cluster_name\": \"Neural Network Architectures and Proposed Methods\"\\n}', 13: '{\\n    \"cluster_name\": \"Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\"\\n}', 5: '{\\n    \"cluster_name\": \"Adversarial Attacks and Robustness in Machine Learning\"\\n}', 48: '{\\n    \"cluster_name\": \"Lyapunov-Based Stability in Reinforcement Learning Algorithms\"\\n}', 61: '{\\n    \"cluster_name\": \"Evaluation of Proposed Method Performance and Data Precision\"\\n}', 65: '{\\n    \"cluster_name\": \"Efficient Neural Network Pruning and Edge Search Methods\"\\n}', 57: '{\\n    \"cluster_name\": \"Neural Architecture Search Methods and Weight Sharing Techniques\"\\n}', 31: '{\\n    \"cluster_name\": \"OOD Data Imputation and Evaluation Metrics\"\\n}', 22: '{\\n    \"cluster_name\": \"Prototype-based Attention Models for Text Classification\"\\n}', 60: '{\\n    \"cluster_name\": \"Novel Batch Normalization Methods and Their Evaluation\"\\n}', 3: '{\\n    \"cluster_name\": \"Reinforcement Learning and Policy Optimization Methods\"\\n}', 47: '{\\n    \"cluster_name\": \"Exploration Strategies in Deep Reinforcement Learning\"\\n}', 26: '{\\n    \"cluster_name\": \"Statistical Dependence Testing and Methods in Neuroimaging\"\\n}', 59: '{\\n    \"cluster_name\": \"Graph Neural Networks and Community Detection\"\\n}', 46: '{\\n    \"cluster_name\": \"Multi-Agent Learning and Game-Theoretic Approaches\"\\n}', 28: '{\\n    \"cluster_name\": \"Model Unification and Sequence Positioning Concepts in PDP Approaches\"\\n}', 40: '{\\n    \"cluster_name\": \"Robustness and Differential Privacy in Adversarial Machine Learning\"\\n}', 39: '{\\n    \"cluster_name\": \"Backdoor and Adversarial Attack Methods and Defenses\"\\n}', 25: '{\\n    \"cluster_name\": \"Program Synthesis and Evaluation Using Human-Labeled Datasets\"\\n}', 33: '{\\n    \"cluster_name\": \"Multilingual Reinforcement Learning for Atari with Advanced Algorithms\"\\n}', 36: '{\\n    \"cluster_name\": \"Invertible ODE Methods for Robustness and Randomized Attacks\"\\n}', 4: '{\\n    \"cluster_name\": \"Learning Algorithms and Quantization Techniques in Shot-based Models\"\\n}', 44: '{\\n    \"cluster_name\": \"Pruning Techniques and Sparsity in Neural Networks\"\\n}', 35: '{\\n    \"cluster_name\": \"Model Simplicity and OOD Detection via CNF Methods\"\\n}'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_64717/3496358311.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfr['_level_0'] = dfr['review_cluster'].map(cmap).fillna('NO_NAME')\n",
      "/tmp/ipykernel_64717/3496358311.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfr['_level_0'] = dfr['_level_0'].apply(robust_json_parse)\n",
      "/tmp/ipykernel_64717/3496358311.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfr['_level_0'] = dfr['_level_0'].apply(get_dict_item)\n"
     ]
    }
   ],
   "source": [
    "from hc import hdbscan_kmeans_refine\n",
    "\n",
    "NAMING_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a program chair of a scientific conference. \n",
    "Given a set of keywords and representative examples of paper review, your job is to extract the main positive or negative comment described.\n",
    "Here are the keywords for each cluster:\n",
    "<KEYWORDS>\n",
    "{{keywords}}\n",
    "</KEYWORDS>\n",
    "Here are the representative documents for each cluster:\n",
    "<REPRESENTATIVE_DOCS>\n",
    "{{representative_docs}}\n",
    "</REPRESENTATIVE_DOCS>\n",
    "Output a JSON object with only one key called 'cluster_name', to denote the title of the group, with no additional information or commentary.\n",
    "Do not leave the JSON empty. Always return a value for 'cluster_name' with your best guess at a descriptive title for the topic.\n",
    "Be as specific as possible and mimic the style of the representative reviews examples.\n",
    "The JSON format is as follows:\n",
    "    {   \n",
    "        \"cluster_name\": <NAME>\n",
    "    } \n",
    "Do not include special characters in the answer besides the JSON format. Make sure to delimit the JSON keys with double quotes.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def cluster_and_name(df):\n",
    "    df['text'] = df['review'].astype(str)\n",
    "    dfr = df[:1000]\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(dfr['text']).toarray()\n",
    "\n",
    "\n",
    "    cluster_labels = hdbscan_kmeans_refine(\n",
    "        X,\n",
    "        hdbscan_min_cluster_size=10,\n",
    "        max_cluster_size=50,\n",
    "        split_k=5,\n",
    "        max_total_clusters=1000,\n",
    "        random_state=42,\n",
    "        minimum_cluster_size=5)\n",
    "\n",
    "    dfr['review_cluster'] = cluster_labels\n",
    "\n",
    "\n",
    "    cluster_keywords, c_tfidf, count_vectorizer = get_cluster_keywords(\n",
    "        dfr,'review',\n",
    "        'review_cluster')\n",
    "\n",
    "    count_vectorizer = CountVectorizer(\n",
    "        stop_words='english',max_features=1000,\n",
    "        ngram_range=(1, 2))\n",
    "    count_vectorizer.fit(dfr['review'])\n",
    "    docs = get_representative_docs(dfr, 'review', 'review_cluster', cluster_keywords, count_vectorizer)\n",
    "\n",
    "\n",
    "\n",
    "    cmap = name_clusters_with_topics(dfr, llm, 'review_cluster', cluster_keywords, docs,  prompt_template=NAMING_PROMPT_TEMPLATE)\n",
    "    print(cmap)\n",
    "    dfr['_level_0'] = dfr['review_cluster'].map(cmap).fillna('NO_NAME')\n",
    "    dfr['_level_0'] = dfr['_level_0'].apply(robust_json_parse)\n",
    "    def get_dict_item(x):\n",
    "        try:\n",
    "            value = list(x.values())[0]\n",
    "            return value\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print (f\"Response: {x}\")\n",
    "            return x\n",
    "\n",
    "    dfr['_level_0'] = dfr['_level_0'].apply(get_dict_item)\n",
    "    dfr['_level_0']\n",
    "    return dfr, cmap\n",
    "\n",
    "dfr_review, cmap = cluster_and_name(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b31e45bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Evaluation of Methods for Noise Handling in Labeled Data\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Audio Feature Analysis and Spectrogram Methods\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Audio Feature Analysis and Spectrogram Methods\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "3D Generative Models and VAE Methods\n",
      "Neural Network Training and Convergence Analysis\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Distillation Techniques for Transformer-Based Language Models\n",
      "Distillation Techniques for Transformer-Based Language Models\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Random Projection Methods for Similarity Search\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Weakly Supervised Object Counting and Recognition in Images\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Evaluation of Methods for Noise Handling in Labeled Data\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Graph Learning Model Feature Analysis\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Learning Model Feature Analysis\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "3D Generative Models and VAE Methods\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Neural Encoder Decoder Models and Training for Automatic Text Summarization\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Neural Network Training and Convergence Analysis\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Neural Network Training and Convergence Analysis\n",
      "Efficient Neural Network Pruning and Edge Search Methods\n",
      "Efficient Neural Network Pruning and Edge Search Methods\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Neural Architecture Search Methods and Weight Sharing Techniques\n",
      "Neural Architecture Search Methods and Weight Sharing Techniques\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "OOD Data Imputation and Evaluation Metrics\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Evaluation of Methods for Noise Handling in Labeled Data\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Evaluation of Methods for Noise Handling in Labeled Data\n",
      "Novel Batch Normalization Methods and Their Evaluation\n",
      "Novel Batch Normalization Methods and Their Evaluation\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Graph Learning Model Feature Analysis\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Graph Learning Model Feature Analysis\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Methods for Noise Handling in Labeled Data\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Reinforcement Learning and Policy Optimization Methods\n",
      "Reinforcement Learning and Policy Optimization Methods\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Random Projection Methods for Similarity Search\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Learning Model Feature Analysis\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Statistical Dependence Testing and Methods in Neuroimaging\n",
      "Graph Learning Model Feature Analysis\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Graph Learning Model Feature Analysis\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Graph Neural Networks and Community Detection\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Multi-Agent Learning and Game-Theoretic Approaches\n",
      "Multi-Agent Learning and Game-Theoretic Approaches\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Graph Learning Model Feature Analysis\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Graph Neural Networks and Community Detection\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "3D Generative Models and VAE Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Learning Model Feature Analysis\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Graph Learning Model Feature Analysis\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Backdoor and Adversarial Attack Methods and Defenses\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Graph Neural Networks and Community Detection\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "3D Generative Models and VAE Methods\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Audio Feature Analysis and Spectrogram Methods\n",
      "Audio Feature Analysis and Spectrogram Methods\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Audio Feature Analysis and Spectrogram Methods\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Neural Network Training and Convergence Analysis\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "OOD Data Imputation and Evaluation Metrics\n",
      "OOD Data Imputation and Evaluation Metrics\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Multilingual Reinforcement Learning for Atari with Advanced Algorithms\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Invertible ODE Methods for Robustness and Randomized Attacks\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Statistical Dependence Testing and Methods in Neuroimaging\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Random Projection Methods for Similarity Search\n",
      "Neural Network Training and Convergence Analysis\n",
      "Distillation Techniques for Transformer-Based Language Models\n",
      "Distillation Techniques for Transformer-Based Language Models\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Training and Convergence Analysis\n",
      "Graph Learning Model Feature Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Multilingual Reinforcement Learning for Atari with Advanced Algorithms\n",
      "Random Projection Methods for Similarity Search\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Pruning Techniques and Sparsity in Neural Networks\n",
      "Pruning Techniques and Sparsity in Neural Networks\n",
      "Efficient Neural Network Pruning and Edge Search Methods\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Training and Convergence Analysis\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Reinforcement Learning and Policy Optimization Methods\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Learning Model Feature Analysis\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Prototype-based Attention Models for Text Classification\n",
      "3D Generative Models and VAE Methods\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Neural Network Training and Convergence Analysis\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Neural Network Training and Convergence Analysis\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Neural Network Training and Convergence Analysis\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Graph Learning Model Feature Analysis\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Neural Network Training and Convergence Analysis\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Neural Network Training and Convergence Analysis\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Neural Encoder Decoder Models and Training for Automatic Text Summarization\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Statistical Dependence Testing and Methods in Neuroimaging\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Learning Model Feature Analysis\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Graph Learning Model Feature Analysis\n",
      "Novel Batch Normalization Methods and Their Evaluation\n",
      "Reinforcement Learning and Policy Optimization Methods\n",
      "Reinforcement Learning and Policy Optimization Methods\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Multi-Agent Learning and Game-Theoretic Approaches\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Neural Architecture Search Methods and Weight Sharing Techniques\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Neural Network Training and Convergence Analysis\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Graph Learning Model Feature Analysis\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Backdoor and Adversarial Attack Methods and Defenses\n",
      "Backdoor and Adversarial Attack Methods and Defenses\n",
      "Backdoor and Adversarial Attack Methods and Defenses\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Backdoor and Adversarial Attack Methods and Defenses\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Multi-Agent Learning and Game-Theoretic Approaches\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "OOD Data Imputation and Evaluation Metrics\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Graph Neural Networks and Community Detection\n",
      "Neural Network Training and Convergence Analysis\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Audio Feature Analysis and Spectrogram Methods\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Random Projection Methods for Similarity Search\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Efficient Neural Network Pruning and Edge Search Methods\n",
      "Efficient Neural Network Pruning and Edge Search Methods\n",
      "Efficient Neural Network Pruning and Edge Search Methods\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Invertible ODE Methods for Robustness and Randomized Attacks\n",
      "Neural Network Training and Convergence Analysis\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Neural Network Training and Convergence Analysis\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Backdoor and Adversarial Attack Methods and Defenses\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Random Projection Methods for Similarity Search\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Graph Learning Model Feature Analysis\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Weakly Supervised Object Counting and Recognition in Images\n",
      "Weakly Supervised Object Counting and Recognition in Images\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Statistical Dependence Testing and Methods in Neuroimaging\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Graph Learning Model Feature Analysis\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Invertible ODE Methods for Robustness and Randomized Attacks\n",
      "Backdoor and Adversarial Attack Methods and Defenses\n",
      "Invertible ODE Methods for Robustness and Randomized Attacks\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Statistical Dependence Testing and Methods in Neuroimaging\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Graph Learning Model Feature Analysis\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Backdoor and Adversarial Attack Methods and Defenses\n",
      "Backdoor and Adversarial Attack Methods and Defenses\n",
      "Backdoor and Adversarial Attack Methods and Defenses\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Novel Batch Normalization Methods and Their Evaluation\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Pruning Techniques and Sparsity in Neural Networks\n",
      "Pruning Techniques and Sparsity in Neural Networks\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Neural Network Training and Convergence Analysis\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Reinforcement Learning and Policy Optimization Methods\n",
      "Audio Feature Analysis and Spectrogram Methods\n",
      "Neural Encoder Decoder Models and Training for Automatic Text Summarization\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Multilingual Reinforcement Learning for Atari with Advanced Algorithms\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Neural Network Training and Convergence Analysis\n",
      "Random Projection Methods for Similarity Search\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Novel Batch Normalization Methods and Their Evaluation\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Weakly Supervised Object Counting and Recognition in Images\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "3D Generative Models and VAE Methods\n",
      "Graph Learning Model Feature Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Statistical Dependence Testing and Methods in Neuroimaging\n",
      "OOD Data Imputation and Evaluation Metrics\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Neural Network Training and Convergence Analysis\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "OOD Data Imputation and Evaluation Metrics\n",
      "Prototype-based Attention Models for Text Classification\n",
      "OOD Data Imputation and Evaluation Metrics\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Invertible ODE Methods for Robustness and Randomized Attacks\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Neural Network Training and Convergence Analysis\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "OOD Data Imputation and Evaluation Metrics\n",
      "3D Generative Models and VAE Methods\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Reinforcement Learning and Policy Optimization Methods\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Multi-Agent Learning and Game-Theoretic Approaches\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Multi-Agent Learning and Game-Theoretic Approaches\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Multi-Agent Learning and Game-Theoretic Approaches\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Backdoor and Adversarial Attack Methods and Defenses\n",
      "Audio Feature Analysis and Spectrogram Methods\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "3D Generative Models and VAE Methods\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Invertible ODE Methods for Robustness and Randomized Attacks\n",
      "Statistical Dependence Testing and Methods in Neuroimaging\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Neural Encoder Decoder Models and Training for Automatic Text Summarization\n",
      "Neural Encoder Decoder Models and Training for Automatic Text Summarization\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Graph Learning Model Feature Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Training and Convergence Analysis\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "3D Generative Models and VAE Methods\n",
      "Multilingual Reinforcement Learning for Atari with Advanced Algorithms\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "3D Generative Models and VAE Methods\n",
      "3D Generative Models and VAE Methods\n",
      "Theoretical Bounds and Generalization in Lipschitz Analysis\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Learning Model Feature Analysis\n",
      "Distillation Techniques for Transformer-Based Language Models\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Weakly Supervised Object Counting and Recognition in Images\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Invertible ODE Methods for Robustness and Randomized Attacks\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Invertible ODE Methods for Robustness and Randomized Attacks\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Random Projection Methods for Similarity Search\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Statistical Dependence Testing and Methods in Neuroimaging\n",
      "Statistical Dependence Testing and Methods in Neuroimaging\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Multilingual Reinforcement Learning for Atari with Advanced Algorithms\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Multilingual Reinforcement Learning for Atari with Advanced Algorithms\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Pruning Techniques and Sparsity in Neural Networks\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "3D Generative Models and VAE Methods\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Novel Batch Normalization Methods and Their Evaluation\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Graph Learning Model Feature Analysis\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Distillation Techniques for Transformer-Based Language Models\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Distillation Techniques for Transformer-Based Language Models\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "3D Generative Models and VAE Methods\n",
      "3D Generative Models and VAE Methods\n",
      "3D Generative Models and VAE Methods\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Neural Network Training and Convergence Analysis\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Graph Learning Model Feature Analysis\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Neural Network Training and Convergence Analysis\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "3D Generative Models and VAE Methods\n",
      "3D Generative Models and VAE Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Distillation Techniques for Transformer-Based Language Models\n",
      "Distillation Techniques for Transformer-Based Language Models\n",
      "Graph Learning Model Feature Analysis\n",
      "3D Generative Models and VAE Methods\n",
      "Graph Neural Networks and Community Detection\n",
      "OOD Data Imputation and Evaluation Metrics\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Neural Architecture Search Methods and Weight Sharing Techniques\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Reinforcement Learning and Policy Optimization Methods\n",
      "Graph Learning Model Feature Analysis\n",
      "Multilingual Reinforcement Learning for Atari with Advanced Algorithms\n",
      "Statistical Dependence Testing and Methods in Neuroimaging\n",
      "3D Generative Models and VAE Methods\n",
      "3D Generative Models and VAE Methods\n",
      "3D Generative Models and VAE Methods\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "3D Generative Models and VAE Methods\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Random Projection Methods for Similarity Search\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet\n",
      "Neural Network Training and Convergence Analysis\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Multilingual Reinforcement Learning for Atari with Advanced Algorithms\n",
      "Weakly Supervised Object Counting and Recognition in Images\n",
      "Graph Learning Model Feature Analysis\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Multi-Agent Learning and Game-Theoretic Approaches\n",
      "Reinforcement Learning and Policy Optimization Methods\n",
      "Program Synthesis and Evaluation Using Human-Labeled Datasets\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Reinforcement Learning and Policy Optimization Methods\n",
      "Reinforcement Learning and Policy Optimization Methods\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Distillation Techniques for Transformer-Based Language Models\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Multilingual Reinforcement Learning for Atari with Advanced Algorithms\n",
      "Evaluation of Proposed Method Performance and Data Precision\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Neural Encoder Decoder Models and Training for Automatic Text Summarization\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Multi-Agent Learning and Game-Theoretic Approaches\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Information Theory and Entropy Estimation in Discrete and Continuous Domains\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Neural Architecture Search Methods and Weight Sharing Techniques\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Neural Architecture Search Methods and Weight Sharing Techniques\n",
      "Audio Feature Analysis and Spectrogram Methods\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Graph Learning Model Feature Analysis\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Neural Network Training and Convergence Analysis\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Amharic Word Embeddings and Sentiment Lexicon Development\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n",
      "Invertible ODE Methods for Robustness and Randomized Attacks\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Exploration Strategies in Deep Reinforcement Learning\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "OOD Data Imputation and Evaluation Metrics\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Neural Encoder Decoder Models and Training for Automatic Text Summarization\n",
      "Gradient-based Architecture Search Methods and Optimization Techniques\n",
      "Neural Encoder Decoder Models and Training for Automatic Text Summarization\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Model Unification and Sequence Positioning Concepts in PDP Approaches\n",
      "Graph Learning Model Feature Analysis\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "OOD Data Imputation and Evaluation Metrics\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Learning Algorithms and Quantization Techniques in Shot-based Models\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Gradient Normalization Techniques and Variance Reduction in Deep Learning\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Reinforcement Learning and Policy Optimization Methods\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Novel Batch Normalization Methods and Their Evaluation\n",
      "Novel Batch Normalization Methods and Their Evaluation\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Lyapunov-Based Stability in Reinforcement Learning Algorithms\n",
      "Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods\n",
      "Backdoor and Adversarial Attack Methods and Defenses\n",
      "Deep Neural Network Training Methods and LBI Approaches\n",
      "Neural Network Architectures and Proposed Methods\n",
      "3D Generative Models and VAE Methods\n",
      "Graph Learning Model Feature Analysis\n",
      "3D Generative Models and VAE Methods\n",
      "Neural Network Training and Convergence Analysis\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Physical GRF Data Analysis Methods and Experiments\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Neural Encoder Decoder Models and Training for Automatic Text Summarization\n",
      "Neural Encoder Decoder Models and Training for Automatic Text Summarization\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Prototype-based Attention Models for Text Classification\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Underwater Image Super-Resolution with GANs\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Unsupervised Domain Adaptation Methods and Evaluation\n",
      "Underwater Image Super-Resolution with GANs\n",
      "3D Generative Models and VAE Methods\n",
      "Audio Feature Analysis and Spectrogram Methods\n",
      "Robustness and Differential Privacy in Adversarial Machine Learning\n",
      "OOD Data Imputation and Evaluation Metrics\n",
      "Adversarial Attacks and Robustness in Machine Learning\n",
      "Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Model Simplicity and OOD Detection via CNF Methods\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Invertible ODE Methods for Robustness and Randomized Attacks\n",
      "Neural Network Training and Convergence Analysis\n",
      "Neural Network Architectures and Proposed Methods\n",
      "Graph Learning Model Feature Analysis\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Falcon Method and Quantization Techniques for Learning with PDF Trees\n",
      "Policy Learning and Decision Making in Reinforcement Learning\n",
      "Multi-Agent Learning and Game-Theoretic Approaches\n",
      "Reward Shaping and Intrinsic Motivation in Reinforcement Learning\n",
      "3D Generative Models and VAE Methods\n",
      "3D Generative Models and VAE Methods\n",
      "Random Projection Methods for Similarity Search\n",
      "Graph Neural Network Methods and Attention-based Pooling\n",
      "Random Projection Trees for Similarity Search Methods\n",
      "Sliding Window Methods for Temporal Pattern Detection in Sequential Data\n"
     ]
    }
   ],
   "source": [
    "for reason in   dfr_review['_level_0']:\n",
    "    print(reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYNTHESIS_NAMING_PROMPT_TEMPLATE = '''\n",
    "you are an program committee chair of a scientific conference.\n",
    "Your task is to identify the main issues in a list of reviews, and to group them into higher level groups.\n",
    "This is an ENUMERATED list of reviews, in the format NUMBER: REVIEW. \n",
    "Your job is to reduce the list to a smaller number of ISSUEs raised by the reviewers. \n",
    "Be as specific as possible. \n",
    "The output is a list of GROUP_NAME that are at a higher level of abstaction and that group together at least two underlying issues.\n",
    "Try to identify groups that put together a minimum of two, but ideally many similar issues, and give them a name that is descriptive of the group and of the differences between\n",
    "items in that group and items in other groups. \n",
    "along with a mapping of the original ISSUE_NUMBERs to the new GROUP_NAME.\n",
    "Only return the map if it contains more than one ISSUE_NUMBERs, that is, if we can actually group some issues into a higher level pattern.\n",
    "Make sure every ISSUE_NUMBER is assigned to a group, and make an effort to group every issue into a higher level group.\n",
    "\n",
    "These is the enumerated list of issues:\n",
    "<ENUMERATED_ISSUE_LIST>\n",
    "{{text_to_synthesize}}\n",
    "</ENUMERATED_ISSUE_LIST>\n",
    "\n",
    "First, briefly describe your reasoning, and then return a JSON dict object. The answer should have the following structure:\n",
    "REASONING: <YOUR REASONING>\n",
    "JSON GROUPS:\n",
    "{\n",
    "  \"<GROUP_NAME_1>\": [\"<ISSUE_NUMBER>\", .., \"<ISSUE_NUMBER>\"],\n",
    "  \"<GROUP_NAME_2>\": [\"<ISSUE_NUMBER>\" .., \"<ISSUE_NUMBER>\"],\n",
    "}\n",
    "Please adhere to the format described in the JSON part of the answer: each json key must contain a list of numbers corresponding to the ISSUE_IDs to be grouped under that key identifying the group name.\n",
    "Do not include special characters in the answer besides the JSON format.\n",
    "'''\n",
    "\n",
    "dfr_review = build_cluster_hierarchy(dfr_review, llm, '_level_0', prompt_template = SYNTHESIS_NAMING_PROMPT_TEMPLATE, MAX_TOP_LEVEL_CLUSTERS=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd3f3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Tree...\n",
      "Detected and using levels (highest to lowest): ['_level_2', '_level_1', '_level_0']\n",
      "\n",
      "Printing Tree Structure:\n",
      "Warning: LEVELS_TO_PRINT (6) is greater than total levels available (3). Printing all levels.\n",
      "- Evaluation, Benchmarking, and Theoretical Analysis (L2, 191)\n",
      "  - Evaluation and Benchmarking of Learning Methods (L1, 131)\n",
      "    - Evaluation of Few-Shot and Self-Supervised Learning Methods for Classification and Anomaly Detection (L0, 40)\n",
      "      * Examples:\n",
      "        -  While this paper tackles an interesting problem. The technical approach is unfortunately too outdated and obvious and not quite the level of ICLR. The dataset is likely too easy given the high accuracy.\n",
      "        -  Summary: This paper considers the addition of self-supervised learning techniques in the few-shot learning setting. Extensive experiments are done to show that it can be helpful, including in cases where the labeled data is corrupted. The paper also considers the domain mismatch issue where unlabeled images come from a different domain. Review: This paper is thorough and clearly written. Applying self-supervised learning techniques to the few-shot learning regime is a simple idea, and this paper clearly shows that it can be beneficial. It includes extensive experiments on a wide variety of image datasets, including many additional studies in the appendix. The only possible criticisms of this paper are that it is limited to the image domain (as are most few-shot learning/self-supervised learning studies, so we can probably ignore this) and that it does not produce a huge delta in understanding compared to Gidaris et al. (2019). Gidaris et al. (2019) was posted to arxiv in June, I*m not sure if this work was also on arxiv around the same time, and even if it wasn*t I*m not sure what to consider *concurrent*. However, as the authors note they include additional experimental settings (like the domain-selection idea) that are not in Gidaris et al. (2019), so the works are somewhat complementary. The only other comment I have is that the paper is quite large in scope for a conference submission and as a result there are many details and experiments that are left for the appendix. I could also see the domain selection experiments constituting their own submission. For example the definition of the **distance* between a pair of domains* is only introduced in passing in the midst of Section 4.2 covering domain shift experiments, and the method for training a domain classifier is similarly only mentioned in passing in 4.2. Of course, it is not really valid to criticize a paper for being too exhaustive. Overall, I recommend acceptance. Specific comments: - Truly a minor suggestion but I suggest moving Figure 1 to the top of page 2. - Snell et al. (2017) needs a citep - You ought to cite *S4L: Self-Supervised Semi-Supervised Learning*, which is related to your discussion of connections between self- and semi-supervised learning (though not few-shot). - The paragraph beginning *The focus of most prior work...* in the Related Work section provides a nice framing of your work and so might make more sense in the introduction. - *we consider self-supervised losses based on labeled data ... that can be derived from inputs x alone* All labels (can be derived from in the inputs x alone, given an oracle (or human labeler). I think you mean *that can be derived automatically without any human labeling*. - You state *Our final loss function combines the two losses*. Is there no scalar multiplier on either loss term to trade-off the importance of each? - The gains from the self-supervised auxiliary tasks are over and above any gains from data augmentation alone. More experimental details are in Appendix A.5.* I don*t see any information to substantiate the claim that the self-supervised tasks result in a bigger improvement than data augmentation alone, can you provide those details? - *However, does more unlabeled data always help for a task in hand?* This question actually was addressed somewhat in *Realistic Evaluation of Semi-Supervised Learning Algorithms*, see sections 4.4 and 4.5 therein.\n",
      "        - Review:###Summary & Pros - This paper proposes a few-shot learning method that uses self-supervision as an auxiliary label and trains primary and auxiliary labels via multi-task learning. - This paper provides extensive experiments for analyzing the effect of self-supervision on various few-shot learning settings: (1) self-supervision can improve various few-shot learning algorithms, ProtoNet & MAML; (2) self-supervision with similar samples can provide more improvements. Concerns #1: Novelty of the proposed method - This paper uses a multi-task learning approach with self-supervision. But this approach is already used in various tasks, e.g., domain adaptation, semi-supervised learning, training GANs. Thus, the proposed method (in Section 3) using a multi-task learning objective with self-supervised losses seems to be incremental. Concerns #2: Somewhat unsurprising experimental results - This paper shows various experimental results, but some experiments seem to be trivial. For example, the performance gap is typically increased when learning harder tasks, e.g., when the number of training samples is decreasing, the performance gap between methods is typically increasing in a fully-supervised setting. Thus I think results in the paragraph *Gains are larger for harder tasks* might be predictable. Other examples are Figure 4a and 4b in Section 4.2 because one can easily expect that using training more in-domain samples can provide more performance gain. - In the case of Figure 4d in Section 4.2, the authors claimed that the effectiveness of SSL decreases as the distance from the supervised domain increases. However, I think Figure 4d is not matched to the claim. For example, in the case of Dogs, better performance is achieved when using a more dissimilar domain for self-supervision except for D_s=D_ss. So I wonder how to draw the lines in Figure 4d. Some experimental results provide meaningful messages, e.g., single self-supervision can improve performance significantly while joint self-supervision does marginally. However, the contribution of the methodology is limited and some experimental results seem to incremental.\n",
      "    - Evaluation of Methods for Noise Handling in Labeled Data (L0, 5)\n",
      "      * Examples:\n",
      "        -  The paper proposes to study how early stopping in optimization helps find confident examples. Overall, the paper is well-organized and easy to read. Although there is some parallel study regarding the theoretical aspect of how early stopping help finds confident examples (i.e., Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks, which has unfortunately not been cited), the paper focuses on the empirical perspective. A thorough empirical study illustrating how early stop works would interest the label noise community. The authors claim that early stopping is efficient to find a maximal safe set. I think it would be necessary to illustrate the maximal safe set for all the datasets. The authors only did this for one case of CIFAR-100, which is not convincing. The small loss based learning has shown the confident examples extracted. It would be essential to compare the proposed method with those methods. The authors are suggested to compare the proposed method with more baselines. There are lots of algorithms exploiting the transition matrix and with statistically consistent estimators. The authors have ignored all those methods. It is unclear from the paper that if the baselines have used the clean validation sets. For fair comparison, those clean data should be used in the training procedures of the baselines. Clothing1M is a more challenging dataset with real-world label noise. The dataset also includes some clean data for validation use. The authors should verify the effectiveness of the proposed method on this dataset. The authors are suggested to make it clear why noise rates are sometimes available for use.\n",
      "        - Review:###The paper presents a regularization technique for conditional density estimation. The method is simple: adding noise to the data points, and training on the noisy data points. The paper also further gives an interpretation of the method, as a form of smoothing the curvature of the density function. It further proves the consistency of the method. Pros: (i) The paper is well written. (ii) The method is simple to implement. (iii) The authors demonstrate a clear intuition of what the method does, i.e., as a form of smoothing. Cons: (i) The method itself is not novel. Adding noise to for regularization is a quite common technique used in many different applications. (ii) The experiments performed in the paper are all very small scaled. Not very convincing. (iii) Continuing the last point, it is unclear whether such technique can be scaled up. The method in another view is to replace the empirical distribution with a kernel density estimate. We know kernel methods don*t scale. It*s hence questionable whether the method can give any benefits in large scale dataset. (iv) If the use of noise regularization is to smooth the density function, how does it compare to methods that enforces Lipschitz continuity? e.g., gradient penalty, or construct a lipschtz constrained network such as in [1]. [1] Sorting out lipschitz function approximation. Anil et al.\n",
      "        -  Summary: The authors first analyze and answer the question: What training examples should be focused and how large the emphasis spread should be? Then, they proposed the gradient rescaling framework serving as emphasis regularization. Strengths: 1. The paper is well organized except the reference citation (read difficultly) 2. The proposed method is very simple and effective. 3. Experiments show the improvements over SOTA. Weakness: 1. The experiments lack the recent important baseline *symmetric cross entropy for robust learning with noisy labels, ICCV2019*, which are the current SOTA. Maybe the author should check the above paper and show the results. 2. The experiments are only conducted on symmetric noise. Actually, asymmetric noise is also important. The author should conduct at least some experiments on asymmetric noise.\n",
      "    - Evaluation of Novel Dropout and Data Augmentation Techniques on ImageNet (L0, 20)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposes a noise regularization method which adds noise on both x and y for conditional density estimation problem (e.g., regression and classification). The writing is good and the whole paper is easy to follow. However, I vote for reject, since the novelty is somehow limited, the claims made in the paper is not well supported and experiments are not very convincing. 1. Adding noise on x (e.g., [1]), y (e.g., [2]) is not new. Though it is claimed that this paper extends previous results on classification/regression to conditional density estimation which is a more general case. This claim is not well supported. Experiments are still evaluated in classification/regression tasks. 2. Theorem 1 & 2 in Sec 4.2 only show the asymptotic case, which are quite obvious and seems helpless in understanding the advantage of adding noise regularization in conditional density estimation. 3. Sec 4.1. The explanation that Page 5, ```*The second term in (6) penalizes large negative second derivatives of the conditional log density estimate...*. It is hard for me to understand. Large positive second derivatives also lead to poor smoothness. [1] Learning with Marginalized Corrupted Features, ICML 2013 [2] Learning with Noisy Labels, NIPS 2013\n",
      "        -  In this paper, the authors performed an empirical study on the importance of neural network weights and to which extent they need to be updated. Some observations are obtained such as from the third epoch on, a large proportion of weights do not need to be updated and the performance of the network is not significantly affected. Overall speaking, the qualitative result in the paper has already been discovered in many previous work, although the quantitative results seem to be new. However, there is large room to improve regarding the experimental design and the comprehensiveness of the experiments. Just name a few as follows: 1) For different models and different tasks, the quantitative results are different. There is no deep discussion on the intrinsic reason for this, and what is the most important factor that influences the redundancy of weight updates. The authors came to the conclusion that from the third epoch on, no need to update most of the weights. “3” seems to be a magic number to me. Why is it? No solid experiments were done regarding this, and no convincing analysis was made. 2) The datasets used in the experiments are not diverse enough and are not of large scale. For example, the CIFA-10 and MNIST datasets are relatively of small scale. What if the datasets are much larger like ImageNet. In such more complicated case, will the weight updates still be unnecessary? Will the ratio and the epoch number change? What is the underlying factor determining these? For another example, there are many NLP datasets for language understanding and machine translation, which are of large scale. Why choosing an image captioning dataset (which I do not agree to be real-life experiments when compared with language understanding and machine translation)? Can the observations generalizable to more complicated tasks and datasets? 3) The models studied in the paper are also a little simple, especially for the text task. Why just using a single-layer LSTM? Why not popularly used Transformer? As a summary, for an empirical study to be convincing, the tasks, datasets, scales, model structures, detailed settings, and discussions are the critical aspects. However, as explained above, this paper has not done a good job on these aspects. Significantly more work needs to be done in order to make it an impactful work. *I read the author rebuttal, but would like to keep my rating unchanged.\n",
      "        - Review:###This paper demonstrates that for regularization, data augmentation usually works better than explicit regularization methods such as weight decay and dropout. The experiments are detailed and also includes theory explanation of why data augmentation works. Data augmentation (or increasing the size of training data) and explicit regularization are standard methods to overcome overfitting. From my understanding, they are two different methods to tune the model quality, and they can combined to further improve the model performance, as discussed in [1]. So this paper is not well motivated. [1] DeVries, Terrance, and Graham W. Taylor. *Improved regularization of convolutional neural networks with cutout.* arXiv preprint arXiv:1708.04552 (2017).\n",
      "    - Evaluation of Proposed Method Performance and Data Precision (L0, 21)\n",
      "      * Examples:\n",
      "        - Review:###This paper adapts Bayesian linear regression to the setting of a limited memory replay buffer. The idea is to calibrate the prior mean and variance when the neural representation of context is updated. Overall the paper is well written and explained clearly. Some experiments are provided to show that the proposed method is able to achieve a performance competitive to Bayesian linear regression with infinite memory. The result of this paper is interesting. But I am not sure if the current experimental results are convincing enough to justify the significance of the proposed method. 1. The results in Section 4.2 seem to be following the setting in Riquelme 2018. These datasets are all in a supervised learning setting. It is a bit disappointing that the proposed method is not tested on RL datasets. 2. No other baseline is provided in the experiments for comparison. a. There are other methods in the literature to overcome catastrophic forgetting of neural networks, e.g regularizing the update of the network. How would that be compared to the proposed method? b. What about other methods, like [1]? 3. Most of the experiment details are missing. For example, how is the reward defined in section 4.3? What is the overhead in computation in practice, especially for the SDP? Other comments: 1. Why would solving a SDP require only O(g^{0.5}) in section 3.1? 2. In the discussion in section 3, even if equation (5) and (6) can be exactly solved, how does the heavy tailed problem mentioned in section 2 been solved? [1] Elmachtoub, Adam N., et al. *A practical method for solving contextual bandit problems using decision trees.* arXiv preprint arXiv:1706.04687 (2017).\n",
      "        - Review:###The paper proposes a method to learn an explanation of black-box systems from its outputs. The method is based on the information bottleneck as the objective function is designed to measure mutual information between input x, system output y, and narrowed information of input t. t is constructed by filtering x with maintaining interpretability of y, so that it is finally assumed as the explanation of the system extracted by the proposed method. The paper is well motivated and well written. Enough experiments were conducted to assess the advantage of the proposed method in the classification tasks. It looks a good paper. Maybe the paper is focused on only tasks that the predictor does not generate much information, such as classification. It is still unclear how the proposed method work when it is applied to the output-rich models, i.e., the model should keep as much information as inputs. The proposed method automatically selects some important chunks from inputs, but the chunks still rely on some task-specific hand-crafted chunking strategies. The paper also conducted some experiments by changing the strategy, but it is still unclear what is the important criteria. It is also good to show how actually the thickness of the bottleneck (controlled by k) works in actual cases, e.g., showing results for the same example with moving k. Trivial comments: * The example in 3.2 *great, great* and *great, thought provoking* looks still ambiguous to explain what the section want to say. * *x_i \times z_j* in p.iv looks ambiguous. * z_j^* in 3rd eqn. of p.v should take l: z_j^{*(l)} * the max operator over l in 3rd eqn. of p.v looks to hide other values than the highest one (specifically, the L1 norm of z_j^* does not become k by this eqn. as the k-hot vector does). Summation looks intuitively better than max. Could you explain how this eqn. was constructed? * f(.) in the 4th eqn. of p.v may be undefined in the main text. *eta_1 of Adam looks to be set to not a standard value (0.9). Is there any reason?\n",
      "        - Review:###The idea of the paper is to learn a distance function between observed and the agent’s behaviors. Once they have the distance function, they can learn the agent’s policy efficiently given a single demonstration of each task. In their formulation, the distance function and the policy are jointly learned. The idea is reasonable and the performance outperforms baselines like GAIL and VAE. However, the paper is not-well written with many relevant equations defined in the supplementary material. The unsupervised data labeling part seems Adhoc with many details in the supplementary material. I wonder if the process stable or not. How many lower than the average performance of the proposed method as shown in F.g 4 are caused by unsupervised data labeling? In Fig. 4b, the manual performance is very strong once converged. Although the proposed method initially reaches high reward, after twice many iterations the manual performance even outperforms the proposed method on average many times. Hence, I am not very convinced about the proposed method will be the best-picked method in practice. Overall, I think the idea is good. But the paper is poorly written and I concern the most about the stability of the unsupervised data labeling process. The experimental results are also not super convincing. Hence, I recommend for weak rejection.\n",
      "    - Novel Batch Normalization Methods and Their Evaluation (L0, 8)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposes a method called *Dimensional Reweighting* for graph convolutional networks. The method involves a reparametrization of GCNs (by adding an extra reweighting block in each layer), which the authors show theoretically can reduce variance. The authors supplement this with extensive empirical experiments showing slightly improved performance by adding their method to existing methods. I would vote to weakly reject this paper for two key reasons - first, I think the writing can be improved and explanations can be clarified, especially for people less familiar with the field like myself. Second, I am not certain how significant the final experimental improvements are (other than on the Reddit dataset), as most of the numbers are not that far apart, and it seems that different methods in the literature already produce fairly different results. Overall, I think the structure of the paper is fairly good. I feel that a few things should be modified for clarity. - You claim a 40% improvement in error rate in the intro, which sounds enormous. I would say *relative error rate* to avoid overclaiming, because 40% improvement sounds like you are reducing (absolute) error from 60% to 20%, while in reality you are reducing error from 3.6% to 2.1%. - In section 2.1, did not know if the projection matrix W was learned or predefined. - I was not sure why you used sigma_g and sigma_s as opposed to just sigma in equation (5). Do you use different activation functions? Also, I did not find what activation function the authors end up using in their experiments. - I may have misunderstood something, but the theory does not seem to match the proposed method exactly. The mean-field theory analysis has the activation function after the reweighting by S but before multiplying by W, while the framework in Section 2.2 has the activation after the reweighting by S and after the multiplying by W. I am not sure how much this difference makes, or if it is significant, but I think it should be explained by the authors. - I also did not understand exactly what the *variance* the authors are reducing is. The authors talks about *reducing the learning variance brought by perturbations on the input,* but when is the input ever perturbed for GCNs? Explaining this more clearly would improve the motivation for this work. - I would appreciate a better intuitive explanation of the measure *K.* I gather that it is related to the *variance* being reduced, but it is different from that. The experimental results are good overall, as the proposed method tends to give the best results (by a small margin) across the board. I especially appreciate that the authors performed many experiments over many different datasets and repeated runs 20 times to try to get confident estimates of how well each method performs. I also appreciate that the authors cleaned up the Citeseer and Cora datasets, and I hope the cleaned datasets will be useful for the research community. With that said, I do not know how significant the improvement is. I think something that would be helpful would be to measure the *variance* that the method is supposed to be reducing (since it sounds like it is not exactly the same thing as K), and showing that in a table as well. This would show experimentally that the method achieves its intended goal. Minor comments - I would recommend that the authors proofread for English grammar and style in updated versions of the paper. For example, in the first paragraph of the introduction, the authors use *is proposed* instead of *were proposed* and typo *broad* as *board.* - Just curious, why did you choose a 2 layer network with 2 activation functions for the Dr block? Why not just have 1 hidden layer?\n",
      "        - Review:###The paper is out of my research area. I could only provide little recommendation. I have tried to read this paper, but it was rather tedious with heavy notations. It would be more friendly to represent the models in visible way for example using diagrams as I can see that the model is a sequence matrix operators with non-linear transformations after that. The paper states that the proposed DrGCNs can improve the stability of GCN models via mean field theory. The experiments were conducted on benchmark datasets and the proposed method was compared to several GCN variations.\n",
      "        - Review:###This paper proposed to improve the regular batch normalization by reducing the skewness of the hidden features. To this end, the authors introduce a non-linear function to reduce the skewness. However, the analysis and experiments are too weak to support the authors* claim. 1. The motivation is not clear. Why is it necessary to reduce the skewness? There is no practical or theoretical evidence to support it. 2. To verify the proposed non-linear transformation can reduce skewness, it*s better to visualize the learned feature distribution to confirm this point. 3. Experiments with more datasets and networks are needed to evaluate the performance of the proposed method.\n",
      "    - OOD Data Imputation and Evaluation Metrics (L0, 12)\n",
      "      * Examples:\n",
      "        - Review:###The paper considers the problem of out-of-distribution (OOD) sample detection while solving a classification task. The authors tackle the problem of OOD detection with exploiting uncertainty while passing a test sample through the neural network. They treat outputs of (some) layers in a NN as random Gaussian-distributed variables and measure uncertainty as variance of these Gaussians. Then when uncertainty is high, OOD is detected. The overall idea behind the paper could be interesting, but its realisation in the current form is questionable. The paper seems totally misusing the reparametrisation trick and stochastic outputs of layers in NNs. Eq. (2) is not the objective of variational inference that seems to be required for stochastic outputs and the reparametrisation trick as presented before the equation. The objective misses the KL-divergence term! Without it what would stop a neural net to set sigmas to 0 and forget about the stochasticity altogether? Not to mention that the current objective is not mathematically justified. If there is no mix and error in eq. (2) and the networks were trained using this loss (and based on provided code they were using this loss), my wild guess of explaining why this may give best results in the experiments is that the models were trained for surprisingly small number of epochs. Therefore, a hypothesis would be that this small number of epochs did not allow the networks to switch sigmas to 0. Until the authors can clarify and justify the objective, I will vote for rejection only based on this ground. However, there are other issues in the paper as well. First of all, its clarity. It seems that the paper requires a lot of polishing. The first paragraph of this review is based on my assumptions from the paper since I am not completely sure I understand it correctly. More about the clarity issues below For strong evaluation, comparison with Malinin & Gales (2018) work seems to be important since it was the only work also using uncertainties for OOD detection in related work. Also related work section does not look like an exhaustive overview. Some of the detailed comments: 1. “In other words, in-distribution samples possess more features that convolutional filters react to than OOD samples” – first of all, this sentence is not easy to parse. Secondly, it is unclear, why this should be true. If OOD samples are still natural images, they would contain edges just the same as in-distribution samples. That is the power of deep learning enabling transfer learning, that low-level features are the transferable across different data and tasks. Therefore, the claim that “Therefore, the uncertainties of the features will be larger when the inputs are in-distribution samples” requires more elaboration and arguments 2. The arguments of the next paragraph regarding uncertainty of deeper layers should be larger for OOD samples are not very convincing either. It is either requires a definition what the authors mean here as uncertainty, or it is not necessarily true that absence of fixed regions for embeddings leads to higher uncertainty. 3. 3rd and 4th paragraphs in Introduction have too many repetitions of phrases between each other. Compare, e.g. the first sentences of the paragraphs or the last sentences. 4. “One cause of the abovementioned problem is that their approaches” and similarly the next paragraph: “their approaches” stylistically sound wrong. It is appropriate in the previous paragraph since there is a link to “previous studies”. It seems that “these approaches” or “the existing approaches” would be a better choice for this and the next paragraph. 5. “Each uncertainty is easily estimated after training the discriminative model by computing the mean and the variance of their features using a reparameterization trick” – conventional discriminative models do not estimate mean and variances of the features. The issue of estimating uncertainty is addressed by several special methods such as Bayesian variational methods used in the referred papers. Therefore, in order to use a reparametrisation trick one need to firstly choose a special class of models, which is not obvious from the text. 6. “Moreover, UFEL is robust to hyperparameters such as the number of in-distribution classes and the validation dataset.” – the size of the validation dataset? In any case neither size of the validation dataset nor the validation dataset itself are not hyperparameters (should not be hyperparameters for out-of-distribution detection). The number of classes can hardly be called a hyperparameter also. 7. “depends on the difference in the Dirichlet distribution of the categorical parameter <…> In our work, the distribution of the logit of the categorical parameters” – what is/are this/these categorical parameter(s)? 8. “Further, they estimate the parameter of the Dirichlet distribution using a DNN and train the model with in-distribution and OOD datasets” – this sentence may mislead to impression that the proposed method does not need OOD dataset for training, which does not seem to be the case, since lambda and \theta are trained based on OOD samples 9. “because they will not be relevant to the classification accuracy” – who are they? 10. “and epsilon is the Gaussian noise” –> the standard Gaussian noise 11. “where z^0 = x” – it seems this should be placed somewhere earlier when z^l is introduced since z^0 is not used in eq.(2) after which this text is placed 12. It is unclear how lambda^l and CNN \theta are learnt 13. It is unclear how the values of features d(x) are used to detect OOD samples 14. “comparison methods, and models” – not clear what models mean here 15. Missing references to datasets in the main text. At least reference to Appendix A.2 is required 16. “We used 5,000 validation images split from each training dataset and chose the parameter that can obtain” – which parameter? 17. “All the hyperparameters of ODIN” – a reader does not know yet that ODIN is used for comparison 18. “which consists of 100 OOD images from the test dataset and 1,000 images from the in-distribution validation set” – it is a bit confusing to call OOD dataset as a test dataset in this context 19. “We tuned the parameters of the CNN in Equation 4 using 50 validation training images taken from the 100 validation images. The best parameters were chosen by validating the performance using the rest of 50 validation images.” – this is confusing. What parameters do the authors talk about in the second sentence if not the parameters of the CNN? 20. “We used TNR at 95% TPR, AUROC, AUPR, and accuracy (ACC),” – Some elaboration is required, at least the reference to Appendix A.1. What is the changing threshold for AUROC and AUPR? Why AUPR-In and AUPR-Out are considered and only a single AUROC is considered. What is the positive class for AUROC? 23. “For LeNet5, we increased the number of channels of the original LeNet5 to improve accuracy” – do the authors mean that they allowed RGB images as input rather than greyscale? If yes, this explicit explanation would be preferable 24. “We inserted the reparameterization trick” – not the best word choice. Reparametrisation trick is a computational/implementation trick/method and it is hard to say that it can be inserted into a network. I believe what the authors mean is that they inserted mean/std outputs instead of point outputs. Conceptually, this means that the output of the corresponding layers is considered to be stochastic rather than deterministic. Also, it is unclear when the authors say they insert it to the softmax layer. According to Section 3 the softmax layer is never considered to output means and stds. 25. The numbers of epochs for training NNs are very small for LeNet and WideResNet in the experiments. Did the models manage to converge during this short training? Minor: 1. “These data were also used” -> “this data”\n",
      "        - Review:###This is a nice piece of incremental work on top of previously published GAN imputation methods. It seems to work well in the limited evaluation and is at least claimed to be easier to use for practitioners. This paper could benefit tremendously from both better evaluation and discussion. The paper would be much clearer if GI contextualized itself relative to GAIN on the one hand (which is the most similar GAN method) and multiple imputation on the other hand (of which this is almost, but not quite, an instance of). Suggestions for improving the introduction & discussion: * The purpose of this paper is to model uncertainties about missing values — you really should say more about probabilistic methods than *A few exceptions exist such as Bayesian models”. At least give some motivation for why certain imputations problems couldn’t be feasibly solved by modeling the missing values in a probabilistic programming framework. * Other GAN methods for imputation (GAIN and MisGAN) are dismissed as *often very complicated to be applied in practical setups by practitioners”. Given that the described method resembles GAIN, is it really much simpler? If so, can you be more specific when characterizing related work? * *This is different from approaches such as multiple imputation where several predictors are trained on different imputed versions of a dataset.” — the main difference between this approach and MI is that you’re interleaving imputation and training a downstream model. Emphasize this earlier on, since it will make the whole technique easier to understand. Suggestions for improving the evaluation: * You’re imputing missing rectangles from an image dataset — please show us the resulting images. I would greatly prefer this to Section 4.5 — which is a very low sample size, low dimensionality example and it’s really unclear how well it generalizes to real data. * *We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution.” — I think this is a mistake, likely motivated by the proposed method doing worse under the RMSE metric. Show us several relevant metrics and then discuss their tradeoffs afterward. * *We run each experiment multiple times (at least 4)” — please report how often each experiment was run, even better if you standardize this number. * For Table 2, please provide accuracy without missing values as a baseline. * Add MICE or some other “standard” imputation method as a baseline. Suggestions for improving readability: * Many sentences start with “in this” (e.g. “in this case”, “in this setting”, &c). Sometimes these sentences even co-occur within the same paragraph. Try to switch up the phrasing and move away from repetition. * Not a complete sentence: *For instance, jointly training multiple generator/discriminator networks, tuning objective functions with multiple hyper-parameters, etc.* Update: I think the latest draft of the paper is a big improvement, the inclusion of a *classical* baseline, improved language and additional appendices are all welcome. I*m leaving the rating as a *weak accept* since the paper still feels rough and could use additional editing/streamlining.\n",
      "        - Review:###The uncertainty of having a missing value is investigated on the prediction by not assigning a single imputed value but N different values generated via an imputer network (based on GAIN). Unlike old-fashion multiple imputation techniques, one predictor is trained on different samples and it induces the uncertainty. The experiments on number of datasets show the proposed predictor is capable of having a fairly better performance. Overall, this paper raises an interesting point about missing data imputation via generative models, and well-written; however, there are number of concerns: 1- The predictor is trained on different version of imputed samples (imputed via the generator); this equates to making noisy version of the real samples, where noise is applied to the missing variables. A side effect of this is generalization of the predictor; thus, have you been careful that the improved accuracy is not due to generalization? In other words, if we imposed the generalization via adding gaussian noise to the imputed samples by GAIN for example, would we get improved accuracy too? 2- Your method known as GI is a modified version of GAIN. You could also use MisGAN, and I am wondering if the results would have been different if generator in MisGAN was used in GI in Figure 2 (b), as MisGAN works better than GAIN. 3- I am also wondering how the GAIN imputation changes by removing the MSE term? GI discards the MSE term in GAIN, and it changes the distribution of the imputed variables by GAIN. Could you maybe fit a Normal distribution on a chosen imputed variable (N=128) and visualize how different it is from the distribution of imputed variables with GAIN with MSE term. 4- In justification for claim 1, it is said “This is equivalent to training models using noisy labels”. This is not accurate: in noisy label prediction, we have one (noisy) y corresponding to each x, in your case there are multiple ys for one sample. 5- In the implementation details, I cannot fully wrap my head around the part “z vector of size 1/8”; how did you choose this 1/8?\n",
      "    - Unsupervised Domain Adaptation Methods and Evaluation (L0, 25)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposes a training strategy for robustness against label noise. The training strategy is simple and straightforward. The neural network will first be trained on the entire dataset with all the noisy labels. After obtaining the network with lowest validation error, the network will be used to make a prediciton on the original training set and select a subset of it to construct a maximal safe set. Finally, the network will be findtuned on this maximal safe set. The training strategy is very similar to tradictional self-training in semi-superivsed learning and co-training for domain adaptation ([Co-training for domain adaptation, NIPS 2011]), except that the proposed prestopping only iterate the procedure once. The paper discusses two important questions for the method: (1) when to early stop the training; (2) how to constuct a maximal safe set. The authors* responses to these questions are very natual but less interesting. Using the lowest validation error to early stop the training could be suboptimal, since the small validation set can not fully capture the data distribution and could make the network empirically overfit to this validation set. The criterion to contruct a maxial safe set is also conventional, and is similar to what a number of papers are doing, for example, [1] Co-training for domain adaptation, NIPS 2011 [2] Self-ensembling for visual domain adaptation, ICLR 2018 [3] A dirt-t approach to unsupervised domain adaptation, ICLR 2018 [4] Iterative learning with open-set noisy labels, CVPR 2018 In experiments, the results are not very surprising. There are some baselines that adopt a similar (iterative) pipeline (learning the network - selecting a subset of the training samples - re-learning the network): [1] Iterative Learning with Open-set Noisy Labels, CVPR 2018 [2] Dimensionality-Driven Learning with Noisy Labels, ICML 2018 [3] Symmetric Cross Entropy for Robust Learning with Noisy Labels, ICCV 2019 The authors can consider to compare to some of these baselines, especially [1] and [2]. The difference between the paper and [1,2] is basically the criterion to construct the maximal safe subset. Besides, I suggest the authors to conduct large-scale experiments on ImageNet or even a subset of ImageNet, since the difficulty of detecting label noise is much higher when the resolution of images become bigger. CIFAR-10 and CIFAR-100 only contain 32x32 images, which is far less challenging. Overall, I think the paper is well written, the idea is clearly presented, and the experiments also seem convinceing. However, the contribution of this paper is very incremental.\n",
      "        - Review:######Summary### This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ### Novelty ### The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty. ###Clarity### Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ###Pros### 1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. 2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. 3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ###Cons### 1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset: DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/ Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/ 2) The organization and presentation of this paper should be polished. Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.\n",
      "        - Review:###This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives. Pros: - The writing is good - Satisfactory empirical results Cons: - The proposed method is very similar to certain methods in the literature Detail comments: (1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited. (2) Experiments - How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available. - What does the * in DRCN* mean in Table 1? - ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin. (3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition. Typos: - In Eq.(1), there is a missing D in the first term. D_s should be mathcal{D}_s to match previous notation. - In Eq.(2), the *0,* is not meaningful given the definition of []^+. Refs - Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR*06) (Vol. 2, pp. 1735-1742). IEEE. - Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham. - Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236. # Update after rebuttal Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are *totally different* can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.\n",
      "  - Theoretical Foundations and Analytical Methods (L1, 60)\n",
      "    - Gradient Normalization Techniques and Variance Reduction in Deep Learning (L0, 21)\n",
      "      * Examples:\n",
      "        - Review:###This paper studies the solution of neural network training in the NTK regime. The trained network can be written as the sum of two terms --- the first is the minimum RKHS norm interpolating solution, and the second term depends on the initialization. When the initialization scale is small, the second term almost vanishes, but when the initialization scale is large, it*s likely that the second term becomes very large, leading to worse generalization. The technical contribution of this paper is pretty low. The most important formula is (14), which only appears in the second half of the paper (the first half of the paper is almost all known results). The bounds in later part of the paper are also straightforward. Moreover, another paper https://arxiv.org/abs/1905.07777 already studied the same question and showed that non-zero output can increase the generalization error. ----------- update: I have read the authors* response. My assessment stays the same since I still think that the technical contribution of this paper is quite limited. Also there is a negative effect of using small init, which the authors might have overlooked: when the init is smaller, you*d need a larger width for the NN to be in the NTK regime. See e.g. *Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks. Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruosong Wang. ICML 2019*.\n",
      "        - Review:###The paper considers the impact of initialization bias on test error in strongly overparameterized neural networks. The study uses tools from recent literature on the generalization of overparameterized neural networks, i.e. neural tangent kernels and interpolating kernel method, to provide useful insights on how the variance of weights initialization affects the test error. I have a few questions about theoretical results, but the paper has a convincing experiment that supports its theoretical claims. Addressing the following points will improve the exposition of the paper. 1. Please provide a little hint on how Lemma 2 rewrites the equation (13) for linearized function for easier readability without referring to the Appendix. 2. In the case of cross-entropy error, would the effect be similar? Could this be verified with a similar experiment as for MSE? 3. To what extent this result is observed in not as strongly overparameterized settings? In other words, it would be interesting to see what happens if you fix the architectural choice while increasing the number of training parameters, how long does the test error effect persist? Minor remark: - a few typos are present on pages 4, 5, 7, 8\n",
      "        - Review:###This paper proposes a method, known as DrGCN, for reweighting the different dimensions of the node representations in graph convolutional networks (GCN). Specifically, the representation of every node is element-wise multiplied with a weight vector, which is parameterized as a function of the average input node representation, where the function is a two-layer neural network. At a conceptual level, this is similar to various existing normalization schemes, such as batch normalization and weight normalization. While it is claimed in Section 4.3 that the difference is that “[batch normalization] reduces variance between samples, while DrGCN reduces variance between dimensions”, I am not sure if this characterization is accurate. Batch normalization actually makes each dimension have unit (sample) variance and so does not make the variance of each dimension small. What it does is to make the sample variance of each dimension equal, which is also what DrGCN tries to do, i.e.: reduce variance across dimensions (since samples in the case of GCNs are the representations of different nodes). DrGCN is also similar to weight normalization because like weight normalization, DrGCN learns a transformation on top of the vanilla representation (in the case of weight normalization, the vanilla representation is the normalized weight vector; in the case of DrGCN, the vanilla representation is the node representation before reweighting). The conceptual contribution therefore seems incremental. Incremental conceptual contributions would be fine if (1) they result in a surprising theoretical result, or if (2) they result in a surprising improvement in empirical performance. Unfortunately, neither seems to be demonstrated in this paper. In the theoretical analysis, there are various occurrences of unjustified leaps of logic; as a result, what is claimed to be shown by the analysis is different from what is actually shown, and it is unclear what is actually shown is substantially related to the proposed method. For example, in Section 3.1, the paper says that “GCNs are different from fully-connected networks only in \tilde{A}, and degrade to fully-connected networks when \tilde{A} = I. So, our analysis can be somehow be generalized to DrGCNs.” The first sentence is true; in other words, it says that fully-connected networks are a special case of GCNs when \tilde{A} = I. However, the second sentence does not follow - just showing a special case (which is what the subsequent analysis does) does not say much about the general case. Relatedly, the architecture that is analyzed is of the form H^l = W^l phi(S^l H^{l-1}) + b^l for l = 1, …, k, whereas the architecture that is proposed is H^l = phi(W^l S^l H^{l-1} \tilde{A}^{l} + b^{l}) for l = 1, …, k. The latter cannot be cast as the former unless \tilde{A} is diagonal. Also, the caveat of the mean field approximation are not stated - whatever result that is shown is only valid at the infinite width limit, which is different from what is claimed in the abstract, which says that *We prove that DrGCNs can reduce the variance of the node representations by connecting our problem to the theory of the mean field.” Additionally, the analysis is done in the case where S is directly parameterized, whereas the proposed method parameterizes S as the output of a two-layer neural network. I presume the reason why latter was done in practice because it worked better empirically. Because this is not explained by the analysis, the theory is incomplete, and so this caveat should be clearly stated in the abstract. In the experimental results, the performance improvement of DrGCN over layer normalization is not statistically significant. Also, DrGCN is only compared to other normalization schemes (batch normalization and layer normalization) on one dataset, and so there is insufficient evidence to conclude that DrGCN generally works better than existing methods empirically, Additionally, in section 2.1, it is claimed that “sampling-based GCNs still lie within the framework of equation (2), as we can set all unsampled edges to 0 in \tilde{A}”. This does not seem to be accurate, because in sample-based GCNs, different edges are sampled in every minibatch, and so there is no fixed choice of \tilde{A} that makes these GCNs equivalent to the formulation in eq. (2). Also, if the goal is to reduce variance across dimensions (as the paper claims in Section 4.3), why was the average node representation fed into the two-layer neural network rather than its reciprocal?\n",
      "    - Information Theory and Entropy Estimation in Discrete and Continuous Domains (L0, 16)\n",
      "      * Examples:\n",
      "        - Review:###This work proposes a framework for solving de-mixing problems. The hard constraints from human inputs about a specific problem are relaxed into continuous constraints (the *slow* reasoning part), and a reconstruction loss measures the fitness of the inferred labels with the observations (the *fast* pattern recognition part). Due to the relaxation inference becomes an optimization problem, and on a Sudoku task and a crystal-structure-phase-mapping recovery task (both de-mixing tasks), the proposed method gets very good performance (100% for all Sudoku tasks including one in the appendix). Pros: 1. The method works well for the two demixing tasks. 2. It *led to the discovery of a new material that is important for solar fuels technology* Cons: 1. The generative decoder seems to be pretrained on both tasks instead of learned (correct me if I misunderstood), and I*m not sure if this approach can work in cases where we don*t have access to such a generative decoder, so branding the approach *deep reasoning network* might be an overclaim. 2. No reasonable baselines are used: The supervised baseline in Sudoku does not use those handcrafted constraints at all. Given pretrained decoders, a reasonable baseline would be randomized optimization methods such as simulated annealing, which might also solve the two tasks listed here. 3. This paper proposes a deep reasoning framework with relaxation and continuous optimization, but it is unclear whether this can solve general reasoning problems such as multi-hop QA or some NP-hard integer programming problems. Questions: 1. In algorithm 1, how are the penalty weights and thresholds adjusted? 2. How to determine whether a run needs to restart? Overall this work points an interesting direction of combining reasoning and pattern recognition in the same network and the proposal works well on two de-mixing problems. However, I am not convinced that the proposed solution can generalize to tasks other than the tasks proposed here, and the usage of pretrained generative decoders undermines the significance of this work. Therefore, I am inclined to reject this paper. ---updates after reading authors* rebuttal---- Thanks for revising the paper and addressing my concerns! However, my concern Con #2 has not been fully addressed. I think a reasonable baseline (at least for Sudoku) is simulated annealing, such as in https://www.researchgate.net/publication/220704743_Sudoku_Using_Parallel_Simulated_Annealing. I believe that with restarts those baselines would also solve the Sudoku problem. Another concern I still have is the claim of *reasoning*, and I*d suggest to narrow down the claim to be only on pattern de-mixing, since the reasoning part seems to be writing down continuous constraints from the discrete constraints (same as the concern in review #3). Although the proposed approach can solve some NP-C integer programming problems, it is unclear based on the experiments here whether it can work for general reasoning tasks (e.g., DROP https://allennlp.org/drop or listops https://arxiv.org/pdf/1804.06028.pdf) without writing new rules manually. Besides, after reading Reviewer 3*s comments, I also feel it unsuitable to train DRNet (generalization) on test set for 25 epochs even though you made it explicit in the revised paper. I*d recommend removing that experiment since it doesn*t change this work that much.\n",
      "        -  PAPER SUMMARY: This paper proposes a fast inference method for Gaussian processes (GPs) that imposes a sparse decomposition on the VI approximation of the posterior GP (for computational efficiency) using the KNN set of each data point. This is further coupled with armortized inference for better scalability. NOVELTY & SIGNIFICANCE: This paper adopts a different approach of characterizing the VI approximation of a GP posterior than original VI approximation that was developed in Titsias (2009): Instead of characterizing the surrogate q(f_I) of p(f_I | Y) for a small collection of inducing inputs, the proposed method characterize q(f) directly where q(f) = int_f_I q(f_I) p(f | f_I)df_I. This is, however, a somewhat strange direction which, to me, seems to raise extra issues that could have been avoided if one follows the conventional VI approximation: (1) As the posterior surrogate is now directly over f instead of f_I, the number of variational parameters is now proportional to the data size which requires several (redundant) extra approximations including armortized inference & the lower-bound on the entropy term that admits a sparse decomposition. (2) This at least creates the armortized and entropy gaps that decrease the expressiveness of the original VI. While I understand that this is in exchange for the ability to encode local information (via KNN) within the surrogate posterior, it is not clear to me why do we need to incur all these computational issues to incorporate such local information. (3) For example, instead of forcing such local information in the posterior surrogate q(f), we could alternatively let it be reflected in the test conditional p(f_* | f_I, Y_n(*)) such that the test output depends on both the inducing output and a local partition of data (e.g., via K-mean), which has been previously explored in [*] and later incorporated in the conventional VI paradigm of Titsias (2009) without incurring extra intractability [**]. (4) This maintains the dense correlation between data points within the same neighborhood while allowing the VI surrogate to be more concisely specified and independent of the no. of training data points. Furthermore, it also helps avoid the data-bound overhead of computing a KNN per test point. [*] Local and global sparse Gaussian process approximations (AISTAT-07) [**] A distributed variational inference framework for unifying parallel sparse gaussian process regression models (ICML-16) To summarize, the practical significance of placing such a VI approximation directly on q(f) to encode such (indirectional) local information is, given the above, questionable to me. Please note that I am not disputing the potential use of this VI form here, which could have been the only way to encode a different (directional) type of information. For encoding KNN information, however, it only seems to create more troubles than it solves. Minor point: The above references, especially [*], should have been cited. TECHNICAL SOUNDNESS: [A] Optimization of the ELBO: (1) The ordering of data (i.e., the directional information) was mentioned repeatedly in the paper but its importance to the fast approximation was neither explained nor discussed. (2) The decomposition form of Eq. (6) also raises a question: How do we know that the term inside the log is positive? There seems to be missing information on the constraint of R. [B] Amortized Inference: (1) The choice of the GCN seems arbitrary here. I am in fact not sure why GCN is necessary for the inference network & furthermore, GCN also brings to the table another heuristic choice of A. (2) How do we set the adjacency graph A? (3) How do we know what is the right complexity for the GCN? [C] Complexity: The complexity analysis is too informal and lacking fine-grained information. Please include a detailed complexity analysis of the training and inference cost in terms of the input dimension, the no. of data points, the size of the neighborhood and the batch size. It is also necessary to factor in the KNN overhead (e.g., the cost of building the K-D tree for low-dimensional embedding of data & the approximation cost of projecting that information to high-dimensional data) EXPERIMENT: The experiment results only show marginal improvement over the baselines, and the size of the dataset for regression is too small. If I read correctly, both have fewer than 20000 data points. SVGP in particular has been tested on a much larger datasets (AIRLINE, UK Housing) comprising millions of data points -- comparison on such dataset should have been reported. Note that the largest dataset used to evaluate the efficiency of fast approximation of GP is on the scale of 6M data points [****]. On that note, eBird and precipitation should not even be considered mid-sized. To demonstrate the efficiency of local information encoding, comparison with [*] should be reported. There is another class of inducing-point methods that use expectation propagation that should have been discussed and/or compared with: [***] A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation (JMLR-18) [****] Distributed Gaussian Processes (ICML-15) CLARITY: The paper is clearly written. REVIEW SUMMARY: This paper adopts a VI approximation that deviates from the conventional form of (Titsias, 2009) to encode the KNN information, which causes extra computational issues (that incurs extra approximations). I find this deviation redundant seeing that the same information could have also been accounted for using the old VI form, which is a lot more computational efficient. I also find the experiment lacking as comparison with fast approximation method such as [*] that incorporate local information is not included. There are also a few technical ambiguities that need to be clarified. ------ Post-Rebuttal Update ------ Thank you for the rebuttal & I have read it in detail. However, it still does not address my concern, which I re-summarize here: I do not dispute the beneficial of exploiting neighborhood information but my point is we could still leverage the same amount of neighborhood information without going through the trouble of incurring extra steps of approximation due to approximating q(f) instead of q(f_I) -- I think I have elaborated this in points (1) - (4) in my original review -- which also creates the amortized gap. I am also not sure what model reuse means (in this context) and whether it is relevant since it appears somewhat noncentric to the objective of this paper. Also, apparently, the experiment is still lacking since to me, comparing with [*] is important in substantiating the contribution claim of this paper.\n",
      "        - Review:###The authors consider active deep learning. They propose decomposing predictive entropy into a) vacuity (lack of evidence) and b) dissonance (contradictory evidence). They frame this in terms of *subjective logic*. In practice this is achieved by having the NN output the parameters of a Dirichlet, which allows an additional degree of freedom describing variance/vacuity. Dissonance is defined in terms of the support of contradictory classes. To get improved estimates of vacuity they augment the loss with a term regularizing the Dirichlet parameters to be small (low precision) at unlabelled points with higher KDE(unlablled points) than KDE(labelled points). They propose initially weighting vacuity and later dissonance as AL proceeds. Encouraging results are presented on simulated 2D data, MNIST and CIFAR10. I think the basic idea of separating vacuity and dissonance is interesting, and the demonstration of the failings of existing ADL approaches is valuable. It wasn*t clear to me how this *subjective logic* theory gets you to the specific definitions of vacuity/dissonance, or whether these were just proposed by the authors. Equation 6 seems to come out of nowhere (whereas the rest of the derivations using the Dirichlet are very intuitive). The idea of encouraging the network to be uncertain far from data is also reasonable. While some Bayesian models such as Gaussian process regression with a RBF kernel give you this for free, it is certainly true that DL methods do not have this characteristic in general. Regularizing the r to be small seems like a reasonable way to do this, but I*m not convinced by the kernel density estimate part. DNNs can operate on very high-dimensional, structured inputs. Even the simplest of these, images, requires some degree of spatial invariance (achieved using convolutions) to obtain meaningful predictions. I find it very hard to believe a KDE can do anything meaningful in such spaces, even if you could find a sensible bandwidth (which isn*t discussed at all). It is possible of course that random selection of unlabelled points to regularize in this way would work just as well. Unfortunately no ablation study is performed, so we don*t know what the individual contribution of the three proposals (moving from vacuity to dissonance, augmented loss and Dirichlet likelihood) is. How sensitive is the method to the vacuity/dissonance weighting? The improvement over competing methods for MNIST and CIFAR10 appears to mostly manifest after the initial 20 or so acquistions. Do these results extend to batch AL? For many applications that*s more important. Overall I thought this paper had some promising ideas but they need to be more thoroughly tested empirically to give some sense of how robust and generalizable the approach is.\n",
      "    - Statistical Dependence Testing and Methods in Neuroimaging (L0, 10)\n",
      "      * Examples:\n",
      "        - Review:###This paper introduces a machine learning pipeline for injury prediction in NFL events. The paper discusses several system settings on data streaming and processing, along with model selection and other hyper-parameter tuning details. The problem itself is very important. However, there are several disadvantages of the current status of the paper. The writing of this paper needs to be largely improved. The description is very redundant and the texts are very hard to read. Therefore, it makes the paper much less clear to the reader. Second, the proposed system is not related to the focus of ICLR and it lacks the novelty. Both the data processing and model selection methods are already well-known practicies. Experiments are also not established well enough to demonstrate the advantage of the proposed solution to this problem. Due to the above issues, I think the paper is not ready for publication.\n",
      "        - Review:####### A. Summarize what the paper claims to do/contribute. Be positive and generous. #### The paper translates the Leaky Integrate and Fire model of neural computation via spike trains into a discrete-time RNN core similar to LSTM. The architecture would be readily amenable to the modern deep learning toolkit if not for the non-differentiability of the hard decision to spike or not. The hard decision is made by thresholding. The paper adopts a simple approximation of backpropagating a *gradient* of 1.0 through the operation if the threshold is within a neighbourhood [thresh - a, thresh + a], and otherwise 0.0, so the system can be trained by backpropagation. The architecture is tested on a few *neuromorphic* video classification datasets including MNIST-DVS and CIFAR-DVS. Experiments are also run on a text summarization task. #### B. Clearly state your decision (accept or reject) with one or two key reasons for this choice. #### The reviewer thinks the paper should be rejected in its current state. The proposed architecture is a straightforward change to a standard LSTM core. Thus it should be compared head-to-head to LSTM on standard datasets for these models (e.g. classic synthetic tasks, language modeling, speech recognition, machine translation, etc) with everything else held constant (hidden size, learning rate, sequence length, etc etc). It also doesn*t really carry over any of the benefits of Spiking Neural Nets even though it is inspired by Leaky Integrate and Fire because it operates in discrete time like a normal RNN, just with an extra binary output produced by spiking. It*s unclear that a spiking inductive bias is actually useful, even though event-driven computation could in theory allow much less computation, the proposed method does not have that property. So the paper doesn*t really provide evidence to back up their claim that the proposed model combines the complimentary advantages of Deep Learning and Spiking Neural Nets. #### C. Provide supporting arguments for the reasons for the decision. While the proposed method is in-spirit inspired by the leaky integrate and fire model, it is operated/trained in discrete time which does not allow it to achieve the benefits of continuous time integrate-and-fire models which allow for less computation and time-discretization-invariance. The conversion of the spiking model to the deep learning framework is rather crude, as the differentiable approximation to the non-differentiable threshold operation is biased and not well-motivated either empirically, intuitively, or theoretically (i.e. there are no comparisons to alternative choices). There are new techniques for marrying continuous-time models and deep learning which seem more promising to investigate to this end (e.g. Neural ODE). So in summary, the method doesn*t have the computational benefits of a biologically plausible spiking algorithms and is not well-tested against competing deep learning methods, making it hard to verify the motivation of pushing toward a performant yet biologically plausible algorithm. #### #### D. Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. #### There are many grammatical and word-choice mistakes which make the paper hard to read. Mainly, from a practical perspective, the paper would be much-improved by showing what benefit the spiking inductive bias confers over a standard LSTM on standard tasks in the deep learning community. The method/landscape should be developed and studied in further detail until claims can be made about combining the strengths of spiking and deep-learning models.\n",
      "        - Review:###This paper deals with neuromorphic computing architectures for solving deep learning problems. Given that the energy-efficiency is a critical need for modern machine learning tools, this is an important study in my opinion. Until now, most of the spiking neuron based works don*t really apply for realistic deep model learning. Therefore, advancements such as these would be very useful. However, my biggest concern is that the paper may be a bit difficult to grasp for the general ML audience. I myself am not an expert in this topic and had quite a bit of difficulty as an ML researcher in understanding parts of the paper. As a matter of fact, some of the supplementary material was quite helpful to understand the entire context. So, I suggest to move some of the basics from there to the main paper. Also, some of the performance comparison results in the supplementary section are more convincing and clear compared to the results in the main paper. Based on my understanding, the algorithmic approach seems logical and the empirical results are convincing. However, I am not in a position to assess the level of novelty. So, I am giving the benefit of the doubt to the authors, given that this is a critical topic for the ML community.\n",
      "    - Theoretical Bounds and Generalization in Lipschitz Analysis (L0, 13)\n",
      "      * Examples:\n",
      "        - Review:###[Summary] This paper studies the impact of initialization noise on the theories of wide neural networks in the Neural Tangent Kernels (NTK) regime. The paper proves that the difference between the trained neural net and the kernel interpolator (with the NTK) can be bounded by O(sigma^L + 1/sqrt{m}), where sigma^2 is the initializing variance of each individual weight entry. Relationships between the generalization error of these two functions are derived from the above bound. [Pros] The general message that this paper conveys is interesting -- the initial network f_{\theta_0}(x), which is typically omitted (or made small by making sigma small) in NTK analyses, can deviate the converged NN from the kernel interpolator in terms of generalization error. [Cons] There are fundamental mistakes in the statements/proofs of Theorem 2, 3, 4: -- Theorem 2: the statement is “whp over W, the bound … holds uniformly for x”. The proof relies on Lemma 3, whose statement is also uniform over x, but the proof applies the Markov inequality *for a single x* and is thus valid only for a single x. (As it’s Markov, it seems not sensible to apply the union bound upon it.) -- Theorem 3: the difference between L^NN_test and L^int_test should be on the order of (sigma^L + 1/sqrt{m}) rather than it squared. To bound the difference in squared loss we have a^2 - b^2 <= O(1) * |a-b| (if a, b are bounded by O(1)). We don’t have a^2 - b^2 <= |a - b|^2. -- Theorem 4: J(X_test) as defined is a vector whose dimension grows with the number of test data points, where the theorem requires it to be a scalar. Indeed the treatment of test data as a fixed matrix (rather than samples from a distribution) is already a bit atypical. *** I have read the authors* rebuttal and the other reviews, and I*m glad to see the issues with Theorem 3 and 4 pointed out above are fixed in the revision. However, I also agree with the other reviewers that the paper in the present stage has not yet demonstrated sufficient technical contributions, and thus I am keeping my original evaluation.\n",
      "        - Review:###Through the lens of Distributional Robust Risk (DRR), this work draws a link between adversarial robustness and Lipschitz constant regularisation. The authors first provide an upper bound of the DRR (with a Wasserstein ball as the ambiguity set) in terms of the true risk and the Lipschitz constant of the loss function under the current model. They show that the standard adversarial risk can be upper bounded by the DRR, emphasizing that the Lipschitz constant regularised loss can be used as a proxy for adversarially robust training. The authors then apply this idea to kernel methods and aim to minimise the true risk under a Lipschitz constant constraint. By using a bound between the Lipschitz constant and the largest eigenvalue of the Gram matrix, and by approximating the Gram matrix using Nystrom approximation, they express the Lipschitz constant constraint as a convex constraint. In the case of multiplicative kernels, this approach is shown to be efficient when only using a polynomial number of sample points for the Nystrom approximation. This method is tested on 3 standard datasets and is shown to outperform state-of-the-art deep learning based approaches over a wide range of perturbation in both l_2 and l_infinity norm. I have concerns about the novelty of the two main contributions of this work, which are Theorems 1 and 2: * Theorem 1 is a direct implication of Kantorovich duality, well known in optimal transport. * Theorem 2 is a rewriting of Proposition 3.1 in [1], which is not cited. My second concern is that the value of n (number of sample points used in Nystrom approximation) has not been specified for the presented experiments (unless I missed it). This is an important parameter to specify since the algorithm requires to invert an n by n matrix when applying Nystrom approximation, and scalability is usually the main criticism to kernel methods. When comparing the methods, would it be possible to estimate the Lipschitz constants of the models trained by the various algorithms ? This would enable to observe the true effect of the methods on the Lipschitz constant. Due to these two concerns above, I tend to reject this submission. A few comments about clarity: * It would help to provide the full kernel method algorithm (with more detail than Algorithm 1 in Appendix), e.g., explicit the convex constraint on gamma when training the SVM. * epsilon notation is used both for the accuracy in the Lipschitz constraint (as in Theorem 3) and the adversarial perturbation (as in equation 3 or Figures 4 and 5). In page 7, paragraph *Attacks*, delta is used once for denoting the perturbation, but epsilon is reused just after. More consistency in the notation would be greatly appreciated. * It is not clear in the main text that L is a bound on the gradient norms at sample points. It is made clearer in the appendix, so one sentence from the appendix should be added to the main text. Typos: * p.6 top: *and analogously for ...(x_1^a,.)* instead of *(x_1^b,.)* [1] Distributionally Robust Deep Learning as a Generalization of Adversarial Training, M. Staib, S. Jegelka, NIPS, 2017\n",
      "        - Review:###In this paper, the authors propose a distributional robust risk minimization method using Lipschitz regularization and give an approach to approximate the Lipschitz constant for product kernels efficiently. My major concerns are as follows. 1. The result in Theorem 1 is similar to Theorem 3 in [1]. The authors use McShane–Whitney extension theorem [2] to generalize the previous result, which is a trivial derivation. 2. Although the proposed method is theoretically applicable for a wide class of models, it is hard to use in practice, since the Lipschitz constant is too hard to compute exactly. To address the problem, the authors give an approach to approximate the Lipschitz constant for product kernels efficiently. However, the applicability of the method is still limited if it can be used only for product kernels. 3. In the abstract, the authors claim that this paper resolves the previous methods* limitations for deep neural networks. However, there is no corresponding theoretical analysis. 4. Some notations are confusing. For example, in the second paragraph on Page 3, there are two different sets denote ; in the definition of , the meaning of the symbol is unclear; in Theorem 2, the symbol is not defined. [1] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. In International Conference on Learning Representations (ICLR), 2018. [2] Iosif Petrakis. Mcshane-Whitney extensions in constructive analysis. arXiv preprint arXiv:1804.06757, 2018.\n",
      "- Learning Methodologies and Algorithms (L2, 253)\n",
      "  - Graph Neural Networks and Community Detection (L1, 5)\n",
      "    * Examples:\n",
      "      - Review:###In this paper, the authors proposed a new kind of graph neural network that can use continuous edge features. Specifically, a variational lower bound is proposed for mutual information and integrated into the GNN model so that MI between edge features and the message passing channel is maximized. Experiments on molecule graph datasets and knowledge graph datasets show the effectiveness of the proposed method compared to the state-of-the-art GNN models. Utilizing continuous edge features in GNN is an important and difficult task. The authors proposed an elegant solution. The paper is well written and extensive experiments on large scale datasets are compared with 9 competitors that are all outperformed by EIGNN on nearly all the cases. To conclude, I think the paper will be a good addition to the conference.\n",
      "      - Review:###The authors present an algorithm for actively learning the nodes in a graph that should be sampled/labeled in order to improve classifier performance the most. The proposed techniques use both the graph structure, and the current classifier performance/accuracy into account while (actively) selecting the next node to be labeled. There seem to be two main contributions in the paper. 1) The propose to sample nodes nodes based on *regional* uncertainty rather than node uncertainty 2) They use an variant of pagerank to determine nodes that are central, and hence most likely to affect subsequent classification in graph convolution classifiers. Both approaches seem to be interesting. There are experiments to show effectiveness of these techniques, and there are some interesting observations (for example, that the APR technique works better for smaller sample sizes, while the regional uncertainty methods do better for larger sampling fractions.). While both techniques seem straightforward extensions of previous approaches (and are well explained in the paper), the experiments indicate that they work better than prior approaches. It would have been nice if the authors had also discussed ways in which one or more of these techniques could be combined though, or discussed how we could pick the right approach (in a more empirical way, since it is not clear what the threshold for high sampling rate/low sampling rate distinction is, or if it varies from problem to problem)\n",
      "      - Review:###Although this paper seems to only combine existing techniques in community detection and node embedding into a co-train process. The idea is simple and easy understood and the paper is well-written. Theoretical analysis is provided for the approximation error for the sampling strategy. However, major concerns are: 1. Experimental results show that co-training node embedding and community detection can improve the performance for node classification. The improvements may result from the assumption that papers with the same class label are associated with the same community in the citation graph. However, in the dataset, there are many cases that there are not dense connections among the same labeled papers. The authors should check the correlation between the detected communities and the original paper labels. 2. No comparison with other community-preserving node embedding methods, such as *Community Preserving Network Embedding* in AAAI17 3. Since this paper aims to combine community detection and node embedding process, a set of baseline should be considered. For example, if considering the downstream node classification of node embedding as an evaluation task, then how about the performance of the following two-step method. We can first detect communities based on the node features then do graph node embedding by considering the communities* membership and node features together (e.g. simply concatenating both community membership features and node features). 4. Efficiency and scalability evaluations are needed. Spectral clustering has a scalability issue when meeting big graphs. Since the spectral process is also applied in the proposed method, efficiency and scalability evaluations are encouraged to provide, especially for big graphs which are not covered in the selected datasets in this paper. 5. In Sec 5.3 and Fig 2, it*s mentioned that trends of the three datasets are different. For the increasing trend, how about the performance for an extreme case where all nodes are considered in one batch. On the other hand, adding more nodes in one minibatch could provide more information, but why there exists a decreasing trend? Though the authors provide a reason in Sec 5.3, it*s better to analyze the reason directly from the datasets.\n",
      "  - Graph-Based Learning Methods and Feature Analysis (L1, 69)\n",
      "    - Graph Learning Model Feature Analysis (L0, 43)\n",
      "      * Examples:\n",
      "        - Review:###This paper introduces a method combining the lexicon-based feature and character ngram model for handling negation in Amharic sentiment classification. The algorithmic contribution is demonstrated in a Amharic dataset. My rating for this paper is Reject because there is no novelty in the approach, it introduces some rules to build a FSA plus a logistic regression model with character-level ngram feature. It is also lack of argument/motivation why the proposed model is special and specific to the problem. The experiment only compares with two baselines in single dataset, which is also not convincing.\n",
      "        -  This paper extends the neural coreference resolution model in Lee et al. (2018) by 1) introducing an additional mention-level feature (grammatical numbers), and 2) letting the mention/pair scoring functions attend over multiple mention-level features. The proposed model achieves marginal improvement (0.2 avg. F1 points) over Lee et al., 2018, on the CoNLL 2012 English test set. I recommend rejection for this paper due to the following reasons: - The technical contribution is very incremental (introducing one more features, and adding an attention layer over the feature vectors). - The experiment results aren*t strong enough. And the experiments are done on only one dataset. - I am not convinced that adding the grammatical numbers features and the attention mechanism makes the model more context-aware. Other suggestions: - The citation format seems weird through out the paper. - Table 1 and 3 look like ablation results. It might be less confusing if it*s presented as *Full system: xx%; without pairwise FA: yy%; without grammatical numbers zz% ...*. - Equation 8 - 10 are quite confusing. What is f(x)? How large is V? What is u? etc. - Please define/explain the *grammatical numbers* feature when it*s introduced in Section 2.2.\n",
      "        - Review:###This paper proposes to use an extra feature (grammatical number) for context-aware coreference resolution and an attention-based weighting mechanism. The approach proposed is built on top of a recent well performing model by Lee et al. The improvement is rather minor in my view: 72.64 to 72.84 in the test set. There is not much in the paper to review. I don*t think the one extra feature warrants a paper at a top conference. The weighting mechanism over the features is also unclear to me why it benefits from attention. Couldn*t we just learn the weights using another layer? It could be context dependent if desired. It is also incorrect to criticise Lee et al. (2018) that they would give the same representation to the same mention every time. Their model is context dependent as they use a BiLSTM over the sentence. Of course the same mentions are likely to get similar representations, but this is desirable.\n",
      "    - Graph Neural Network Methods and Attention-based Pooling (L0, 26)\n",
      "      * Examples:\n",
      "        -  The authors proposed a novel method to find the Maximum Common Subgraph (MCS) of two graphs. I am familiar with the quadratic assignment problem (QAP) based graph matching and I am not very familiar with the MCS problem. The authors adopt Graph Matching Networks (GMN) for feature embedding, and then similarity matrix X can be generated by computing the similarities between the embeddings. The similarity matrix is then normalized using a similar way as the sinkhorn procedure in [1-3]. The Assignment matrix then can be given from X. Then a novel procedure, named Guided subgraph Extraction (GSE, which is considered as the main contribution of this paper), is used to get an MCS from assignment matrix. Here the authors may consider a simple baseline, which is to use QAP to give the assignment matrix, and then run GSE to obtain the MCS. Overall the paper is well written, and the experiment is good and solid. Some suggestions: The GCN based GMN might not be the best choice for graph embedding. The authors may consider stronger Graph Neural Networks such as DGCNN (used in [3]) or Message Passing Neural Network (used in [4] and [5]) as the graph embedding module in the future work. [1] Deep Learning of Graph Matching, CVPR18 [2] Learning Combinatorial Embedding Networks for Deep Graph Matching. ICCV19 [3] Deep Closest Point: Learning Representations for Point Cloud Registration. ICCV19 [4] Deep Graphical Feature Learning for the Feature Matching Problem, ICCV19 [5] Neural Message Passing for Quantum Chemistry, ICML17\n",
      "        - Review:###This work proposes a subgraph attention mechanism on graphs. Compared to the previous graph attention layer, the node in the graph attends to its subgraph. The subgraph is represented by an aggregated feature representation with a sampled fixed-size subgraph. The methods are evaluated on both node classification and graph classification problems. I have major concerns about the novelty, and experiments in this work. 1. The motivation is not clear. Using a subgraph or neighborhood to represent a node is reasonable. However, this work samples a subset of nodes from the one-hop neighborhood and aggregates them for attention mechanism. It is very similar to a GCN + GAT. The sampling process even loses some neighborhood information in the graph. 2. The experimental setups are very strange. In Table 2, the methods are compared to GCN and GAT on node classification problems. The performance of GAT is too low and even lower than that reported in GAT. Can authors explain this? It is highly recommended to use the same experimental settings as in GCN and GAT. The same problem exists in Table 3. Can authors provide a performance comparison based on the same settings in GIN? 3. The performance improvements are very unstable and marginal. In Table 3, the proposed methods can not compete with previous methods especially on large datasets like IMDB-MULTI. I wonder how the proposed methods perform on very large datasets such as reddit. 4. Can authors provide comparisons with a simple GCN+GAT?\n",
      "        - Review:###This paper introduces a subgraph attention method for graphs. Recently, many papers have shown that attention is a very important concept. However, there was no attention method for graph input structures, while a particular subset of nodes is very crucial to make the output. This paper first proposes the graph attention mechanism and hierarchical graph pooling idea. The attention basically subsamples subtrees so that each node can have the same number of attention candidates. Then, we can the attention network as many other papers. Experimental results show that the proposed attention based algorithm outperforms other algorithms. I think this paper attacks a very important issue *graph attention* and have a very nice algorithm and results. Overall, my recommendation is *accept*. Cons. It would better if the authors test some other different attention networks along with the current way. ================================================ I*ve read all discussions and changed my score. The novely of this work is not enough as R4 pointed out.\n",
      "  - Reinforcement Learning Algorithms and Stability Analysis (L1, 53)\n",
      "    - Lyapunov-Based Stability in Reinforcement Learning Algorithms (L0, 28)\n",
      "      * Examples:\n",
      "        -  This very well written and executed paper synthesizes several ideas recently published in the field of deep reinforcement learning-based goal-driven navigation. It elegantly combines these ideas together by presenting a neural agent architecture that consists of: * a perception module (e.g. a convnet) that extracts coarse visual feature maps s_t from an RGBD image * a differentiable map canvas M_t that is rotated at each step based on affine egocentric velocity (dx_t, dy_y, d phi_t) * differentiable inverse projection mapping, which uses known camera parameters, projective geometry and the depth channel of the image to project the visual feature vectors s_t onto a 2D map and add it to the existing canvas M_t * a recurrent module (GRU) for update a state h_t that is used for computing the policy distribution and value function * additional inputs to the policy and value function, that include a global map read r_t, as well as a query q_t (produced by the policy head) based retrieval of features from the map * position indexing of features retrieved from the map The algorithm is trained end-to-end, without extra supervision, using Advantage Actor-Critic (A2C) RL. Based on the strong inductive biases regarding the map, namely affine transforms of the map given information about relative movement, and projective geometry transformations of visual features in the map frame, it seems that the question of where to write is solved, and that the network only needs to learn what to write in the differentiable map. Evaluation is done on 3 games in VizDoom: finding the exit of the Labyrinth, object retrieval and find and return / Minotaur. Criticism: The authors could justify better the choice of using the projective geometry inductive prior. They use sentences like *We argue that projective geometry is a strong law imposed on any vision system working from egocentric observations* (not quite related to grid and place cells, despite being in that section) and *this inverse mapping operation is second nature to many organisms* without giving any reference. Several papers have been published in the last two years, focusing on differential memory architectures with a 2D map structure, projective geometry. This paper goes further by building and iteratively updating a 2D occupancy map using visual features and image geometry, just like RGBD-SLAM (which would merit a citation, e.g., [1] and [2]). This paper essentially combines existing ideas (see table 1): projective geometry, reward-based learning of M_t, RL, multitask navigation, semantic features. While this is not novel, seeing all this combined in a single technique does have merit. What is disappointing, given that this is a combination paper, is that the environment is so simply, and that photorealistic environments were not tested. For example, the VizDoom environment uses 2D sprites for objects, making the visual feature extraction from objects much simpler. Would the method work equally well with the objects in DeepMind Lab, which are seen from multiple view points? And what in an environment like AdobeIndoorNav? [1] Henry et al (2010) *RGB-D mapping: Using depth cameras for dense 3D modeling of indoor environments* [2] Izadi et al (2011) *KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera*\n",
      "        -  This paper studies how to build semantic spatial maps for the purpose of navigation in 3D environments. The paper presents a differentiable policy network that pastes together semantic map predictions into a spatial map. Information is read out from this map using a global read operation (that looks at the entire map) and a self-attention read operation. This information is used to produce actions. The paper presents experimental results in 3D VizDoom scenarios and reports improvements over a vanilla LSTM, and another spatial memory based method (Neural Map). Strengths: 1. I very much like the proposed formulation for tackling navigation problems. Using learning to leverage semantic reasoning, and structuring the computation spatially makes a lot of sense. 2. In my view, the proposed formulation advances current models in the following ways: a. Maintaining and updating allocentric maps, and reading off egocentric maps. This alleviates need for repeated rotations of the map, and thus prevents aliasing. 3. The paper provides ablations for the various parts of the system and provides qualitative analysis of the learned spatial representations. 4. Very good placement of work in current literature. I really like Table 1. Shortcomings: 1. The central contribution of the paper is the design of the egocentric spatial memory, how to build and maintain it over time, and its use in deep RL. The paper does this by using components from previous papers and presents a very nice summary of this in Table 1. Unfortunately, modulo the component described above (that of maintaining allocentric maps and reading off egocentric maps as and when needed), all other components are borrowed from existing papers, as can be seen in Table 1 already. The paper lists its contributions in Introduction on page 2, and each of those contributions has been studied in previous papers (though I will that admit no single paper does all these things together). Thus, I believe the paper falls short in terms of technical contributions. 2. Following on from point above, putting everything together and showing that it works, could also be a reasonable contribution, though it would warrant more extensive and systematic experiments for the different design choices, possibly in more realistic environments. For example, a) is the projective projection important, or could that have been learned, b) do repeated rotations indeed lead to blurred representations, c) what is critical to get such models work with RL, that past models that used imitation learning couldn*t, d) other claimed differences from past works in this space. 3. Experiments and analysis: a. The paper compares against NeuralMap, and reports improvements, but doesn*t give a reason as to why this happens. b. Past works have demonstrated these ideas in visually realistic environments (similar to those in Gibson / Habitat, see semantic tasks in CogMap). Current paper only investigates proposed ideas in VizDoom environments. Thus, while I like the direction of research and the fact that the paper presents an architectures that uses latest techniques in the area, I believe the paper doesn*t have enough technical contribution of its own, and experiments are limited to synthetic VizDoom environments.\n",
      "        - Review:###[Note: I wrote this review after John Schulman*s first comment, before any reply, and before Gehrard Neumann*s comment] The authors propose an actor-critic algorithm based mostly on regression. Being off-policy, the algorithm can learn from multiple policies. It can also be applied to continuous as well as to discrete actions, and it can be trained in a batch RL setting. They compare it to a set of state-of-the-art algorithms in standard openAI gym continuous action benchmarks and show competitive performance despite a much simpler implementation of the algorithm. I basically subscribe to John Schulman*s comment below, both about empirical results and about citing Self-Imitation Learning, but I have a stronger point against the paper, which is insufficient positionning with respect to the relevant literature. The paper does not cite or discuss the one below, though it looks VERY close: @inproceedings{neumann2009fitted, title={Fitted Q-iteration by advantage weighted regression}, author={Neumann, Gerhard and Peters, Jan R}, booktitle={Advances in neural information processing systems}, pages={1177--1184}, year={2009} } This paper also starts from RWR and performs weighted regression based on the advantage rather than on the return. So to me it is exactly the same idea, and it is mandatory that the authors clearly establish what is the novelty of their work with respect to this previous paper. Less importantly, the authors may also want to have a look at : @article{zimmer2019exploiting, title={Exploiting the sign of the advantage function to learn deterministic policies in continuous domains}, author={Zimmer, Matthieu and Weng, Paul}, journal={arXiv preprint arXiv:1906.04556}, year={2019} } which also uses ideas along the same line. To me, a good way to improve the novelty of this work would be to perform a detailed empirical study of the inner mechanisms of the algorithm on very simple benchmarks where the value function and policy could be visualized. In particular, how stable is the estimation of the value function? This is known to be an issue, as most algorithms avoid approximating it and prefer estimating the Q-function.\n",
      "    - Policy Learning and Decision Making in Reinforcement Learning (L0, 25)\n",
      "      * Examples:\n",
      "        - Review:###This paper unfortunately violates the blind-review policy: its acknowledgement exposes the authors. I thus support desk rejection.\n",
      "        - Review:###This paper presents visual imitation with reinforcement learning (VIRL), an algorithm for learning to imitate expert trajectories based solely on visual observations, and without access to the expert’s actions. The algorithm is similar in form to GAIL and its extensions, learning a reward function which captures the similarity between an observed behavior and the expert*s demonstrations, while simultaneously using reinforcement learning to find a policy maximizing this reward, such that the learned policy will replicate the demonstrated behavior as well as possible. A key feature of this method is that the learned reward function is defined by a learned distance metric, which evaluates the similarity between the agent*s current trajectory, and the nearest demonstrated expert trajectory. The network describing the distance metric is recurrent, such that the distance is defined between trajectories rather than individual states. The distance function network is trained via a negative sampling approach, where expert trajectories are randomly reordered to produce examples that dissimilar to the expert trajectories. The distance network also defines a variational autoencoder, and the reconstruction of the target trajectories is treated as an auxiliary task to help train better representations of the trajectory space. While previous work has considered the problem of visual imitation learning, the approach taken here is novel in its architecture and loss function, and significantly outperforms the baselines in terms of the similarity between the resulting behavior and the expert behavior. The clarity of the technical presentation could be improved, however. In particular, it would be helpful for the reader if the definitions of the negative sampling loss and the autoencoder losses were given before the combined loss, and if we saw the form of the loss for both positive and negative sequence pairs. Equation 4 could also be made explicit, with the full summation term included.\n",
      "        - Review:###**Summary of the paper: The paper proposes an IL method named support-guided adversarial IL (SAIL), which is based on generative adversarial IL (GAIL) (Ho and Ermon, 2016) and random expert distillation (RED) (Wang et al., 2019). The key idea of SAIL is to construct a reward function by multiplying reward functions learned by GAIL and RED. This multiplication yields two benefits; 1) it handles the issue of biased reward in GAIL, since state-action pairs outside the expert’s support are assigned low reward values. 2) SAIL’s reward is more reliable than RED’s reward for state-action pairs inside the expert’s support. The authors show that SAIL is at least as fast as than GAIL in terms of the sample complexity. Experiments on continuous control benchmarks show that SAIL is overall more stable than GAIL. **Rating: The paper proposes a simple but effective combination of existing methods. The proposed method is well motivated and performs well on benchmarks. Still, the paper has some issues regarding justification, clarity, and evaluation, which should be addressed (see below). I vote for weak acceptance. **Major comments/questions: - No guarantee of the optimality of the learned policy. Can it be guaranteed that SAIL learns the expert policy? (assuming the expert policy is realizable). Propositions 1 and 2 show the convergence of the support estimation, but these results are not related to the optimality of a policy learned with the reward function. This is an important point for justifying SAIL, since SAIL does not perform distribution matching to learn the expert policy, and it also does not perform IRL to learn the reward function. Therefore, SAIL lacks the optimality guarantee from both distribution matching and IRL perspectives. Please address and clarify this point. - Clarity in the theoretical analysis. In the theoretical analysis, the paper assumes a rate of GAIL for support estimation. This is quite confusing, since GAIL performs distribution matching and does not estimate the support. Also, given that r_gail = -log D(s,a), the reward’s upper-bound (R_gail) is infinity and the bound in Eq. (9) is not informative. - The reward r_red is constant at the optimal. Eq. (2) and Eq. (3) imply that, for state-action pairs from the expert’s state-action distribution, r_red is constant at the optimal. Specifically, the optimal solution of Eq. (2) is hat{\theta} = \theta, which yields to a constant value of r_red(s,a) in Eq. (3). In this scenario, SAIL is equivalent to GAIL for the expert state-action distribution. This means that Eq. (2) should not be optimized until optimal, and some early stopping criteria are required. Does this scenario (constant value of r_red) occur in the experiments? - IRL baseline methods. The paper should compare SAIL to methods which aim to handle the bias in reward function, e.g., DAC (Kostrikov et al. 2019). While DAC requires the time limit, this time limit is known in the benchmark tasks. Also, IRL methods such as AIRL (Fu et al., 2018) should be compared, since IRL methods are better than GAIL at handling bias in reward function (Kostrikov et al. 2019). **Minor comments/questions: - Typos: *offline RL algorithms* should be *off-policy RL algorithms*. Line 5 of Algorithm 1 should perform gradient ascent instead of gradient descent. An expectation over state-action distribution of expert is missing from Eq. (2). - What are the bold numbers in table 1 and 2 indicating? Why does the Hopper task have two bold numbers, but the other tasks have only one? --After author response-- I have read the author response and other reviews. I thank the authors for including additional experiments. However, the authors* arguments regarding optimality do not fully address my comments (see below). I will keep the vote of weak acceptance. The authors argue that *In this asymptotic case, SAIL is equivalent to performing distribution matching via GAIL with the additional *constraint* that candidate distributions need to have the same support of the expert distribution*. However, the support of the expert distribution may coincide with the entire state-action space, which makes the additional constraint uninformative in the asymptotic case. Specifically, the expert distribution coincides with the state-action space when the expert policy has an infinite support (e.g., the expert policy is Gaussian). Assuming the asymptotic case, the support estimation in RED will give an indicator function over an entire state-action space, and the support constraint in SAIL is always satisfied. In other words, SAIL is exactly equivalent to GAIL in this case. For these reasons, the authors* arguments regarding optimality do not fully address my comments. I think additional assumptions are required, e.g., the expert policy needs to have a finite support or be deterministic.\n",
      "  - Reinforcement and Sequential Decision Learning (L1, 66)\n",
      "    - Exploration Strategies in Deep Reinforcement Learning (L0, 21)\n",
      "      * Examples:\n",
      "        -  The paper proposes a method for learning options that transfer across multiple learning tasks. The method takes a number of demonstration trajectories as input and attempts to create a set of options that can recreate the trajectories with minimal terminations. I currently recommend rejection. The evaluation of the proposed method is rather weak and does not clearly demonstrate that the author’s goals have been achieved. The paper could also do a better job of situating the approach with regard to existing option learning approaches. Detailed comments: - The paper strongly emphasizes reusability of learnt options over multiple tasks as a key goal. This aspect is largely absent from the practical part of the paper, however. The proposed algorithm largely ignores the multi-task aspect beyond requiring demonstrations from different tasks - also see the next remark. In the experiments, the multi-task transfer is not emphasized. In the 4rooms domain options are learnt on training tasks and evaluated on test tasks, but the effect of task distribution or task diversity on generalisation is not investigated. Moreover, the larger scale ATARI experiments do not seem to include any multi-task aspects at all, with options immediately being learnt on the target task. - The objective in (1) omits any explicit mention of different tasks. It would be good to indicate explicitly how it depends on the distribution of tasks and what the expectation is taken over. - The authors indicate that their learning objective needs the transition function P for the MDP. This is never further discussed. Do the experiments assume known transition functions? If not, how are these functions estimated? If a model is known, does it still make sense to learn option policies from samples or would it be better to use planning based options (see e.g.[1])? - While the paper cites a number of option learning approaches, it could do a better job of situating the research within the literature. There are a number of option induction approaches that explicitly focus on reusability of options - see e.g. [1], [4]. There have also been a large number of approaches that focus on hierarchical learning to represent a distribution over demonstration trajectories: see eg. [3],[5], [6], [7]. Some of these approaches might also be better baselines than OptionCritic which doesn’t explicitly take into account learning from demonstrations or multi-task transfer. [1] Mann, T. A., Mannor, S., & Precup, D. (2015). Approximate value iteration with temporally extended actions. JAIR, 53, 375-438. [2] Konidaris, G., & Barto, A. G. (2007). Building Portable Options: Skill Transfer in Reinforcement Learning. In IJCAI, [3] Konidaris, G., Kuindersma, S., Grupen, R., & Barto, A. (2012). Robot learning from demonstration by constructing skill trees. IJRR, 31(3), 360-375. [4] Andreas, J., Klein, D., & Levine, S. (2017). Modular multitask reinforcement learning with policy sketches. ICML [5] Henderson, P., Chang, W. D., Bacon, P. L., Meger, D., Pineau, J., & Precup, D. (2018). Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement learning. AAAI. [6] Co-Reyes, J. D., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., & Levine, S. (2018). Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings.ICML [7] Daniel, C., Van Hoof, H., Peters, J., & Neumann, G. (2016). Probabilistic inference for determining options in reinforcement learning. Machine Learning, 104(2-3), 337-357. Minor comments: - The results on ATARI seem to have been ended before reaching final learning performance - I couldn’t find details for how the transition function in 4-rooms is changed - Does the optionCritic comparison include the deliberation cost? Since this paper aims to minimise option terminations that seems to be the most logical comparison. - Why don’t the ATARI results compare against other approaches? - The influence of the KL penalty isn’t really examined in results beyond looking at performance. How does it influence the trade-off between representing trajectories and diversity?\n",
      "        - Review:###In this paper, the authors aim to learn a locally interpretable model via the reinforcement learning approach, to address the fundamental challenge which is that the previous locally interpretable model has smaller representation capacity than black-box models, and causes under-fitting with conventional distillation techniques. Overall speaking, the paper is well organized, and the proposed approach is well tested, but in my opinion, there is a conceptual error. You claimed your method is REINFORCEMENT LEARNING based, but the REINFORCEMENT LEARNING definition for your task is weird, or wrong. In section 3, you didn*t give an explicit explanation for the state transition. With your given RL-like objective function, it seems that the state transition is from features to features. However, there is no specific correlated explanation in your paper on why you make such an assumption. Besides, the state transition in RL relies on decision making at each time step, while it has not reflected in your paper and code, namely, the state-transition independents on the decision making. To sum up, I don’t think the proposed method is RL-based, it would be more appropriate to define it as a MAB problem, and this paper should solve this problem before publishing.\n",
      "        - Review:###This paper proposes an off-policy reinforcement learning method in which the model parameters have been updated using the regression style loss function. Specifically, this method uses two regression update steps: one update value function and another one update policy using weighted regression. To compare the proposed method with others [main comprasion], 6 MuJoCo tasks are used for continuous control and LunarLander-v2 for discrete space. -- Even though this paper has done a good job in terms of running different experiments, the selection of some of the benchmarks seems arbitrary. For example, for discrete action space, this paper uses LunarLander which is rarely used in any papers so it makes very difficult to draw a conclusion based on these results. Common 49 Atari-2600 games should have been used for comparison. The same thing about experiments in section 5.3 is true too as those tasks are not that well-known. -- The proposed method doesn*t outperform previous off-policy methods on Mujoco task (Table 1). Since the main claim of this paper is a new off-policy method, outperforming the previous off-policy methods is a fair game. The current results are not convincing enough. -- There are significant overlaps between this paper, *Fitted Q-iteration by Advantage Weighted Regression*, *Model-Free Preference-Based Reinforcement Learning *, and *Reinforcement learning by reward-weighted regression for operational space control* which makes the contribution of this paper very incremental. -- The authors used only 5 seeds to run Mujoco experiments. Given the sensitivity of Mujoco for different starting points, the experiments should have been run at least with 10 different seeds. Questions: 1) Shouldn*t be an importance sampling ratio between pi and mu in the equations? starting from eq.5. 2) Does the algorithm optimize the respect to as well? (eq. 15) if yes, why it is not mentioned in algorithm 1? Plus, since is uniform dist. (at least this is assumed for implementation), eq. 14,15,and 16 (wherever there is d(s)), those can be simplified, e.g. hat{V} = sum(w_i V_i), wouldn*t be better just introduced simplified version rather than current ones? (referring to the only equation above section 3.3) 3) Is this the same code used to report results in this paper? if yes, I didn*t see any seed assignment in the code?! and what is *action_std* in the code? There are a couple of recent works in merging on-policy with off-policy updates which you might want to cite them.\n",
      "    - Multi-Agent Learning and Game-Theoretic Approaches (L0, 10)\n",
      "      * Examples:\n",
      "        - Review:###Summary: The paper proposes a method for improving the scalability of communication-based cooperative multi-agent reinforcement learning. While existing approaches assume a fixed underlying network topology over which agents communicate, in the proposed method, this network topology is dynamic (changes at each time step) and learnable (by assigning a weight to each node and *rewiring* nodes in a particular way based on these weights). Authors highlight the importance of having a topology that is roughly similar to a collection of star-topologies. The center of stars (central nodes) further form a complete graph. They argue that such a topology can achieve global cooperation while reducing the number of messages exchanged as compared to the case where all agents can communicate with each other. To learn a dynamically changing topology, the method assigns a weight (an integer between 0 and 4) to each agent based on its local observation. An existing method (CBRP) is then used to establish connections between agents based on weights assigned to them. A graph neural network (GNN) is used for computing the messages that are exchanged among agents. Communication uses the following 3 steps: (i) agents talk to the central agent(s) to which they are connected, (ii) central agents exchange information among themselves, and (iii) central agents transmit information to the agents that are connected to them. Each agent uses a deep Q-network - the parameters of this network are shared across agents. This Q-network receives rewards from the environment. Gradients flowing through Q-network are also used to update GNN. Since CBRP is non-differentiable, the parameters for network that computes weights for all the agents are also updated using a Q-network that gets the same reward from the environment as the first Q-network. Experiments done on MAgent environment demonstrate that: (i) communication is useful and (ii) method scales well as number of agents increases. Additional qualitative studies have also been performed to understand the content of messages and the learned strategies. Authors have also experimented with the StarCraftII environment. Comments: The paper deals with an interesting problem, however, the presentation can be significantly improved as there are multiple grammatical mistakes in the manuscript. Unfortunately, the work does not position very well with the existing literature. The motivation and the impact of the contributions are not very clear. I would rate contributions as marginal. It is not clear what POSs* terms in Algorithm 1 mean. It would be interesting to see which agents become central agents over time. As central agents form a complete graph, if there are many central agents then the approach will be inefficient. Under the message visualization heading on p9, it is not clear how one decides whether a message was a *move* message or an *attack* message. Questions to the Authors: 1. On p2, it is written that when concatenation or mean operation is used for aggregation, then inter-relationship between agents are not captured. What does this mean? Why does this problem not apply to GNN based solution which may also use mean for aggregation? 2. On p10, first line, it is written that all methods were made to react to the same initial state. How was this state chosen? A few questions are also embedded in the comments above.\n",
      "        - Review:###1. Summary The authors learn structured communication patterns between multiple RL agents. Their framework uses a Structured Communication Network Module and Communication-based Policy Module. These use a hierarchical decomposition of the multi-agent system and a graph neural network that operates over the resulting abstract agent (groups). The authors evaluate on two environments, where this approach outperforms other ways to communication protocols. 2. Decision (accept or reject) with one or two key reasons for this choice. Weak accept. 3. Supporting arguments Scalable communication with many agents will require a (learned) trade-off between structural priors and learning representations of communicaton (protocols). This work seems like an interesting step in analyzing how to implement this. 4. Additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. 5. Questions\n",
      "        - Review:###This is a very complete submission. There is a novel analysis, simulations, as well as some results on real data. The authors propose Lyapunov-based safe RL algorithms that can handle problems with large or infinite action spaces, and return safe policies both during training and at convergence. As far as I can tell the approach is novel, makes sense, and requires a lot of technical innovations. I was impressed with the method and the analysis behind the method. The incorporation of the Lyapunov idea from control theory makes a great deal of sense in this application. However, it is not trivial to get from using this tool to a working method.\n",
      "    - Multilingual Reinforcement Learning for Atari with Advanced Algorithms (L0, 9)\n",
      "      * Examples:\n",
      "        - Review:###This paper propose a modified U-net architecture to segment the stagnant zone during silo discharging process. It lacks novelty and the improvement is marginal. More importantly than all of that, this paper violates the double blind review rule and is same with [1]. So I think this paper is not suitable for acception. [1]Waktola S, Grudzien K, Babout L. Stagnant zone segmentation with U-net[C]//2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering (AIKE). IEEE, 2019: 277-280.\n",
      "        -  In this work, the authors improve the work (Raab & Schleif, 2018) by (1) reducing the computational complexity, (2) neglecting the sample size requirement, and (3) achieving a low-rank projection through Nystrom approximation. More specifically, the feature dimensionality is reduced by using only s biggest eigenvalues and eigenvectors, and the sample size is coordinated through Nystrom approximation. Class-wise sampling is used for the source, and uniform sampling is used for the target. Experimental studies on three datasets have been done. This paper should be rejected because (1) the paper lacks important latest references on domain adaptation, (2) the paper misuses the notations that makes the paper is not easy to follow, (3) the algorithm is not well justified either by theory or experiments, and (4) the presentation should be further polished. Here are some detailed comments: (1) A lot of recent deep domain adaptation methods are missing. These deep works achieve the state-of-the-art results on many transfer benchmark tasks. Without comparison with them (the paper does not mention any deep works), it is very unconvincing to conclude the paper makes new contributions to the transfer community. (2) In line 2 of page 5, the authors claim that BT assumes S_Z ~ S_X, which is not true. The key idea of BT is to construct new source data using target basis and source eigenvalues. Similarly, the authors make the same claim in section 4.1 for NBT, which is also not valid. (3) The notations in eq. (12) and (13) are very misleading. Eq. (12) follows the notations of the first line in page 5, but in the first line below eq. (12), why R_X in R^{d \times s}, S_X^2 in R^{s \times s}? I understand that X_s is the low dimensional X, then X_s = XR_s with R_s is the dimensionality reduction matrix whose size is d \times s (using biggest s eigenvectors is fine). With this, eq. (13) is incorrect as X should be L_X*S_X*R_X^T, but not L_s*S_s*R_s^T. Moreover, it is also unclear why X is decomposed into product of two matrices in this work, is there any benefit of doing so for transfer purpose? (4) In section 4.1, A_Z and A_X have exactly same form with X and Z, i.e., L_Z*S_Z*R_Z^T and L_X*S_X*R_X^T, please clarify. How eq. (14) X_s = \tide{L_X}*S_X come from? Is it the same as eq. (13)? (5) The title highlights low-rank, but it is not very clear how low-rank matters in the proposed method. I do not find contents stating the low-rank property of the proposed algorithm in the main technical sections. (6) Regarding section 4.2, what is the benefit of using class-wise sampling for the source? Have you tried to use uniform sampling for both the source and target domains? (7) The bound in eq.(16) is not very meaningful as s << n, m, d. Moreover, I am also not convinced by the claim this reduces the distribution differences, please give more theoretical justifications. (8) Regarding the experimental studies, why not use accuracy as many existing works do? The baselines are all subspace-based papers, and are out-of-the-date. The latest subspace papers, e.g., JGSA and MEDA, should be included. Moreover, deep methods are completely missing, which makes the empirical evaluation much less convincing. The improvements of NBT to BT are very marginal, 0.6. (9) Some typos and unclear points (please further polish the paper): (a) The last sentence of para 2, X and Z should be data, not features/ (b) Para 3, it is unclear what are the implicit alignment and explicit alignment of domain distributions. (c) The first sentence in section 2, it should be homogeneous. (d) Page 5 above eq. (15), it should be S_Z ~ S_X. (e) Section 4.2 the second line, it should be “in the data matrix” (f) The second last line of page 6, it should be inequality (16). (g) Some references miss page information.\n",
      "        - Review:###This paper proposes a representation learning algorithm for RL based on the Information Bottleneck (IB) principle. This formulation leads to the observed state X being mapped to a latent variable Z ~ P(Z | X), in such a way that the standard loss function in actor-critic RL methods is augmented with a term minimizing the mutual information between X and Z (which can be seen as a form of regularization). This results in a loss that is difficult to optimize directly in the general case: the authors thus propose to approximate it through a variational bound, using Stein variational gradient descent (SVGD) for optimization, which is based on sampling multiple Z_i’s for a given state X, so as to compute an approximate gradient for the parameters of the function mapping X to Z. Experiments show that when augmenting the A2C algorithm with this technique, (1) the mutual information I(X, Z) decreases more quickly (better « compression » of the information), and (2) better sample efficiency is observed on 5 Atari games (with also encouraging results with PPO on 3 Atari games). In spite of the interesting theoretical contributions, I have to recommend rejection as the current empirical evaluation of the proposed approach is extremely limited, making it difficult to assess its benefits over more straightforward algorithms. On the positive side, the authors derive a sensible approach to IB representation learning in RL, and provide solutions to the optimization challenges it leads to. I did not have time to check all the maths in the Appendix (I only went through the derivations in A.1 and A.2), but they seem to make sense overall (though it is unclear to me if the new algorithm proposed in A.5 is a practical one, so I am not taking it into account in this evaluation). The key negative point is definitely the weak empirical evaluation. The main results are from a limited sample of 5 Atari games, when the full Atari benchmark has 10x more games and is known to exhibit high variance among games when comparing RL algorithms (the additional results from the Appendix on 3 additional games also show situations where the proposed method does not seem to help much, confirming that larger scale experiments are needed for a proper evaluation). In addition it seems like each algorithm is run only once (instead of using multiple seeds) and only over ~200K timesteps, which is three orders of magnitude lower than results typically reported on Atari. Another issue is that there is no comparison to other representation learning techniques (like those mentioned in the related work section, or the recent *Unsupervised State Representation Learning in Atari*), nor to a natural and more straightforward variant of the proposed method where Z would simply be sampled from a (learned) Gaussian distribution Z ~ N(mu(X), var(X)), which at first sight seems like an easier-to-optimize objective (using the reparameterization trick)… I may be wrong, but then this should probably be explained in the paper (I realize that the proposed approach is more general, but then it should be shown how this extra flexibility can lead to improved results). Finally, the impact on runtime performance is not analyzed: how much slower do A2C / PPO become when optimizing the mutual information term with SVGD? Overall it is really unclear that better RL results can be obtained through this technique. Another important issue is that I found the paper rather difficult to follow, due to some inconsistent / unclear notations or equations. Here are the main ones I noted: • The discount factor is not accounted for in the derivation of the objectives in eq. 1-2 (I know this is often the case in practice, but the reason for dropping it should at least be mentioned) • The jump from V^pi(X) to V^pi(Z) at the end of Section 3 is explained too succintly. It suggests that V^pi(Z) must be constant (equal to V^pi(X)) over all values of Z that may be sampled from X, which as far as I can tell is not the case in the rest of the paper. It is also unclear whether pi depends on Z or X. And the notation J(Z) makes it look like J does not depend on X, but it seems like it does because even if pi depends only on Z, by its definition V^pi(Z) actually still depends on X (this also leads to weird equations like eq. 33 where X does not appear in the right-hand side). Overall this is rather confusing. • The « for every X » at the top of p. 4 does not make sense to me, due to the term I(X, Z) in eq. 3 where X is a random variable and thus does not take a specific value. • The paragraph below eq. 4 is a bit confusing. It looks like it amounts to saying that Y is a Gaussian around V(Z)? • Eq. 5 suggests that R depends on Z, but shouldn’t it depend on X? If it depended on Z, then wouldn’t it influence the optimization since by modifying P(Z|X) we can control the distribution on Z and thus the distribution on R? • In eq. 10 it is unclear whether L1 and L2 contain the expectation, also L2 is defined as a function of both theta and phi but seems to depend only on theta • Below eq. 15 it is said that « P is the distribution of Z » but P does not appear in eq. 15 • In eq. 16 should the phi on the left hand side be phi* as in eq. 15? • Below eq. 16 it is said « Notice that C has been omitted », but it is unclear whether it was not included to alleviate notations or because it disappears naturally in the mathematical derivation of eq. 16 • The motivation for introducing zeta below eq. 17 is unclear, especially since it seems to play an important role considering that zeta = 0.005 << 1 is used in the experiments (with no explanation as to how this specific value was selected) • hat{L}(Z_i, theta, phi) (between eq. 17 and 18) does not seem to be defined • What is the motivation for using the Gaussian U(Z) as described in Section 5? In particular I find it weird that it depends on X_i, while U(Z) is supposed to replace the marginal P(Z) and not the conditional P(Z | X) Minor points: • In the definition of Y_t = R_t = sum_i=0^n-2 … above eq. 4, I think the sum should be up to n-1 for an n-step return • Eq. 4 uses R_t on the right hand term instead of Y_t which looks weird • Theorem 1 states « Assume that for any epsilon > 0, … »: « for any » should probably be « there exists », since if the inequality was true for any epsilon, it would imply both mutual informations are equal (also the formulation of the theorem does not make it clear that the last inequality is the main result) • Footnote 1 p. 4: I(X, Y) should be I(X, Z) • In eq. 15 the phi below the argmax should be in bold • Please add a reference that the reader can refer to in order to understand where eq. 20 is coming from • « Apparently » is used in a couple of places but should probably be replaced with another word • When U is uniform, how do you choose its support? • After « median of pairwise distances between the particles », I think i=1 should be j=1 • It is unclear to me what « A2C with noise is »: it is said that the same phi(X, epsilon) is used « as A2C with our framework », but is it the same phi as A2C with uniform SVIB or A2C with Gaussian SVIB? And whichever it is, how does it differ from the one it is equal to? • « we add 21 to all four curves in order to make exponential moving average »: I do not understand that sentence • « we set the number of samples as 26 for the sake of computation efficiency »: I fail to see how going from 32 to 26 is going to make a major difference in computational efficiency Update: thank you for your response, but in the absence of a revised version addressing my concerns (as well as those from the other reviewers), I cannot increase my rating\n",
      "    - Reinforcement Learning and Policy Optimization Methods (L0, 12)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposes an approach for improving adversarial imitation learning, by combining it with support-estimation-based imitation learning. In particular, the paper explores a combination of GAIL (Ho and Ermon, 2016) and RED (Wang et. al., 2019), where the reward for the policy-gradient is a product of the rewards obtained from them separately. The motivation is that, while AIL methods are sample-efficient (in terms of expert data) and implicitly promote useful exploration, they could be unreliable outside the support of the expert policy. Therefore, augmenting them by constraining the imitator to the support of the expert policy (with a method such as RED) could result in an overall better imitation learning algorithm. While the motivation and intuition are clear to me, I have reservations about the claims made in the abstract and the experimental sections: 1. SAIL is an effective method for solving the reward bias in AIL. The reward in SAIL is “always” non-negative (product of 2 non-negative terms), making the method a very ad-hoc way of getting around the reward bias problem, especially when compared to other methods such as those which estimate the value function of the absorbing state (Kostrikov et. al. 2019). Consider a simple chain MDP with 3 states A, B and a terminal state T. The actions are left/right from each state. Let the expert trajectory be A->B->T. Also, for SAIL, consider perfect support estimation with an optimal RED-network. When at B, the agent can terminate with a right-action and collect some reward. But taking left and collecting 0 reward (due to perfect support estimation) makes it land in A, from where it can now achieve a positive reward for the A->B transition, and repeat the process. Hence, one could always create MDPs where the Q value of B->A is higher than B->T. The Lunar-Lander environment (with certain parameters) in Section 4.1 appears to present a scenario where SAIL get arounds the reward bias, but this doesn’t remove my doubts over the generalization of this approach. Also, in Table 1, why does GAIL not hover above the landing spot even in the default case? If the reward bias is strong there, with sufficient exploration, the agent should converge to the same policy as in the modified case. Figure 3 is concerning for the same reason as above. It shows the immediate reward at the goal state, and points that SAIL has large reward for no-op action. The issue is that RL optimizes for actions that have the maximum Q value, not the action with the maximum immediate reward. 2. I would recommend that the authors refer to the original GAIL algorithm as “GAIL” in the experiments section, and their practical stabilization trick as “GAIL-bounded” (or something to that effect). Referring to original algorithm as GAIL-log, and the modification as GAIL could be misleading to readers. 3. The authors claim that SAIL has better training stability, leading to more robust policies. If this is due to the algorithmic contribution of combining AIL and Support-Estimation-IL, then GAIL-log and SAIL-log in Table 2. should show this in the standard deviation numbers. This doesn’t appear to be the case. Also, Figure 4 (Half-Cheetah) has unusually large variance for SAIL-log. 4. Figure 4 and Table 2 numbers are very different. Take Humanoid for instance. From Figure 4, it seems that SAIL is way better than GAIL. But if you look at Table 2, they both achieve mean-score in excess of 10k. What’s the difference between Table 2. and final performance in Figure 4?\n",
      "        - Review:###The paper proposes an imitation learning algorithm that combines support estimation with adversarial training. The key idea is simple: multiply the reward from Random Expert Distillation (RED) with the reward from Generative Adversarial Imitation Learning (GAIL). The new reward combines the best of both methods. Like the GAIL reward, the new reward encourages exploration and can be estimated from a small number of demonstrations. Like the RED reward, the new reward avoids survival bias and is more stable than the adversarial reward. I have a concern regarding the Lunar Lander experiment. Were the demonstrations generated in the modified environment? If they were generated in the original environment (with early termination), this may have unintentionally created a state distribution mismatch between the demonstration environment and training environment that unfairly hurts the GAIL baseline*s performance. If the demonstrations were instead generated in the modified environment (without early termination) where the agent is actually trained, the demonstrations would contain many self-loop transitions at the goal state, and GAIL would likely not exhibit survival bias. I am also a bit concerned about the MuJoCo results. The stochasticity of the demonstrations and the evaluation trajectories may have a significant effect on the standard deviation of rewards. Was a stochastic policy or a deterministic policy used to generate the demonstrations? Were the evaluation trajectories generated by rolling out the stochastic imitation policy, or by rolling out a deterministic version of the imitation agent? Also, could the authors provide the mean and standard deviation of rewards in the demonstrations in Tables 1-2 and Figure 4? It would be nice to establish a rough upper bound on the performance of the imitation methods. Update: After reading the author response, I have increased my score from 3 to 6.\n",
      "        - Review:###The authors propose using a recent method for adversarial inverse reinforcement learning (AIRL) for the task for generating high-quality image captions. Leveraging the GAN framework, a discriminator is trained to distinguish real captions from those produced by the generator, while the generator is optimized with policy-gradients (REINFORCE) to maximize the pseudo-reward from the discriminator. The main difference from prior work seems to be that the discriminator acts on a word-level, rather than sentence-level (as done, for instance in Dai et. al. 2017). Correspondingly, the generator policy is updated with the objective of 1-step reward maximization (more like contextual bandits), rather than with a long-term sequential decision-making objective (as done in Dai et. al. 2017). The evaluation is done using 2 data splits – standard and robust, with various metrics such as SPICE, CIDEr, BLEU, CHAIR. Diversity analysis and ablations are also performed to dissect the performance of the proposed approach. My 2 main issues with the paper are confusing motivation (in section 1) and various imprecise parts (in section 3 and 4). 1. The authors argue that current GAN-based captioning models provide ill-defined rewards due to the “reward ambiguity problem”. This problem is not explained or motivated well in the paper, but instead the readers are referred to the AIRL paper. “Reward ambiguity” in inverse-RL arises because there could be many reward functions that yield the same optimal policy. The AIRL algorithm recovers one of these possible reward functions, and since such a recovered reward could be shaped by the environment dynamics, AIRL attempts to disentangle the reward from the dynamics. The motivation there is to use the recovered reward on a new system with different transition dynamics. In the context of this paper though, I would like to understand the angle of reward ambiguity. The authors disentangle the sentence reward into word-wise rewards; however, I’m not sure if there’s any relation between this and the disentanglement done in AIRL for solving reward ambiguity. 2. One of the objectives is learning compact rewards. It is claimed that addition of a constant term to the reward provided to the generator policy results in this, but what’s the intuition behind this? As for evaluation, it needs to be shown that words with similar semantics have similar discriminator score. How do we conclude this from Figure 2.? Also, please include Up-Down method results in Figure 2. 3. Section 3 questions a. How is state s_t defined? It is very hard to follow sections 3 and 4 without a clear definition and example for this. b. “Finn et. al. 2016 proved that IRL is mathematically equivalent …” --- this is imprecise. Maximum-Entropy-IRL is equivalent to the GAN formulation, not general IRL. c. p_theta(a,s) is referred to as “reward distribution”. I don’t think it’s a distribution. d. Equation 3. AIRL defines g only as a function of state (s_t) for the disentanglement, and not like what the authors have written. e. Equation 4. How is s_t sampled? 4. Section 4 questions a. 4.1 says discriminator “maximizes the divergence”. This doesn’t seem correct. b. f is referred to as state-value. This doesn’t seem correct. c. Shouldn’t the -1 term in Equation 8 disappear under expectation? d. Don’t understand how second line of Equation 11 is arrived at. There are quite a few other sources of mathematically imprecise writing that I noticed. I would recommend the authors to be more robust in their presentation.\n",
      "    - Reward Shaping and Intrinsic Motivation in Reinforcement Learning (L0, 14)\n",
      "      * Examples:\n",
      "        - Review:###The paper proposes a framework (Scoring-Aggregating-Planning (SAP)) for learning task-agnostic priors that allow generalization to new tasks without finetuning. The motivation for this is very clear - humans can perform much better than machines in zero-shot conditions because humans have learned priors about objects, semantics, physics, etc. This is achieved by learning a scoring function based on the final reward and a self-supervised learned dynamics model. Overall, the paper is very clear and easy to follow. The presented task is realistic and important, and the paper seems to address it in a reasonable approach. However, the evaluation seems lacking to me - the evaluation convinced me that SAP works, but I am not convinced that it works better than existing approaches (see below), and especially did not convince me that it is better in the zero-shot test environment. The (anonymized) website contains nice videos that support the submission. Questions for the authors: 1. Page 3, 3rd paragraph of Section 3: the paper says that *The proposed formulation requires much less information and thus more realistic and feasible* - I agree that this is more realistic, but is it really more feasible? The requirement of much less information makes the proposed formulation much more sparse. 2. A basic assumption in the SAP framework is that a local region score is a sum of all the sub-regions. As phrased in the paper: *in the physical world, there is usually some level of rotational or transnational invariance*. I*m not sure that this assumption makes sense neither in the Mario case or in other tasks, e.g., robotics. Doesn*t it matter if you have a *turtle* right in front of you (which means that the turtle is going to hit you), or below you (which means that you are going hit the turtle)? 3. A question about the planning phase - page 5 says: *We select the action sequence that gives us the best-aggregated score and execute the first action*. Do you select the entire sequence of actions in the new environment in advance? Can the agent observe the new state after every action, and decide on the next action based on the actual step that the action has reached, rather than on the state that was approximated in advance? In other words - what happens if the first action in the new test environment yields an unexpected state, that was not predicted well by the dynamics model; does the agent continue on the initial planned trajectory (that ignores the *surprise*), or does it compute its next action based on the unexpected state? 4. Experiments: in Gridworld and Mario - are there any stronger baselines in the literature, or reductions of known baselines to the zero-shot scenario? Are the chosen *Human Priors*, BC-random and BC-SAP just strawmen? Since the main goal of this paper is the zero-shot task, what would convince me is a state-of-the-art model that does possibly *better than SAP on the training level*, but *worse than SAP in generalizing to the new level*. Additionally, are there other baselines that specifically address the zero-shot task in the literature? Minor (did not impact score): Page 2, 1st paragraph: *... we show that how an intelligent agent*... Page 3, 3rd paragraph: *... in model-free RL problem* - missing an *a* or *problem*s**? Page 3, 3rd paragraph: *. Model based method ...* - missing an *a* as well? Page 4, 1st paragraph:: *... utilizing the to get the ...* Page 4, last row: missing a dot after the loss equation, before the word *In*. Page 7, Table 1: *BC-random* is called *BC-data* in the text. Aren*t they the same thing?\n",
      "        - Review:###I am not from this area and don*t know much about reinforcement learning. The paper discusses zero shot generalization (adaptation) into new environments. The authors propose an approach and then show results on Grid-World, Super Mario Bros, and 3D Robotics. In the training environment E1 = (S, A, p) the algorithm sees a bank of exploratory trajectories \tau_i = {(s_t, a_t)}_{t=1}^{T} but not rewards. The authors then say that algorithm is tested on the test environment E2. They * propose to only inform the new task per trajectory terminal evaluation r(? ) in E1* to give the training signal (where r is the reward). I am a bit confused by this setting. The model never sees any rewards for E1 but it does see rewards for E2? How is this zero shot? The authors then propose their approach, I wish some of it had been described more rigorously with math (e.g. the loss etc.) so it was easier to understand for people not in the domain and familiar with some of the terminology. Empirically the authors show results for 3 datasets and this seems thorough.\n",
      "        - Review:###This paper proposes a method of learning a hierarchical communication graph for improving collaborative multi-agent reinforcement learning, particularly with large numbers of agents. The method is compared to a suitable range of baseline approaches across two complex environments. The initial results presented seem promising, but further work is needed to ensure the results are reproducible and repeatable. To enable reproducibility, please include details of all hyperparameters used for all approaches in both domains. These should include justification of how the hyperparameters were tuned. Without understanding how these values were set I cannot support acceptance. To ensure the results are repeatable, repeated runs of training should be completed and the variation in performance quantified in the results. These repeats may have already been performed as Table 3 and Figure 5 discuss average results, but if not they must be completed before the work can be published due to the known issues with high variance in performance that commonly occur in deep RL. I would also argue against the justification of excluding ATOC from the StarCraft II experiments as its performance in MAgent with 25 agents is comparable to the other baseline methods that were tested. However, this is lower priority than the issues above provided there is no significant change in the relative performance of methods when the variance across multiple runs is documented in all existing experiments. Minor Comments: The following are suggestions for improvements if the paper is accepted or for future submissions. In Section 2, centralised critic methods are grouped as *communication-free* however I don*t think this is the best term to explain this approach as each agent has to communicate both its observations and actions to a centralised node (e.g. COMA) or all other agents (e.g. MADDPG). I also think this section should include coverage of other methods of utilizing graph neural networks in multi-agent reinforcement learning - e.g. *Deep Multi-Agent Reinforcement Learning with Relevance Graphs.* Malysheva et al. Deep RL Workshop @ NeurIPS 2018 and *Relational Forward Models for Multi-Agent Learning* Tacchetti et al. ICLR 2019. In Section 3, the acronyms CBRP and HCOMM are used on page 4 before they are introduced in full on page 5 for CBRP and never for HCOMM. HCOMM is also not used in Figure 4 or in the text description of the method. I believe it is the module described in Section 3.2 but this should be made clearer. Many claims in the paper are worded too strongly and should be revised. In Section 2, it is claimed that DQN *is one of the few RL algorithms applicable for large-scale MARL* - However, there are now many successful applications of deep RL to multi-agent systems (some of which are cited earlier in this same section) that use a variety of algorithms other than DQN. It is also claimed in this section that *DQN has excellent sample efficiency* despite the sample efficiency of deep RL being a known issue, open research question and a barrier to its widespread use in practice. In Section 3.2 the authors conclude *Overall, our LSC algorithm has advantage in the communication efficiency* despite in the same section noting two cases where ATOC has better efficiency (N_msg and N_b-r). I would suggest removing this sentence entirely as the paragraph above already contains a balanced account of the relative merits of each approach. On pages 8 and 9 the authors make references to guarantees and in the Appendix to proofs that are not supported by theory only empirical results. Without supporting theory these words should be avoided. The writing is also often informal to the detriment of presenting important information clearly. Notably, on page 3 *due to explosive growing number of agents* and on page 4 *The overall task of the MARL problem can be solved by properly objective function modeling.* The second of these, particularly the word *solved* is also related to the issue above of using the words guarantee and prove. Finally, the paper would benefit from a thorough grammar check. I note the following issues as simple changes that can be made to improve the readability of the paper: - In the abstract, the sentence *but also communication high-qualitative* does not parse. Perhaps this could be shortened to the brackets following? i.e. *is not only scalable but also learns efficiently.* - Page 2, *or employing the LSTM to* -> *or use the LSTM to* - Page 2, *still hinder the* -> *still hinders the* - Page 2, *the communication structure need be jointly* -> needs to be jointly - Page 6, *policy module motioned below* -> discussed below - Page 10, *the map size are is 1920x1200* -> the map size is 1920x1200 - Page 10, *We do not compare with ATOC because its poor performance in MAgent* -> because of its - Page 10, *we will to improve* -> we will improve - Page 10, *practical constrains* -> practical constraints\n",
      "  - Similarity Search and Random Projection Methods (L1, 34)\n",
      "    - Random Projection Methods for Similarity Search (L0, 10)\n",
      "      * Examples:\n",
      "        - Review:###The paper describes a method that aims to learn task-agnostic priors for zero-shot generalization. The main idea is to employ the following modeling approach on top of the model-based RL framework: a local convolution network is used to compute a score for each local state action pair, and then another network is used to aggregate all the scores. While the problem being studied is important and the experimental results seem positive, there are a few concerns. First, the baselines presented in the experiments are relatively weak. In Related Work, the authors discuss the differences between the proposed method and the related methods, but few of the related methods are used as baselines for comparison with the proposed method. Moreover, the experiments are quite insufficient in terms of ablating different components of the proposed methods. Second, essentially the proposed method is trying to solve the zero-shot generalization by parameter initialization; a model is pretrained on related tasks and used as initializations for target tasks. The authors claim that it is different from prior work mainly because of the neural architecture that deals with sparse rewards via score aggregation. While the proposed architecture might be more suitable for solving tasks with sparse rewards, it is not intuitive whether it has something to do with learning zero-shot generalization. And apparently, the method will also rely on the similarity between the pretrained task and the target task, and such a scope constraint is not discussed in the paper. In other words, I*m not quite sure a better architecture is fundamental progress towards zero-shot RL.\n",
      "        - Review:###This paper proposes a new method for measuring pairwise similarity between data points. The idea is to define the similarity between two data points to be the probability (over the randomness in constructing the trees) that they are close in an RP tree. More concretely, the proposed method constructs a collection of RP trees (albeit with some modifications), and takes the similarity to be the average over different RP trees of a strictly decreasing function of the distance between the leaf nodes containing the data points in each RP tree. The key modification to the RP tree is to limit the number of projection vectors used in an RP tree and re-use previous projection vectors. I believe the method is more or less equivalent to Euclidean distance, for the following reason. Two data points would have the highest similarity under the proposed similarity measure if they are in the same leaf node. For this to happen, both points must be on the same side of the dividing hyperplane corresponding to each of the ancestor nodes. Because the threshold along the projection vector is chosen randomly at uniform, this means that for the two data points to have high similarity consistently, the distance between them along the projection vector must be small (so that the probability of splitting them is small). Because the projection vectors themselves are chosen randomly on the unit sphere, this implies that this must hold along most projection vectors for similarity to be high consistently, which means that the Euclidean distance between the two points must be low. If true, this raises several questions: 1) Why does the proposed method work better than distance similarity (which I assume means Euclidean distance) in the experiments? Are there situations when the proposed method would yield a high similarity consistently whereas Euclidean distance wouldn’t? Are the results in Fig. 6 just for a single run of the proposed method? If so, many more runs need to be performed since the decisions of the RP tree should vary significantly depending on the projection vectors and thresholds. Both the mean and standard deviation should be reported. 2) In Sect. 1.2, the authors critiqued distance-based similarity because it often does not correspond to intuitive notions of similarity/perceptual similarity. However, it does not appear that the proposed method would correspond to perceptual similarity either. For example, consider a dataset where some coordinates are more perceptually important than others (this is the case for example for the wavelet coefficients of a natural image - the lower frequencies are typically more perceptually important than higher frequencies). A more perceptually meaningful distance than Euclidean distance would be a Mahalanobis distance (which can essentially weight different coordinates differently), but the random projections use standard inner products and so are unable to capture the appropriate weighting of the different coordinates. So, why would one expect the proposed method to be more perceptually meaningful? 3) In Sect. 1.2, the authors critiqued multi-partition-based similarity because it does depend on the data distribution and cited the elimination of “prior knowledge dependence” in Sect. 1.3 as one of the benefits of the proposed method. This appears to be at odds with the goal of devising a similarity measure that is perceptually aligned, because such a similarity measure must depend on the representation of the data (e.g.: if the data is represented in the wavelet domain, one needs to know which order the different dimensions are arranged, i.e. from lowest frequency to highest frequency or the other way around). Overall, it is unclear if the desiderata makes sense, and if the proposed method achieves the objectives. Other questions: 4) For the X-Projection tree (which re-uses projection vectors), it seems to be equivalent to a layer-by-layer RP tree with larger branching factor. If so, the presentation of the method should be changed to this, because a layer-by-layer RP tree with larger branching factor is both conceptually clearer and simpler to analyze. If not, the proposed method should be compared to a layer-by-layer RP tree with larger branching factor, to justify the increased conceptual complexity of the X-Projection tree. 5) For the experiments, comparisons should also be made to multi-partition-based similarity, like Multiple RP+EM and RF similarity. 6) In addition, the proposed method should be compared to two simpler baselines that computes the average and the minimum distance of the two points along multiple random projection vectors, in order to justify the increased conceptual complexity of RP trees. 7) In Sect. 1.3, the paper claims that “it is well known that in an RP Tree, data points that are closely distributed, indicating their high level of similarity in space, are always partitioned into the same subset”. This is not true, since hyperplane could divide a cluster down the middle for example. 8) One of the claimed contributions in the abstract that “we introduce randomness into partition to eliminate its reliance on prior knowledge”. Note that just by introducing randomness, prior knowledge is not necessarily eliminated. For example, the way in which random projection is performed (i.e. standard inner product vs. other inner products) assumes knowledge of the distance metric, which is induced from the inner product. 9) In Fig. 6, what is the distance metric used for the baseline? Minor issues: pg. 2: “project all data points into one random vector” -> “project all data points along one random vector” pg. 3: “leading to unsatisfied results“ -> “leading to unsatisfactory results” pg. 3: “nearest neighbours finding” -> “nearest neighbour search” pg. 3: “pattern discovering” -> “pattern discovery” pg. 4: “similarly data points” -> “similar data points” pg. 4: “01 matrix” -> “0-1 matrix”\n",
      "        - Review:###In the paper, the authors propose a novel optimization method for training deep learning models. The idea is from the LARS and AdamW. The authors then test the proposed method on multiple experiments, results showing that the proposed method works better than other compared methods. The following are my concerns: 1) No convergence guarantee in the paper. There are too many papers claiming faster convergence these days, proof of convergence guarantee is always preferred. 2) The proposed method is straightforward and easy to understand. It is just a combination of AdamW and LARS. I am worried about the novelty of this paper. 3) In the experiments, why the compared methods are usually different. For example, compared methods are Adam, SGD, and NovoGrad in table 4 and compared methods are Adam, AdamW, and NovoGrad in table 6. Why not compare all these methods? 4) When the batch size varies, is it required to tune beta_2 accordingly? I didn*t find it clearly mentioned in the paper, could authors explain how to set it? 5) I am confusing that NovoGrad method works much better than Adam or AdamW in Table 6 with no weight decay, more explanations are required. 6) It is unclear why NovoGrad is better than LARS. LARS normalizes learning rate through |w|_2/|g|_2. The authors should explain why normalizes using layerwise |g|_2 is better. Although the idea is straightforward, the proposed method may be helpful for the community. I will consider increasing the score if authors can address my concerns.\n",
      "    - Random Projection Trees for Similarity Search Methods (L0, 24)\n",
      "      * Examples:\n",
      "        - Review:###The paper proposes SSE-PT for sequential recommendation, which is an extension of previous work SASRec by adding user embedding with SSE regularization [Wu et al. 2019] . They further extend SSE-PT to SSE-PT++ to handle longer sequence. Experiments on five datasets show that the SSE-PT and SSE-PT++ outperform several baseline approaches. Detailed comments: 1) The technical contribution seems to be scattered: user embedding is introduced, effect of different types of regularization is studied and sampling based approach is added to address long sequence. It could be better if the author could make clear what the major contribution of this paper is. Also, SSE [Wu et al. 2019] is existing technique and simply applying it to sequential recommendation is a bit incremental. 2) In addition to SASRec, there are some other transformer based model (e.g., [1]) for sequential recommendation and the paper discuss how the proposed method differ from them. 3) In SSE-PT++, would sampling start index v based on the recency (e.g., with exponential decay) make more sense than uniform probability? 4? Overall, experiments look comprehensive: The baseline methods include both non-deep-learning methods and recent deep learning based methods for sequential recommendation; ablation study is conducted; case study is performed on MovieLens to show how the attention weights differ from SASRec; running time is compared against baselines and sensitivity analysis on hyper-parameters are also provided. To summarize, the paper is a bit incremental/scattered in terms of technical contribution but the execution of this paper looks solid. I would give a “weak accept” to this paper given the reasons listed above. [1] F. Sun et. al. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer\n",
      "        - Review:###This paper proposes a novel algorithm NeuralMCS for maximum common subgraph (MCS) identification. The proposed algorithm consists of two components. One is a neural-network model based on Graph Matching Networks (GMN, Li et al, 2019) to learn a node-to-node matching matrix from examples of the ground-truth MCS results. Another is the algorithm called GSE (Guided Subgraph Extraction) to obtain an MCS by making an explicit assignment from the estimated matching matrix by the NN model. Experimental comparisons are made to other NN-based approaches combined with threshold-based assignments by the Hungarian algorithm (w.r.t the accuracy) and to a state-of-the-art exact algorithm MCSplit, and show the effectiveness of NeuralMCS. This paper proposes an interesting method for MCS detection which would have large application interests. Though the basic idea is nice, the reported performance gains would be a bit less convincing due to the following evaluation problem and its weak novelty. The algorithm has two parts, and the first NN part to learn a matching matrix is mostly based on the already existing algorithm of GMN (Li et al, 2019). The novel part would be primarily in post-processing normalization (described in 3.1) for the matching matrix and seems to also be applicable to other NNs (for example, GAT?). The second part GSE to get an explicit subgraph also seem to be applied independently to the first part. We can see that combining these two parts worked, but it is unclear how each component contributes to the performance gain compared to any possible alternatives of each part. I understand that MCS detection from a matching matrix (and node state vectors) is not exact if we just use Hungarian-like linear assignment problem (LAP) solvers for a submatrix obtained by a simple thresholding, but both post-processing normalization and GSE parts (which brings the novelty) can be more carefully evaluated through some *ablation studies* using some simple alternative substitutes.\n",
      "        -  ** post rebuttal start ** After reading reviews and authors* response, I decided not to change my score. However, I feel that this paper is somewhat under-evaluated initially, so I hope the authors have an opportunity in another venue with their revision. Detailed comments: 1.1. I recommend to add an algorithm box describing the learning scheme. It is not end-to-end learning, so it is hard to catch (and potentially, replicate) the learning part. I am also a bit skeptical about the convergence (with non-zero sigma), as Reviewer 2 has a concern about it. 1.3. *We hypothesized that the value of the uncertainty is different depending on whether the inputs are OOD or in-distribution inputs. The results of the ablation study listed in Table1 demonstrate that this hypothesis is true.* 2. *In order to use the data uncertainty, we used the value of sigma.* -> Table 1 proves that your proposal (playing with sigma) is effective, but it does not mean that sigma is the uncertainty which is only essential component for detecting OOD. I recommend the authors to validate their hypothesis, maybe by conducting more experiments to show that the role of mu and sigma is as expected. At least, if mu is proven to have no effect on OOD detection by some experiment, then it can be a clue. Minor comment: I hope ICLR papers are cited as ICLR papers at least in ICLR submissions, not arXiv preprint.. Alemi*s paper is ICLR*17 paper, for example. ** post rebuttal end ** - Summary: This paper proposes to train an OOD detection model from a portion of modified latent vectors; more specifically, similar to VAE, they assume unimodal Gaussian distributed latent space at each layer and use the collection of standard deviation to train an OOD detector. Experimental results on several OOD benchmarks with different backbone networks show that their method outperforms ODIN (Liang et al., 2017). - Decision and supporting arguments: Weak reject. 1. Though the idea of extracting uncertainty is interesting, but I think the motivation and explanation is not enough, so I couldn*t find a rationale why we should do this. I have several questions that I couldn*t find an answer in the submission, could you answer them? 1.1. Are the classification loss and OOD detection loss optimized jointly? 1.2. Is it reasonable to assume unimodal Gaussian distribution over all latent spaces without a carefully designed learning objective? More specifically, to make it learnable, don*t you need a learning objective other than the conventional cross-entropy loss, e.g., *Bayes by backprop* proposed in the early work (Blundell, 2015)? 1.3. Why only the standard deviation values are useful for the OOD detection performance? If they are really useful, how the standard deviation values are related to the OOD detection performance? Blundell et al. Weight Uncertainty in Neural Networks. In ICML, 2015. 2. More ablation study is required to verify the effectiveness of their method. Again, I am not sure why mu and sigma should be split, and why mu should be discarded for the OOD detection part. 3. The architecture design of CNN in Figure 7 also looks arbitrary. 4. Comparison with more state-of-the-art methods is required. ODIN (Liang et al., 2017) is a powerful method but it is somewhat old and many recent works actually combine their method with ODIN for better performance. Why don*t you compare the proposed method with the Mahalanobis distance-based classifier (Lee et al., 2018)? They also estimate the uncertainty by measuring the Mahalanobis distance on the feature spaces & combine them for better OOD detection. - Comments: 1. I couldn*t find any statement about the classification accuracy, does the proposed model have a good classification performance as well? Since {a half of the model capacity is spent to split mu and sigma} and {it should take account of uncertainty in the forward pass}, I am not sure it maintains a good classification performance, compared to the standard classification model with the same capacity.\n",
      "  - Supervision, Labeling, and Weak Supervision Methods (L1, 26)\n",
      "    - Program Synthesis and Evaluation Using Human-Labeled Datasets (L0, 20)\n",
      "      * Examples:\n",
      "        -  This paper proposed an interesting idea on how we can leverage the lead bias in summarization datasets to pretrain abstractive news summarization models on large-scale unlabelled corpus in simple and effective way. For pre-training, they collected three years of online news articles data. Then, they take the top 3 sentences of the article as summary and the rest of the article as input document. For better choosing such article-summary pairs, they employ effective data cleaning and filtering process. Overall, they collected 21.4M articles for the pretraining. Overall, the pretrained model does decent on three summarization datasets without any fine-tuning. After fine-tuning the respective datasets, the gains seem significant. Especially on the XSum dataset, the improvements are remarkable. I believe that the idea is interesting but the experiments are incomplete and more investigation is required to make this paper stronger. Therefore I suggest to reject this paper. Arguments: 1) The important experiments that are missing in this paper are evaluating the proposed method on better human written summarization datasets -- DUC. The real world summarizations resemble more like the ones in the DUC dataset and it would be interesting to see if the transfer results of the pretrained model on the DUC datasets. The important question is to understand whether the pretrained model which took advantage of lead-bias could achieve good summaries on real summarization samples. This would also answer whether the pretrained just took advantage of the lead-bias issue of many large summarization datasets or does it really learn good summarization model. 2) This paper has good idea but mainly missing ablation studies. For example, how does the proposed model do compared with GPT-2 in the fine-tuning setting, and how do these two models perform on the DUC datasets. 3) During the dataset filtering/collection, a check on the quality of the filtering process by doing a small human study would have been a great addition. Also, instead of showing the output examples (which can go in the supplementary), human study comparing the quality of the pretrained model with fine-tuning and a baseline (can be from previous work) would have been better. Other minor questions 1) “we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest” -- is there any reason on fixing to these numbers? How did you make this decision ? 2) Even though the performance gains look visibly significant, I would suggest to report the statistical significance scores.\n",
      "        -  This paper introduces deep audio prior (DAP), which uses CNN*S prior to perform classical tasks in audio processing: source separation, denoising, texture synthesis, co-separation. This paper gets inspiration from deep image prior and adapts it to audio, by introducing a lot of insights in the audio domain, which I believe is a good amount of contribution. I have the following questions on the paper: (1) The source generation part is a bit confusing. When testing on real-world examples, do you need to generate sources? I want more explanations on the ablations studies in Section 4, temporal/dynamic sources. What are the input/output, how is the noise generated, how is the model trained. (2) In general, I expect more details in the paper, like the model architecture (does that affect performance), how training is performed (iteration, convergence, etc). (3) Some notations are missing, e.g. equation (4).\n",
      "        -  This paper presents a technique based on genetic programming to generate a suitable training corpus of programs and I/O examples using a trained discriminator network. Given a set of human useful programs and the corresponding I/O examples, the main idea of the approach is to use genetic programming to iteratively construct new corpus leveraged by a discriminator that aims to make the I/O examples of the corpus closer to the I/O examples of the human-useful set. This approach is evaluated on 40 human useful corpus of array programs and it is able to synthesize more programs than random corpus or genetic programming based baselines. Overall, this paper presents an interesting idea of automatically generating corpus for training neural program synthesis architectures, where most previous techniques sample synthetic programs uniformly from the space of DSL programs. (Shin et al. 2019) also point out a similar issue in neural program synthesis approaches, but this paper presents an automated technique to construct a better training corpus of synthetic programs and the corresponding I/O examples. I like the idea and the overall direction, but the current paper looks a bit preliminary both in evaluation as well as presentation. First, the description of the overall method is too high-level. It would be better to formalize exactly the network architectures for both synthesis as well as the discriminator networks. Having precise inputs to the networks as well as equations would help with the description. The description of the genetic programming framework to generate child corpus from parent corpus also seems a bit high-level. It wasn’t clear what are the inputs to the discriminator network (in Section 3.5). Does it only take a single I/O example as input or a pair of I/O examples? Does it also take the current corpus program as input? How is the discriminator network used to select programs for the child corpus? I was trying to better understand the context in which such a dataset generation might be useful. It seems for such a technique, one has to come up with a dataset of human useful corpus of tasks. Doesn’t it mean that one has to possibly identify all possible programs that a user might want to synthesize upfront? It would be interesting to evaluate how well the technique works when synthesizing for programs that are also interesting to users but not provided as a part of the human useful corpus. It was surprising to see genetic programming baseline solving more unique programs in the test set. Is it the case that with the discriminator based corpora, the approach is overfitting to one class of problems and not on others? Evaluating the technique on unseen programs not in the human useful set might help better evaluate this point as well. For the comparison in 4.2 with uniformly generated data, only 5000 programs were used for training, unlike typical approaches that train on millions of synthetic programs. Why not train the network on larger amount of programs? It was also interesting to see that the identity program was difficult to synthesize. Wouldn’t enumeration based approaches first start with that identity program? From the description it was also not clear how big the total search space for programs was in the language. Why wouldn’t enumeration based approaches work here? Also, it would be good to better understand why Sketch (Solar-Lezama 2008) based symbolic approaches won’t be able to synthesize these programs. It would also help to provide some examples of synthesized programs and their I/O examples, and maybe also the child corpus that lead to successfully synthesizing them. The paper mentions that DeepCoder based baseline isn’t applicable in this setting because of the generality of the language. Is the search space too large? Alternatively, can the presented technique be applied to DeepCoder to train on a generated corpus to improve its performance? Karel (Shin et al. 2019) might be another domain used in previous literature to show the usefulness of such an approach. On page 4, the paper mentions that the first 3 examples were provided with a specific form. Is there any intuition why such examples are useful and whether such constraints are used for all newly generated corpus as well? What happens when only random I/O examples are used? Minor: References are not formatted correctly in the paper. page 4: value between 8 and 8\n",
      "    - Weakly Supervised Object Counting and Recognition in Images (L0, 6)\n",
      "      * Examples:\n",
      "        -  This article proposes a method for object counting which can be trained with weak supervision. Object counting methods are often trained with point annotations, i.e., one click-point per object. In this article, a weaker way of annotation is used: count-based annotation, i.e., the number of objects of each class present in the image are given as annotation but no precise location of the objects. This article is an extension of density-based object counting methods for weakly supervision. This article analyzes the effect of pooling layers on WSL. The findings indicate that the pooling layer constrained the objects* locations to a predefined grid and accordingly they remove it. When removing this effect is alleviated. Moreover, the authors realized that a sift on the predictions also happen. Accordingly, they learn rotated versions of the model for centering the predictions. As a countereffect, several points per object are detected, and then a gini impurity regularization is proposed to reduce the number of detected objects. Then, the authors connect this formulation with VAE. The experiments are conducted on a toy example, i.e., circle finding, and on the well established Mall dataset. Main concerns: The experimental section should make experiments on standard datasets (i.e., USCD, Trancos, Mall, PkLot, Shangai, Penguins) using standard evaluation protocols (MAE, GAME). The current evaluation only shows the qualitative results on one frame of the Mall dataset. The results of the proposed method should be compared with baselines. For instance, Glance method, which is only trained with count-level annotation, and with *C-WSL: Count-guided Weakly Supervised Localization*. It should be also compared with WSL methods based on image-level labels such as *Object Counting and Instance Segmentation with Image-level Supervision* or *Where are the Masks: Instance Segmentation with Image-level Supervision* and then with methods that use point supervision as an upper bound such as density method or Where are the blobs. Moreover, some ablation studies about the effect of each component of the model would be needed. Regarding the model I have some concerns: - Removing the pooling layers does not seem a very good idea. This will remove part of the ability to detect objects at multiple scales and moreover will increase its computational power. Maybe for easy cases as circles and heads of mall people where the resolution changes are small could work. But not for more challenging datasets. - The rotation of the model could be ok for circles or heads. But making the model to learn rotation-invariant features could be challenging in more difficult datasets. - Eq.5 and 8 which are used in the model seem to use point supervision and not just the count based. Which makes the method not a weakly supervised one. Are these equations the ones used or others?\n",
      "        - Review:###The paper presents a way to solve a localization task for in images without providing any localization supervision. This allows tackling image reconstruction and classification problems that involve multiple object instances. At the core of the approach is a suggestion to resolve the non-differentiable top-k selection process: It is done by introducing axuilliary variables which allows the derivation of an alternating optimization procedure for their framework. I like the paper and I think the problem it*s tackeling is at the core for all many important image understanding tasks. When thinking about reasons that speak against the paper, I*m mostly concerned about the fact that the idea is not yet fully worked out: * The biggest unresolved aspect is the number *k* of instances needs to be provided. Assuming real-world relevant instance detection tasks, images in the test set never have *k* provided. * All patches need to be squared -- again, for real-world tasks this is not the case. * In the case of image reconstruction, what is the *reconstruction error* exactly? Assuming a scene with background clutter, how would this work here? It seems that the demonstrated examples have a black background so working additively is not a problem. The conclusion mentions *a first step*, so I understand that the authors may be aware of the shortcomings. As such, I*d like to have limitations (and potential resolutions) to be pointed out in the paper. With respect to related work, I*m missing *Attention-based Deep Multiple Instance Learning*, by Ilse et al. I*m not certain if it could be applied to the reconstruction task, but it seems that it should be a baseline for the classification task. When talking about weak supervision, the paper *Weakly supervised object recognition with convolu-tional neural networks* by Oquab should also be mentioned in the related work section. On a more detailed level, Figure 1 has no symbols for the newtork H_n and T_tau.\n",
      "        - Review:###This paper proposes a method for multi-instance object classification and reconstruction that does not require any location-based supervision. The main contribution is the introduction of a differentiable top-K region proposal that allows to train the whole model with only a supervision of the total number of instances (and their class) in the image. They test the performance of their method in simple visual tasks like cluttered MNIST, street digit recognition and finding the basis of procedural texture generation. The paper is well written and motivated. The proposed method is clear and well formalized. Their reported results seem substantially better than the baselines they compare against. The additional experiments in their Appendix Fig. 7, analyzing the evolution of the heatmap loss, is interesting, although I think the heatmap visualization could be improved to better understand what the model is learning across iterations (improve legend to include at what iteration the heatmap is taken, give on what image this is evaluated). Unfortunately, their tasks seem quite easy, and it is hard to assess the impact of their method when working with more real-world data-sets, where the number of instances of every class is more loosely defined (we could always describe more objects in a real image from the COCO dataset for example). It seems of great importance to evaluate the limitations of their method in this direction, as the source of supervision might be too weak in the cases where the generated dataset might not have all the combinations of number of instances per image (as the cluttered MNIST has given that it’s procedurally generated). The results on SVHN are a little bit confusing, and it’s unclear what the “Supervised” method is, specially knowing that there are available methods that do obtain much better performance on this task using all the supervision available. It should also be better explained why the performance drops so drastically when the IoU threshold is increased. Finally, the texture generation experiments are very hard to interpret and are even further from real-world tasks. Given my concerns on how this unsupervised approach can scale to real-life datasets, I suggest a weak reject, but I think the proposed method has some interest for the community and I strongly encourage the authors to provide further evidence of performance of their method on more complex vision tasks.\n",
      "- Neural Network Architecture, Training, and Efficiency (L2, 461)\n",
      "  - Distillation, Compression, and Model Simplification (L1, 25)\n",
      "    - Distillation Techniques for Transformer-Based Language Models (L0, 10)\n",
      "      * Examples:\n",
      "        - Review:###The manuscript proposes SSE-PT, a sequential recommendation model based on transformer and stochastic shared embedding (SST). Experiments on several datasets show that SSE-PT outperforms a number of baseline methods. Some analytical results are also provided. Overall, I think this work is not suitable for ICLR due to following reasons. The novelty of this work is limited. This work is based on SASREC [W Kang, ICDM2018] and uses transformer to encode user-item interactions in sequential manner. The difference is that this work adds user embedding in bottom layer and utilizes SSE for regularization as well as designs SSE-PT++ by sampling. To me, there is little extension or novelty. The experiment results are not convincing. Most of results are copied from [W Kang, ICDM2018] except HGN in Table 1. Table 1 shows SASREC is much better than HGN [C Ma, KDD2019]. However, I checked the results in HGN paper and found HGN is much better than SASREC. Even though datasets are different, most of them are from Amazon data. I was not convinced by this result due to the large difference. In addition, I did not understand why the authors change evaluation metrics in Table 3, i.e., from NDCG/Recall@10 to NDCG/Recall@5. I found SSE-PT without regularization and with different regularizations are much worse than the best result, which makes me concern about the effectiveness of personalized transformer. I did not see ablation study or discussion about this. Update: I have considered author rebuttal. I appreciate the extensive hyper-parameter sensitivity and ablation study in the paper, while these cannot be a key factor in evaluating paper as most of them can be done easily. I main concerns still lie in the novelty and experimental results. I still think this work is not suitable for ICLR and I keep my score.\n",
      "        - Review:###In this paper, the authors study an important recommendation problem, i.e., sequential recommendation, and design a novel and improved model called SSE-PT (Stochastic Shared Embedding - Personalized Transformer). Specifically, the authors mainly follow the previous works of the Transformer model and the stochastic shared embedding (SSE) regularization technique. For the part of the personalized transfer (PT), the authors introduce the user embedding for each user , i.e., , shown in Eq.(2) and illustrated in Figure 1. For the part of regularization, the authors find that the SSE technique works well in terms of avoid overfiting in context of other regularization techniques. Extensive empirical studies on five datasets show the effectiveness of the proposed approach compared with other related methods. Overall, the paper is very well presented, in particular of the introduction and discussion about the related works, and the analysis of the experimental results. My major concern is that the technical novelty is somehow limited in terms of the two closely related works of Transformer and stochastic shared embedding (SSE). I thus recommend weak acceptance. Some suggestion: Some important baseline methods may be included to make the results more convincing, e.g., Fossil, MARank, and/or BERT4Rec. Some minors: Typo: in the paragraph below Eq.(3): user -> user Typo: FPMF, PFMC in different places\n",
      "        - Review:###Summary: The paper proposes adding two mechanisms to the BERT architecture for NLU. The first is based on integrating information from all layers of the encoder via a method called Squeeze and Excitation. The second uses Gaussian blurring to encourage information sharing among neighboring words. The proposed method improves modestly on BERT on the GLUE suite of problems. It also substantially improves on BERT with respect to a class of examples that are designed to confound models that learn superficial heuristics based on word occurrence. I learn toward rejecting this paper. The method shows some performance gains over BERT on some GLUE tasks, but these are fairly small for the most part, and BERT outperforms the proposed method by a similar amount on a similar number of tasks. The strongest result is the HANS *lexical_overlap* case, where the proposed method has a clear advantage. I have no experience with these kinds of NLU models, so I can*t say with confidence whether the architectural additions proposed are well-motivated, but to me it feels like there is not a strong justification for adding these particular features to the BERT architecture, and the results do not clearly demonstrate their utility except in the *lexical_overlap* case. Details / Questions: * It seems to me that the GLUE results might be within the margin of error. Is it feasible to replicate training with different random seeds to see what the variance in the performance numbers might be? I suspect that a statistical analysis [1] might conclude that BERT and the proposed method are indistinguishable on the GLUE suite. * Were the proposed architectural additions conceived with the HANS *counterexamples* in mind (i.e. is there a specific reason to think that these types of methods would avoid the *superficial* reasoning that these examples are supposed to reveal)? Were other methods of adding context considered? * I suggest using the same x-axis scale on the two charts in Figure 3 to avoid confusion about the magnitudes of the differences. References: [1] Demšar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7(Jan), 1-30.\n",
      "    - Model Simplicity and OOD Detection via CNF Methods (L0, 15)\n",
      "      * Examples:\n",
      "        - Review:###This paper propose an active deep learning model. By leveraging subjective Logic, they propose to decompose the entropy of a predicted class distribution into vacuity (lack of evidence) and dissonance (conflict of strong evidence). Instead of using the predicted class distribution, they estimate the supporting evidence for each class. In the actual data sampling stage, they first sample from those high-vacuity dense region, to shape the true decision boundary, and then gradually sample from those high-dissonance region to fine-tune the decision boundary. They show better performance than the baselines on both synthetic and real datasets. First of all, for the readers who are not familiar with Subjective Logic or probabilistic logic in general, it is a bit hard to follow the reasoning behind the equations in Sec 3 and 4. Specifically, what is the intuition behind the dissonance of an opinion in Eq (6)? What is the advantage of using Subjective logic framework. In the experiments on MNIST, it would be best to visualize the image samples selected by the active learning model in the early and later stages, so that we can have a more intuitive understanding of vacuity vs. dissonance.\n",
      "        - Review:###This paper introduces a variational objective to train a model which can jointly select keyframes of a video and generate the intervening frames to produce a resultant video. The model is provided an initial set of frames as context. At training time the model always learns to produce N*J frames, where N is the number of keyframes and J is a fixed number of frames to generate for each keyframe. The authors compare their method for selecting informative keyframes on a number of baselines and show an improvement over these baselines. The problem is interesting and well-motivated, but I have some concerns with the proposed approach and experiments. As such, I am a weak reject. comments / questions: - Equation 3 lacks context. Initially, when looking at the authors* objective it seems that the inner expectation should be taken with respect to the joint time indices for the current and next keyframe. Only later after equation 4 do they mention that they always predict a fixed number of frames J. - The need for normalizing over the first T timesteps in equation 4 seems quite messy. Is it guaranteed that all of the needed keyframes will actually be within the first T timesteps? How does this work in practice? - Many important details of the inference procedure are relegated to the appendix. For example, there are no details for extracting which of the 60 keyframes that were trained for a sequence (due to the fixed length sequences) should be selected at test time. Looking at the appendix, it is clear that the approach requires an extensive planning algorithm at inference time, which seems like an important component. - The authors prominently highlight that their method is fully differentiable, yet they train in two stages while freezing weights. Why isn*t the model trained end-to-end? The stated reason for doing so is that this *simple* two-stage procedure improves optimization. What exactly happens if you don*t do this two stage training process? Does it fail to learn? Some experimental numbers would be nice to see. - The authors do not compare their method to any strong keyframe prediction baselines. Considering there is existing work in keyframe prediction, it seems important to highlight the difference between other competing models, rather than relying on simple baselines. Why don*t they use self-information/surprisal as a baseline i.e., by training an autoregressive model on the frames and then picking the N frames with the largest -log(p)? This is a metric that has been investigated frequently and has better interpretability than defining a new measure of surprise. Note that Kipf et al. (2019) uses this notion of surprisal as well. - Sauer et al. (BMVC 2019) should likely be cited as it does very similar keyframe analysis. Also, as the ICML 2019 conference had already concluded by the ICLR submission deadline, is it really fair to state the work with Kipf et al. (2019) was conducted in parallel? - Why does the model trained to learn a fixed number of timesteps for the intermediate frames? Did they investigate jointly predicting the indices for the current and next timesteps? It seems like it would greatly simplify their inference scheme if they did this. If they tried that approach and it failed, maybe that should be mentioned in the paper (with an explanation as to why it fails). - In the literature review, when discussing hierarchical temporal structure, the authors state: *However, these models rely on autoregressive techniques for text generation and are not applicable to structured data, such as videos.* Autoregressive techniques have been investigated in relation to videos; in fact, the authors later describe papers that have used autoregressive techniques for modeling videos.\n",
      "        - Review:###The paper proposed a word embedding model to incorporate the sentiment information. The paper provided both maximum likelihood estimation and maximum posterior estimation for the proposed framework. Improved experiment results on word similarity and low frequency embeddings are presented. Overall, the paper incorporates the sentiment information in a neat way. And my main concern is the around the Bayesian inference and the prior knowledge distilled into the model. Detail comments are as following, 1. The model employed Laplace approximation for posterior distribution. Not quite sure this is a good idea for the Bernoulli case since Laplace approximation is trying to use Gaussian distribution to approximate the region around the mode. How will the MAP solution compare with a full Bayesian solution such as VB or sampling-based methods? 2. Another concern is the prior introduced into the model. Normally prior information will be washed away as the training data grow. Not the case for the low frequency examples that the model performed well on. Would it possible that the improved performance on low frequency example is just a side effect of the biased introduced by the prior? How sensitive will the embedding perform with respect to the prior selected?\n",
      "  - Neural Network Architecture, Pruning, and Efficiency (L1, 64)\n",
      "    - Efficient Neural Network Pruning and Edge Search Methods (L0, 6)\n",
      "      * Examples:\n",
      "        -  The paper targets on learning the number of channels across all layers, under computation/model size/memory constraints. The method is simple and the results seems promising. However, the following issues need to be resolved: 1. The main method is based on published *slimmable networks,* such that the novelty is limited; 2. The method is very simpler to DropPath in [1], which uses DropPath to learn important branches while this paper uses it to learn channels. They are similar. 3. Better ablation studies are required in Table 1. This table should be simplified. As the method cannot learn architectures but channel numbers, the only useful pairs of comparisons are those having the same architecture, such as a pair of MobileNet vs AutoSlim-MobileNet. 4. an important detail is missing: where does the AutoSlim start from? Does it start from a larger model than the baseline? In the set of *500M FLOPs* experiments, I see the size of *AutoSlim-MobileNet v1* (4.6M) is larger than *MobileNet v1 1.0x* (4.2M), this implies that AutoSlim start from a *MobileNet v1 Nx* and N > 1.0. What is exactly N? 5. If AutoSlim starts from a larger baseline model with N times (N > 1.0) width, then the pruning baseline methods (AMC and ThiNet) should also start from the same larger models for fair comparison. In general, starting from a larger model and pruning it down can achieve a better accuracy vs. size trade-off. 6. *300 epochs with linearly decaying learning rate for mobile networks, 100 epochs with step learning rate schedule for ResNet-50 based models*, are baselines trained in the same way? Minor: 1. missing captions in a couple of figures, e.g., Figure 5. 2. *the importance of trained weights* vs *the importance of channel numbers* is trivial [1] Bender, Gabriel, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. *Understanding and simplifying one-shot architecture search.* In International Conference on Machine Learning, pp. 549-558. 2018.\n",
      "        - Review:###This paper proposes a simple and one-shot approach on neural architecture search for the number of channels to achieve better accuracy. Rather than training a lot of network samples, the proposed method trains a single slimmable network to approximate the network accuracy of different channel configurations. The experimental results show that the proposed method achieves better performance than the existing baseline methods. - It would be better to provide the search cost of the proposed method and the other baseline methods because that is the important metric for neural architecture search methods. As this paper points out that NAS methods are computationally expensive, it would be better to make the efficiency of the proposed method clear. - According to this paper, the notable difference between the proposed method and the existing pruning methods is that the pruning methods are grounded on the importance of trained weights, but the proposed method focuses more on the importance of channel numbers. It is unclear to me why such a difference is caused by the proposed method, that is, which part of the proposed method causes the difference? And how does the difference affect the final performance?\n",
      "        - Review:###This paper presents an offline and online pruning method for CNNs where RL is used for tuning the sparsity. Pruning methods are important for real-time applications of CNNs on low resourced devices, so the paper addresses a genuine need. However, the paper is poorly written and the experimental results are not as convincing. 1) Majority of the description of the models and architecture is written in text and is very difficult to parse, while this could*ve been avoided by usage of mathematical notations for operations. This is specifically evident for early parts of sec 3 which makes parsing and reading very difficult. 2) It*s not clear why the baseline performances in the experimental section are different across different methods. If I understand correctly, all the original non-pruned models should be the same for this experiment to make sense. This is specially important for Tab 2 where the closest competing algorithms (FBS and CGNN). Due to using different baselines, the numbers are not comparable. 3) One thing that is not clear in the text is the computation taken by the runtime pruner architecture. Are these rolled in the estimations on the speedup?\n",
      "    - Neural Architecture Search Methods and Weight Sharing Techniques (L0, 6)\n",
      "      * Examples:\n",
      "        -  The authors propose a search for neural architectures with different resource requirements by training a single model only. Furthermore, models found at the end of the search require no additional post-processing and are ready for deployment. A weight-sharing technique is applied to make this happen. The authors argue that there are multiple adaptions required to make it work. This includes the child model sampling strategy, use of model distillation, weight initialization, learning rate schedule, regularization and batch norm calibration. The work seems to be an incremental extension of Yu & Huang (2019b) and phrased as a NAS algorithm. Many techniques considered vital for the proposed method make use of techniques proposed by Yu & Huang (2019b) (sandwich rule, inplace distillation, batch norm calibration and the way how weights are shared). Other required techniques are either proposed by others (initialization) or very simple (learning rate schedule and regularization). The authors claim that they extend the work by Yu &Huang (2019b) *to handle a much larger space where the architectural dimensions, including kernel sizes, channel numbers, input resolutions, network depths are jointly search*. They never clarify why this is a non-trivial step and they might want to point this out in their rebuttal. Besides this, the authors did a very good job. The paper is well-written, references are given wherever needed and all the closest related work is covered sufficiently. The experimental part conducts several ablation studies which supports their various decisions. Unfortunately, all results reported use heavy data augmentation which makes a comparison to other methods (besides EfficientNet) impossible. 600M is considered the upper bound for mobile architectures by the NAS community. Unfortunately, no such model is considered making it even harder to compare to existing NAS methods. The EfficientNet numbers reported don*t match the ones reported in the original paper as far as I see. A red dot in Figure 3 could be added for the BigNASModel with 660M parameters.\n",
      "        - Review:###Parameter sharing (PS) is an important approach to speed up neural network search (NAS), which further allows the development of differential architecture search (e.g., DARTS) methods. However, PS also deteriorates the performance of learning models on the validation/testing set. This paper first changes the search space from a DAG (micro+marco) in e.g., DARTS to a stacked one based on MBConv; and then, propose to use several tricks to train the super-net well. Finally, a search method is constructed for the supernet to find the desired architectures. Overall, the paper is too experimental. The method is an ensemble of existing approaches, i.e., every single component in the paper has been visited in the literature. Expect for experimental results, I do not see many general lessons we can learn from the paper. Finally, why the proposed method can be better than others is not well-explained and clarified. Please see the questions below: Q1. Is NAS a method only for ImageNet? Can the method generalize to more applications/datasets? - While ImageNet is a good dataset for CV experiments, I think NAS should be a method for deriving architectures with certain requirements. - So, with so many tricks proposed in the paper, I wish authors can carry on experiments on other data sets as well, e.g., CIFAR and MNIST, which can still be preferred. Q2. On motivation, could authors explain more about the difficulties of combining all these techniques? - Each method is brought from some other paper, what motivate authors to combine them together? What makes them believe this is possible? Q3. On presentation, could authors draw a figure of the search space in the main text and give an overall algorithm for Section *3.2 COARSE-TO-FINE ARCHITECTURE SELECTION*. It is hard for a reader to see novelties there. Q4. *We also use the swish activation (Ramachandran et al., 2017) and fixed AutoAugment V0 policy* - are all other compared methods using swish activation and AutoAugment V0 policy? Q5. How about the search efficiency of the proposed method? Only the accuracy is reported in the paper. Q6. Could authors give STD (i.e., gray area to represent STD) in Figure 4, 5 and 7? Some curves are too close, I am not sure they are statistically different. Q7. How is the performance of the super-net? Q8. Could the authors add an ablation study on this point? - *The motivation is to improve all child models in our search space simultaneously, by pushing up both the performance lower bound (the smallest child model) and the performance upper bound (the biggest child model) across all child models.* - It is important to avoid fine-tune - From the paper, I am not sure whether the problem is solved by changing the space or the proposed training method (See Q1).\n",
      "        - Review:###Summary: This paper borrows the idea of word-to-vector from NLP and applies it in reinforcement learning based Neural Architecture Search (NAS). It suggests a pretrained encoder to transform the search space to a dense and continuous architecture-embedding space. First it trains the architecture-embedding encoder and decoder with self-supervision learning like Auto-Encoder. Then it performs reinforcement learning based Neural Architecture Search(NAS) in the architecture-embedding space. Strength: There is no architecture prior, such as cell, in the searching process. Thus it*s more general and can explore more architectures possibilities. Because it performs architecture search in a continuous space, a CNN based controller is used instead of a RNN controller. The result of the proposed method on CIFAR-10 is comparable with other popular NAS approaches. It reduces the number of searching architectures to <100 in <12 GPU hours without using tricks such as cell or parameter sharing. Weakness: The evaluations are highly insufficient. It only performs experiment on CIFAR-10, and the generalization ability on other datasets is unclear. In many NAS works. CIFAR-100 and ImageNet are commonly used to evaluate the performance. Besides, there is no comparison with more recent and related important methods such as DARTS and the method proposed by Luo et al. (2018). Actually its performance is not as good as Darts or the best performance reported in ENAS.\n",
      "    - Neural Network Architectures and Proposed Methods (L0, 47)\n",
      "      * Examples:\n",
      "        -  The paper presents the empirical observation that one can freeze (stop updating) a significant fraction of neural network parameters after only training for a short amount of time, without hurting final performance too much. The technical contribution made by this paper is an algorithm for determining which weights to freeze, called partial backpropagation, and an empirical validation of the algorithm on various models for image recognition. The observation that weights can be frozen is somewhat interesting, although similar findings have been reported before. It*s not clear the proposed algorithm is useful. The authors mention that fully parameterized models are expensive to run, but they don*t demonstrate any speed-ups using their approach. Such speed-up would also not be expected since the forward pass of the algorithm cannot get faster by freezing weights, and the impact on the backward pass is limited. I*d be willing to raise my rating if the authors can convince me of the usefulness of their algorithm.\n",
      "        - Review:###The authors show that parameters of a DNN do not satisfy the i.i.d. prior assumption and that neural layer activations considered as i.i.d. are not valid assumptions for all hidden layers of the network. One can therefore not rightfully use GPs to describe the network’s hidden layers. The authors suggest formulating the neurons per layer as energy functions thereby rendering a hidden layer as a Gibbs distribution and the connection between adjacent hidden layers as a PoE model. The paper is well written and well postulated. > Fig 4: What is the information presented by each neuron? How would this have looked with the i.i.d. prior in place. > There are places in the paper where one must refer to the supplementary, for example sections H and J with the simulations. Do consider moving these crucial sections to the main paper. > One recurring thought I had when the authors bring up Bayesian Hierarchical model, is that most of the BHMs rely on i.i.d assumptions both in the prior space and with the observations. How would you stand by your claim of explaining a DNN*s layers to be modelled as a BHM?\n",
      "        -  This paper proposed a new graph neural network to utilize the edge features. In particular, it proposes the Edge Information maximized Graph Neural Network (EIGNN) that maximizes the Mutual Information (MI) between edge features and message passing channels. The MI is reformulated as a differentiable objective via a variational approach. The experimental results have some improvement over existing methods. Overall, the idea is novel and well presented. Pros: 1. The idea of utilizing edge features looks novel. 2. The writing is clear. 3. Extensive experiments are done to verify the performance of the proposed method. Cons: 1. The theoretical analysis is just a regular routine. 2. How does the hyper-parameter affect the performance? In other words, how does the component of the edge features affect the model performance?\n",
      "    - Pruning Techniques and Sparsity in Neural Networks (L0, 5)\n",
      "      * Examples:\n",
      "        - Review:###This work introduces a Reinforcement Learning based framework that simultaneously learns both a static and dynamic pruning strategy. The combination allows the static pruner to decrease the required storage while the dynamic pruning can optimize the required compute using input-dependent pruned weights. The RL agent can dynamically learn the optimal sparsity distribution among the different layer of the networks while staying under a resource constraint as opposed to other methods which often enforce a layer level sparsity ratio. It demonstrates the efficacy of the algorithm on CIFAR10 and ILSVRC2012 and showed the effect of the tradeoff between static and dynamic pruning. Overall I believe this paper is a borderline accept. It proposes a unified framework to manage the trade-off between static pruning to decrease storage requirements and network flexibility for dynamic pruning to decrease runtime costs. The empirical results demonstrate the capability of the framework but would benefit from some clarification and additional ablations. Pros: Proposed an RL formulation of a unified framework for static and dynamic channel pruning. The empirical results demonstrate the ability of the model to achieve high accuracy while sparsifying the compute and balancing storage consumption and accuracy. Strong results are shown for CIFAR10 and ILSVRC2012. Demonstrated a tradeoff between static pruning and runtime-pruning through ablation of R_r. Cons: It is unclear how speed-up calculated. Is it wall clock and benchmarked on what device? What was the cost of running the RL agent during runtime? If you are computing MACs, that should be reported as such unless a strong correlation can be proven with the particular pruning scheme. MAC reduction does not translate directly to speedup in hardware. It could be clearer if the ablation of R_r also demonstrated a storage/accuracy tradeoff. It is unclear if similar results may be achieved by first running a static pruning and then separately training a dynamic pruning algorithm on the already statically pruned network? It might benefit from an ablation study with and without simultaneously training static pruning? Some of the algorithmic details could benefit from some clarification. In section 3.2, it is unclear to me the effect of R_r during training. It seems that the agent could learn to over select channels to prune to adapt to R_r. Section 3.4 seems to lack inclusion of R_r in the number of statically pruned values. The treatment of M_0 in section 3.2 separate from M_r and M_s seems to make the number of statically pruned filters during training depend on the layers chosen by dynamic pruning. It seems like it may cause differing sparsity between training and inference time.\n",
      "        - Review:###This paper proposes to learn static and dynamic channel pruning policies for convolutional neural networks. Static pruning depends only on the training dataset and is computed once before the model is deployed. Dynamic pruning is input-dependent. The policies are obtained with deep reinforcement learning on the training dataset using a combination of the loss function and storage/computation resource budgets as a reward signal. The key novelty in this paper is to combine static and dynamic pruning which can obtain the benefits from both worlds. Experimentally, the learned pruning policies are competitive with recent dynamic pruning approaches on CIFAR-10 and ILSVRC2012, in terms of both final test accuracy and number of parameters/inference time. Overall, I do like the idea of combining static and dynamic pruning, the DRL approach is reasonable and it seems to do well in practice. However, there are some key issues that must be addressed by the authors. In summary, these are: 1- Additional baseline methods: why not compare against a simple combination of the best published static pruning method, and one of the best published dynamic pruning methods (e.g. FBS or RNP)? I*d like to understand what the added value of jointly learning the static and dynamic pruning policies is. If a simple combination of existing static and dynamic methods works well, you will need to justify the need for the more complicated DRL approach you propose in this paper. 2- Writing: it has to be substantially improved; please see the sections Writing and Minor below. 3- Training cost: how stable is the training process described in Section 4.1? Does it much longer to train compared to pure dynamic pruning methods? Related work: - Discuss connections to routing networks which adaptively route layer outputs to the next layer*s modules: Rosenbaum, Clemens, Tim Klinger, and Matthew Riemer. *Routing networks: Adaptive selection of non-linear functions for multi-task learning.* arXiv preprint arXiv:1711.01239 (2017). Writing: - Sections 3.3 up to Section 5 need lots of rewriting; I suggest some changes in *Minor* below but please make a full pass as the paper is difficult to read at the moment. - Section 4.1 is extremely difficult to read and so I don*t really understand how you train the pruning policies. Please improve the writing and summarize the process in pseudocode or illustrate it. Also, I believe 4.1 should be a section of 3 rather than in experiments. It is extremely important to understand how the network+DRL agents are trained! Clarification: - Number of pruned channels, runtime vs static: ceil{d_r C} vs (C - ceil{d_s C}); why are these different in form? Seems like the static formula prunes ceil{(1-d_s) C}. Why is that? - Tables 1-2: what is the *Baseline acc.* and why is it different for each method? Isn*t this the accuracy of the same network before any pruning? Minor: - Title is too long: particularly the expression *DYNAMIC FLEXIBLE RUNTIME CHANNEL*. Perhaps you can think of shorter titles. - *We consider a standard form of reinforcement learning an agent* --> *We consider a standard form of reinforcement learning: an agent* - *are the the number of channels* --> *are the number of channels* - *output feature F_out u_r is predicted* --> *output feature F_out, u_r, is predicted* - *many existed dynamic* --> *many existing dynamic* - *Since C_in is various among layers* --> *Since C_in varies among layers* - *To avoid over-prune the filters and crashed in training, we set a minimum sparsity ratio +?, then the action space change to a_t^r ? (+?, 1].* --> *To avoid over-pruning the filters and crashing the training, we set a minimum sparsity ratio +?, such that the action space becomes a_t^r ? (+?, 1].* - *The reward function is proposed to consider both of network accuracy and computation budget.* --> *The reward function is considers both the network accuracy and computation budget.* - *and it may be various in scale* --> *and it may vary in scale* - *large at begin of training and small near converge* --> *large at the beginning of training and small near convergence* - *which filters can be to prune permanently* --> *which filters can be pruned permanently* - *on two popular dataset* --> *on two popular datasets* - *4.1 IMPLEMENT DETAILS* --> 4.1 IMPLEMENTATION DETAILS - *For CNN fintuning* --> *For CNN finetuning* - *Noted that for fair* --> *Note that for fair* - Tables 1-2: *Compare to state-of-the-art* --> *Comparison to state-of-the-art* - *Our methods has* --> *Our method has*\n",
      "        -  This paper proposes a masking process to improve the pruning of DNN. In addition, the algorithm proposes to automatically allocate the pruning rates over layers. On the positive side: - I do believe the main contribution is automatically allocating the pruning rates over the network. Related works: - How does this relate to methods using gating to prune in the presence of residual layers or BN? - Paper claims multiple times that related works need a train-prune-retrain process which is only valid for post-processing works. - I missed sparsity promoting works related to training from scratch where similar masks are implicitly used during training (even not explicitly stated) to maintain the zeros in the network. At least in [1,2] the training time is the same as the time used for training a network from scratch (same claim as in this paper). Method: - The mask is trained using a sigmoid function and claims this will be representative of the relevance of the *neuron* within the layer. How is this really related? - Why the initialization of the mask is to the mean? I think I am confused there. For initialization, I would argue all the neurons/parameters are relevant, right? - The claim that this method is *much simpler* is a bit subjective. I do not see why. Please elaborate. - I am not sure if the claim of pruning filters and not considering the bn layer is correct. I would tend to think that, if pruning a neuron, the corresponding BN module should be modified (that is, propagating the zero to subsequent layers). - I do like the extension to residual connections. It would be great to have more and clearer details on how is this done. Would this also apply to the UNET type of architecture? - What is the intuition behind Eq 4 and how it is related to the relevance of a parameter? - The extension to multiple metrics is of interest, however, there is little detailed there. How is the automatic allocation done? This is repeated in section 3.3 but details are missing. - My understanding is that the regularization multiplier is affected by the learning rate, therefore their effect is lower as the training progresses. In this case, seems the opposite, right? (page 6 before 3.4). - The part with the sparsity budget is interesting. What guarantees are that the newly enabled neurons are actually useful? Could it be possible that the budget suggested is not the right for the task at hand and, therefore, the additional parameters are not really relevant / needed? Experiments: - There is loss with little sparsity when it comes to Imagenet (15 and 17% pruning) does not seem very promising even the training time is similar. - Seems like the experiments and comparisons to L1-pruning are not very surprising. It would be nice to have more comprehensive numbers. For imagenet, if using Resnet-50, would be easier to compare to other numbers. - In the imagenet comparison, L1-pruning is the version-A of the paper. What about the others or why that particular model? - How are the actual groups made? Minor details: - Please improve figure 1. It is not easy to understand. Same with Figure 3. What is seen in Figure 4. - I do believe the WideResNet-28-10 number of parameters for BAR is not correct. - Section 4.2 is a bit overselling. I do not see *much-higher* parameter sparsity. The claims are mostly valid for VGG type of networks in this particular setting. [1] Learning the Number of Neurons in Deep Networks, NeurIps 2016 [2] Compression-aware training of DNN, NeurIps 2017\n",
      "  - Neural Network Training, Optimization, and Analysis Techniques (L1, 177)\n",
      "    - Deep Neural Network Training Methods and LBI Approaches (L0, 42)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposes a two-phase training method for learning with label noise. On the positive side, this paper focuses on the idea of prestopping and proposes several relevant definitions to formalize their idea and come up with a heuristic algorithm. However, I believe the paper has missed several very relevant papers that provides very similar ideas. Both [2] & [3] provide theoretical analysis to why early stopping matters in learning with label noise for DNNs. Before these two papers, [1] also observed that the learning trajectories for clean and noisy samples are different in label noise problem, and they used early stopping in their experiments to address this issue. Given these existing literatures, the contribution of this paper should be considered more properly. [1] Learning with Bad Training Data via Iterative Trimmed Loss Minimization, Yanyao Shen, Sujay Sanghavi, ICML 2019. [2] Hu, Wei, Zhiyuan Li, and Dingli Yu. *Understanding Generalization of Deep Neural Networks Trained with Noisy Labels.* arXiv preprint arXiv:1905.11368 (2019). [3] Li, Mingchen, Mahdi Soltanolkotabi, and Samet Oymak. *Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks.* arXiv preprint arXiv:1903.11680 (2019).\n",
      "        - Review:###This paper studies the importance of a neural networks weights and to which extend do they need to be updated. Particularly, the authors show that freezing weights which have small gradient in the very beginning of the training only results in a very slight drop in the final accuracy. This paper should be rejected because (1) the paper only provides some empirical results on freezing network network weights, I don*t think there are much insights and useful information; (2) To my knowledge, the phenomenon that only a few parameters are important has been observed before by many papers. Given that, I vote for a rejection.\n",
      "        - Review:###Main contribution of the paper - The paper argues that the base assumption, the i.i.d. of the activated elements (activations) in the hidden layers, the existing methods (lee.et.al 2018) hold is not convincible. - Instead, the author proposes a new way to probabilistically model the hidden layers, activations, and layer/layer connections. - Based on the probabilistic model, the paper proposes a new regularizer. Methods - The author argues that the activation is not iid by empirically showing that the trained MLP (in most cases) does not un-correlated. - The author proposes a new probabilistic model for MLP, and CNN assuming the Gibbs distribution to each activation and also assuming the product of expert (poE) model to explain the layer/layer relationship. - And according to their model, CNN will be explained by the MRF model. - The author proposes a regularization term regarding layer/layer connection. - They argue that the SGD training can be seen as a first-order approximation of the inference of the hidden activations in MLP. Questions - See the Concerns Strongpoints - The probabilistic explanation of the MLP and the CNN seems novel and was interesting to the reviewer - The proposed explanation assumes a weaker condition compared to the existing methods. Concerns - The main concern is that the reviewer cannot fully convince that i.i.d. assumption is wrong. Even though the trained MLP does not support the i.i.d. condition, one can suppose that the reason would be the typical training method (SGD), just finding the local minima in a deterministic way. Maybe the proof in Appendix.G. supports the argument of the author, but the reviewer failed to clearly agree with the argument. A clear explanation regarding the issue would be required. - As far as the author understands, the paper proposes a probabilistic (Bayesian) model for explaining MLP, but it seems that they just used SGD for training the model. In that case, the reviewer is little suspicious of the role of the proposed regularization in that the regularization comes from Bayesian formulation, but the model was trained in a deterministic way. The reviewer wants to ask the author that (1) is it possible to infer the model in a Bayesian manner such as sampling? (2) Is there any justification for using SGD when conducting the experiments regarding the regularization? If it is related to Appendix.G, clearer explanation would be appreciated. - As far as the reviewer understands, the regularization deals with the practical part of the paper. It would be better to see the effect of the regularization of widely used networks such as small-layered ResNet or others. If the proposed formulation has other practical strongpoints, it would be nice to clarify them. - The explanation using Gibbs distribution and PoE looks similar to RBM. The reviewer strongly wants a clear explanation of the difference and the strongpoints compared to RBM. Conclusion - The author proposed a new probabilistic explanation of the neural network, which seems novel and worth reporting. - However, the reviewer failed to fully agree on some steps in the process of the paper. Therefore, the reviewer temporary rates the paper as weak-reject, but this can be adjusted after seeing the answers of the author. Inquiries - See the concerns parts.\n",
      "    - Gradient-based Architecture Search Methods and Optimization Techniques (L0, 24)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposes an interesting heuristic of batch construction from samples. Instead of the usual random sampling, the authors to sample based on some measures of the ``uncertainty”. To be specific, the uncertainty is measured as a normalized entropy estimated from a window of historical predictions. I like the idea of designing more sophisticated ways to encourage more exploration over the samples that the model is not good at. The thought is similar as active learning. It is interesting to see how similar thought can be used to improve the performance of the algorithm in the general batch gradient descent setting. On the other hand I am not quite convinced the proposed way is truly better. The main concern is the experiments do not quite show the state-of-the-art result at all. It is not even close on MNIST, CIFAR-10 and CIFAR-100. Also those datasets are relatively small one. Can authors add results on larger datasets such as tiny image net? Besides this main concern I also have some worries about the design of the algorithm. I listed them below: 1. The vanilla stochastic gradient descent can be roughly justified since the expectation of the stochastic gradient is the true gradient of the loss. Now with the proposed heuristic will this still be true? 2. Is there any guarantee the algorithm can converge? It is not clear to me as the optimization proceeds the ``uncertainty” may oscillate. Is there any condition when the convergence is guaranteed? 3. As the number of classes grows the estimation of the entropy itself is a tough problem. Is there any way to mitigate this issue other than increase the window size? Another minor comment: Could the authors add more explanation on equation (4)? For example, is related to the maximum entropy led by a uniform distribution, and the summation term in (4) is related to the empirical entropy.\n",
      "        -  Summary: In ES the goal is to find a distribution pi_theta(x) such that the expected value of f(x) under this distribution is high. This can be optimized with REINFORCE or with more sophisticated methods based on the natural gradient. The functional form of pi_theta is almost always a Gaussian, but this isn*t sufficiently flexible (e.g. multi-modal) to provide a good optimization algorithm. In response, the authors advocate for using a flexible family of generative neural networks for pi_theta. Using NICE as a generative model is desirable because it maintains volumes. This means that we can adjust volumes in latent space and this directly corresponds to volumes in x space. Doing so is useful to be able to tune how concentrated the search distribution is and to explicitly reason about the mode of the search distribution. Overall, I found that there were a number of technical details that were well motivated, such as how to leverage the *mode preservation* of NICE, how to use importance sampling to be able to use samples from multiple rounds of optimization when updating theta and the fact that any existing ES algorithm can be used to do the optimization in the latent space. *We found that the PGES algorithm (naive stochastic gradient descent of (8) with the score-function estimator) applied to the NICE distribution suffers from the same limitations as when applied to the Gaussian; it is inable to precisely locate any local minimum.* I don*t understand this. Can*t the Gaussian become very concentrated? You write: *ES implicitly balance the need for exploration and exploitation of the optimization landscape. The exploitation phase consists in updating the search distribution, and exploration happens when samples are drawn from the search distribution’s tails.* This is a weak form of exploration, since there is no explicit mechanism that encourages f(x) to be evaluated at regions that it has never been evaluated on before. The search distribution*s tails will have low probability mass, so exploration unlikely. Your proposed method uses a pi(x) that is flexible enough to represent multi-modal distributions. However, how can you ensure that your search procedure actually uses this flexibility? In other words, how is your proposed method any better at exploration that the baseline ES method? It would be great to have a slightly more detailed alg. box in the main text for your proposed method, instead of having it in the appendix. Some details I found confusing, such as whether you perform one step of alternating optimization per call to f(x) or if you perform many steps of alternating optimization. Your *Mode preserving properties* trick is cool. However, I don*t fully understand how it is used. Surely you need to be able to be able to change the mode of the distribution some time? Do you only use the mode preservation trick at certain optimization steps? Again, incorporating this in the alg box would be helpful. In the results, I was disappointed that you required restart strategies. I thought that one of the key advantages of using NICE was that you could capture a multi-modal search distribution. Can you explain? What do you mean by the *global volume of the distribution?* What is the volume of a distribution? I understand what concept you*re trying to convey, but can you be more precise?\n",
      "        - Review:###This paper studies loss landscape of Non-negative matrix factorization (NMF) when the matrix is very large. It shows that with high probability, the landscape is quasi-convex under some conditions. This suggests that the optimization problem would become easier as the size of the matrix becomes very large. Implications on deep networks are also discussed. The NMF problem is known to be NP-hard. In case that the matrix X to factorize is large, the author(s) uses concentration property of random matrix to show that along any random positive matrix U,V and U’,V’, the MSE loss of NMF is convex with high probability. The extra assumption is that the rank of U and V should also be large enough. Section 3 is devoted to prove this. It seems to me there are some typos which are quite serious and make the equation (3) incorrect. However, the main result (Theorem 1) still seems to hold. The equation (3) should replace W_2 with 2 W_2. The reason is in the appendix D.2, the definition of W_2 has missed this constant 2, which is the hat X’’(lambda) at lambda=0. Therefore, all the constants in the equation (4) need to be modified accordingly. In D.2, to derive the equation (9), it seems to me the McLaurin series should give 2 l’’’’(0) l’’(0) >= (l’’’(0))^2, isn’t it? The whole proof is quite long to check. In Fact 1, is the mu the mean of z? In Lemma 1, what is lemma 9? Fact 10 has a constant 2 which seems to be forgotten. Therefore significant modification is needed to correct all the errors. Regarding experiments, some data-set in Table 1 does not seem to me relevant to the paper (Assumption 1), in particular those with r < 10. Figure 6 shows that the gradient flow is close to a straightly line, suggesting that the gradient descent algorithm follows a convex landscape. The Figure 6(b) seems to me have not converged yet, as at step 10,000, the cosine is not as flat as the others. This means that maybe the gradient flow does not converge to the local minima (U^*,V^*). Further explanation about this is needed in the paper. Regarding optimization efficiency, it is not that convincing since even in the over-parameterized regime: the landscape become more convex, but there can be a lot of local minima which are not as good as the global minima. Therefore from an optimization perspective, finding global minima still remain challenging. I think it would be better to mention this somewhere in the paper. Minor typo includes: equation (5), write ||W_2||_F^2. Equation (12), hat 1 should be 1\n",
      "    - Learning Algorithms and Quantization Techniques in Shot-based Models (L0, 29)\n",
      "      * Examples:\n",
      "        - Review:###Summary The paper proposes an architecture for few-shot video prediction in which a number of videos are summarized through global pooling operations and passed into a video predictor that learns to leverage them for adaptation, similar in spirit to the RNN-based meta-learning approaches such as Santoro’16, Duan’16. Due to global image and feature pooling operations, the proposed approach is computationally efficient. Proof-of-concept experiments are presented in a simple simulated physical prediction setting. It is claimed that the proposed model achieves generalization to longer sequences and larger board sizes, as well as a larger number of objects. Decision The work considers an interesting task and shows promise, but the paper lacks a substantial contribution at this time. I recommend weak reject. To improve the paper, I recommend considering harder prediction tasks and lifting some of the restrictions of the method. Pros The paper proposes and evaluates a model for few-shot video prediction, which is an interesting and well-motivated task. Cons 1. The method is only evaluated on a toy task. While a model is proposed that can successfully solve the presented toy task, certain assumptions, such as averaging directly in the image space, may prevent the findings of this paper to scale to more complex tasks, which demands further investigation. 2. It is claimed that the method generalizes to longer sequences and larger board sizes, however, the performance of the method seems to quickly deteriorate in both settings. Authors present anecdotal evidence that the method generalizes to larger number of objects, however, quantitatively the results again are much inferior to the method trained on a larger number of objects. While the presented qualitative generalization findings are interesting, they seem limited in scope, especially when taking into account the simplicity of the considered data. Minor comments 1. On page 3, the paper states “We are not the first to consider a similar learning problem, although most prior works do require some form of external supervision.” While there is a substantial body of literature that uses extra supervision, the following foundational works do not: Finn et al, 2016a,b; Fragkiadaki, 2016. In addition, “a similar learning problem” is considered without supervision in many papers on video prediction, including: Ranzato et al., 2014; Srivastava et al., 2015; Mathieu et al., 2015; Denton et al., 2017, 2018; Villegas et al., 2017, 2019; Wichers et al., 2018; Castrejon et al., 2019. While it might not be necessary to cite all of these as they do not necessarily apply their models to intuitive physics, the statement in the paper seems rather unsubstantiated. 2. A similar meta-learning prediction problem is addressed in Nagabandi’19a,b, which are not cited. 3. The paper is somewhat cluttered with notation, which overall hampers the flow for the reader. The paper would benefit from slight reorganization to improve the flow. This could be done e.g. be structuring the paper to first present the computational task that it addresses, namely few-shot video prediction, and how each component of the model works to address that problem, and only later go into the detail of how the data were collected and how each individual module was implemented Santoro et al, Meta-learning with memory-augmented neural networks. Duan et al, RL2: Fast reinforcement learning via slow reinforcement learning. Nagabandi et al, LEARNING TO ADAPT IN DYNAMIC, REAL-WORLD ENVIRONMENTS THROUGH META-REINFORCEMENT LEARNING, Nagabandi et al, DEEP ONLINE LEARNING VIA META-LEARNING: CONTINUAL ADAPTATION FOR MODEL-BASED RL --------------------- Update 11.19 ----------------------- I appreciate the newly provided qualitative generalization results (in response to R3). However, it is clear that the network has trouble maintaining the exact number of spheres in the scene, especially after collisions or interactions with obstacles. While this generalization finding is undoubtedly interesting, I am not convinced it is enough for a publication in ICLR. Furthermore, to support the authors* claim that the prediction performance on long sequences is satisfactory, I suggest comparison with a hand-crafted baseline, such as one suggested by R1, or one that makes use of explicit physics.\n",
      "        - Review:###I am not sure about my assessment of this paper. The authors propose their approach as a mixture of two other approaches. Then, they proceed with an illustrative 2-D example where they apply the same hyperparameters for all tested methods. This does not seem appropriate especially given that later in the paper they show that SGD with momentum and Adam use very different hyperparameter settings such as learning rate and weight decay. Basically, SGD and NovoGrad perform very similar in Figure 6 and the difference might be due to a too large learning rate for SGD (and other approaches). Increasing it further does not help as Figure 7 suggests. It is not the case for NovoGrad because of its gradient normalization. However, what the authors don*t show us is a different experimental setup with some trivial objective function where it would take NovoGrad an enormous amount of steps to converge to the optimum IF the algorithm is initialized far enough from the optimum. Since NovoGrad is based on normalized gradients, its steps are pretty much just signs of the original gradient multiplied by learning rate, especially in 2D. Thus, with some initial learning rate of 0.1 and when initialized in say (1, 10^10), it would take NovoGrad about 10^9 steps to converge independently on the scale of the objective function. This is not the case for SGD whose gradients are not normalized. This is actually a strong counterargument to the main claim of the paper that *NovoGrad is robust to the choice of learning rate and weight initialization*. This argument in the paper is based solely on the results of the toy problem where other methods used large learning rates. The paper includes a set of experiments containing different methods and their hyperparameters. When the authors use some cosine function to schedule learning rate in Table 1, they do it for AdamW and NovoGrad but they use polynomial schedule for SGD. Why? Weight decay and learning rate values for SGD and NovoGrad are very different in the same table, why? Do the authors explain why this difference may happen, they don*t. I believe that there is a better answer than just *this is the result of hyperparameter tuning*. The authors note that when beta=0, NovoGrad becomes layer-wise NGD with decoupled weight decay. They suggest beta to be in [0.2, 0.5] and I am wondering which beta were used in different experiment (the default suggested value is not given). The experimental results suggest that NovoGrad performs slightly better than SGD with momentum but it has more hyperparameters and we don*t know whether the results are due to different computation efforts in hyperparameter tuning. Figure 5 compares Adam and Novograd on WikiTex-103 and shows that Adam converges faster in terms training perplexity. However, Adam*s test perplexity is worse than the one of NovoGrad. Interestingly, NovoGrad*s test perplexity is better than its training perplexity especially in the beginning. It seems that NovoGrad is good here because it does not converge well, i.e., the results are likely due to regularization which is problematic in the original Adam.\n",
      "        - Review:###This paper aims to conduct few-shot learning on unlabeled data (instead of on training tasks with few-shot labeled data per task). The proposed algorithm is a trivial combination of existing clustering method and a few-shot learning method, i.e., the clustering provides pseudo labels, from which a series of few-shot training tasks are generated, and then traditional few-shot learning method can be applied afterward. Many existing techniques are integrated, e.g., k-reciprocal Jaccard distance, DBSCAN clustering, prototypical network, triplet loss, etc. The paper reported the experimental results on Omniglot and Market1501. The paper suffers from several drawbacks: 1) lack of novelty and originality; 2) problem setting is not convincing enough; 3) only a comparison on a very easy and small dataset (Omniglot) is shown, while the comparison on Market1501 is missing; 4) lack of comparison and discussion of other related works. Detailed comments: 1. A simple combination of two existing techniques (clustering followed by few-shot learning) without any in-depth analysis cannot be justified as a reasonable contribution for an ICLR paper. 2. The unsupervised few-shot learning setting does not make much sense in practice: we can usually collect many labeled data to generate few-shot learning tasks since these labeled data is not required to be drawn from the same distribution as the target few-shot task (e.g., the test tasks). For example, for few-shot image classification, we can always generate training tasks from ImageNet data. In practice, we only suffer from few-shot labeled data on test tasks. Assuming that none of labeled data is available can lead to unnecessary degrade on few-shot learning performance. The authors need to present more evidence or practical application scenarios to support the practical value of this problem setting. 3. Omniglot is too easy and too small for most of the recent few-shot learning methods. More challenging datasets such as ImageNet or at least its easier subsets (e.g., miniImageNet, tiredImageNet) should be considered. 4. No comparison to any baseline is shown on the other dataset Market1501. 5. More related works need to be compared and discussed: -Constructing learning tasks via clustering has been studies in “Hsu, K., Levine, S. and Finn, C., 2018. Unsupervised learning via meta-learning. arXiv preprint arXiv:1810.02334.” This paper shares the same idea when constructing the unsupervised episode training tasks. -Self-supervised techniques to boost the performance of few-shot learning has also been studies in “Su, J.C., Maji, S. and Hariharan, B., 2019. Boosting Supervision with Self-Supervision for Few-shot Learning. arXiv preprint arXiv:1906.07079.” and “Gidaris, S., Bursuc, A., Komodakis, N., Pérez, P. and Cord, M., 2019. Boosting Few-Shot Visual Learning with Self-Supervision. arXiv preprint arXiv:1906.05186.” This paper uses a self-supervised method called “pseudo labeling”, but other self-supervised methods should also be compared. ---------------------- Update after rebuttal: Thanks for the new experiments on miniImageNet! However, I still have concerns about the novelty of the proposed idea (clustering+few-shot learning) and its similarity to previous works. In addition, the new results on miniImageNet is not very encouraging (it is necessary to provide a fair comparison). Hence, I will keep my rating unchanged.\n",
      "    - Neural Network Training and Convergence Analysis (L0, 48)\n",
      "      * Examples:\n",
      "        - Review:###This paper studies overparameterized fully-connected neural networks trained with squared loss. The authors show that the resulting network can be decomposed as a sum of the solution of a certain interpolating kernel regression and a term that only depends on initialization. Based on this, the authors also derive a generalization bound of deep neural networks by transferring it to a kernel method. My major concern about this paper is the novelty and significance of its results: In terms of connection to NTK, It seems that the connection between neural networks trained with squared loss and the result of NTK-based kernel regression has already been well-studied by Arora, Sanjeev, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. *On exact computation with an infinitely wide neural net.* arXiv preprint arXiv:1904.11955 (2019). which is a missed citation. Without a clear explanation on the difference between the submission and this paper above, I don’t think this paper is ready for publication. In terms of generalization, it is also very difficult to judge whether this paper*s result is novel. In fact this paper misses almost all citations on generalization bounds for neural networks. Moreover, the generalization bound given in this paper does not seem to be very complete and significant, since the authors do not show when can L_{test}^{int} be small. To demonstrate the novelty and significance of the result, the authors should at least compare their generalization result with the following generalization bounds for over-parameterized neural networks in Section 4: Allen-Zhu, Zeyuan, Yuanzhi Li, and Yingyu Liang. *Learning and generalization in overparameterized neural networks, going beyond two layers.* arXiv preprint arXiv:1811.04918 (2018). Cao, Yuan, and Quanquan Gu. *A generalization theory of gradient descent for learning over-parameterized deep relu networks.* arXiv preprint arXiv:1902.01384 (2019). Arora, Sanjeev, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. *Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.* arXiv preprint arXiv:1901.08584 (2019). Cao, Yuan, and Quanquan Gu. *Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks.* arXiv preprint arXiv:1905.13210 (2019). Overall, I suggest that the authors should make a clear discussion on the relation of this paper to many existing works mentioned above. As long as the authors can give a convincing demonstration of the novelty and significance of their results, I will be happy to increase my score. A minor comment: how can the bound in Theorem 3 be derived based on Theorem 2? Should there be a constant factor in the bound?\n",
      "        - Review:###I acknowledge reading the rebuttal of the authors. Thank you for your clarifications and explanation. My point was this paper would make a good submission to ICLR if it was better motivated presented and explained to a wider audience. Unfortunately in its current form it can only reach a limited audience. #### Summary of the paper: The paper proposes accelerated information flows under Wasserstein or Fisher rao metric . i.e a method for solving minimization of probability functional , where the probability space is either endowed with the Wasserstein or the Fisher distance. Gradient descent methods in euclidian spaces can be accelerated using a type of momentum , this paper extends this to gradient flows, using similar formalism of Hamiltonian that appeals to the dynamic of a the particles and the velocity (or momentum, ). Writing down the lagrangian one obtains two co-evolving PDE one of the dynamic of the density and one for the potential . The PDEs are specified for both Fisher Rao, and Wasserstein distance. The hamiltonian for example for the Wasserstein distance . and PDEs amounts a continuity equation for the density evolving with drift , the evolution of the momentum is also given by a PDE. Proposition 2 of the paper gives the particles differential equation corresponding to the system of PDEs. For the energy being the KL divergence an explicit expression is given , this expression remains difficult in practice since it needs the knowledge of the density . Authors propose in the application section to use gaussian approximation , or using a kernel density estimators. The Bandwidth of the kernel is then choosen using a heuristic proposed in the paper. The paper then focuses on deriving expression for flows when the densities are centered gaussians, and this amounts to an ODE on the covariance , the ODE is discretized in Appendix D.2 to lead to computational method. Then convergence of the flow is analyzed for the wasserstein accelerated flows, under eta- convextiy* in the wasserstein sense of the functional. Some experiments of the particle based method are shown on synthetic experiments and in bayesian logistic regression. Review: Contribution/ Clarity: The main contribution of the paper is in deriving the accelerated gradient flow for the wasserstein distance this was also addressed in a recent paper [Taghevia and Mettha 2019]. The technical contribution is interesting but given that this field of flows in probability space is still not very well spread in the ML community, I wonder if ICLR is the best fit for this type of work. I support good theoretical work, but I think the authors could have done a better job in exposing the ideas how they extend form euclidean space, to manifolds, to probability spaces gradually. Simple derivations of euclidean space Hamiltonian will help the reader that is not exposed to such literature. I think the paper will benefit from a less technical writing in introducing the ideas coming from euclidean space and in conveying the intuitions. Comments: - In the proof of Proposition 2 you give the expression of evolution of by conservation of the momentum. Could you please elaborate more how you obtain this expression, and where you proved the conservation of momentum? - In term of damping if ones uses the Wasserstein Fisher Rao flows , one obtain also accelerartion , maybe you can comment on that ? since you analyze both flows , would be interesting to discuss the relation to Global convergence of neuron birth-death dynamics, that shows that an acceleration is obtained via WFR flows, since it will introduce a damping as well. - since MCMC and BM method lead to similar result what is the advantage of the wasserstein accelerated flow? one could also implement also an accelerated langevin dynamic\n",
      "        - Review:###Summary: This work provides a memory-efficient nonlinear bandit algorithm based on deep neural networks. More specifically, the algorithm in this work only uses part of history information to save the memory usage. To overcome the catastrophic forgetting problem, the authors provided novel covariance matrix approximation method. Experiment results also suggest that Pros: The writing of this paper is very well. It provides enough introduction of the background of nonlinear bandit problems. The experiment settings and results are convincible. Cons: - The core idea lacks solid theoretical supports. There is no regret bound result in this paper. The reason why I think the authors should add such theoretical proof is that it seems that the idea to construct new prior matrix instead of old one to avoid the catastrophic forgetting is not related to deep neural network at all. Thus, given existing regret analysis for Thompson sampling on linear bandit problems, the authors should also provide a simple analysis on linear case to show that the construction of prior matrix is indeed meaningful. - The experiment part does not show the accuracy the SDP solve needs. As the authors mentioned in Discussion part, below equation 6, it is very crucial to decide the accuracy the SDP solver needs. I suggest the authors add more details about the SDP solver in the experiment part. Minor comments: - The authors used DNN to minimize equation 3. Have the authors tried a regularized MSE instead of (3)? I think to add a regularizer can further improve the results. - At page 13, below equation 8: why the first equality lacks?\n",
      "    - Optimization Algorithms and Convergence Analysis in Stochastic Gradient Methods (L0, 34)\n",
      "      * Examples:\n",
      "        - Review:###The paper proposes a variance reduction based algorithm to solve compositional problems. The idea comes from the stochastically controlled stochastic gradient (SCSG) methods. The paper applies the idea from SCSG to estimating the inner function G(x) and the gradient \n",
      "abla f_k to solve compositional problems. The paper provides a theoretical analysis of the query complexity of the algorithm in both convex and non-convex setting. The experiments show the performance of the proposed algorithm is better than other recent methods. The paper seems to be the first attempt to extending stochastically controlled functions to the compositional problems. However, I vote for rejecting this submission for the following concerns. (1) Since SCSG is a member of the SVRG family of algorithms, the difference between this paper and [Xiangru Lian, Mengdi Wang, and Ji Liu, 2017] is not significant enough, especially in the algorithm design and the proof of the theoretical theorem. (2) The formulation of the compositional problems comes from reinforcement learning, risk-averse learning, nonlinear embedding, etc. However, the experiments are only performed on nonlinear-embedding problems. I think performing the experiments on different kinds of problems will be helpful to justify the significance.\n",
      "        - Review:###This paper proposes a new method for empirical composition problems to which the vanilla SGD is not applicable because it has a finite-sum structure inside non-linear loss functions. A proposed method (named SCCG) is a combination of stochastic compositional gradient descent (SCGD) and stochastically controlled stochastic gradient (SCSG). In a theoretical analysis part, a linear convergence rate and a sub-linear convergence rate are derived under the strong convex and non-convex settings, respectively. In experiments, the superior performance of the method to competitors is verified on both strongly convex and non-convex problems. Clarity: The paper is clear and well written. Quality: The work is of good quality and is technically sound. Significance: The problem treated in this paper is important and contains several applications as mentioned in the paper. Hence, developing an efficient method for this problem is important and interesting. Although derived convergence rates are better than existing primal methods, this paper lacks a comparison with the recently proposed primal-dual method by [A.Devraj & J.Chen (2019)]. [A.Devraj & J.Chen (2019)] Stochastic Variance Reduced Primal Dual Algorithms for Empirical Composition Optimization. NeurIPS, 2019. A convergence rate obtained in [A.Devraj & J.Chen (2019)] seems faster than that of SCCG for ill-conditioned strongly convex problems. However, there exists a certain setting (large-scale setting) where SCCG outperforms their method. Thus, the contribution of the paper is not lost, but it is better to compare SCCG with the method in [A.Devraj & J.Chen (2019)], empirically and theoretically. If the authors can show an empirical advantage over their method, it will make the paper stronger. ----- Update: I thank the authors for the response and hard work. I am convinced of the advantage of the proposed method. I would like to keep my score.\n",
      "        -  In the paper, the authors consider composition problems and use the stochastically controlled stochastic gradient method (SCSG) to approximate the gradient G(x) and \n",
      "abla f(x). The authors also provide convergence analysis of the proposed method for strongly convex problems and non-convex problems. Authors then conduct experiments on the mean-variance optimization in portfolio management task and the nonlinear embedding problem, results show that the proposed method is faster. The following are my concerns: 1) There are several important related works missing in the paper, e.g., [1][2]. 2) The convergence results of the proposed method in the paper are not state-of-the-art. For a strongly convex case, the result in the paper is O( n+ k^2 min(n, 1/u^2) log1/e), it is not necessarily better than O(n+k^3 log 1/e) in [1] or O(n+kn^{2/3} log 1/e). For a non-convex case, the result in the paper is O(min{1/e^{9/5}, n^{4/5} / e}), it is not necessarily better than O(n^{2/3}/e) in [1] or [2]. 3) More compared results should be conducted in the experiments, e.g. [1][2]. [1]Huo, Zhouyuan, et al. *Accelerated method for stochastic composition optimization with nonsmooth regularization.* Thirty-Second AAAI Conference on Artificial Intelligence. 2018. [2] Zhang, Junyu, and Lin Xiao. *A Composite Randomized Incremental Gradient Method.* International Conference on Machine Learning. 2019.\n",
      "  - Representation Learning and Feature Extraction (L1, 195)\n",
      "    - 3D Generative Models and VAE Methods (L0, 27)\n",
      "      * Examples:\n",
      "        - Review:###Review of “Improving Evolutionary Strategies with Generative Neural Networks” Typically in ES, the distribution of solution candidates come from a hand-engineered distribution (i.e. multivariate Gaussian, or other parametrized distributions). In place of hand-engineered distribution choices, they introduce the use of GANs as a tool for Evolution Strategies. I liked the novelty of combining the use of GANs in new directions (namely ES/GA). The core idea is to model the density function using Generative Neural Networks (GNN, MacKay 1995), and find the parameters of this GNN using tools from the normalizing flows literature for their NICE invertible properties (okay pun intended :) along with GAN-style training using historical data from the ES process. They demonstrate their method on traditional blackbox optimization toy tasks (such as Rosenbrok and Rastrigin functions), and also on a few continuous control RL benchmark tasks, to demonstrate improved performance over a strong representative ES algorithm (XNES). Overall, I liked the work as it provides a fresh way of using GANs with another subfield (ES/GA). If they want to improve the work, I would suggest demonstrating that their approach can solve certain difficult tasks that traditional ES methods (or RL methods) cannot solve. Although the experiments chosen are not difficult ones, I believe they were chosen for clarity to showcase the method, so I think that is fine (in case there are complaints that they experiments are too simple). (For the record, I was looking to give a score of 7, but the ICLR system made me choose between 6 and 8, and I chose 8.)\n",
      "        - Review:###This paper introduces a deep reasoning networks for de-mixing overlapping patterns with some logic constraints. There are two applications considered in the paper: de-mixing overlapping hand-written digits and inferring crystal structures of materials from X-ray diffraction data. The experiments indicate the proposed method work pretty well on these tasks. I like the general idea of this paper, since it has the flavor of combining deep learning with logic rules, although I feel weird to view the generative decoder as thinking fast and the reasoning modules as thinking slow. The notion of thinking fast and slow in the model does not well match the intuition given in the first paragraph of the introduction. The so-called reasoning module is essentially some contraints (i.e., regualrization losses) and a training data sampler. It is far away from the concept of (symbolic or logic) reasoning. There is not too much reasoning happening here. The way the paper relaxes the discrete logic constraints to continuous and differentiable objective that can be jointly optimized by SGD is interesting, which is similar to [Harnessing deep neural networks with logic rules, ACL 2016]. The carefully designed training data sampler that samples data according to a constraint graph also resembles GraphRNN, as the authors have mentioned in the paper. I feel the combination of these techniques is definitely interesting but also somehow incremental. I am not a big fan of some big claims in the paper. The reasoning modules are not what I expect. For the experiments, I think the authors do a good job presenting these experimental details and evaluations. These experiments are interesting and also show some advantages of the propose method. However, some baselines are also doing pretty well, indicating that the task is not difficult in general.\n",
      "        - Review:###This paper proposes a meta learning approach based on data valuation for reinforcement learning tasks. The core idea is to train a second network (the data value estimator) in conjunction to a regular predictor network. The predictor is then trained with samples chosen via the data value estimation. The authors motivate this construction with the goal to filter out unreliable and corrupted data. It*s well established that RL poses a difficult learning problem, and as such the goal to improve the RL process is definitely a good one. To the best of my knowledge the approach proposed here is new. The exposition of the paper is also quite clear, and all parts of the approach are explained nicely. In addition, the submission contains a thorough evaluation of the method. A central point for the method seems to be the validation data set which is used to train the data value estimator. The text emphasizes that this data set can be *small* several times, and the discussion and results of section 4.5 try to shed light here. However, Figure 5 indicates that a fairly large fraction of samples is needed to identify, e.g., more than 50% of the corrupted samples. Another cricital aspect for meta-learning approaches such as this one is also the training time. RL is already expensive, so if the meta learning introduces a large factor, the training could quickly become infeasible. Here, the text gives a factor of about 3, which is noticeable, but not overly big. This still seems practical. Potentially, the estimator could also be reused (at least partially) for repeated training runs. The tests in figure 2 are somewhat unituitive at first - I was expecting results of models trained on datasets with different samples being removed beforehand, rather than removing samples based on a trained estimator. However, this test makes sense on second sight, and e.g., the significant drop in performance after removing the most important samples indicates that the estimator was able to correctly identify a certain portion of data that is actually important for successful predictions. In addition to the noisy label and domain adaptation tests, this paints a positive picture. The method seems to yield useful (be it somewhat small) improvements in terms of learning performance. One aspect that could be justified better in my opinion is the choise for a discrete representation for the data value estimator. Shouldn*t the method likewise work with a continuous representation here? The paper explain how the discrete model is trained with quite some detail, but the choice itself could be motivated more clearly. However, overall I think the method is interesting and yields nice performance improvements. It is described and evaluated in detail, so I think the paper could me included in the ICLR program.\n",
      "    - Amharic Word Embeddings and Sentiment Lexicon Development (L0, 21)\n",
      "      * Examples:\n",
      "        -  This paper is quite difficult to read. The figures are pixelated. There is almost no organization to the text of the work. Descriptions are imprecise and lax: for example, *We apply basic preprocessing on Amharic News Comments. These include normalization of Amharic script symbols, tokenization, stop word removal, punctuation mark removal and so on. Amharic writing system is expressed using only consonants. To handle the features of the language is very challenging. We require conversion of Amharic scripts to consonant-vowel form. Particularly, before performing negation handling and stemming, the algorithm converts each Amharic word to its consonant vowel form.* What kind of normalization? What kind of tokenization? Did you use outside tools? If so, cite them or refer to the code. If not, then please explain your method more thoroughly. Don*t assume all your readers know what consonant-vowel form is. *and so on* is not appropriate. You should explain what the so on is in a paper submitted to a conference. Don*t bother telling us the features of the language are challenging. Just tell us how you did it. Switching between words like *cue* and *clue* Even the font of the text changes part way through the paper. Figure 2 is almost entirely uninformative. The algorithms are poorly displayed and nearly unreadable. Experiments are too limited.\n",
      "        - Review:###This paper suggests generating a large news summarization dataset by taking advantage of the fact that in news articles it is often the case that first few sentences contain the most important information. I have the following criticisms of this paper: - the idea is not novel. The XSUM dataset cited had used this to create a large dataset based on BBC articles as the editorial guidelines are such that the first sentence is a summary of the article. The lead1 baseline doesn*t make sense, as it is the actual reference of the dataset. As implemented, it actually picks the second sentence of the original article, and unsurprisingly works worse than the lead-X for the other two datasets. - the filtering based on word overlap between the initial sentences and the rest of the document means that the training dataset will encourage models copying words; good summaries don*t have high word overlap necessarily. - no human evaluation is not conducted; ROUGE indicates small differences, but it can*t be trusted without confirmation by human evaluation - I don*t agree that using positional information is bad for the models. The point is that we need to do better than that, but we should still take it into account\n",
      "        - Review:###This paper proposes a novel BERT based neural architecture, SESAME-BERT, which consists of “Squeeze and Excitation” method and Gaussian blurring. “Squeeze and Excitation” method extracts features from BERT by calculating a weighted sum of layers in BERT to feed the feature vectors to a downstream classifier. To capture the local context of a word, they apply Gaussian blurring on output layers of the self-attention layer in BERT. The authors show their model’s performance on GLUE and HANS dataset. Strengths *This paper claims the importance of the local context of a word and shows an effect of their method on the various datasets: GLUE, and HANS. Weaknesses * It seems like the self-attention layer can learn the local context information. Finding important words and predicts contextual vector representation of a word is what self-attention does. So, if using local-context information, which is information in important near words, is an important feature for some downstream tasks, then the self-attention layer can learn such important near words by training the key, query, and value weight parameters to connect the near important words. It would be nice if the authors provide some evidence that self-attention can*t learn such a local-context feature. *In table 1, their experimental results show a slight improvement by using their method, but it*s not significant. * On HANS dataset, they show using local-context can prevent models from easily adopting heuristics. How Gaussian blurring can prevent that problem? More explanation about the relation between local-context and adopting heuristics is required.\n",
      "    - Audio Feature Analysis and Spectrogram Methods (L0, 10)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposes an analysis of convolutional neural networks (CNNs) features the basis for making perceptual quality comparisons. The analysis is based on the proposed Perceptual Efficacy (PE) Score that measures spatial frequency and orientation selectivity of CNN features. The hypothesis put forward by the authors is that a CNN features with high PE score can be used to formulate a perceptual loss (Eq. 1) that correlates well with human image quality judgement. The authors use a dataset of human image quality judgements to assess their hypothesis. One issue I see with the hypothesis as stated is that in the definition of frequency selective features, the authors make use of the Contrast Sensitivity Function (CSF) which quantifies the dependency of human perceptual characteristics on frequency. So in the definition of the PE score we have embedded knowledge of human perceptual sensitivity. Is it therefore surprising that we see correlation between the high PE features and human judgements of quality? Experimental results: The scatterplot presented in Figure 4 does not say to me what the authors claim it should. I do not see a significant difference between the low-PE features and the high-PE features in terms of their correlation with human image quality judgement (as measures in this case by the DMOS). I also find the large table of number in Table 1 to be rather impenetrable. I would recommend an alternative method of presentation to make the desired point. Clarity: There are many undefined terms and acronyms (eg. SISR, HVS are not defined, while DMOS and SROCC are not described). Also, the description of visual masking in Sec. 4.3 was confusing and difficult to follow. Otherwise the writing was reasonably clear. Impact and significance: Overall, the findings of the paper are not terribly surprising and as discuss above, given the use of the CSF (quantifying human perceptual characteristics) in the definition of the CNN Perceptual Efficacy (PE) score, it would seem rather surprising that a correlations would not be found. As a result of this as well as the rather narrow nature of the study involved, I am inclined to think that the impact potential of this paper would be rather low.\n",
      "        - Review:###In the article the authors propose to measure quality of CNN-features by quantifying the orientation tuning and spatial frequency sensitivity of the features. The underlying hypothesis is that properties of features in the human visual cortex are also indicators for quality in CNNs. The authors devise an experiment similar to experiments performed on mammals to check which features are active under which types of basic patterns. Afterwards, a loss-function is devised that uses proportions of the best or worst features according to the metrics and it is shown that features that have high values on the metrics also lead to good performance. ----------------------------------------------------------------------------------------- I have a problem understanding some of the metrics used. In the introduction, the following claim is made: *The first attribute is sensitivity to spatial frequencies at which there is minimal contrast masking in human visual perception*. To my understanding, the metric to measure this is (2) in 4.2. a_m^k is to my understanding the average response of the feature when given an image with orientation-frequency f. therefore, the derivative should be *the change of activation under change of frequency*. I feel unable to connect this with the initial hypothesis, as it does not mention change of frequencies. i would have expected the correlation between CSF(f) and a^k_m(f), did you have a reason why you did not chose correlation? Similarly, for mu_2 in (3) you are using the maximum value. Is there a reason not to use the mean? in that case mu_2 would be the variance, which would be a natural measure for orientation selectivity. ------------------ Is there a way to make Table 1 more pleasing for the human eye wrt the discussion of the results?\n",
      "        -  The authors detail a set of priors for unsupervised decomposition of individual spectrograms into their component parts. The introduce reasonable constraints on temporal coherence (consistency and dynamic shifts) and mask activations (at least one component always activated). They also regularize sources to not overlap spectrotemporally. Decomposition is performed by training the weights of a U-Net on a single spectrogram as in deep image priors. The authors demonstrate quantitative improvements on blind source separation over other data-agnostic techniques, and qualitative use of the model for interactive editing, audio texture synthesis, and audio watermark removal. The work also performs an ablation study to qualitatively demonstrate the importance of each element for the prior. The experiments are performed well and explained clearly. They also introduce a dataset of diverse mixtures for future comparisons. Pros: * Important motivation for why audio has different properties than images (even if it can be represented as a *image* spectrogram). The priors are well-motivated by the dynamics of audio. * Good ablations and quantitative comparisons to baselines. Cons: * Some details could be better demonstrated / explained (even if only in the appendix). For example the paper cites the network architecture, but a local description would be helpful. Similarly, the latent dynamics are carefully regularized, so visualizing them would be helpful to understand the dynamics. * The scaling of the technique is not supported by the current experiments. The authors claim they have extended to 4 sources, but all experiments in the paper seem to only involve two sources. * More motivation could help in terms of the value of non-amortized methods like deep priors, vs. other approaches such as pretraining or self-supervised methods. While it is difficult to get lots of labeled data for a specific task, the argument was not convincingly made that methods like deep priors should outperform methods that use pretrained priors on adjacent tasks (where collecting data is easy).\n",
      "    - Falcon Method and Quantization Techniques for Learning with PDF Trees (L0, 48)\n",
      "      * Examples:\n",
      "        - Review:###This paper explores a well motivated but very heuristic idea for selecting the next samples to train on for training deep learning models. This method relies on looking at the uncertainty of predictions of in the recent history of statements and preferring those instances that have a predictive uncertainty over the recent predictions. This allows the training method to train on instances that are neither too hard nor too easy and focus on reducing the uncertainty whenever it has the greatest potential gain to do so. There are two extra components that make this method work: - Windowing: only looking at the recent history of the instances which has two effects: firstly, the current state of the model is explored which gives a more recent assessment relative to the current state of the model. Secondly, it makes the algorithm faster by reducing the overhead of analyzing the prediction history of samples. - Annealing the selection bias: as the training goes on the selection becomes more random and less biased. This approach is evaluated in on three simple data-sets: MNIST, CIFAR-10 and CIFAR-100. Although this is a very limited subset of models, the results are consistent and statistically significant, although their effect is not really huge. The paper gives very little theoretical justification or analysis of the results but gives only the presented empirical evidence which seems to support the hypothesis on the efficacy of the approach. Another drawback of the approach is that it introduces new hyperparameters: those governing the annealing schedule for the selection bias. Since the approach seems efficient in a relatively constrained setup, it can be reasonably expected that it might be helpful in more general situations, therefore. On the other hand, since it is only evaluated on three very similar tasks, it limits the conclusiveness of the results. That*s why I would for weak accept. In the presence of more empirical (or even theoretical) evidence, I would vote for strong accept.\n",
      "        - Review:###This paper presents a training approach on label noise datasets and outperforms state-of-art methods. It defines the samples whose average probability on assigned label in recent q iterations is largest among all labels as memorized samples, in the sense of the network memorize these samples. Then authors proposed two stage method which firstly early-stops at minimum validation error (or memorized rate), and then trains on maximal safe set that gathers memorized samples. The experiments compared several state-of-art approaches and showed that the proposed method benefits from early-stopping and safe set. Authors also showed that the prestopping idea can also be used to improve other approaches. Pros: The proposed method achieves better performance than state-of-art methods. Authors have good experiments which evaluate on multiple datasets and algorithms. Authors also investigate the relation between model complexity and performance of co-teaching+ Cons: Many recent papers indicate the “error-prone period”, authors should include related works about early-stopping on label noise training. https://arxiv.org/pdf/1901.09960.pdf fig1 https://arxiv.org/pdf/1903.11680.pdf fig5 https://arxiv.org/pdf/1906.05392.pdf fig3 Although the method achieves good performance, since the idea is a bit straightforward especially after exploring above papers, I am slightly worried about novelty of the ideas.\n",
      "        -  Summary: As the title of the paper states, this paper tries to improve evolution strategies (ES) using a generative neural network. In the standard ES candidate solution is generated from a multivariate normal distribution, where the parameters of the distribution are adapted during the optimization process. The authors claim that the gaussian distribution, i.e., the ellipsoidal shape of the sampling distribution, is not adequate for the objective functions such as multimodal functions or functions with curved ridge levelsets such as the well-known Rosenbrock functions. The motivation is clearly stated. The technique is interesting and non-trivial. However, the experimental results are not very convincing to conclude that the proposed approach achieves the stated goal. Moreover, this paper may fit more to optimization conferences such as GECCO. Because of the empirical results, I would rate this paper as the border line (around 5), but due to the slightly annoying rating system the rate appears as 6. Comments: P2: *Efficient Natural Evolutionary Strategies (xNES) (Sun et al., 2009) has been shown to reach state- of-the-art performances on a large ES benchmark.* This algorithm is *eNES* and this algorithm is not competitive with the state-of-the-art ES such as CMA-ES. The authors might want to refer to exponential NES, which is xNES, proposed by Glasmachers et al 2010. P5: *Indeed, other bijective GNN models like the Real-NVP (Dinh et al., 2016) introduce non-volume preserving transformations, which can easily overfit and lead to premature concentration and convergence.* Has it been reported in a reference? If so provide the reference. If not, the authors should state that it has been observed the authors preliminary study. In any case, I think it depends how the model is used or trained, and this statement itself is not universally true. P7: *By using f? instead of g? as the push-forward map of the NICE model, we ensure that the flexibility brought by the GNN only impacts the tails of the search distribution. As detailed in an ablation study presented in Appendix F, this additional tool turns out to be essential in order to use GNNs for ES.* I barely understood this point. Please make is clearer. P8: Experimental results are not very convincing. The experiments are limited to dimension 2, 5, 10 and only a few functions are selected from the BBOB test function suite. How about on 20D? What happens if the target is 1e-8, which is the default setting in BBOB? Figure 3 looks interesting, and this is what the authors are trying to achieve. Therefore, it looks like the authors reached the stated objective. However, this is only 2D. No results are provided to convince that the proposed strategy achieved the stated objective. Figure 4 simply looks that the proposed algorithm failed to reach the *flexibility* stated in Section 2: *Another limitation of classical search distribution is their inability to follow multiple hypothesis, that is to explore at the same time different local minima. Even if mixture models can show such flexibility, hyper-parameters like the number of mixtures have optimal values that are impossible to guess a priori.* From these results, I am not convinced that the proposed strategy really achieved more flexible distribution than the classical methods, and whether the flexibility contributes to improve the performance. Another critical point to be discussed is its usefulness. Since this algorithm is proposed to *improve evolution strategy* as a black-box optimizer (not for specific tasks), I expect to improve the state-of-the-art performance. Are the reported results outperform the CMA-ES? Based on Glasmachers et al (2010), xNES tends to require more objective function evaluations than CMA-ES, especially for higher dimensional cases. I am curious to know if the proposed approach outperforms the CMA-ES on Rosenbrock functions.\n",
      "    - Model Unification and Sequence Positioning Concepts in PDP Approaches (L0, 24)\n",
      "      * Examples:\n",
      "        - Review:###This paper studies the problem of interpreting predictions of blackbox models. In particular, they study local interpretable models, which are used to study interpretability at the level of one or a few data points. A key challenge is that in order for local models to be interpretable, they need to be simple in form and therefore lower in capacity (i.e. linear); thus, if they are trained on entire datasets they will underfit. The aim of this work is to address this issue by learning how to select representative samples from a dataset for training local linear models to reproduce predictions of black-boxes. In particular, they propose to use RL to learn to weight instances from datasets; RL is required as they make hard (i.e. non-differentiable) decisions to select a subset of the dataset. This work is closely related to Ren et al. 2018 [1], which proposes to meta-learn how to weight samples in a batch so as to maximize performance on a validation set. By analogy, in this paper, the samples are taken from the entire dataset (i.e. the dataset is subsampled), while the validation loss is effectively an imitation loss formed by treating the black-box model predictions as a target. The data subsampling operation introduces the added complication of non-differentiability. The contribution of this paper is thus the use of RL for meta-learning how to subsample a larger dataset in order to maximize some validation loss. * Pros: * Considers an interesting dataset subsampling variant of the sample weighting meta-learning problem. * Novel application of meta-learning for improving locally-linear models. * Extensive quantitative evaluation shows that the method seems to perform better than baselines, though it might be that a differentiable approximation could do as well while being more sample efficient. * It is a nice result that the l1 penalty actually works well in reducing the number of samples chosen by the * I found the discussion and figures presented in 4.2 to be quite nice and informative. * Cons: * Given the lack of a differentiable approximation baseline, I am not entirely convinced that the use of RL is absolutely necessary/optimal. * I.e. if the weighting function is actually high-entropy, randomly sampling a (large) batch and weighting it might work just as well. * Though there is discussion of the complexity of the overall method it would be nice to see a discussion and figures related to the sample efficiency of REINFORCE? * This would be strongest if given with a comparison to differentiable alternatives (mentioned above) as well. * This would help elucidate whether RL is optimal in this setting: fitting a linear model on more data might be cheaper learning to subsample with REINFORCE. * While the sample weighting function is fast at inference time, most of the overhead comes at training time. This function needs be updated in settings where the underlying dataset changes. * This is a minor issue, but this pushes the burden of interpretability further up to the black-box sample weighting function. While this interpretability problem is less critical, it still exists. * Other comments/requests: * While the use of RL is certainly motivated in order to solve the problem in an unbiased way, it would be nice to see a comparison to a differentiable approximation as a baseline? A few ideas: * Randomly sample a (possibly large batch) and learn to weight it (closely related to the straight through estimator) * Randomly sample a batch and apply [1] * Would be nice to show the sizes of datasets and how many samples end up being used for different values of lambda. * Would be nice to understand which samples are chosen and why. This is probably tricky to analyze, but it would be interesting to see if certain samples are often chosen, or if the weighting distribution has an interesting shape (i.e. is low or high-entropy). I’ve given a weak accept, conditioned on being provided more evidence regarding 1) comparisons to simple differentiable alternatives, 2) sample efficiency of the RL method, and 3) basic analysis of the weighting function. [1] Learning to Reweight Examples for Robust Deep Learning. Mengye Ren, Wenyuan Zeng, Bin Yang, Raquel Urtasun. https://arxiv.org/abs/1803.09050\n",
      "        - Review:###This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance. Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN*s and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis. There are several parts that are confusing/missing in the paper: - In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this. - On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples? - Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch. - I think the statement in the first paragraph in Section 4.4 that *MSE loss term would act as a denoising loss smoothing noisy missing pixels* could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear. I think the paper would benefit if the authors could explain/show: - Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?\n",
      "        -  - This paper simply proposes to use UNet for the segmentation of stagnant zones in X-ray CTs. While the applicability of this model may represent an advance in the particular field of the authors, the technical contribution of this paper is far from the level expected in this conference. - As the paper reads, the main contribution of the paper is the modified version of UNet *proposed* by the authors, which major modification consists on replacing SGD by Adam. Nevertheless, this cannot be considered a contribution, as changing the optimizer in a deep model is a marginal change, from a methodological point of view. - Overall, the quality of the paper is below the standards of ICLR (content, technical contribution, length). - The submission is not anonymized (authors included their names and affiliations).\n",
      "    - Neural Encoder Decoder Models and Training for Automatic Text Summarization (L0, 10)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposes a new encoder-decoder framework that combines prior knowledge-based regularization and constrained reconstruction for unsupervised and weakly-supervised classification in structure rich scenarios. This framework injects prior knowledge in the form of relaxed constraints that act as regularization during the training of the encoder network. Some of the constraints concern sets of training examples. In this case, the paper proposes corresponding sampling schemes. Three experiments demonstrate the efficacy of the model. The first is a synthetically created 4x4 Sudoku made of overlaid MNIST digits. The other two are based on predicting crystal structures from x-ray diffraction measurements. Here, the first experiment is on simulated data for the Al-Li-Fe oxide system, while the other is performed on real measurements for the Bi-Cu-V oxide system. Overall, I believe that the proposed framework could be a significant contribution to the fields of representation learning and constrained optimization. However, the paper exhibits serious shortcomings, which require revision. First, the positive aspects of the paper: • The framework is simple yet ingenious. It makes intelligent use of constraints in the form of regularization to guide the training of the encoder. Furthermore, it enables the direct design of the latent representation through the use of (pre-trained) generative models for constrained reconstruction of data points. • The proposed entropy-based method for relaxation of discrete constraints is intuitive and potentially adaptable for further constraints. • The experiments presented in this paper are well chosen. They demonstrate the contribution of the model to both general CV data as well as a specialized domain, where it can solve both simulated and real scenarios. • The paper provides an extensive literature survey, which makes it easy to embed the presented work in the proper context. However, I propose to remove the paragraph titled “Other less closely related work” as the connection to the current work is not clear, and the space could be used more effectively (see below). Unfortunately, this paper has a couple of major flaws: • The results for DRNets (Generalization) on the MNIST Sudoku are compromised because the model trained on the test set for 25 epochs after being trained on the training set. Honestly, I was baffled to read the following sentence in the appendix: “Note that, during the test, instead of predicting the overlapping digits directly as other networks, we further optimize DRNets on the test set for 25 epochs to achieve a better result.” What is more, the main paper does not even mention this fact! • Although this paper relies on empirical verification of its proposition, the experimental results are almost impossible to interpret with just the information provided in the paper. Both experiments are poorly described, and even after reading the appendix several times, some serious detective work was necessary to piece together what happened in the experiments. The XRD experiments are especially hard to decipher, even with a physics background. Many vital components remain shrouded in mystery: What is a composition graph, and how are the paths sampled from it? How does the restart method, which is part of the results, work? Why are only six phases shown in the phase concentration visualizations if there were 159 possible phases? Are these the first six, a random subset, or were the other phases not realized? • The paper introduces the constraint-aware SGD algorithm to incorporate batching rules into the training of the encoder. On pages 2 and 6 and in Algorithm 1, I found the statement that the weights for each constrained are updated dynamically. However, that is where the information on the dynamic update method ends. Nowhere in the paper or the appendix did I find an explanation of how this is done. As this mechanism is a critical component of the proposed framework, the absence of an explanation is a significant oversight. Other remarks: • In the context of global constraints, the paper talks about a constraint graph. If I understand the creation of this graph correctly, this graph has several connected components in which every element connects to every other element. As such, this seems to be a collection of sets rather than a real graph. This is especially confusing in the case of XRD, where all data points are in the same global constraint, leading to a fully connected “graph”. • Although I appreciate the reference to Kahneman’s model of the mind, I suggest to remove the first two paragraphs from the introduction and use the space to motivate the de-mixing problem instead. While it is a compelling (but not novel) observation, the analogy to system 1 and system 2 does not benefit the proposed work in the slightest. • In general, I fail to see the connection between reasoning and the proposed work. The model itself is an encoder-decoder network that cannot reason. It does not discover any new rules during training. All the reasoning has to be done manually beforehand to be then incorporated in the form of constraints. To clarify, I do believe that there is value in the presented work, but not necessarily in the way, it is advertised.\n",
      "        -  1) Summary The manuscript proposes a k-nearest-neighbor (KNN) Gaussian process (GP) approximate inference scheme to render computations more scalable. 2) Quality Although the application is clear and the methodology is well established, the quality of the submission can be improved by a more thourough empirical evaluation in particular a proper evaluation in terms of runtime, approximation accuracy and comparison to baseline methods. 3) Clarity The manuscript is reasonably well written and most of the technical and experimental content is accessible. There are some typos and some glitches in the notation. See *Details*. There are some open issues regarding the KNN computations. See Questions. 4) Originality The use of a localized (in the KNN sense) set of inducing inputs to improve GP inference but the impact needs to be better quantified empirically. 5) Significance The proposed method is aiming at improving the established setting of GP inference. The modification is rather marginal and the empirical evaluation makes it hard to judge the relative merit of the proposal. 6) Reproducibility The data is from published sources (toy, ebirds, precipitation, digits) and the code for the baseline methods and for the LAIM method itself is available. However, there is no code for the experiments, which makes the results slightly tricky to exactly reproduce. 7) Evaluation The evaluation does not consider simple baselines like dense GPs or sparse approximations such as FITC and VFE. Also plain NN should be considered. 8) Questions A) How do you set the parameter delta? B) How are the nearest-neighbors computed in the first place? Does it require computing the dense covariance matrix? C) How accurate is the NN computation? How much of the computational effort (percentage) of the overall pipeline is required for the NN computation? D) How much better is the proposed approach than directly using NN predictions? E) Does *most correlated* in the footnote on page 2 really mean correlation or is it about covariance? The latter would involve a diagonal rescaling of the covariance matrix. 9) Details a) Abstract: *Gaussian Processes* -> *Gaussian processes* b) Intro: *GP poses a Gaussian prior* -> funny sentence, *with some special * c) Intro: *with some special structures* -> fix d) Background: *q(f)~N(mu,V)* -> imprecise notation, rather *q(f)=N(mu,V)* e) Background, footnote: *distance metrics* -> Are you talking about *distance* or *metric*? f) *Experiment* -> *Experiments* g) References: capitalization not correct e.g. Gaussian, Fourier, Bayes\n",
      "        - Review:###Summary: This paper proposes a latent spatial-temporal origin-destination model to address the OD flow prediction problem. The main contributions are summarized as follows: 1. The authors propose a purely convolutional framework to learn both short-term and long-term spatio-temporal features simultaneously from dynamic origin-destination flow data. 2. The authors propose a novel SACN architecture to capture the relevance of OD flows by modeling each OD flow map as an adjacency matrix. 3. The authors design a periodically shift attention mechanism to model the long-term periodicity. 4. The results demonstrate that the proposed model outperforms state-of-the-art methods in OD flow prediction, with 6.5% to 15.0% improvement of testing RMSE. However, my major concerns are as follows: 1. On Page 4, the authors explain the reasons why they use TGCNN instead of RNN-based architectures to capture the temporal representation. However, it would be more convincing if quantitative analysis or empirical results are provided. 2. On Page 4, readers might be confused with the symbols in the formulation (3) that are not defined clearly. 3. On Page 7, the experiment only considers one metric, i.e., RMSE. The efficiency comparisons between the proposed model and the baselines are missing.\n",
      "    - Prototype-based Attention Models for Text Classification (L0, 25)\n",
      "      * Examples:\n",
      "        - Review:###*an information theoretic principle, information bottleneck principle* in the abstract is quite redundant with the use of *principle* twice **great, great* and *great, thought provoking*. They have the same level of sparsity.* What kind of sparsity are you referring to with this example? Why can*t sparsity reduce semantic redundancy? Please explain further. *However, the first explanation has a large MI with the input document where *great* occurs a lot.* What example input document are you referring to? You should save the explanation of how your method in Equation 2 differs from the original information bottleneck of Equation 1 until after you have actually written out Equation 2. As it is now, you are referencing Equation 2 before it has been seen. I find Equation 2 confusing. Is it possible to make the dependence of the expression on z more explicit. It isn*t clear from the equation itself how p(z|x) influences either quantity in Equation 2. Perhaps you should wait to introduce this equation until you have first explained how z relates to x and t. As it is, z is not clearly defined. I can gather information about it from the figure, from how you describe the difference in your method from the original information bottleneck, but the relationship is not clear enough by reading only the text of the paper before Equation 2 is presented. Can you explain briefly how your *hierarchical LSTM* works in the main text of the paper? Its an unusual enough term that I would want to see a citation or brief explanation right away rather than having it deferred to the Appendix. Why not use a state-of-the-art model for IMDb? Are you not using the standard splits for IMDb? the In Appendix B.1 *output vector is averaged and followed by log-softmax calculation. The final layer is formed to return a log-probability indicating which cognitive chunks should be taken as an input to the approximator* The single output vector of the biLSTM is averaged and followed by log-softmax? the final layer is formed? What does this mean? I find the phrasing of *Negative Sentiment if any negative words* and the corresponding title for positive in Fig 2 confusing. What do you mean by *if any*? The phrasing makes it sound like the prediction of the model somehow depends on a logical step based on whether there are any negative/positive words found. I find the lack of a comparison to some kind of attentional method somewhat glaring in the IMDb example, since I would expect that many classifiers with attention would simply attend to the same words. What does your method give us that attention would not? The same can be said for the MNIST example regarding an attention map. *by the human intelligences* sounds quite robotic Can you provide some sense of inter annotator agreement for labeling the images and sentences? It does seem that there is key information like the definition of approximator fidelity in the appendices which is crucial to actually understanding the paper.\n",
      "        - Review:###1. Summary The authors address the problem of efficiently employing the SURE estimator as a network training regularizer. They show that for CNN autoencoders this can be efficiently computed. Their other contribution is a bagging/boosting technique which is proved to avoid trivial solutions. The proposed architecture, motivated by the theoretical statements, is shown to outperform classic and 2019 state of the art image reconstruction algorithms in MRI and EDX. 2. Decision and arguments Unfortunately this paper is outside my expertise so I can’t evaluate the novelty of the theoretical accomplishments. However taking that as a given, they well-motive the proposed architecture and achieve impressive experimental results. The experiments are well described. 3. Questions a) Why do Table 1 and Figure 3 provide different PSNR and SSIM values? b) Is there any way to measure accuracy to ground-truth with the EDX data? Or are the results just qualitative? c) With respect to Figure 2, and in general for autoencoders, the input and output have the same dimension. So how do you reconcile this with undersampled MRI and EDX data? I understand you train on fully sampled data—then how do you input undersampled data? Are the unknown samples set to zero?\n",
      "        - Review:###This paper presents a sample-based self-explaining method for image classification. The basic idea is adopt the attention mechanism to learn the relation between the latent representation of the query sample and training samples, and identify the training samples with higher similarity as the prototype. The classification decision is based on the label consistency between the identified prototypes (with the relation score in attention mechanism as the weight of different prototypes in determining the label agreement) The proposed model is intrinsically interpretable since the prototypes with higher weights can play as the decision explanation. And the authors have conducted experiments to show that such self-explaining mechanism based on attention model can achieve comparable classification accuracy with original black-box models. The presentation of the paper is clear and easy to follow. But I have several concerns regarding the choice of prototypes and the evaluation of the interpretation: 1) According to Eq. (2), it seems that all training samples are used as the prototypes (but with different weights). Why not just use the top few prototypes? Would this such setting introduce a lot of noise, since many training samples are from different classes? 2) Since one focus of the paper is to provide interoperation of the classification model, some more experiments are needed to evaluate how well the interoperation is. For example, some crowdsourcing experiments to check if the provided prototypes can help human users correctly guess the model prediction. 3) I think the authors should also compare with the black-box model when we use the attention mechanism as a post-hoc interpretation. One straightforward baseline is that use the black-box model for classification, and pick the top *prototypes* with the highest similarity in the latent representation. Such comparison can help to validate if incorporating attention mechanism in the model design can provide better quality prototypes.\n",
      "    - Scheduled Sampling Methods and Initialization Techniques in Learning Object and Tensor Representations (L0, 15)\n",
      "      * Examples:\n",
      "        - Review:###This paper presents a method to train a network for counting/localizing objects in a weakly supervised framework. The network is optimized based on weak supervision about target object counts; training images have only the ground truth of the total number of object counts without their positions in images. Through analyzing the feature maps of the network trained on toy bubble images, the authors propose two regularization techniques for rotation robustness and sparseness of the map in order to improve performance of object localization in the feature map. The experimental results on object counting tasks using the Mall dataset show that the proposed method produces favorable performance. This paper is leaning toward rejection due to the following two reasons. (1) The presented techniques are limited to the specific task of object counting. (2) They are derived in an ad-hoc way based on less theoretical background and thus lack novelty. The detailed comments are as follows. * The technique to embed rotation invariance by Eq.(4) is presented in an ad-hoc way. This seems to be limited to the tasks on toy data such as bubble images shown in the paper. In the real-world tasks/images to count the more complicated objects, the spatial correlation among object parts is an important clue to provide discriminative features for detecting objects such as torso below the head in human figures. Such spatial dependency could be missed by imposing the rotation invariance on the network. * The regularization by Gini impurity is a well-known technique to induce sparsity, lacking novelty. There is a large body of researches for enhancing sparsity, and thus to validate the regularization, the authors should compare the Gini impurity with the other sparsity-inducing regularization such as entropy. For example, it could be possible to argue the regularizations in terms of their derivatives (gradients) to be used for back-propagation. This paper lacks detailed analysis and discussion about the regularization in this counting framework. * It is unclear how the perspective from the (beta-)VAE contributes to the analysis of the proposed method. There seems to be less connection between Eq.(8) and the object counting task/framework. The authors just show the similarity between the presented method and VAE in terms of formulation. Since the proposed method belongs to just a simple optimization with sparsity-inducing regularization, the reviewer cannot find any convincing reason to discuss the connection to VAE. * The method is not fully validated in the experiments. The authors provide only one experimental result on the Mall dataset which is insufficient to validate the effectiveness of the proposed method. Considering that the method is limited to the specific task of object counting, it is necessary to qualitatively and thoroughly evaluate the performance on various datasets that exhibit various environmental conditions regarding such as lighting and occlusion. Minor comments: - Show the index for summation in Eqs.(1,4,6,7,8). - It is confusing to show q_Theta(D|I) in Eq.(3), even though the authors aim to discuss the connection to VAE. Show the mathematical definition of q_Theta(D|I) before that.\n",
      "        - Review:###This paper proposes a framework to predict valence and arousal tasks in videos. The framework mainly employs LSTM in a two-time-scale-structure to take multimodal inputs. In general, the proposed framework groups well-studied techniques to solve a well-known task of multimodal learning. DNN based Multimodal learning has been heavily investigated for a long time. Numerous frameworks have been proposed with various success. Although the proposed framework is technically sound, the proposed *residual-based training strategy* and *long temporal fusion* are kind of trivial or lackluster. I can hardly identify any significant contributions that support a publication in top machine learning conferences such as ICLR.\n",
      "        -  ## Overview The paper tackles object detection without predefined anchors (sliding windows). The paper is well motivated and existing detection methods often rely on anchors, which may limits their potential. So this paper is solving an interesting problem and seems novel. The paper provides experiments to support the proposed architecture empirically. ## Summary of the contribution: 1. The paper proposed an object detection approach called FoveaBox that does not rely on anchors (sliding widows). 2. The paper shows that FoveaBox outperforms some existing object detection methods. 3. The paper shows that FoveaBox can also be used for object proposals by changing the classification target to class-agnostic head. ## My feedback: I think the paper is well motivated and has value to the object detection community. So I am leaning positive to accept this paper. The major reasons are: 1. The paper studies an interesting architecture which does not rely on sliding windows and shows its effectiveness. The problem seems not very well studied in the existing object detection literature. 2. The paper provides sufficient ablation study to analyze and understand the proposed methods. However, I believe the paper could be significantly improved especially in the writing. So my position is not strong. ## Improvements 1. The writing can be significantly improved. The paper reads a bit confusing and unclear especially at the technical part. The paper could benefit from a clear overview figure about the proposed approach. Figure 4 seems to be doing that illustration but giving only the tensor shape seems quite confusing. I also listed some questions below about the actual algorithm. 2. The paper could be improved with experiments and ablation on another dataset. Current ablation shows that eta should be 2.0 for best performance but without a second dataset it is hard to say this value is general. So one may have to tune this parameters in different datasets. 3. The scale is still discretized. So that is essentially anchors in the scale space. I wonder how could the approach applies to scale to? ## Questions 1. How would the proposed approach address the issue when there are multiple bounding boxes around the same pixels? It seems the current approach is predicting a box per pixel? 2. What is the inference computation cost? And do you have a comparison with existing methods on the compute cost?\n",
      "    - Sliding Window Methods for Temporal Pattern Detection in Sequential Data (L0, 15)\n",
      "      * Examples:\n",
      "        -  This paper proposes Recency Bias, an adaptive mini batch selection method for training deep neural networks. To select informative minibatches for training, the proposed method maintains a fixed size sliding window of past model predictions for each data sample. At a given iteration, samples which have highly inconsistent predictions within the sliding window are added to the minibatch. The main contribution of this paper is the introduction of sliding window to remember past model predictions, as an improvement over the SOTA approach: Active Bias, which maintains a growing window of model predictions. Empirical studies are performed to show the superiority of Recency Bias over two SOTA approaches. Results are shown on the task of (1) image classification from scratch and (2) image classification by fine-tuning pretrained networks. +ves: + The idea of using a sliding window over a growing window in active batch selection is interesting. + Overall, the paper is well written. In particular, the Related Work section has a nice flow and puts the proposed method into context. Despite the method having limited novelty (sliding window instead of a growing window), the method has been well motivated by pointing out the limitations in SOTA methods. + The results section is well structured. It*s nice to see hyperparameter tuning results; and loss convergence graphs in various learning settings for each dataset. Concerns: - The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. Despite the paper stating that there have been earlier work (Joseph et al, 2019 and Wang et al, 2019) that attempt mini-batch selection, the paper does not compare with them. This is limiting. Further, since the proposed method is not specific to the domain of images, evaluating it on tasks other than image classification, such as text classification for instance, would have helped validate its applicability across domains. - Considering the limited results, a deeper analysis of the proposed method would have been nice. The idea of a sliding window over a growing window is a generic one, and there have been many efforts to theoretically analyze active learning over the last two decades. How does the proposed method fit in there? (For e.g., how does the expected model variance change in this setting?) Some form of theoretical/analytical reasoning behind the effectiveness of recency bias (which is missing) would provide greater insights to the community and facilitate further research in this direction. - The claim of 20.5% reduction in test error mentioned in the abstract has not been clearly addressed and pointed out in the results section of the paper. - On the same note, the results are not conclusively in favor of the proposed method, and only is marginally better than the competitors. Why does online batch perform consistently than the proposed method? There is no discussion of these inferences from the results. - The results would have been more complete if results were shown in a setting where just recency bias is used without the use of the selection pressure parameter. In other words, an ablation study on the effect of the selection pressure parameter would have been very useful. - How important is the warm-up phase to the proposed method? Considering the paper states that this is required to get good estimates of the quantization index of the samples, some ablation studies on reducing/increasing the warm-up phase and showing the results would have been useful to understand this. - Fig 4: Why are there sharp dips periodically in all the graphs? What do these correspond to? - The intuition behind the method is described well, however, the proposed method would have been really solidified if it were analysed in the context of a simple machine learning problem (such as logistic regression). As an example, verifying if the chosen minibatch samples are actually close to the decision boundary of a model (even if the model is very simple) would have helped analyze the proposed method well. Minor comments: * It would have been nice to see the relation between the effect of using recency bias and the difficulty of the task/dataset. * In the 2nd line in Introduction, it should be *deep networks* instead of *deep networks netowrks*. * Since both tasks in the experiments are about image classification, it would be a little misleading to present them as *image classification* and *finetuning*. A more informative way of titling them would be *image classification from scratch* and *image classification by finetuning*. * In Section 3.1, in the LHS of equation 3, it would be appropriate to use P(y_i/x_i; q) instead of P(y/x_i; q) since the former term was used in the paragraph. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Some of my queries were clarified. However, unfortunately, I still think more needs to be done to explain the consistency of the results and to study the generalizability of this work across datasets. I retain my original decision for these reasons.\n",
      "        - Review:###This paper proposes a brain-inspired recurrent neural network architecture, named Recurrent Leaky Integrate-and-Fire (RLIF). Computationally, the model is designed to mimic how biological neurons behave, e.g. producing binary values. The hope is that this will allow such computational models to be easily implemented on neuromorphic chips and the solution will be more energy-efficient. On neuromorphic MNIST and CIFAR, the proposed model achieves higher classification accuracy than other listed methods. On ROGUE, a text summarization benchmark, the proposed model achieves competitive performance. I am leaning towards rejecting this paper. The main advantage of the proposed computational model was not supported by evidence in the paper. The presented evidence only suggests that the computational model has the capacity for learning to solve real-world tasks to a degree that is on par with other existing computational models. But what supposedly distinguishes the proposed one from the rest, i.e. being more hardware-friendly and energy-efficient, was not demonstrated.\n",
      "        -  This work develops a differentiable spectral projection layer to enforce spatial PDE constraints using spectral methods, to achieve the introduction of the physical constraints in the end-to-end network without damaging the intrinsic property of the network. Analysis of computational cost shows the proposed layer is cheaper than the convolutional layer. The experimental comparison demonstrates the superiority of the proposed method. In my viewpoint, the novelty of this paper is somewhat novel. This paper focuses on designing the PDE layer to constrain the network output without additional loss. Some constraint sets for comparison are clearly performed and then authors present the proposed spectral projection layer. From the mechanism of solving, the FFT (IFFT) operator is important component in this layer. It is curious about the role and importance between FFT (IFFT) and spectral projection. If possible, the authors maybe provide some analyses of these two components to deeply recognize the proposed layer. A series of compared experiments are conducted to verify the effectiveness of the proposed PDEL. But it is a little confusing in Table 1, e.g., why the second column obtains the second better in the all mean values. I know its result has a high score of residue. It will be better if authors can clarify their causes and analysis clearly.\n",
      "- Physical GRF Data Analysis Methods and Experiments (L2, 19)\n",
      "  * Examples:\n",
      "    - Review:###The paper considers the problem of parametric conditional density estimation, i.e. given a set of points {(x_n, y_n)} drawn from a distribution , the task is to estimate the conditional distribution p(x|y). The paper considers parametric estimation where in given a parametrized family of distributions f_{theta} we wish to minimize the likelihood of seeing the given data over theta. The parametric family in a lot of applications consists of highly expressive families like neural networks, which leads to the issue of overfitting in small data regimes. This has been tackled via regularization over the parameter space which might be hard to interpret as the associated inductive bias is not well understood and depends on the parametric family under consideration. On the other hand the paper proposes to add explicit noise in the examples used during training, i.e. irrespective of the optimization procedure (which could be mini-bath sgd) the paper proposes to draw examples from the data set, explicitly add noise onto the examples and create a proxy objective over the augmented data set. The paper establishes two theoretical results. First is a simple taylor approximation based analysis to highlight the effect of the variance of noise. The conclusion is that higher variance penalizes the high curvature areas of the resultant density and hence this kind of noise addition could be seen as making resulting density smoother. The second contribution is to show that this procedure is asymptotically consistent, i.e. as n goes to infinity and the number of augmented data points go to infinity, the resulting density converges to the target density. This of course requires the noise variance to follow a decreasing schedule to 0. The main merit of the idea is in its agnostic nature, as it can be applied to any parametric family and the experiements show that it seems to uniform improvement across models, The basic idea proposed by the error, has existed in the space of deep learning based methods forever. This is the same idea behind image augmentation which forms a crucial part of training supervised models in vision. The authors claim that this idea is novel in the space of parametric density estimation, however I do not know enough about the area to verify the claim. It would surprise that this very natural idea has not been tried before. I have gone through the theoretical derivations in the paper and they look sound to me. However the results are all asymptotic in nature without establishing explicit rates which is a little bit of disappointment. Since I am not completely familiar in nature, but I guess such asymptotic consistency might be achievable using other forms of regularization under suitable assumptions. In that light, the theoretical contributions while being sound did not lend much intuition about why such a method might outperform others. Intuition does arise from the derivation for the effect of noise on the objective which helps understand the nature of the noise, but one wonders if similar intuitions could be derived for other forms of regularization as well. It would be great to see this derivation being extended to some concrete scenarios under well understood parametric families and seeing the effect explicitly. Regarding the experiments - The experiments definitely look promising as the improvments seem uniformly good across the cases considered. I am not an expert however in this setting so it is hard for me to judge the quality and significance of the benchmarks. The experiment methodology nevertheless looks sound.\n",
      "    - Review:###1. Summary of the paper This paper describes the decision boundaries of a certain class of neural networks (piecewise linear, non-linear activation functions) through the lens of tropical geometry. An earlier result by Zhang et al. (2018) is extended to multi-class classification problems (technically, only the result for a binary classification is given in the main text, though). Similar to this earlier work, the network is shown to be represented as a tropical rational function. The dual subdivision of this function is shown to be represented as the convex hull of two zonotopes. This characterisation is used to explain different phenomena of neural network training, viz. the *lottery ticket hypothesis*, network pruning, and adversarial attacks. 2. Summary of the review This is a highly interesting paper with a very relevant subject. I think that the perspective of tropical geometry leads to valuable insights. My background is *not* in tropical geometry, so this paper required several passes to fully grasp. I like the novel insights that this paper creates; it is very interesting to observe known phenomena via tropical geometry. I appreciate the thorough description of all concepts in this paper. This is to some extent both *boon and bane*: on the one hand, the paper contains a lot of information and concepts that need to be understood; on the other hand, the experiments go *wide* but not *deep*. I suggest to accept the paper, but to fully endorse it, I would recommend to work on the following issues: - Clarity & exposition: In some places, the paper could build intuition for non-experts (such as myself) better. This is closely tied to the second point. - Focus: I would maybe pick *one* or *two* of the experimental areas and use the remaining space to explain all concepts in more detail, build some intuition, and provide a more in-depth setup. Nothing has to removed of course; it could still be put in the appendix. This paper has the potential to be a very strong insightful contribution to to our community; the authors are to be commended! 3. Detailed comments (clarity) The paper describes its concepts well and has a high information density. At times, there is the risk that readers are provided with too much information in the main text, leaving the necessary intuition somewhat lacking (unless they are already experts in the subject matter, in which case a lot of the information can be skipped). I realise that writing a paper based on methods that are not yet well-established is no small feat; the authors are to be commended for that! Here are some suggestions from someone with a background in differential topology: - The introduction and contributions are somewhat repetitive; I would suggest merging the *Contributions.* paragraph with the one preceding it - Even though it *should* be a well-known definition, I would briefly explain that the semiring lacks an additive inverse - Add an explanation of the tropical quotient to Definition 1; I find the current phrasing of Definition 3 to be confusing at first glance - What are *upper faces*? Faces with a specific coordinate fixed? - A definition of is required. It is my understanding that is a projection function that *drops* the last coordinate. Is this correct? If so, it should be briefly mentioned on p. 3; else, the discussion about the bias-free case on p. 4 cannot be understood. - Theorem 2 lacks an *intuitive* formulation; the results are stated in a terse mathematical fashion, but it would be helpful (also in light of the subsequent discussion) to briefly comment on their *meaning*. For example, the first result could be restated as *the decision boundary is a subset of a tropical hypersurface of the polynomial *. The paragraph *Theorem 2 bridges the gap...* could maybe also be moved to _precede_ the theorem statement. - The paper refers to as a super-set, but in my understanding, it is a *level set* because Definition 4 uses an equality, not an inequality. Am I misunderstanding this? One potential misinterpretation of my part could be that superset refers to the fact that ; so not a superset in the sense of level set analysis, but rather a superset in terms of set theory. If this is the case, maybe rephrase the sentence above to something like *boundaries through their superset according to the first statement of Theorem 2*. - What does the colour map in Figure 2 depict? The number of iterations? Moreover, I find the polytope though to understand at first glance; how is it related to the decision boundaries that are shown in the leftmost figure? - I would suggest to place Figure 1 after stating Theorem 2, since it is only referenced later on. Furthermore, the red structures are somewhat confusing. According to Theorem 2, the decision boundary is a subset of the hypersurface, right? What is the relation of the red structures in the convex hull visualisation? The caption states that they are normals, but as far as I can tell, this has not been formalised anywhere in the paper (it is used later on, though). - To what extent is the existence of the functions described by Theorem 2 unique? On p. 5, in Section 4, the paper alludes to *not* using the functional form of the network directly because it does not seem to be unique. I would like this to be explained a in more details, as I found the justification of why the dual subdivision is used quite hard to follow. - In Section 4, how many experiments of the sort were performed? I find this a highly instructive view so I would love to see more experiments of this sort. Do these claims hold over multiple repetitions and for (slightly) larger architectures as well? Please also see my comments on Figure 2 above. - The claim that orientations are preserved should be formalised. I immediately understand the intuition behind this concept, but if possible, I would like a quantification of this. Might it be possible to *measure* changes in orientation with respect to an original polytope? If so, it should be possible to provide more experiments about these effects and summarise them accordingly. Maybe it would also be interesting to investigate whether other initialisations can be compared in terms of their orientations? - In Section 5, I would give a brief link to the appendix for the definition of a Minkowski sum. - Section 5 has (in contrast to the other sections) a lot of details containing the experimental setup, but it is missing a description of the competitor methods. Adding to what I wrote above, I feel that the paper should rather pick *one* area in which experiments are performed; the pruning (together with the lottery ticket hypothesis explanation, which could be seen as a motivating example) strikes me as a good candidate for this. I really like this concept of tropical pruning, by the way---it is an elegant, principled description! - The plots in Figure 4 should summarise multiple pruning runs, if possible. Why not show a standard deviation as well? Given the stochasticity of training, I would think this highly necessary. - In Section 6, I find the comment on normals generating a superset to the decision boundaries hard to understand. - The perspective of perturbing the network such that the decision boundaries change is really interesting, but I am missing a *take-away message* or a discussion of the insights. Currently, this section seems more like a feasibility study: it appears to be possible to use the tropical description to find new parameters that misclassify a given input. I would propose a discussion of the implications of these findings. - Concerning future extensions of this work, are there some promising results or directions for CNNs or GCNs? If so, it would strengthen the conclusion if they were mentioned. 4. Minor style issues The paper is well-written. I found some minor style issues: - *piece-wise* --> *piecewise* (occur multiple times) - *recently demonstrated* --> *demonstrated* - *Thereafter, tropical hypersurfaces divide* --> *Tropical hypersurfaces divide* - *If set --> *Letting B* - I am not sure if I would call adversarial attacks a *nuisance*; maybe rather a *problem*? - Use *operatorname* or *mathrm* to typeset the loss in Eq. 5 5. Update after rebuttal The authors addressed all my important comments; their efforts in rewriting and revising the paper in such a short time period are to be commended. I am very happy to raise my score.\n",
      "    - Review:###[Due to the rebuttal, my score was raised from a weak reject to a weak accept] Summary The paper addresses the problem of interpreting predictions/decisions of a black-box classifier/regressor by masking the parts of the input that were most relevant. The proposed approach consists of, first, manually designing “cognitive chunks” of input data, e.g. individual words for sentiment classification or fixed-size image-patches for image-classification. Then, a variational IB framework is used to infer which of these chunks are relevant for the classifier’s decision. Additionally, there is a (hard) constraint, making sure that only a fixed (small) number of chunks is used. The bottleneck variable, in this case, is a sparse-chunk representation of the data. The latter is obviously a more compressed representation of the data, but importantly it is a more compressed representation that contains the largest possible amount of relevant information about the decision (because of properties of the information-bottleneck objective). Both factors together, according to the paper, constitute a “good” (i.e. brief but comprehensive) explanation which allows for interpretability and attribution of the black-box system’s decision. The method is evaluated on three tasks (sentiment prediction, image classification, TCR to epitope binding prediction) and performance is reported to be on-par or better than state-of-the-art methods. Contributions -) Application of the IB-method for generating summaries of decision-relevant input-data, which are good candidates for interpretability. The theoretical properties of the IB objective are appealing for producing interpretable data-summaries. -) Adaptation of the variational IB framework, using bits and pieces reported in the literature such that the bottleneck variable is a sparse, binary vector over “cognitive chunks”. -) Experimental evaluation, where human judges rate the “interpretability“ of various state-of-the-art attribution methods. Quality, Clarity, Novelty, Impact The paper addresses a timely and important problem, particularly the IB framework could add some solid theoretical footing (the “theory of relevant information”) to the field of interpetability methods. The paper is well written (though it needs another pass for typos, etc.), related methods and literature are discussed and compared against, and the specific variational IB objective is introduced nicely. Large parts of the method (deep variational IB, VI with categorical variables) have been published before, but these parts are combined in a novel and original way. My main issues with the current paper are (I) interpretability and comprehensiveness are not necessarily the same as maximum compression of maximally relevant information, (II) the method (in theory) depends strongly on the quality of the approximator, this is currently not mentioned and not explored, (III) the experimental section is currently not very strong, in particular the MNIST experiment. See more details for the main issues below. Overall, I personally think that the main idea of the paper is interesting, mature and fleshed out enough for a publication, however, the experimental section is somewhat lacking and (II) is missing from the current manuscript. While the method has theoretical advantages, empirically it seems to perform more or less equal to L2X (but the chunks produced seem qualitatively different which is interesting). I am therefore slightly leaning towards suggesting a major revision of the paper, but I am happy to be convinced otherwise by the other reviewers and the authors during discussion/rebuttal. (I would rate the paper as *borderline*, but it seems that this year*s review system only allows for *weak reject* or *weak accept*, so I*ll go for *weak reject* for now). Improvements / major issues (I) Good compression of highly relevant information is not (always) the same as good interpretability/comprehensiveness. In the limit, the bottleneck variable captures a minimal sufficient statistic, i.e. a maximally compressed version of all relevant information - for finite beta, the bottleneck approximates such a minimal sufficient statistic. From a theoretical point of view this is very appealing, since it is guaranteed to cover a maximal amount of information (given a certain level of compression). But the way this information is represented matters a lot for interpretability - any reversible mapping of the bottleneck variable does not change its information content but can have substantial effects on interpretability, e.g. consider encrypting or randomly perturbing elements of the explanation (i.e. the selected cognitive chunks). This is a major open problem, and some theoretical grounding in the IB framework helps by talking about this problem in very concrete terms. While I would not expect the paper to solve the problem in full generality, some discussion, and perhaps adding a “shortcomings” section would be nice. (II) The relevance of information is measured via I(t;y), which ultimately boils down to the approximation q(y|t). The quality of this approximation is crucial, which can of course be seen by how it influences the tightness of the bound. While I appreciate that the paper investigates the quality of the approximation to some degree (by inspecting the approximator fidelity), I would highly appreciate a thorough discussion of this issue (because ultimately the method will produce cognitive chunks that are relevant for q(y|t), not p(y|x) - the interpretations can be trusted only if q matches fairly well). It would be very interesting to see how quickly interpretability degrades with lower-quality q(y|t) - the latter would of course require more experiments with human “interpreters” which I would not expect to be easily feasible within the rebuttal period. Another interesting experiment to test the match between q(y|t) and p(y|x) would be to “minimally intervene” on the input-chunks suggested by the method and see whether that actually affects the predictions of the black-box models. E.g. do small random perturbations to the selected cognitive chunks in the MNIST digits change the prediction of the black-box classifier? Compare this against small random perturbations in arbitrary chunks of the input. (III) Experimental section: I’m fairly happy with the IMDB experiment, and the TCR to Epitope binding is a nice non-standard application but I find the quality and significance of the results a bit hard to judge. My main concern though is the MNIST experiment: what I would have expected was the following: cognitive chunks are shown to participants and they need to guess the correct number (just like in the IMDB experiment). In the experiment reported in the paper, I’m afraid that there’s a certain bias for judges favoring explanations that lie on the digits rather than off digits. It remains unclear whether they simply prefer the chunks selected by VIBI over other methods, or whether they have actually gained more understanding of how the black-box makes decisions. (IV) Table 2 can easily be misleading because entries with highest mean-accuracy are marked in bold, regardless of whether confidence intervals overlap with other entries or not. Please fix this by either only marking entries in bold where the error bars don’t overlap with an entry in the same row, or marking all entries in bold that lie within the error bars of the best-performing entry. Particularly for “Approximate Fidelity” VIBI often does not perform significantly better than L2X but performs roughly equally well. Of course it would also help to run more repetitions to potentially shrink confidence intervals. (V) Please state the (parametric form) of the prior r(z*) used for the experiments. Also state the analytical expression for the KL-term in the final objective that this prior leads to. Minor comments a) Please add some discussion on how the method depends on hand-crafting cognitive chunks, and how hard/easy this might be for different domains. b) Rather than fixing r(z), other papers have proposed to optimize the prior as well (typically in the context of VAEs / VIB) which is well justified from an IB perspective. It might be interesting to explore these possibilities for VIBI as well in the future. [1] Fixing a broken ELBO. Alemi et al. 2017 [2] The beta-VAE’s Implicit Prior. Hoffman et al. 2017 c) Instead of fixing the number of cognitive chunks in advance, it could also be interesting to infer that number as well (as a future extension of the method). This could either be achieved via a sparsity-inducing prior r(z), or perhaps by borrowing some ideas from the Deterministic IB [3], and its variational version. [3] The deterministic information bottleneck. Strouse and Schwab. 2016\n",
      "- Robustness, Privacy, and Specialized Applications (L2, 76)\n",
      "  - Adversarial Robustness, Privacy, and Attack Defenses (L1, 52)\n",
      "    - Adversarial Attacks and Robustness in Machine Learning (L0, 15)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposed a framework based on a mathematical tool of tropical geometry to characterize the decision boundary of neural networks. The analysis is applied to network pruning, lottery ticket hypothesis and adversarial attacks. I have some questions: Q1: What benefit does introducing tropical geometry brings in terms of theoretical analysis? Does using tropical geometry give us the theoretical results that traditional analysis can not give us? If so, what is it? I am trying to understand why the authors use this tool. The authors should be explicit in their motivation so that the readers are clear about the contribution of this paper. More specifically, from my perspective, tropical semiring, tropical polynomials and tropical rational functions all can be represented with the standard mathematical tools. Here they are just redefining several concepts. Q2: In “Experiments on Tropical Pruning”, the authors mentioned “we compare our tropical pruning approach against Class Blind (CB), Class Uniform (CU), and Class Distribution (CD) methods Han et al. (2015)”. What is Class Blind, Class Uniform and Class Distribution? There seems to be an error here “Figure 5 shows the pruning comparison between our tropical approach ...”, i think Figure 5 should be Figure 4. Q3: In the adversarial attack part, is the authors proposing a new attack method? If so, then the authors should report the test accuracy under attack. Also, the experimental results should not be restricted to MNIST dataset. I am also not sure about the attack settings here, the authors said “Instead of designing a sample noise ? such that (x0 + ?) belongs to a new decision region, one can instead fix x0 and perturb the network parameters to move the decision boundaries in a way that x0 appears in a new classification region.”. Why use this setting? Are there any intuitions? Since this is different from traditional adversarial attack terminology, the authors should stop using adversarial attacks as in “tropical adversarial attacks” because it is really misleading. =================================================================== Thanks the authors for the response. I still have two questions: Q1: The authors say that this theory provides a deeper understanding to Lottery Ticket Hypothesis (LTH). Then another paper “Rethinking the Value of Network Pruning” [1] suggests something different than LTH. [1] suggests that we do not need the initialization of large networks to train the pruned network from scratch to achieve high accuracy. Since the authors claim that their theory is related to LTH, then what would the proposed theory say about [1]? Q2: Since you redesign the task of adversarial attacks, I am still not convinced why this setting is interesting? The reason why people are interested in adversarial attacks is because it could happen during test time. What is the application of this setting? Why this new setting is important and worth studying? They are not clear to me. Also, as i wrote in my initial review, “the authors should stop using “adversarial attacks” as in “tropical adversarial attacks” because it is really misleading.”. I hope the authors can address this concern, or otherwise new readers may also find this part difficult to understand. [1] Rethinking the Value of Network Pruning. Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell. ICLR 2019.\n",
      "        - Review:###====== Updates ====== I appreciate the authors* time and effort in the response. I have read the rebuttal, but I am not convinced by the authors* argument on using L2 (or L_infty) constraints. No matter whether L2 or L_infty constraint is used, the authors* method is not directly comparable to methods in Song et al. (2018), making the results in Table 2 and Table 3 meaningless and confusing. - Song et al. (2018) indeed constraints the search region of latent code to be within a small L2 ball of a randomly sampled anchor latent code. However, this anchor latent code is not directly related to any given image in the dataset, and therefore the generated adversarial examples are not close to any existing image. In contrast, the authors* attack is still basically a norm-bounded attack, which is not directly comparable to the unrestricted attack in Song et al. (2018). - Song et al. (2018) is a white box attack, while the attack in this paper is black box. ====== Original review ======= This paper proposes to generate semantic preserving adversarial examples by first learning a manifold and then perturbing data along the manifold. In this way the generated adversarial examples can be semantically close to the original clean examples, and the perturbations can be hopefully more natural. For manifold learning, the authors propose to use a similar approach to that proposed in Pu et al. (2017), which uses SVGD to train a VAE. After the VAE is trained, the authors use GBSM to train a model to produce semantic adversarial examples efficiently. I have many concerns for this paper: - The approach is not well motivated. It is unclear why using a fully Bayesian framework and employing SVGD to learn the VAE model is preferred for conducting semantic adversarial attacks. Many choices in the algorithm seem to be arbitrary, and there are many approximations in the method whose accuracies have no guarantees. For example, the recognition networks are used to approximate the updated parameters of the encoder from SVGD. Sampling from the posterior distribution of z is approximated by first doing Monte Carlo over Theta. For *manifold alignment* another recognition network is used to approximate the updates from SVGD. It is hard to predict how those approximation errors accumulate when all pieces are combined together to form a very complicated algorithm. - In Equation (6) the authors hard-constrain the generated adversarial example such that they cannot differ from the original data by some pre-specified l_2-norm. This leads to many unfair comparisons in the experiments: 1. The authors compare their approach to other attacking methods on the success rates of attacking Madry*s model and Kolter & Wong*s certified model. However, both Madry and Kolter & Wong*s model are for attacks using the l_infinity norm. It is unfair that the authors* attack uses l_2 norm. In fact, it is known that models robust to l_infinity norm attacks are generally not robust to attacks using other norms. 2. The authors also compare their approach to methods in Song et al. (2018) and Zhao et al. (2018a). However, the two previous approaches did not directly constrain the distance between generated adversarial examples and the corresponding clean inputs. Therefore, when using human evaluation to assess the image quality of generated adversarial examples, the two previous methods are naturally at a huge disadvantage. In stark contrast, the authors* adversarial images are constrained to be close to the corresponding unperturbed images under a small l_2 norm, which naturally have higher image quality.\n",
      "        - Review:###The paper presents an approach to generating adversarial examples that preserve the semantics of the input examples. To do so, the approach reconstructs the manifold where the input examples lie and then generates new examples by perturbing the elements of the manifold so as to ensure the new elements remain in the manifold to preserve the semantics of the elements. In the presented system the manifold is learned by means of Stein Variational Gradient Descent, while the perturbation is made by applying the Gram-Schmidt process which ensures that the perturbed elements still reside in the manifold. To generate adversarial examples the approach presented in the paper considers a scenario in which only the predictions of the classifier are known, to be able to compute and optimize the loss function. The presented approach has been tested on toy examples regarding images (both numbers from MNIST or SVHN and images from CelebA datasets) and texts (SNLI dataset). The performance presented in the paper is promising. The results show that the manifold shape is preserved while creating perturbed elements. The system also achieves good results in terms of adversarial success rate, which however I believe should be called *attack success rate*, a term widely used in literature. The paper is well written and seems to me to be mathematically sound. I have very few comments on the paper which does not present any important lack in my opinion. I suggest moving algorithm 2 to a new page in order to have the whole pseudo-code together. Moreover, I have found two typos, one on page 4 in the last equation where z*_m I think it is wrongly written as z^*_m (the * is far from the z), and one on page 6 in the *adversarial examples* paragraph where the word *data* is misspelt as *dat*.\n",
      "    - Backdoor and Adversarial Attack Methods and Defenses (L0, 12)\n",
      "      * Examples:\n",
      "        - Review:###In general, this paper follows the min-max training framework for adversarial robustness. Instead of using a gradient-based attack to solve the inner maximization, the authors use a neural network to learn the attack results. From the experimental results, this method can effectively defend against CW and PGD on CIFAR-10 and CIFAR-100. But the clean accuracy is lower than Madry et al. Also, there are too many works on robustness defense that have been proven ineffective (consider the works by Carlini). Since this is a new way of robust training and there is no certified guarantee, I suggest the authors refer [1] to evaluate the effectiveness of the defense more thoroughly to convince the readers that it really works. Especially, a robustness evaluation under adaptive attacks is necessary. In other words, if the attacker knows the strategy used by the defender (such as the attacker network structure), it may be possible to break the model. I am not convinced by the limiting cycle claim in Figure 1. I do not think this scenario (gradient descent goes along a cycle) is possible. If we take the integral of the gradient along this cycle from x to itself, we will get 0=f(x)-f(x)=-int_{t on cycle}f*(t)dt<0, which means that the function is not continuous at x. I suggest the authors have a surface plot of the function if they think this is possible. [1] Carlini, Nicholas, et al. *On evaluating adversarial robustness.* arXiv preprint arXiv:1902.06705 (2019).\n",
      "        - Review:###This paper proposed a *learnable* trojan by training a neural network that takes a sample (e.g. an image) as an input and generates a programmable trigger pattern as an output. The authors argue that the proposed method can support dynamic and out scope target classes, which are particularly applicable to backdoor attacks in the transfer learning setting. The authors conducted experiments on large-scale models (ImageNet) in two settings: (1) outsourced training attack; (2) transfer learning attack. Although the idea of making a backdoor attack more robust (e.g. more transferrable) and programmable is interesting, I don*t think the current results fully substantiate the claimed benefits. Below are my concerns: 1. Lack of performance comparison: in the outsourced training attack, the attack success rate is quite low ( the best top-1 attack rate is ~50% on VGG). In the existing literature, the backdoor attack success rate can be made nearly 100%. This makes me wonder whether the low attack success rate only occurs in the proposed attacking method. Since there are no results from existing attacks, there is no way to evaluate how good the proposed attack is. 2. How about small dataset/model? I understand that the authors want to emphasize the scalability of their attack to large models like ImageNet. However, there are no comparisons on ImageNet (my comment 1). The authors are suggested to compare performance on standard datasets in backdoor attack literature (e.g. CIFAR-10, traffic sign) 3. It was unclear how *dynamic* the proposed method can be. Based on the attack formulation in equation (3), in order to make the attack *dynamic* in terms of changing different target classes, the attackers need to train a neural trojan network for every target class, which does not seem to be dynamic to me. Can the authors further justify the advantage of the dynamic feature in the proposed attack? And I have concerns about how many target classes an attacker can *dynamically* change. Some experiments showing the number of target classes vs attack performance and clean data accuracy will be very helpful. 4. The defense argument against detection methods is weak. Unless the authors can show the proposed attack has the ability to simultaneously backdoor all possible target classes, simply arguing the attack is dynamic and thus can evade detection is not convincing, not to mention in the backdoor setting, attacker should make the first move before the defender takes action. *** Post-rebuttal comments I thank the authors for the clarification. However, I feel my comments have not been fully addressed, especially on the part on justifying 50% attack success rate on VGG should be considered as significant in the considered setup. Without any valid comparisons, I find it difficult to assess the contributions. In addition, the authors did not add new empirical evidence regarding my questions but mainly re-iterated the applicability of the proposed method, so I will remain my review rating. ***\n",
      "        -  This paper proposes a general framework for constructing Trojan/Backdoor attacks on deep neural networks, specifically in cases where the end user plans to perform transfer learning on the backdoored classifier. To accomplish this, this work proposes the use of a trigger generator that is trained alongside the trojaned network, that allows the attacker to cause any image to be misclassified as any other. The proposed method is general, and is shown to work across a variety of datasets. Although the threat model here is slightly less general than the standard backdoor attacks threat model (in that the adversary here also needs to be involved in training the model, and not just supplying the data), the authors do well to motivate the threat model, and the attack is sufficiently general to be an interesting security model. The method is, to the best of my knowledge, novel, and is an innovative way to launch trigger attacks even when the adversary does not know what the final classes or task will be. The evaluation is thorough enough, though the results themselves could be better. However, as this is the first attack of this kind, the results seem sufficient to warrant publication. My major concern with this work is readability---many of the sections are in need of significant proofreading, and there are many grammatical/word choice mistakes that make the paper somewhat difficult to read. (For example, *P is derivative* in Section 4.2 should probably say *P is differentiable*; the use of frontend/backend is somewhat counterintuitive, as the backend should be the what is given to victims, and the victim trains a frontend reliant on the backend). The method itself could also be presented more clearly (section 4.2 specifically could use less equations and more exposition). Overall the paper requires significant written revision in order to be up to the standard of publication, but given that these concerns can be addressed in the revision period I am (weakly) recommending acceptance.\n",
      "    - Invertible ODE Methods for Robustness and Randomized Attacks (L0, 10)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposes a regularization method for achieving robustness to noisy inputs, with relatively less computation compared to standard data augmentation approaches. Specifically, the authors analyze the analytic expression of the loss on the noisy inputs, and using Jensen’s inequality, propose to minimize a surrogate loss over the expectation of noisy inputs. To minimize the loss over the expectation, the authors impose a regularization over the first moment of the network weights. The authors validate the model with the proposed regularization technique for its robustness against Gaussian attack and other types of attacks, whose results show that the model is robust. Pros - The general idea of the regularization that replaces the generation of noisy samples and optimization over it is conceptually appealing and seems practically useful. - The derivation of the moment-based regularization makes sense. - The proposed regularizer seems to be effective to a certain degree, on the sets of experiments done by the authors. Cons - Experimental validation seems highly inadequate due to lack of baselines. Thus it is difficult to assess the degree of robustness the proposed model achieves. The authors should perform extensive evaluation against state-of-the-art techniques against multiple types of attacks, in order to demonstrate the effectiveness of the proposed method. - While the authors emphasize the computational efficiency of the method, the authors do not report computational cost or actual runtime. - The types of non-Gaussian attacks should be better described. Which ones use L-infinity attacks and which use L2 attacks? - Figure 3 doesn’t seem like a very favorable result to the proposed model, since we are generally more concerned with adversarial examples generated with small perturbations, as large perturbations may change the input semantics. In sum, while I like the overall idea and find the work novel and potentially practical, it is difficult to properly evaluate the work due to lack of comparison against state-of-the-art data augmentation methods for achieving robustness. Therefore I temporarily give this paper a weak reject, but may change the rating with more experimental results provided in the rebuttal.\n",
      "        - Review:###Summary: The goal of this paper is to train neural networks (NNs) in a way to be robust to adversarial attacks. The authors formulate training a NN as finding an optimal controller for a discrete dynamical system. This formulation allows them to use an optimal control algorithm, called method of successive approximations (MSA), to train a NN. The authors then show how constraints can be added to this optimization problem in order to make the trained NN more robust. They show that the resulted constraint optimization problem can be formulated as a semi-definite programming and provide some experimental results. Comments: - Although the problem studied in the paper is important and the approach is interesting, it seems the paper has been written in rush and in my opinion is not ready for publication. The writing is not good. The introduction and related work sections are incomplete and not very informative. It is not clear what has been done before and what is the contribution of this paper. The main technique/algorithm of the paper has not been explained clearly that someone can easily understand and implement it. The experimental results are not convincing. - There are strong claims in the paper such as *experiments show that our method effectively improves deep model*s adversarial robustness*, this is too strong, given the quality of the experiments of the paper. Or *the constraint optimization problem can be formulated as a semi-definite programming (SDP) problem and hence can be solved efficiently*, to the best of my knowledge, SDP solvers are limited to small problems and cannot solve the large problems efficiently. - The area of making NNs robust to attacks is a very active area and there are many attacks and solutions out there, which require more comprehensive empirical studies of any new method. I do not see this in the paper. - Overall, I think this paper requires a major revision in order to be evaluated better and to be more useful for the community.\n",
      "        - Review:###This paper proposes applying randomization to the output layer of a DNN to defend against query-based attacks based on finite difference estimates. Then some theoretical analysis is provided, showing that with perturbation of a suitable scale, the randomization layer will not affect the accuracy of the model, while causing a large estimation error of finite difference methods that prevents finite-difference based attacks. Empirical results verify that the proposed defense is still effective against adaptive attacks where the randomness is averaged. Pros: The proposed method is simple, straightforward, yet novel. Its working mechanism is easy to understand and analyze, so it should be useful against finite-difference based attacks. Limitation: The proposed method is not useful to defense against white-box attacks and transfer-based attacks, since basically it does not change the predictive model. Some other randomization methods like [26], by contrast, change the predictive model, hence they may be useful against white-box attacks and transfer-based attacks. Questions and suggestions: This part is my main concern. It seems that the experimental results are very good. For example, in Figure 3a, the defense is effective even if sigma^2<1e-6. However, by the analysis in Section 4.2 (the formula below Line 3, Page 6), when sigma^2=1e-6, |E[g_i-gamma_i]| should be rather small, hence it should not block finite-difference based attacks. I think more explanation is needed for the good performance in the experiments. Finite differences are extremely sensitive to small random perturbation of the function value when the spacing (step size) h is small. For example, g_i=frac{L(f(x+he_i))-L(f(x-he_i))}{2h}, when h is very small, f(x+he_i) and f(x-he_i) is very close, hence adding perturbation to them will change g_i a lot. To present stronger adaptive attacks to output randomization, my suggestion is that a larger h can be adopted. It will be better if the results are investigated against attacks with different values of h. A mistake: In Section 4.1 on Page 4: *we can express the probability that x is misclassified in the vector d(p) as: sum_{i=2}^C P(d(p_i)>d(p_m))*. I think this is wrong, since P(A or B happens)=P(A happens)+P(B happens) only when A and B are mutually exclusive. However, *d(p_i)>d(p_m)* and *d(p_j)>d(p_m)* are not mutually exclusive. Hence, the probability that x is misclassified in the vector should be less than or equal to that sum of probabilities. By the way, the writing in Section 4.1 is not clear: - The overall misclassification probability is presented first, but after that K only represents the misclassification probability into a specific class. The connection between them is unclear. - At the beginning of Section 4.1, the distribution of epsilon is epsilonsimmathcal{N}(mu,sigma^2cdotmathbf{I}_C): a unique sigma is used. But after that, the variance of epsilon_i becomes sigma_i^2 instead of sigma^2. - In the second to the last line on Page 4, *level of noise (sigma^2) can be set for each class separately*, but the authors did not explain how to set them, and in the experiments sigma^2 is set as the same scalar. - In Figure 1b, the line style of *K=5.0e-3* and *K=1.0e-1* in the legend is very similar. The line style of *K=2.0e-01* in the legend is not clear: I do not know whether it refers to *-.-.-.* or *------*. Typos: Section 6, Page 8: *unintentionlly* => *unintentionally* Some missing spaces after punctuation: - Section 4.2, Page 5, *... gradient estimate.When the ...* => should add a space before *When* - Section 5, Page 7, *In addition,input randomization ...* => should add a space before *input*\n",
      "    - Robustness and Differential Privacy in Adversarial Machine Learning (L0, 15)\n",
      "      * Examples:\n",
      "        - Review:###The paper proposes a new way of adversarial training by placing another neural network called *attacker* network, and let the attacker to learn how to generate adversarial examples during training. This training scheme is formulated to solving a joint training according to min-max problem. Experimental results show that the method outperforms existing adversarial training in CIFAR-10/100 once the gradient information can be provided into the attacker network. In overall, the paper is well-written with an interesting message: There are certain features useful across all data samples in the inner maximization problem, which can be induced from gradient information. The experimental results are presented clearly, demonstrating its effectiveness and efficiency in running time. Section 4 seems to support the main claim in a novel way as well. The general motivation or justification on the proposed methods, e.g. the *limiting cycle* argument or the visualization part were not that convincing or seems slightly over-claimed to me, nevertheless. - I think adding a discussion on how to generalize the framework into other threat models, e.g. L2 or (even) unrestricted attacks would further strengthen the paper. I feel the current framework may suffers some training difficulties on these other threat models, even such kinds of discussion would also valuable to understand the method. - As the current formulation can generalize the general inner maximization optimization process, comparing or applying the method with a more recent form of adversarial training, e.g. TRADES, would be nice to demonstrate the general applicability of the method.\n",
      "        - Review:###This paper proposes a regularizer to encourage the robustness to the random contamination for training deep neural networks. The idea is straightforward and intuitive, but not that exciting. The experiments show some improvement. However, I have a few serious concerns: (1) Why do we care about the random noise, especially Gaussian noise? There has been a large amount of literature on training robust network, but they are also for the robustness to adversarial examples. The Gaussian noise is too simple and easy to defend. We can even apply a simple denoiser to preprocess the data, which does not even involve training a sophisticated neural network. (2) The proposed moment regularizer is very delicate. I do not think it can generalize to other noises or contaminations. This is because for other noises, the moment approximation can be fairly loose. (3) The Alexnet was proposed in 2011. Consider that it is already late 2019, the authors INDEED need to do experiments using more advanced and recent models, e.g., ResNet34/50 or even powerful ones, e.g, ResNeXt, DenseNet, Wide ResNet.\n",
      "        - Review:###Summary: This paper uses results from distributional robustness to provide bounds of p-norm-constrained adversarial risk which depend on the Lipschitz constant of the underlying classifier. The bulk of the paper focuses on sample-efficient mechanisms to approximate the Lipschitz constants of kernel methods so that a constraint on this Lip constant can be enforced during training. Empirically, the kernel methods are compared to existing deep learning approaches and are shown to be competitive at this scale. Overview: This is a technical paper which draws on theoretical results in Kernel methods, distributional robustness, and numerical approximation. I have been unable to verify the correctness of results from all of these areas but to the best of my knowledge the results seem reasonable. Despite the technical depth, the paper is well-written and largely easy to follow. 1) In section 3.1 you should emphasize the threat model more clearly. Distributional robustness may provide tools to evaluate very general forms of adversarial risk but in this work only the p-norm ball threat model is considered. This is a fine restriction and theoretical statements are made correctly but this does not encompass all forms of adversarial risk. 2) I had a hard time understanding Figure 1. The adversarial risk (Eqn 3) is upper-bounded by the variable-radius risk (Eqn 4 LHS) but Figure 1 shows that the adversarial risk of radius may permit larger perturbations than the variable-radius risk. Could you please explain how this Figure should be interpreted? The caption and main-text description of the Figure are very slim. 3) I like Figures 2 and 3 as they clearly highlight the benefits of both DRR and the proposed approximations in this work. This paper already presents thorough theoretical analysis but a further analysis into the conditions required for to guarantee tighter bounds on the Lipschitz constant would be a valuable addition. 4) Unfortunately I was unable to follow the analysis in Section 4.1 due to a lack of prior knowledge. To my understanding, the per-dimensional product factorization allows cheap exact computation of the off-diagonal elements of . This means that only the diagonal elements need be computed by the Nystr*om approximation. However, I cannot see intuitively how this prevents the sample complexity from scaling with the dimensionality (the number of diagonal elements still grows with ). 5) The experimental set-up is mostly standard with the exception that the CIFAR-10 robustness is measured with respect to a learned representation. This is a reasonable setting for validating the theoretical contributions of this work but is not a realistic interpretation of the adversarial threat model considered. The Lipschitz constant of the embedding function (the pre-trained ResNet) is not known and will likely lead to vacuous bounds (for robustness with respect to the input-space). 6) The deep learning experiments utilize the Parseval networks regularization scheme. What is the regularization constant ( ) used for this method? Typically, a small value is chosen which enables easier learning but prevents the Lipschitz constant from being tightly enforced during training. Is the final Lipschitz constant upper-bound computed from the learned weight matrices to ensure that the orthogonality constraint is not violated? 7) There is some disparity between the experimental results presented and the theory in the paper. In particular, this paper explores methods to provide robustness certificates to Lipschitz bounded classifiers but does not compute and validate these certificates empirically. 8) The gradient visualizations were surprising to me. The Par-MaxMin model has interpretable gradients in that the target class is visible through the gradients. However, for the Gauss-Lip model the gradients seem to only depend on the images current class and are suggestive of a form of gradient obfuscation. It may be useful to visualize *large perturbation* adversarial examples for the Gauss-Lip classifier --- what characteristics do these images have? Similarly, one could generate so-called *Distal Adversarials* which are generated by sampling random noise and optimizing them towards a target class. Does the Gauss-Lip model generate realistic looking images or is noise classified accurately? 9) Overall, the experiments seem sufficient to conclude that the Gauss-Lip model is a promising approach to adversarial learning. It is not clear how this method could be scaled to high dimensional data (while there are more obvious avenues for the deep learning alternatives). Minor: - In Theorem 1 statement, *If additionally if is convex*.\n",
      "  - Underwater Image Super-Resolution with GANs (L1, 24)\n",
      "    * Examples:\n",
      "      - Review:###The submission aims to analyze deep neural network (DNN) features in terms of how well they measure the perceptual severity of image distortions. It proposes to characterize each DNN feature in terms of two well known properties of the human visual system: a) sensitivity to changes in visual frequency and b) orientation selectivity. Both properties are evaluated with respect to the known human Contrast Sensitivity Function (CSF) and measured empirically from the feature’s response to (oriented) sinusoidal gratings. The results are quantified by a composite score termed Perceptual Efficacy (PE). In a set of comprehensive experiments (several pre-trained DNNs, several layers per DNN and two different datasets of distorted images with human perceptual quality annotations) it is demonstrated that feature representation consisting of a layer’s features with high PE better agree with human perceptual quality judgments than low PE feature representations from the same layer. I believe the submission convincingly demonstrates a statistical association between the proposed PE score of a DNN feature and human perceptual quality assessments. Though, it remains unclear whether the characteristics captured by the PE score are necessary or sufficient to explain the success of DNNs to guide image generation tasks by providing a perceptual loss function. For that I believe it is necessary to demonstrate that the present empirical results can be used to improve results of an image generation task, e.g. super-resolution. Furthermore the limits of the PE score could be explored by hand-crafting image representations with maximal PE score and comparing their usefulness in guiding e.g. a super-resolution task compared to a pre-trained DNN. Thus, overall I believe the submission reports interesting initial results but falls short of showing that they capture general properties that can be transferred to improving perceptual loss functions.\n",
      "      - Review:###I thank the authors for their detailed response. Some of my questions have been addressed in the rebuttal, but as far as I can tell very few modifications have been made to the paper: mainly an additional super-resolution experiment in the appendix, which goes in a good direction, but currently comes across as quite preliminary. So I think the paper still has most of the weaknesses I mentioned and thus it is not quite fit for publication. But I do encourage the authors to strengthen the experiments (for instance by addressing the points I raised, or by some other means) and resubmit elsewhere. --- The paper proposes an approach to analyzing the properties of *perceptual metrics* used in deep learning image generation methods. *Perceptual metrics* are computed by measuring distances between images not in the pixel space, but i na feature space of a pre-trained cNN. The proposed analysis method, inspired by studies of human perception, is based on measuring the response of these features to sinusoidal gratings of varying frequency or orientation. Based on these responses, the paper proposes a *Perceptual Efficacy Score* that should measure the importance of certain feature in the feature maps for the performance of a perceptual metric. Experiments show that indeed distances measured between features with high score better correlate with human judgement of image similarity than distances between features with a lower score. I find the paper quite interesting, but lean towards rejection at this point. This i smainly because the experiments seem somewhat anecdotal and incomplete, see below for further details. Pros: 1) Application of methods from psychology/neuroscience to artificial neural networks is an interesting avenue of work. Moreover, better unsderstanding of *perceptual metrics* is of wide interest for various image processing applications. 2) The proposed score seems to indeed correlate quite well with the importance of features for human judgement of image similarity. 3) Presentation is mainly clear. Cons: 1) Experiments are not very exhaustive and at times a bit confusing. For instance: 1a) Results are sometimes presented in a confusing way. In Figure 4 first of all it is not quite clear what points correspond to I guess each point is an image) and, second, it is not very obvious that the correlation is higher i none of the plots. In tables 1 and 2 it is confusing that different percentiles for H and L are used for different networks/layers. Is this based on some tuning? Then the tuning process should be clearly explained. Moreover, it might be useful to report the full curves of performance as a function of the percentage of features used. 1b) There are no baselines and there is not much justification of computing the *Perceptual Efficacy* score the way it is computed. What if one uses only the orientation-based score? Or only the frequency-based? What if one selects the most relevant features in a data-driven way (based on correlation on a training set)? What if one selects subsets of features randomly? 1c) While the method is inspired by methods used for studying natural vision systems, there is no connection to human experiments. It would be interesting to see a comparison of frequency and orientation tuning of features in a CNN to human cells (as I understand, the latter should be available in prior works?). 1d) It would be great to see the selected features be used not only for offline image similarity assessment, but also for training image processing models - in the end, this has been the main use of *perceptual metrics*. Do they lead to improved results? 1e) Since the paper is about (subjective) image quality, it might be useful to show some qualitative results, potentially in the appendix if space is an issue. 2) There are some issues with the presentation: 2a) I had a hard time understanding what exactly *Contrast Sensitivity Function* and *contrast masking* are. 2b) Minor issues: - In the abstract: *trained object detection deep CNNs* - I guess image classification is meant - Beginning of Section 2: *Section. 2*, *convolution layer as collection channels* - Section 4: *corresponds the the peak: - Section 5.2 *Berkeley-Adobpe*\n",
      "      - Review:###The main contribution of the paper is the extension of techniques for weakly supervised localization, i.e. given ground truth counts of objects in a given image, one can do training to generate hidden layer density maps that allow for feature detection and localization of objects. The main contribution of the paper seems to be the regularization of the density map by incorporation of a Gini impurity penalty and the contrasting of the regularizer against beta-variational autoencoder formulation. The experiments show the utility of the method in a toy example and an example involving pedestrians in video surveillance. While the extensions are interesting, I find that the paper contributions are incremental and do not offer sufficient experimental results. The experiments are qualitative (just illustrations of results on a few images). While I understand the context of the contribution from authors point of view and can appreciate the study of weakly-supervised learning, I find it important to ask how the present framework compares with a traditional method that uses background subtraction (thus gaining appearance invariance to an extent) followed by geometric prior driven indexing of counts and locations (see for example: Lan Dong et al (ICCV 2007 - Fast Crowd Segmentation via Shape Indexing). The inductive bias and interpretability of design is extremely transparent and obvious. Can you please elaborate how your work compares in performance to such a technique and what you mean by feature interpretability in your context? My rating is based on the fact that the results are preliminary and further quantitative experimentation is needed.\n",
      "Calculated maximum depth for CSV columns: 3\n",
      "\n",
      "Saving indented hierarchy data to hierarchy.csv...\n",
      "Save complete.\n",
      "NOTE: View 'hierarchy.csv' in spreadsheet software for indentation effect.\n",
      "\n",
      "Processing Complete.\n"
     ]
    }
   ],
   "source": [
    "print_clusters(dfr_review, 'review', 'hierarchy.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a655c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0b99ced",
   "metadata": {},
   "source": [
    "### Add a refinement step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08127622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 12:16:53,674 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Response:\n",
      "Certainly. Below, I present a **Refined Cluster Hierarchy** for your dataset. The key steps taken were:\n",
      "\n",
      "- **Merging Highly Similar or Redundant Clusters**: Many cluster names described nearly the same content but with slight differences; I combined/standardized these as much as possible.\n",
      "- **Standardizing Terminology**: I adopted more conventional or concise titles for major themes.\n",
      "- **Condensing where Appropriate**: Some \"leaf\" clusters were merged if they appeared to be broken-up subtopics of a broader area.\n",
      "- **Providing the Hierarchical Structure**: The refined hierarchy is explicitly presented as a tree.\n",
      "\n",
      "---\n",
      "\n",
      "# REFINED CLUSTER HIERARCHY\n",
      "\n",
      "**Level 2 (Top Level):**\n",
      "\n",
      "1. Neural Network Architectures, Training, and Efficiency  \n",
      "2. Evaluation, Benchmarking, and Theoretical Analysis  \n",
      "3. Learning Methodologies and Algorithms  \n",
      "4. Robustness, Privacy, and Security  \n",
      "5. Physical GRF Data Analysis and Experiments  \n",
      "6. Specialized Applications (GANs, OOD, etc.)\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Neural Network Architectures, Training, and Efficiency\n",
      "\n",
      "#### 1.1 Architecture, Pruning, and Model Efficiency\n",
      "- Neural Network Architectures and Methods\n",
      "- Pruning and Sparsity in Neural Networks\n",
      "- Neural Architecture Search and Weight Sharing\n",
      "- Distillation, Compression, and Model Simplification\n",
      "\n",
      "#### 1.2 Training, Optimization, and Analysis Techniques\n",
      "- Gradient-based Architecture Search and Optimization\n",
      "- Deep Neural Network Training Methods and LBI Approaches\n",
      "- Optimization Algorithms/Convergence in Stochastic Methods\n",
      "- Gradient Normalization and Variance Reduction\n",
      "\n",
      "#### 1.3 Representation Learning and Feature Extraction\n",
      "- Feature Extraction and Representation Methods\n",
      "- Audio Feature Analysis and Spectrogram Methods\n",
      "- Scheduled Sampling and Initialization\n",
      "- 3D Generative Models and VAEs\n",
      "- Prototype-based Attention for Classification\n",
      "- Language Modeling and Summarization (Enc-Dec)\n",
      "- Model Unification and Sequence Positioning (PDP)\n",
      "- Amharic (or Multilingual) Embeddings and Sentiment Analysis\n",
      "- Falcon and Quantization Techniques in PDF/Tree Models\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Evaluation, Benchmarking, and Theoretical Analysis\n",
      "\n",
      "#### 2.1 Evaluation of Learning Methods\n",
      "- Evaluation and Benchmarking of Algorithms\n",
      "- Evaluation of Dropout/Data Augmentation/BatchNorm\n",
      "- OOD Data Imputation and Evaluation\n",
      "- Evaluation of Few-Shot/Self-Supervised Methods\n",
      "- Evaluation of Proposed Method Performance\n",
      "\n",
      "#### 2.2 Theoretical Foundations and Analytical Methods\n",
      "- Statistical Dependence Testing and Neuroimaging\n",
      "- Information Theory and Entropy Estimation\n",
      "- Lipschitz Analysis, Generalization, and Theoretical Bounds\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Learning Methodologies and Algorithms\n",
      "\n",
      "#### 3.1 Reinforcement & Sequential Decision Learning\n",
      "- Reinforcement Learning and Policy Optimization\n",
      "- Reward Shaping and Intrinsic Motivation\n",
      "- Multi-Agent and Game-Theoretic RL\n",
      "- Exploration Strategies in RL\n",
      "\n",
      "#### 3.2 Reinforcement Learning Algorithms and Stability Analysis\n",
      "- Policy Learning and Decision Making in RL\n",
      "- Lyapunov-Based Stability in RL Algorithms\n",
      "\n",
      "#### 3.3 Supervision & Labeling Methods\n",
      "- Supervision and Weak Supervision in Training\n",
      "- Program Synthesis and Human-Labeled Datasets\n",
      "- Weakly Supervised Object Counting and Recognition\n",
      "\n",
      "#### 3.4 Graph-Based Learning Methods\n",
      "- Graph Neural Networks (GNNs) and Attention-based Pooling\n",
      "- Graph Community Detection\n",
      "- Graph Model Feature Analysis\n",
      "\n",
      "#### 3.5 Similarity Search, Random Projection, and Model Search\n",
      "- Similarity Search and Random Projection Methods\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Robustness, Privacy, and Security\n",
      "\n",
      "#### 4.1 Adversarial Robustness and Attack Defenses\n",
      "- Adversarial Attacks and Robustness in ML\n",
      "- Backdoor Attacks and Defenses\n",
      "- Robustness and Differential Privacy\n",
      "- Invertible ODEs for Robustness and OOD Detection\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Physical GRF Data Analysis and Experiments\n",
      "- Methods and Experiments in GRF Data Analysis\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Specialized Applications\n",
      "- Underwater Image Super-Resolution with GANs  \n",
      "- Other unique domains (if not fitting above)\n",
      "\n",
      "---\n",
      "\n",
      "## **NOTES on Mapping**\n",
      "\n",
      "- **Many repetitive cluster names** (esp. for neural networks/convergence/optimization/etc) have been unified.\n",
      "- **Falcon/Quantization/PDF Trees** merged into \"Falcon and Quantization Techniques in PDF/Tree Models\".\n",
      "- **Distillation/Compression/Model Simplification** merged under one node.\n",
      "- **Adversarial/Robustness/Privacy/Security** unified given strong thematic overlap.\n",
      "- **GANs/Underwater Imaging/OOD-Related Works** placed under a \"Specialized Application\" branch for unique domains, unless they clearly fit elsewhere.\n",
      "\n",
      "---\n",
      "\n",
      "### **If you want to see how any particular original label or row maps to the refined cluster, let me know!**\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from jinja2 import Template\n",
    "\n",
    "# Sample data representing the cluster hierarchy tree.\n",
    "\n",
    "# Define a jinja2 template to create the prompt.\n",
    "prompt_template = \"\"\"\n",
    "You are given a cluster hierarchy tree where each row represents a series of cluster names from the bottom level (leaf, _level_0) to higher levels (_level_1, _level_2, etc.). The current hierarchy is as follows:\n",
    "\n",
    "{% for index, row in df.iterrows() %}\n",
    "Row {{ index + 1 }}:\n",
    "{% for level in levels %}\n",
    "  {{ level }}: \"{{ row[level] }}\"\n",
    "{% endfor %}\n",
    "{% endfor %}\n",
    "\n",
    "Please refine this cluster hierarchy. In your refined hierarchy, modify names if necessary and merge similar clusters (i.e. merge nodes in the tree that represent similar themes) while preserving the hierarchical structure.\n",
    "\n",
    "Return the refined hierarchy in a clear, structured format.\n",
    "\"\"\"\n",
    "df_remap = dfr_review[[c for c in dfr_review.columns if 'level_' in c]] \n",
    "# List the hierarchy levels in order (assuming the DataFrame columns are ordered as the hierarchy).\n",
    "levels = list(df_remap.columns)\n",
    "\n",
    "# Render the prompt using the jinja2 template.\n",
    "template = Template(prompt_template)\n",
    "prompt = template.render(df=df_remap, levels=levels)\n",
    "\n",
    "# # Log the prompt (for debugging purposes) before sending it to the LLM.\n",
    "# print(\"Prompt sent to the LLM:\")\n",
    "# print(prompt)\n",
    "\n",
    "# Call the LLM with the prompt.\n",
    "# Assume that 'llm' is an instance of your LLM class with _call implemented.\n",
    "refined_hierarchy = llm._call(prompt)\n",
    "\n",
    "print(\"\\nLLM Response:\")\n",
    "print(refined_hierarchy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43099dde",
   "metadata": {},
   "source": [
    "2025-04-16 12:16:53,674 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
    "\n",
    "**LLM Response:**\n",
    "\n",
    "Certainly. Below, I present a **Refined Cluster Hierarchy** for your dataset. The key steps taken were:\n",
    "\n",
    "- **Merging Highly Similar or Redundant Clusters**: Many cluster names described nearly the same content but with slight differences; I combined/standardized these as much as possible.\n",
    "- **Standardizing Terminology**: I adopted more conventional or concise titles for major themes.\n",
    "- **Condensing where Appropriate**: Some \"leaf\" clusters were merged if they appeared to be broken-up subtopics of a broader area.\n",
    "- **Providing the Hierarchical Structure**: The refined hierarchy is explicitly presented as a tree.\n",
    "\n",
    "---\n",
    "\n",
    "# REFINED CLUSTER HIERARCHY\n",
    "\n",
    "**Level 2 (Top Level):**\n",
    "\n",
    "1. Neural Network Architectures, Training, and Efficiency  \n",
    "2. Evaluation, Benchmarking, and Theoretical Analysis  \n",
    "3. Learning Methodologies and Algorithms  \n",
    "4. Robustness, Privacy, and Security  \n",
    "5. Physical GRF Data Analysis and Experiments  \n",
    "6. Specialized Applications (GANs, OOD, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Neural Network Architectures, Training, and Efficiency\n",
    "\n",
    "#### 1.1 Architecture, Pruning, and Model Efficiency\n",
    "- Neural Network Architectures and Methods  \n",
    "- Pruning and Sparsity in Neural Networks  \n",
    "- Neural Architecture Search and Weight Sharing  \n",
    "- Distillation, Compression, and Model Simplification\n",
    "\n",
    "#### 1.2 Training, Optimization, and Analysis Techniques\n",
    "- Gradient-based Architecture Search and Optimization  \n",
    "- Deep Neural Network Training Methods and LBI Approaches  \n",
    "- Optimization Algorithms/Convergence in Stochastic Methods  \n",
    "- Gradient Normalization and Variance Reduction\n",
    "\n",
    "#### 1.3 Representation Learning and Feature Extraction\n",
    "- Feature Extraction and Representation Methods  \n",
    "- Audio Feature Analysis and Spectrogram Methods  \n",
    "- Scheduled Sampling and Initialization  \n",
    "- 3D Generative Models and VAEs  \n",
    "- Prototype-based Attention for Classification  \n",
    "- Language Modeling and Summarization (Enc-Dec)  \n",
    "- Model Unification and Sequence Positioning (PDP)  \n",
    "- Amharic (or Multilingual) Embeddings and Sentiment Analysis  \n",
    "- Falcon and Quantization Techniques in PDF/Tree Models\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Evaluation, Benchmarking, and Theoretical Analysis\n",
    "\n",
    "#### 2.1 Evaluation of Learning Methods\n",
    "- Evaluation and Benchmarking of Algorithms  \n",
    "- Evaluation of Dropout/Data Augmentation/BatchNorm  \n",
    "- OOD Data Imputation and Evaluation  \n",
    "- Evaluation of Few-Shot/Self-Supervised Methods  \n",
    "- Evaluation of Proposed Method Performance\n",
    "\n",
    "#### 2.2 Theoretical Foundations and Analytical Methods\n",
    "- Statistical Dependence Testing and Neuroimaging  \n",
    "- Information Theory and Entropy Estimation  \n",
    "- Lipschitz Analysis, Generalization, and Theoretical Bounds\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Learning Methodologies and Algorithms\n",
    "\n",
    "#### 3.1 Reinforcement & Sequential Decision Learning\n",
    "- Reinforcement Learning and Policy Optimization  \n",
    "- Reward Shaping and Intrinsic Motivation  \n",
    "- Multi-Agent and Game-Theoretic RL  \n",
    "- Exploration Strategies in RL\n",
    "\n",
    "#### 3.2 Reinforcement Learning Algorithms and Stability Analysis\n",
    "- Policy Learning and Decision Making in RL  \n",
    "- Lyapunov-Based Stability in RL Algorithms\n",
    "\n",
    "#### 3.3 Supervision & Labeling Methods\n",
    "- Supervision and Weak Supervision in Training  \n",
    "- Program Synthesis and Human-Labeled Datasets  \n",
    "- Weakly Supervised Object Counting and Recognition\n",
    "\n",
    "#### 3.4 Graph-Based Learning Methods\n",
    "- Graph Neural Networks (GNNs) and Attention-based Pooling  \n",
    "- Graph Community Detection  \n",
    "- Graph Model Feature Analysis\n",
    "\n",
    "#### 3.5 Similarity Search, Random Projection, and Model Search\n",
    "- Similarity Search and Random Projection Methods\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Robustness, Privacy, and Security\n",
    "\n",
    "#### 4.1 Adversarial Robustness and Attack Defenses\n",
    "- Adversarial Attacks and Robustness in ML  \n",
    "- Backdoor Attacks and Defenses  \n",
    "- Robustness and Differential Privacy  \n",
    "- Invertible ODEs for Robustness and OOD Detection\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Physical GRF Data Analysis and Experiments\n",
    "\n",
    "- Methods and Experiments in GRF Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Specialized Applications\n",
    "\n",
    "- Underwater Image Super-Resolution with GANs  \n",
    "- Other unique domains (if not fitting above)\n",
    "\n",
    "---\n",
    "\n",
    "## **NOTES on Mapping**\n",
    "\n",
    "- **Many repetitive cluster names** (esp. for neural networks/convergence/optimization/etc) have been unified.\n",
    "- **Falcon/Quantization/PDF Trees** merged into \"Falcon and Quantization Techniques in PDF/Tree Models\".\n",
    "- **Distillation/Compression/Model Simplification** merged under one node.\n",
    "- **Adversarial/Robustness/Privacy/Security** unified given strong thematic overlap.\n",
    "- **GANs/Underwater Imaging/OOD-Related Works** placed under a \"Specialized Application\" branch for unique domains, unless they clearly fit elsewhere.\n",
    "\n",
    "---\n",
    "\n",
    "### **If you want to see how any particular original label or row maps to the refined cluster, let me know!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc32c56",
   "metadata": {},
   "source": [
    "## Cluster and Synthesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbcaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============\n",
      "Level _level_0 has 200 unique labels. When clustered, they become 9 clusters.\n",
      "Processing cluster 1 of 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:53:53,386 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 2 of 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:54:02,055 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 3 of 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:54:08,833 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 4 of 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:54:18,157 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 5 of 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:54:26,322 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 00:54:34,488 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 6 of 9...\n",
      "Processing cluster 7 of 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:54:43,324 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 00:54:52,050 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 8 of 9...\n",
      "Processing cluster 9 of 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:55:00,525 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have gone from 200 clusters to 65 clusters.\n",
      "\n",
      "============\n",
      "Level _level_1 has 65 unique labels. When clustered, they become 4 clusters.\n",
      "Processing cluster 1 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:55:12,491 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 2 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:55:16,808 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 3 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:55:20,697 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 4 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:55:27,303 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 00:55:32,130 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have gone from 65 clusters to 19 clusters.\n",
      "\n",
      "============\n",
      "Level _level_2 has 19 unique labels. When clustered, they become 2 clusters.\n",
      "Processing cluster 1 of 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:55:36,945 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 2 of 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 00:55:45,454 - INFO - Final Level _level_2 has 19 clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping synthesis at level _level_3 with 9 clusters.\n"
     ]
    }
   ],
   "source": [
    "# dfr_cs = pd.read_csv('reject_reviews_clustered.csv')\n",
    "SYNTHESIS_NAMING_PROMPT_TEMPLATE = '''\n",
    "you are an program committee chair of a scientific conference.\n",
    "Your task is to identify the main issues in a list of reviews, and to group them into higher level groups.\n",
    "This is an ENUMERATED list of reviews, in the format NUMBER: REVIEW. \n",
    "Your job is to reduce the list to a smaller number of ISSUEs raised by the reviewers. \n",
    "Be as specific as possible. \n",
    "The output is a list of GROUP_NAME that are at a higher level of abstaction and that group together at least two underlying issues.\n",
    "Try to identify groups that put together a minimum of two, but ideally many similar issues, and give them a name that is descriptive of the group and of the differences between\n",
    "items in that group and items in other groups. \n",
    "along with a mapping of the original ISSUE_NUMBERs to the new GROUP_NAME.\n",
    "Only return the map if it contains more than one ISSUE_NUMBERs, that is, if we can actually group some issues into a higher level pattern.\n",
    "Make sure every ISSUE_NUMBER is assigned to a group, and make an effort to group every issue into a higher level group.\n",
    "\n",
    "These is the enumerated list of issues:\n",
    "<ENUMERATED_ISSUE_LIST>\n",
    "{{text_to_synthesize}}\n",
    "</ENUMERATED_ISSUE_LIST>\n",
    "\n",
    "First, briefly describe your reasoning, and then return a JSON dict object. The answer should have the following structure:\n",
    "REASONING: <YOUR REASONING>\n",
    "JSON GROUPS:\n",
    "{\n",
    "  \"<GROUP_NAME_1>\": [\"<ISSUE_NUMBER>\", .., \"<ISSUE_NUMBER>\"],\n",
    "  \"<GROUP_NAME_2>\": [\"<ISSUE_NUMBER>\" .., \"<ISSUE_NUMBER>\"],\n",
    "}\n",
    "Please adhere to the format described in the JSON part of the answer: each json key must contain a list of numbers corresponding to the ISSUE_IDs to be grouped under that key identifying the group name.\n",
    "Do not include special characters in the answer besides the JSON format.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfr_cs_frombase_nomap = df[:200].copy()\n",
    "# dfr_cs_frombase_nomap['_level_0'] = dfr_cs_frombase_nomap['review'].astype(str)\n",
    "dfr_cs_frombase_nomap.loc[:, '_level_0'] = dfr_cs_frombase_nomap['review'].astype(str)\n",
    "dfr_cs_frombase_nomap = build_cluster_hierarchy(dfr_cs_frombase_nomap, llm, '_level_0', prompt_template = SYNTHESIS_NAMING_PROMPT_TEMPLATE, MAX_TOP_LEVEL_CLUSTERS=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4b113f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Tree...\n",
      "Detected and using levels (highest to lowest): ['_level_3', '_level_2', '_level_1', '_level_0']\n",
      "\n",
      "Printing Tree Structure:\n",
      "Warning: LEVELS_TO_PRINT (6) is greater than total levels available (4). Printing all levels.\n",
      "- Clarity and Presentation (notation, terminology, formatting, equation definition, and algorithm organization) (L3, 1)\n",
      "  - Review:###The paper presents an approach to generating adversarial examples that preserve the semantics of the input examples. To do so, the approach reconstructs the manifold where the input examples lie and then generates new examples by perturbing the elements of the manifold so as to ensure the new elements remain in the manifold to preserve the semantics of the elements. In the presented system the manifold is learned by means of Stein Variational Gradient Descent, while the perturbation is made by applying the Gram-Schmidt process which ensures that the perturbed elements still reside in the manifold. To generate adversarial examples the approach presented in the paper considers a scenario in which only the predictions of the classifier are known, to be able to compute and optimize the loss function. The presented approach has been tested on toy examples regarding images (both numbers from MNIST or SVHN and images from CelebA datasets) and texts (SNLI dataset). The performance presented in the paper is promising. The results show that the manifold shape is preserved while creating perturbed elements. The system also achieves good results in terms of adversarial success rate, which however I believe should be called *attack success rate*, a term widely used in literature. The paper is well written and seems to me to be mathematically sound. I have very few comments on the paper which does not present any important lack in my opinion. I suggest moving algorithm 2 to a new page in order to have the whole pseudo-code together. Moreover, I have found two typos, one on page 4 in the last equation where z*_m I think it is wrongly written as z^*_m (the * is far from the z), and one on page 6 in the *adversarial examples* paragraph where the word *data* is misspelt as *dat*. (L2, 1)\n",
      "    * Examples:\n",
      "      - Review:###The paper presents an approach to generating adversarial examples that preserve the semantics of the input examples. To do so, the approach reconstructs the manifold where the input examples lie and then generates new examples by perturbing the elements of the manifold so as to ensure the new elements remain in the manifold to preserve the semantics of the elements. In the presented system the manifold is learned by means of Stein Variational Gradient Descent, while the perturbation is made by applying the Gram-Schmidt process which ensures that the perturbed elements still reside in the manifold. To generate adversarial examples the approach presented in the paper considers a scenario in which only the predictions of the classifier are known, to be able to compute and optimize the loss function. The presented approach has been tested on toy examples regarding images (both numbers from MNIST or SVHN and images from CelebA datasets) and texts (SNLI dataset). The performance presented in the paper is promising. The results show that the manifold shape is preserved while creating perturbed elements. The system also achieves good results in terms of adversarial success rate, which however I believe should be called *attack success rate*, a term widely used in literature. The paper is well written and seems to me to be mathematically sound. I have very few comments on the paper which does not present any important lack in my opinion. I suggest moving algorithm 2 to a new page in order to have the whole pseudo-code together. Moreover, I have found two typos, one on page 4 in the last equation where z*_m I think it is wrongly written as z^*_m (the * is far from the z), and one on page 6 in the *adversarial examples* paragraph where the word *data* is misspelt as *dat*.\n",
      "- Clarity, Reproducibility, and Presentation of Methods (L3, 65)\n",
      "  - Clarity and Reproducibility of Technical and Methodological Details (L2, 24)\n",
      "    - Confusing or incomplete modeling/notation (L1, 4)\n",
      "      - Review:###------------- updated after rebuttal ------------------- I thank the authors for clarifying and correcting the notations in Lemma 3. Though I still think the current state of the derivation is presented in a suboptimal way, and as a result, can be misleading to people. The Fourier analysis used to give the results that the exact gradient equals (eq. 5) is totally unnecessary: Despite it might seem fancy as a Fourier coefficient, it is just another way of writing the local expectation estimator (Tokui, S., & Sato, I., 2017), if we expand it using the definition . The authors argue that the Fourier analysis is essential to show the bias of the estimator. However, the only conclusion they draw from Fourier analysis is eq.5. And all the bias analysis follows by using Taylor expansions of it. The paper can be greatly simplified if they remove all boolean analysis parts and start from eq. 5 (which has a straightforward proof), using the conventional notation instead of Fourier coefficients. During writing this, I read the bias correction section again and had another concern, the bias correction effect is only justified for functions with small mixed degree terms: *For functions with small mixed degree terms, this can lead to bias reduction, at the cost of an increased variance because of sampling an auxiliary variable* For general multivariate functions, it is even not clear whether the proposed estimator has a smaller bias than the straight-through one. This weakness has a deep reason behind it because they are trying to generalize a bias reduction technique from a univariate function to multivariate functions, which, if done natively, would require K evaluations of the function (K is the number of input dimensions) (as I pointed out in the original review). Overall, I argue rejecting the paper in its current form. ---------------------------------------------------------------- This is not my first time reviewing this paper. Previous concerns on clarity has been addressed and the paper is now more readable. Though I still believe that the boolean analysis part is unnecessary for deriving the final estimator (which can be easily derived from the exact local-expectation estimator E_p[f(z_i=1) - f(z_i=0)] and applying f(1) - f(0) = int_0^1 f*(x) dx approx f(e), e~Unif[0,1].) plus some importance sampling trick. I think the proof of Lemma 3 is incorrect (though the conclusion is correct). f is never multi-linear in the continuous space. It is only for the boolean space, that f has an multi-linear form with Fourier expansion. So the claim that f is multi-linear then E_{p(z}}[f(z_1, ..., z_n)] = f(mu_1, ..., mu_n) is incorrect. This can only be true when f is also linear in the continuous space (which is not true for typical vaes). (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###------------- updated after rebuttal ------------------- I thank the authors for clarifying and correcting the notations in Lemma 3. Though I still think the current state of the derivation is presented in a suboptimal way, and as a result, can be misleading to people. The Fourier analysis used to give the results that the exact gradient equals (eq. 5) is totally unnecessary: Despite it might seem fancy as a Fourier coefficient, it is just another way of writing the local expectation estimator (Tokui, S., & Sato, I., 2017), if we expand it using the definition . The authors argue that the Fourier analysis is essential to show the bias of the estimator. However, the only conclusion they draw from Fourier analysis is eq.5. And all the bias analysis follows by using Taylor expansions of it. The paper can be greatly simplified if they remove all boolean analysis parts and start from eq. 5 (which has a straightforward proof), using the conventional notation instead of Fourier coefficients. During writing this, I read the bias correction section again and had another concern, the bias correction effect is only justified for functions with small mixed degree terms: *For functions with small mixed degree terms, this can lead to bias reduction, at the cost of an increased variance because of sampling an auxiliary variable* For general multivariate functions, it is even not clear whether the proposed estimator has a smaller bias than the straight-through one. This weakness has a deep reason behind it because they are trying to generalize a bias reduction technique from a univariate function to multivariate functions, which, if done natively, would require K evaluations of the function (K is the number of input dimensions) (as I pointed out in the original review). Overall, I argue rejecting the paper in its current form. ---------------------------------------------------------------- This is not my first time reviewing this paper. Previous concerns on clarity has been addressed and the paper is now more readable. Though I still believe that the boolean analysis part is unnecessary for deriving the final estimator (which can be easily derived from the exact local-expectation estimator E_p[f(z_i=1) - f(z_i=0)] and applying f(1) - f(0) = int_0^1 f*(x) dx approx f(e), e~Unif[0,1].) plus some importance sampling trick. I think the proof of Lemma 3 is incorrect (though the conclusion is correct). f is never multi-linear in the continuous space. It is only for the boolean space, that f has an multi-linear form with Fourier expansion. So the claim that f is multi-linear then E_{p(z}}[f(z_1, ..., z_n)] = f(mu_1, ..., mu_n) is incorrect. This can only be true when f is also linear in the continuous space (which is not true for typical vaes).\n",
      "      - Review:###Although I assume somebody well-versed in the recent collaborative filtering literature would not have trouble, I had too much difficulty understanding the setup and the model to be able to recommend the paper for acceptance. Perhaps if you clarify the following questions in your rebuttal and the final version of the paper, I can view the paper more favorably: - In the Problem Statement section, it says that we assume that there is an unknown user preference vector a_i for each user with elements in reals. How should we think about what a_i is? Is it a binary vector? A probability distribution over items? An arbitrary vector of reals? Nonnegative reals? - In the problem statement, we have that *recommender predicts preferred items . . . and the user generates feedback to the recommender.* Can you make this concrete? Would feedback be like a binary label for whether each of the recommended items was of interest? Or whether any were of interest? - In general, I had a very difficult time figuring out the difference between the information in x_i and *feedback*. Is it that x_i only has information about what user i selected, but no information about what the user was presented and did not select? Is the feedback then the combination of what was presented and whether or not it was selected? - In Section 2.2, we discuss a_i^t as the recommended items provided by the recommender, sampled from the action space A -- this seems like a different a_i from the one in the Problem Statement section? If so, this is an unfortunate notation clash. - If we*re producing sets of items at a time, as your wording frequently indicates, should we think of the action space A as a set of sets of items? - When describing the *recommendation-feedback loop as a Markov Decision Process*, you introduce a transition probability over user preferences given actions. Are you understanding a user*s preferences to be changing over time? Or is it just our estimate of the user*s preferences that are changing over time? - In Figure 2, does the Recommender have state that can accumulate all the virtual feedbacks v_i in each step? - After equation (1), it says that v_i is *the embedding of user feedback to historical recommended items*, but in Figure 2 it seems like v_i can only have information about the preceding recommendation and estimated reward, rather than cumulative. Can you clarify? - Is the reward estimator estimating something that could actually be observed? Suppose we had a user in the loop -- can you give an example of what a reward would be for a particular set of actions? Would it be 0/1 for whether the user clicked on one of the actions? - In the setup for the learning task, Section 3.1 says that x_i is the *historical behaviors of user i derived from the user-item matrix X and \tilde{a}_i is the ground truth of the recommended item for the user based on his/her behavior x_i.* I don*t understand this. x_i has all the items the user has clicked. What new information is in \tilde{a}_i? Is it referring to the a_i in the problem statement, some vector representing preferences? Is such a vector ever known during training? Please clarify. - In the paragraph before equation 7, you say *given the recommended policy and the feedback generator* -- should that be *given the recommender policy*? - In Figure 2, we see the recommender producing what seems to be a vector a_i. Is this a set of recommendations? Or a distribution over items from which we could sample recommendations? Or a single recommendation, in which case the graphic should change? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Although I assume somebody well-versed in the recent collaborative filtering literature would not have trouble, I had too much difficulty understanding the setup and the model to be able to recommend the paper for acceptance. Perhaps if you clarify the following questions in your rebuttal and the final version of the paper, I can view the paper more favorably: - In the Problem Statement section, it says that we assume that there is an unknown user preference vector a_i for each user with elements in reals. How should we think about what a_i is? Is it a binary vector? A probability distribution over items? An arbitrary vector of reals? Nonnegative reals? - In the problem statement, we have that *recommender predicts preferred items . . . and the user generates feedback to the recommender.* Can you make this concrete? Would feedback be like a binary label for whether each of the recommended items was of interest? Or whether any were of interest? - In general, I had a very difficult time figuring out the difference between the information in x_i and *feedback*. Is it that x_i only has information about what user i selected, but no information about what the user was presented and did not select? Is the feedback then the combination of what was presented and whether or not it was selected? - In Section 2.2, we discuss a_i^t as the recommended items provided by the recommender, sampled from the action space A -- this seems like a different a_i from the one in the Problem Statement section? If so, this is an unfortunate notation clash. - If we*re producing sets of items at a time, as your wording frequently indicates, should we think of the action space A as a set of sets of items? - When describing the *recommendation-feedback loop as a Markov Decision Process*, you introduce a transition probability over user preferences given actions. Are you understanding a user*s preferences to be changing over time? Or is it just our estimate of the user*s preferences that are changing over time? - In Figure 2, does the Recommender have state that can accumulate all the virtual feedbacks v_i in each step? - After equation (1), it says that v_i is *the embedding of user feedback to historical recommended items*, but in Figure 2 it seems like v_i can only have information about the preceding recommendation and estimated reward, rather than cumulative. Can you clarify? - Is the reward estimator estimating something that could actually be observed? Suppose we had a user in the loop -- can you give an example of what a reward would be for a particular set of actions? Would it be 0/1 for whether the user clicked on one of the actions? - In the setup for the learning task, Section 3.1 says that x_i is the *historical behaviors of user i derived from the user-item matrix X and \tilde{a}_i is the ground truth of the recommended item for the user based on his/her behavior x_i.* I don*t understand this. x_i has all the items the user has clicked. What new information is in \tilde{a}_i? Is it referring to the a_i in the problem statement, some vector representing preferences? Is such a vector ever known during training? Please clarify. - In the paragraph before equation 7, you say *given the recommended policy and the feedback generator* -- should that be *given the recommender policy*? - In Figure 2, we see the recommender producing what seems to be a vector a_i. Is this a set of recommendations? Or a distribution over items from which we could sample recommendations? Or a single recommendation, in which case the graphic should change?\n",
      "      - Review:###The paper proposes to learn a *virtual user* while learning a *recommender* model, to improve the performance of the recommender system. The *virtual user* is used to produce the *reward* signal for training the recommendation system (which is trained using RL). Jointly learning the recommender and the *virtual user* provides a synthetic feedback loop that helps to improve the performance of the recommendation system. The paper formulates the training of the *virtual user* as an inverse RL problem and uses adversarial regularizer. The paper proposes an interesting idea but more experiments (and explanation) is needed to bring out the usefulness of the work. In general, the writing needs to be polished as well. =============== Following are the items that need to be improved: ## Significance of the results * The results in Tables 3 and 4 provide only marginal improvements over the baseline. These improvements do not appear to be statistically significant. It would help if the authors comment on why these results appear significant and also provide variance values/curves for the results. ## Role of the feedback general F * Is there any separate loss for training F or is it always trained along with pi? * Is the feedback capturing some sort of *memory* or *past preferences/behaviors* of the user? If yes, wouldn*t using a recurrent recommender also capture these effects without needing the feedback model F? Note that I am not criticizing the choice of F. I am trying to understand the role played by F (in addition to the recommender pi). * If there is no separate loss for F, I wonder how would the performance change if the F network was to be removed and the reward value was to be fed into a recurrent recommended. (The paper seems to have considered a special case where a non-recurrent recommender is used with the reward value). The reason I am stressing on this is that one of the key distinctions of the authors* work is the use of feedback generator and it would be useful to quantify the benefits on this modification. ## Others * How is the static representation, x, computed? From eq 16 (appendix), it appears that x is a binary vector that captures what items have been purchased/reviewed. Is that correct? If yes, wouldn*t x have an enormous dimensionality? * The recommendation system is operating in a sequential decision-making setup. The formulation of R and F do no consider any sequential information. For example, let us say that the recommender recommends the items a1, a2 and a3 in 3 timesteps. The reward at time 3 is a function of x and a3 and the information of a1 and a2 is not used. * The loss function has many components and I want to make sure I understand what gradients flow where. So please correct me if I missed something; * supervised learning loss (from the real data) trains pi and F (equation 6). * pi and F collaborate to get a high reward from R (since we do not have the ground truth corresponding to R). No gradient flows to R. * Adversarial game between pi and R - which is used to update R. * I understand that the loss is defined according to the output of the last step but do the gradients flow through all the intermediate steps? * The training/inference procedure seems to be doing something strange. Let us assume that T = 5. So the 5 items are recommended to the *virtual user* and only the 5th item is recommended to the real user. Now the recommendation of the 5th item depends on the first 4 items that the real user has not seen. * In equation 15, what does the subscript F stand for? * What algorithm is used to train the policy pi? ================= The following are the items that should be corrected in the future iterations of the paper. Please note that these things did not impact the score. * It seems that the irrelevance of an item (for a user) is treated the same way as missing information about the relevance of the item. Could the authors discuss this more in the paper? * Is there any reason why this approach is only used with CF methods? * Some writing choices seem to make the problem statement more complex than it is. For example, the authors discuss how their work is different than traditional RL instead of simply stating that their work is set up as a POMDP. * Articles are missing (or mixed) up at many places. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper proposes to learn a *virtual user* while learning a *recommender* model, to improve the performance of the recommender system. The *virtual user* is used to produce the *reward* signal for training the recommendation system (which is trained using RL). Jointly learning the recommender and the *virtual user* provides a synthetic feedback loop that helps to improve the performance of the recommendation system. The paper formulates the training of the *virtual user* as an inverse RL problem and uses adversarial regularizer. The paper proposes an interesting idea but more experiments (and explanation) is needed to bring out the usefulness of the work. In general, the writing needs to be polished as well. =============== Following are the items that need to be improved: ## Significance of the results * The results in Tables 3 and 4 provide only marginal improvements over the baseline. These improvements do not appear to be statistically significant. It would help if the authors comment on why these results appear significant and also provide variance values/curves for the results. ## Role of the feedback general F * Is there any separate loss for training F or is it always trained along with pi? * Is the feedback capturing some sort of *memory* or *past preferences/behaviors* of the user? If yes, wouldn*t using a recurrent recommender also capture these effects without needing the feedback model F? Note that I am not criticizing the choice of F. I am trying to understand the role played by F (in addition to the recommender pi). * If there is no separate loss for F, I wonder how would the performance change if the F network was to be removed and the reward value was to be fed into a recurrent recommended. (The paper seems to have considered a special case where a non-recurrent recommender is used with the reward value). The reason I am stressing on this is that one of the key distinctions of the authors* work is the use of feedback generator and it would be useful to quantify the benefits on this modification. ## Others * How is the static representation, x, computed? From eq 16 (appendix), it appears that x is a binary vector that captures what items have been purchased/reviewed. Is that correct? If yes, wouldn*t x have an enormous dimensionality? * The recommendation system is operating in a sequential decision-making setup. The formulation of R and F do no consider any sequential information. For example, let us say that the recommender recommends the items a1, a2 and a3 in 3 timesteps. The reward at time 3 is a function of x and a3 and the information of a1 and a2 is not used. * The loss function has many components and I want to make sure I understand what gradients flow where. So please correct me if I missed something; * supervised learning loss (from the real data) trains pi and F (equation 6). * pi and F collaborate to get a high reward from R (since we do not have the ground truth corresponding to R). No gradient flows to R. * Adversarial game between pi and R - which is used to update R. * I understand that the loss is defined according to the output of the last step but do the gradients flow through all the intermediate steps? * The training/inference procedure seems to be doing something strange. Let us assume that T = 5. So the 5 items are recommended to the *virtual user* and only the 5th item is recommended to the real user. Now the recommendation of the 5th item depends on the first 4 items that the real user has not seen. * In equation 15, what does the subscript F stand for? * What algorithm is used to train the policy pi? ================= The following are the items that should be corrected in the future iterations of the paper. Please note that these things did not impact the score. * It seems that the irrelevance of an item (for a user) is treated the same way as missing information about the relevance of the item. Could the authors discuss this more in the paper? * Is there any reason why this approach is only used with CF methods? * Some writing choices seem to make the problem statement more complex than it is. For example, the authors discuss how their work is different than traditional RL instead of simply stating that their work is set up as a POMDP. * Articles are missing (or mixed) up at many places.\n",
      "      - Review:###This paper studies supervised classification problems where features are unstructured. For these problems, the authors propose a new neural network architecture that first reorganize the features into groups, then builds feed-forward networks on top each group, and finally aggregate the hidden nodes of each group to produce the final output. Empirical and ablation studies are conducted to show the performance of this approach. My detailed comments are as follows. 1. The intuition of this approach should be better explained. In equation (1) the features are group together using a binary matrix. Then the authors suggest using a relaxed version in equation (2) involving softmax function. What is the intuition here? If the feature is very high-dimensional, it seems that the normalization factor in equation (2) might be hard to compute. Moreover, what is the relationship between , , and ? 2. It seems that the neural network architecture is a simple variation of the standard MLP, except that the bottom layer is changed to a linear layer , where is defined using and a softmax operation. It seems that the contribution of the network structure is rather incremental. 3. In terms of the experiments, it seems that the results are very similar to that of the MLP, although slightly better. Moreover, the datasets used seem small, with no more than 10^6 data points in all datasets. The largest dataset is the Permutation invariant CIFAR-10, which has 50000 data with 3072 features. It would be interesting to see how this method works for high-dimensional datasets where the number of features is large. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper studies supervised classification problems where features are unstructured. For these problems, the authors propose a new neural network architecture that first reorganize the features into groups, then builds feed-forward networks on top each group, and finally aggregate the hidden nodes of each group to produce the final output. Empirical and ablation studies are conducted to show the performance of this approach. My detailed comments are as follows. 1. The intuition of this approach should be better explained. In equation (1) the features are group together using a binary matrix. Then the authors suggest using a relaxed version in equation (2) involving softmax function. What is the intuition here? If the feature is very high-dimensional, it seems that the normalization factor in equation (2) might be hard to compute. Moreover, what is the relationship between , , and ? 2. It seems that the neural network architecture is a simple variation of the standard MLP, except that the bottom layer is changed to a linear layer , where is defined using and a softmax operation. It seems that the contribution of the network structure is rather incremental. 3. In terms of the experiments, it seems that the results are very similar to that of the MLP, although slightly better. Moreover, the datasets used seem small, with no more than 10^6 data points in all datasets. The largest dataset is the Permutation invariant CIFAR-10, which has 50000 data with 3072 features. It would be interesting to see how this method works for high-dimensional datasets where the number of features is large.\n",
      "    - Hyperparameter Selection and Implementation/Reproducibility Details (L1, 3)\n",
      "      -  This paper proposes to leverage prototypes to solve the mismatch problem in unsupervised domain adaptation. It further imposes intra-class compactness to help ambiguous classes. Experiments show it achieves new state-of-the-art results in several datasets. pros: + intra-class compactness to help ambiguous classes concerns: -- Prototypes does not come from nowhere. They come from predictions. If you worry about the quality of target predictions (pseudo labels), then Eq. 8 and Eq. 9 are questionable. The intra-class compactness relies on p_t, too. The authors should explain why prototypes are superior than pseudo labels in [1]. -- How does the authors select hyper-parameters? There are lots of magic numbers in Section 4.1 about hyper-parameters but no clues about how to tune them. Recently there is a paper [2] about model selection for UDA, maybe the authors should try it. details: - terminology: *intra-class* is better than *within class* - separate citations: e.g. entropy minimization, mean-teacher, and virtual adversarial training, have been successfully applied to UDA (Vu et al., 2019; French et al., 2018; Shu et al., 2018) -> entropy minimization (Vu et al., 2019), mean-teacher (French et al., 2018), and virtual adversarial training (Shu et al., 2018), have been successfully applied to UDA - confusion: At the last of Section 3.2, it says hat{f}=M^{T}p. But in Eq. 9, hat{f} and M^{T}p are concatenated, which is confusing: why do you concatenate two identical vectors? - Implementation Details: Section 4.1, paragraph 4: lambda^{f}_{adv} =5e-3, lambda^{f}_{adv} and lambda^{p}_{adv} increase from 0 to 1. It is confusing that lambda^{f}_{adv} both is a constant and changes continuously. [1] Conditional adversarial domain adaptation, Long et.al, in NeurIPS 2018 [2] Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation, You et.al, in ICML 2019 (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper proposes to leverage prototypes to solve the mismatch problem in unsupervised domain adaptation. It further imposes intra-class compactness to help ambiguous classes. Experiments show it achieves new state-of-the-art results in several datasets. pros: + intra-class compactness to help ambiguous classes concerns: -- Prototypes does not come from nowhere. They come from predictions. If you worry about the quality of target predictions (pseudo labels), then Eq. 8 and Eq. 9 are questionable. The intra-class compactness relies on p_t, too. The authors should explain why prototypes are superior than pseudo labels in [1]. -- How does the authors select hyper-parameters? There are lots of magic numbers in Section 4.1 about hyper-parameters but no clues about how to tune them. Recently there is a paper [2] about model selection for UDA, maybe the authors should try it. details: - terminology: *intra-class* is better than *within class* - separate citations: e.g. entropy minimization, mean-teacher, and virtual adversarial training, have been successfully applied to UDA (Vu et al., 2019; French et al., 2018; Shu et al., 2018) -> entropy minimization (Vu et al., 2019), mean-teacher (French et al., 2018), and virtual adversarial training (Shu et al., 2018), have been successfully applied to UDA - confusion: At the last of Section 3.2, it says hat{f}=M^{T}p. But in Eq. 9, hat{f} and M^{T}p are concatenated, which is confusing: why do you concatenate two identical vectors? - Implementation Details: Section 4.1, paragraph 4: lambda^{f}_{adv} =5e-3, lambda^{f}_{adv} and lambda^{p}_{adv} increase from 0 to 1. It is confusing that lambda^{f}_{adv} both is a constant and changes continuously. [1] Conditional adversarial domain adaptation, Long et.al, in NeurIPS 2018 [2] Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation, You et.al, in ICML 2019\n",
      "      - Review:###This paper proposes a self-supervised reinforcement learning approach, Mutual Information-based State-Control (MISC), which maximizes the mutual information between the context states (i.e. robot states) and the states of interest (i.e. states of an object to manipulate). For this, they first split the entire state into two mutually exclusive sets of the context states and the states of interest. Then, the neural discriminator is trained to estimate the (lower-bound of) mutual information between the two states. The (mutual-information) intrinsic reward is computed by the trained neural discriminator, which is used for policy pre-training. Experimental results show that MISC helps to improve the performance of DDPG/SAC and the learned discriminator can be transferred to different environments. Detailed comments and questions: - In the paper, the states are represented by only object positions (x, y, z). Is this sufficient? (e.g. velocity is unnecessary?) - For MISC, the additional assumption is required: the agent should know that which parts of the states are its own controllable state and object*s state respectively. Is this additional assumption realistic enough and has it been adopted in other previous works? Is there any way to discriminate robot states and object states automatically? - Can MISC deal with the problems where the number of objects of interest is more than two? In this case, how can we define mutual information? - In Eq. (4), T(x_1:N, y_1:N) is assumed to be decomposable into the sum of T(x_t, y_t) / N. Can this make the lower bound (Eq. (3)) arbitrarily loose since the class of functions becomes very limited? - Detailed experimental setups are missing. e.g. network architecture, hyper-parameters (e.g. I_tran^max), and how they were searched. - Similarly to the problem of sparse reward, if the robot and the object are far apart and it is difficult to reach the object with random exploration, it would also be difficult to train the mutual information discriminator. How was the discriminator trained? How many time steps were used to train MI discriminator? - It seems that the MI discriminator learns to estimate the *proximity* between the robot and the object. Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task. - For the MISC+DIAYN, what if we train the agent using MISC and DIAYN at the same time, instead of pre-training MISC first and fine-tuning DIAYN later? - It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization. - Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper. - It seems that MISC is beneficial when the robot should get closer to the object for the success of the task. Then, how about the opposite situation? What if the task requires that the robot should *avoid* the object of interest? Does MISC still work? Is it helpful for the improvement of sample efficiency? - In order to pre-train the discriminator network, additional (s,a,s*) experiences are required, thus it seems difficult to say that it is better for exploration than VIME. - In section 4.3, what happens if we transfer the learned discriminator to Pick&Place from Push that has a gripper fixed to be closed, rather than the opposite direction (i.e. from Pick&Place to Push)? Does the MISC-t still well work? Can the learned MI discriminator be transferred to different tasks even when the state space is different? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a self-supervised reinforcement learning approach, Mutual Information-based State-Control (MISC), which maximizes the mutual information between the context states (i.e. robot states) and the states of interest (i.e. states of an object to manipulate). For this, they first split the entire state into two mutually exclusive sets of the context states and the states of interest. Then, the neural discriminator is trained to estimate the (lower-bound of) mutual information between the two states. The (mutual-information) intrinsic reward is computed by the trained neural discriminator, which is used for policy pre-training. Experimental results show that MISC helps to improve the performance of DDPG/SAC and the learned discriminator can be transferred to different environments. Detailed comments and questions: - In the paper, the states are represented by only object positions (x, y, z). Is this sufficient? (e.g. velocity is unnecessary?) - For MISC, the additional assumption is required: the agent should know that which parts of the states are its own controllable state and object*s state respectively. Is this additional assumption realistic enough and has it been adopted in other previous works? Is there any way to discriminate robot states and object states automatically? - Can MISC deal with the problems where the number of objects of interest is more than two? In this case, how can we define mutual information? - In Eq. (4), T(x_1:N, y_1:N) is assumed to be decomposable into the sum of T(x_t, y_t) / N. Can this make the lower bound (Eq. (3)) arbitrarily loose since the class of functions becomes very limited? - Detailed experimental setups are missing. e.g. network architecture, hyper-parameters (e.g. I_tran^max), and how they were searched. - Similarly to the problem of sparse reward, if the robot and the object are far apart and it is difficult to reach the object with random exploration, it would also be difficult to train the mutual information discriminator. How was the discriminator trained? How many time steps were used to train MI discriminator? - It seems that the MI discriminator learns to estimate the *proximity* between the robot and the object. Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task. - For the MISC+DIAYN, what if we train the agent using MISC and DIAYN at the same time, instead of pre-training MISC first and fine-tuning DIAYN later? - It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization. - Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper. - It seems that MISC is beneficial when the robot should get closer to the object for the success of the task. Then, how about the opposite situation? What if the task requires that the robot should *avoid* the object of interest? Does MISC still work? Is it helpful for the improvement of sample efficiency? - In order to pre-train the discriminator network, additional (s,a,s*) experiences are required, thus it seems difficult to say that it is better for exploration than VIME. - In section 4.3, what happens if we transfer the learned discriminator to Pick&Place from Push that has a gripper fixed to be closed, rather than the opposite direction (i.e. from Pick&Place to Push)? Does the MISC-t still well work? Can the learned MI discriminator be transferred to different tasks even when the state space is different?\n",
      "      - Review:###This paper proposes an off-policy reinforcement learning method in which the model parameters have been updated using the regression style loss function. Specifically, this method uses two regression update steps: one update value function and another one update policy using weighted regression. To compare the proposed method with others [main comprasion], 6 MuJoCo tasks are used for continuous control and LunarLander-v2 for discrete space. -- Even though this paper has done a good job in terms of running different experiments, the selection of some of the benchmarks seems arbitrary. For example, for discrete action space, this paper uses LunarLander which is rarely used in any papers so it makes very difficult to draw a conclusion based on these results. Common 49 Atari-2600 games should have been used for comparison. The same thing about experiments in section 5.3 is true too as those tasks are not that well-known. -- The proposed method doesn*t outperform previous off-policy methods on Mujoco task (Table 1). Since the main claim of this paper is a new off-policy method, outperforming the previous off-policy methods is a fair game. The current results are not convincing enough. -- There are significant overlaps between this paper, *Fitted Q-iteration by Advantage Weighted Regression*, *Model-Free Preference-Based Reinforcement Learning *, and *Reinforcement learning by reward-weighted regression for operational space control* which makes the contribution of this paper very incremental. -- The authors used only 5 seeds to run Mujoco experiments. Given the sensitivity of Mujoco for different starting points, the experiments should have been run at least with 10 different seeds. Questions: 1) Shouldn*t be an importance sampling ratio between pi and mu in the equations? starting from eq.5. 2) Does the algorithm optimize the respect to as well? (eq. 15) if yes, why it is not mentioned in algorithm 1? Plus, since is uniform dist. (at least this is assumed for implementation), eq. 14,15,and 16 (wherever there is d(s)), those can be simplified, e.g. hat{V} = sum(w_i V_i), wouldn*t be better just introduced simplified version rather than current ones? (referring to the only equation above section 3.3) 3) Is this the same code used to report results in this paper? if yes, I didn*t see any seed assignment in the code?! and what is *action_std* in the code? There are a couple of recent works in merging on-policy with off-policy updates which you might want to cite them. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes an off-policy reinforcement learning method in which the model parameters have been updated using the regression style loss function. Specifically, this method uses two regression update steps: one update value function and another one update policy using weighted regression. To compare the proposed method with others [main comprasion], 6 MuJoCo tasks are used for continuous control and LunarLander-v2 for discrete space. -- Even though this paper has done a good job in terms of running different experiments, the selection of some of the benchmarks seems arbitrary. For example, for discrete action space, this paper uses LunarLander which is rarely used in any papers so it makes very difficult to draw a conclusion based on these results. Common 49 Atari-2600 games should have been used for comparison. The same thing about experiments in section 5.3 is true too as those tasks are not that well-known. -- The proposed method doesn*t outperform previous off-policy methods on Mujoco task (Table 1). Since the main claim of this paper is a new off-policy method, outperforming the previous off-policy methods is a fair game. The current results are not convincing enough. -- There are significant overlaps between this paper, *Fitted Q-iteration by Advantage Weighted Regression*, *Model-Free Preference-Based Reinforcement Learning *, and *Reinforcement learning by reward-weighted regression for operational space control* which makes the contribution of this paper very incremental. -- The authors used only 5 seeds to run Mujoco experiments. Given the sensitivity of Mujoco for different starting points, the experiments should have been run at least with 10 different seeds. Questions: 1) Shouldn*t be an importance sampling ratio between pi and mu in the equations? starting from eq.5. 2) Does the algorithm optimize the respect to as well? (eq. 15) if yes, why it is not mentioned in algorithm 1? Plus, since is uniform dist. (at least this is assumed for implementation), eq. 14,15,and 16 (wherever there is d(s)), those can be simplified, e.g. hat{V} = sum(w_i V_i), wouldn*t be better just introduced simplified version rather than current ones? (referring to the only equation above section 3.3) 3) Is this the same code used to report results in this paper? if yes, I didn*t see any seed assignment in the code?! and what is *action_std* in the code? There are a couple of recent works in merging on-policy with off-policy updates which you might want to cite them.\n",
      "    - Missing Details Necessary for Reproducibility or Replication (L1, 3)\n",
      "      - Review:###1. Typos: (1) “We will consider slighly weaker notation of…” in page 3 should be “We will consider slightly weaker notation of…”. (2) “The same holds along the line line…” in page 3 should be “The same holds along the line…”. (3) A comma should be added between and in the Subsection ‘Proof sketch of unobserved Data’ in page 5. 2. Figure 3 presents the loss surface of NMF on straight paths connecting two random points for 8 real-world datasets. From this figure, we can see that the minima is obtained around lambda equals to 0.5. Whether the authors can explain this phenomenon? Besides, the authors should annotate the differences among each curve in each subfigure in Figure 3 and Figure 4. 3. Experiments utilize 8 datasets to demonstrate the good performance of the proposed planted model. The decomposition rank are given in the previously literature. However, in practice, the rank of a new dataset is unavilable. How to handel this situation? 4. There are unknow elements in Goodbooks, Movielens and Netflix datasets. These can be processed by the given sparsity. However, there is no items in the proposed planted model that can handle the sparsity. 5. Open codes about this paper. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###1. Typos: (1) “We will consider slighly weaker notation of…” in page 3 should be “We will consider slightly weaker notation of…”. (2) “The same holds along the line line…” in page 3 should be “The same holds along the line…”. (3) A comma should be added between and in the Subsection ‘Proof sketch of unobserved Data’ in page 5. 2. Figure 3 presents the loss surface of NMF on straight paths connecting two random points for 8 real-world datasets. From this figure, we can see that the minima is obtained around lambda equals to 0.5. Whether the authors can explain this phenomenon? Besides, the authors should annotate the differences among each curve in each subfigure in Figure 3 and Figure 4. 3. Experiments utilize 8 datasets to demonstrate the good performance of the proposed planted model. The decomposition rank are given in the previously literature. However, in practice, the rank of a new dataset is unavilable. How to handel this situation? 4. There are unknow elements in Goodbooks, Movielens and Netflix datasets. These can be processed by the given sparsity. However, there is no items in the proposed planted model that can handle the sparsity. 5. Open codes about this paper.\n",
      "      - Review:###The authors present an algorithm for actively learning the nodes in a graph that should be sampled/labeled in order to improve classifier performance the most. The proposed techniques use both the graph structure, and the current classifier performance/accuracy into account while (actively) selecting the next node to be labeled. There seem to be two main contributions in the paper. 1) The propose to sample nodes nodes based on *regional* uncertainty rather than node uncertainty 2) They use an variant of pagerank to determine nodes that are central, and hence most likely to affect subsequent classification in graph convolution classifiers. Both approaches seem to be interesting. There are experiments to show effectiveness of these techniques, and there are some interesting observations (for example, that the APR technique works better for smaller sample sizes, while the regional uncertainty methods do better for larger sampling fractions.). While both techniques seem straightforward extensions of previous approaches (and are well explained in the paper), the experiments indicate that they work better than prior approaches. It would have been nice if the authors had also discussed ways in which one or more of these techniques could be combined though, or discussed how we could pick the right approach (in a more empirical way, since it is not clear what the threshold for high sampling rate/low sampling rate distinction is, or if it varies from problem to problem) (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The authors present an algorithm for actively learning the nodes in a graph that should be sampled/labeled in order to improve classifier performance the most. The proposed techniques use both the graph structure, and the current classifier performance/accuracy into account while (actively) selecting the next node to be labeled. There seem to be two main contributions in the paper. 1) The propose to sample nodes nodes based on *regional* uncertainty rather than node uncertainty 2) They use an variant of pagerank to determine nodes that are central, and hence most likely to affect subsequent classification in graph convolution classifiers. Both approaches seem to be interesting. There are experiments to show effectiveness of these techniques, and there are some interesting observations (for example, that the APR technique works better for smaller sample sizes, while the regional uncertainty methods do better for larger sampling fractions.). While both techniques seem straightforward extensions of previous approaches (and are well explained in the paper), the experiments indicate that they work better than prior approaches. It would have been nice if the authors had also discussed ways in which one or more of these techniques could be combined though, or discussed how we could pick the right approach (in a more empirical way, since it is not clear what the threshold for high sampling rate/low sampling rate distinction is, or if it varies from problem to problem)\n",
      "      - Review:###This paper proposes a method of learning a hierarchical communication graph for improving collaborative multi-agent reinforcement learning, particularly with large numbers of agents. The method is compared to a suitable range of baseline approaches across two complex environments. The initial results presented seem promising, but further work is needed to ensure the results are reproducible and repeatable. To enable reproducibility, please include details of all hyperparameters used for all approaches in both domains. These should include justification of how the hyperparameters were tuned. Without understanding how these values were set I cannot support acceptance. To ensure the results are repeatable, repeated runs of training should be completed and the variation in performance quantified in the results. These repeats may have already been performed as Table 3 and Figure 5 discuss average results, but if not they must be completed before the work can be published due to the known issues with high variance in performance that commonly occur in deep RL. I would also argue against the justification of excluding ATOC from the StarCraft II experiments as its performance in MAgent with 25 agents is comparable to the other baseline methods that were tested. However, this is lower priority than the issues above provided there is no significant change in the relative performance of methods when the variance across multiple runs is documented in all existing experiments. Minor Comments: The following are suggestions for improvements if the paper is accepted or for future submissions. In Section 2, centralised critic methods are grouped as *communication-free* however I don*t think this is the best term to explain this approach as each agent has to communicate both its observations and actions to a centralised node (e.g. COMA) or all other agents (e.g. MADDPG). I also think this section should include coverage of other methods of utilizing graph neural networks in multi-agent reinforcement learning - e.g. *Deep Multi-Agent Reinforcement Learning with Relevance Graphs.* Malysheva et al. Deep RL Workshop @ NeurIPS 2018 and *Relational Forward Models for Multi-Agent Learning* Tacchetti et al. ICLR 2019. In Section 3, the acronyms CBRP and HCOMM are used on page 4 before they are introduced in full on page 5 for CBRP and never for HCOMM. HCOMM is also not used in Figure 4 or in the text description of the method. I believe it is the module described in Section 3.2 but this should be made clearer. Many claims in the paper are worded too strongly and should be revised. In Section 2, it is claimed that DQN *is one of the few RL algorithms applicable for large-scale MARL* - However, there are now many successful applications of deep RL to multi-agent systems (some of which are cited earlier in this same section) that use a variety of algorithms other than DQN. It is also claimed in this section that *DQN has excellent sample efficiency* despite the sample efficiency of deep RL being a known issue, open research question and a barrier to its widespread use in practice. In Section 3.2 the authors conclude *Overall, our LSC algorithm has advantage in the communication efficiency* despite in the same section noting two cases where ATOC has better efficiency (N_msg and N_b-r). I would suggest removing this sentence entirely as the paragraph above already contains a balanced account of the relative merits of each approach. On pages 8 and 9 the authors make references to guarantees and in the Appendix to proofs that are not supported by theory only empirical results. Without supporting theory these words should be avoided. The writing is also often informal to the detriment of presenting important information clearly. Notably, on page 3 *due to explosive growing number of agents* and on page 4 *The overall task of the MARL problem can be solved by properly objective function modeling.* The second of these, particularly the word *solved* is also related to the issue above of using the words guarantee and prove. Finally, the paper would benefit from a thorough grammar check. I note the following issues as simple changes that can be made to improve the readability of the paper: - In the abstract, the sentence *but also communication high-qualitative* does not parse. Perhaps this could be shortened to the brackets following? i.e. *is not only scalable but also learns efficiently.* - Page 2, *or employing the LSTM to* -> *or use the LSTM to* - Page 2, *still hinder the* -> *still hinders the* - Page 2, *the communication structure need be jointly* -> needs to be jointly - Page 6, *policy module motioned below* -> discussed below - Page 10, *the map size are is 1920x1200* -> the map size is 1920x1200 - Page 10, *We do not compare with ATOC because its poor performance in MAgent* -> because of its - Page 10, *we will to improve* -> we will improve - Page 10, *practical constrains* -> practical constraints (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a method of learning a hierarchical communication graph for improving collaborative multi-agent reinforcement learning, particularly with large numbers of agents. The method is compared to a suitable range of baseline approaches across two complex environments. The initial results presented seem promising, but further work is needed to ensure the results are reproducible and repeatable. To enable reproducibility, please include details of all hyperparameters used for all approaches in both domains. These should include justification of how the hyperparameters were tuned. Without understanding how these values were set I cannot support acceptance. To ensure the results are repeatable, repeated runs of training should be completed and the variation in performance quantified in the results. These repeats may have already been performed as Table 3 and Figure 5 discuss average results, but if not they must be completed before the work can be published due to the known issues with high variance in performance that commonly occur in deep RL. I would also argue against the justification of excluding ATOC from the StarCraft II experiments as its performance in MAgent with 25 agents is comparable to the other baseline methods that were tested. However, this is lower priority than the issues above provided there is no significant change in the relative performance of methods when the variance across multiple runs is documented in all existing experiments. Minor Comments: The following are suggestions for improvements if the paper is accepted or for future submissions. In Section 2, centralised critic methods are grouped as *communication-free* however I don*t think this is the best term to explain this approach as each agent has to communicate both its observations and actions to a centralised node (e.g. COMA) or all other agents (e.g. MADDPG). I also think this section should include coverage of other methods of utilizing graph neural networks in multi-agent reinforcement learning - e.g. *Deep Multi-Agent Reinforcement Learning with Relevance Graphs.* Malysheva et al. Deep RL Workshop @ NeurIPS 2018 and *Relational Forward Models for Multi-Agent Learning* Tacchetti et al. ICLR 2019. In Section 3, the acronyms CBRP and HCOMM are used on page 4 before they are introduced in full on page 5 for CBRP and never for HCOMM. HCOMM is also not used in Figure 4 or in the text description of the method. I believe it is the module described in Section 3.2 but this should be made clearer. Many claims in the paper are worded too strongly and should be revised. In Section 2, it is claimed that DQN *is one of the few RL algorithms applicable for large-scale MARL* - However, there are now many successful applications of deep RL to multi-agent systems (some of which are cited earlier in this same section) that use a variety of algorithms other than DQN. It is also claimed in this section that *DQN has excellent sample efficiency* despite the sample efficiency of deep RL being a known issue, open research question and a barrier to its widespread use in practice. In Section 3.2 the authors conclude *Overall, our LSC algorithm has advantage in the communication efficiency* despite in the same section noting two cases where ATOC has better efficiency (N_msg and N_b-r). I would suggest removing this sentence entirely as the paragraph above already contains a balanced account of the relative merits of each approach. On pages 8 and 9 the authors make references to guarantees and in the Appendix to proofs that are not supported by theory only empirical results. Without supporting theory these words should be avoided. The writing is also often informal to the detriment of presenting important information clearly. Notably, on page 3 *due to explosive growing number of agents* and on page 4 *The overall task of the MARL problem can be solved by properly objective function modeling.* The second of these, particularly the word *solved* is also related to the issue above of using the words guarantee and prove. Finally, the paper would benefit from a thorough grammar check. I note the following issues as simple changes that can be made to improve the readability of the paper: - In the abstract, the sentence *but also communication high-qualitative* does not parse. Perhaps this could be shortened to the brackets following? i.e. *is not only scalable but also learns efficiently.* - Page 2, *or employing the LSTM to* -> *or use the LSTM to* - Page 2, *still hinder the* -> *still hinders the* - Page 2, *the communication structure need be jointly* -> needs to be jointly - Page 6, *policy module motioned below* -> discussed below - Page 10, *the map size are is 1920x1200* -> the map size is 1920x1200 - Page 10, *We do not compare with ATOC because its poor performance in MAgent* -> because of its - Page 10, *we will to improve* -> we will improve - Page 10, *practical constrains* -> practical constraints\n",
      "    - Specific Technical or Algorithmic Concerns (L1, 6)\n",
      "      - Review:###Major caveat: I have published in the area of adversarial attacks on NLP models, but the specifics of the methods presented in this paper are quite outside of my expertise, and I do not have time to become familiar with them for this review. I hope there are other reviewers that are more qualified than I am to check the specifics of the methods. This paper presents a new technique for generating adversarial examples, by first learning the data manifold in an embedding space, then finding an adversarial example that lies on the manifold. I like this idea, it intuitively seems like a promising method for obtaining semantically meaningful adversarial examples. As I said above, I do not feel qualified to review whether the method should _theoretically_ accomplish its goals, so my judgment of this paper is on the intuition behind the idea (which I like), and the results that I can see (which are less promising). In order to have a *semantics preserving* attack, the method needs to (1) remain on the data manifold, and (2) not change the label a human would give to the input. For (1), this appears to have been accomplished on most datasets, though it seems pretty hard to argue that the artifacts seen in the MNIST examples shown are on the data manifold - there are no such artifacts in any of the inputs, or in the clean reconstruction. How do the authors claim that this actually did a reasonable job of staying in the data manifold? For (2), most of the images do indeed look like they should retain their human labels, which is good (but also not hard for adversarial images). Almost all of the textual examples, however, have correct predictions from the model after the adversarial change to the input. You can*t really argue that these are *semantics preserving*, or even *successful attacks*, as they change the expected input label. This is why semantics-preserving attacks are so hard in NLP, and I don*t think that this method has accomplished its goal here at all, at least for text. The authors should consult with experts in NLP before making claims about successfully constructing semantics preserving attacks on NLP models. I*m pretty on the fence about this paper, as I like the intuition, and the method appears to work reasonably well for vision. It does not work as claimed for text, however, and that should be fixed before this paper is published (either with softened claims or with better results). Hopefully people from other perspectives can pipe in and give a more clear picture on this paper. EDIT: See discussion below for my justification for reducing my score from a 3 to a 1. EDIT 11/14: The authors* revisions have satisfied my concerns about how the NLP attacks are described. I*m a little bit nervous about how the examples were changed - it seems that nothing changed about the method itself, so the authors probably cherry-picked better examples - but that*s not sufficiently worrying to me to justify rejection. The pilot study is also quite weak, as the number of inputs that were evaluated was only 20, and the questions presented don*t appear to ask about changes in the label. I don*t know how you could get 100% on that given the examples that I saw in the previous version of the paper. This is all to say that I don*t think the NLP attacks are actively problematic anymore, as they were previously, now they are just weak. The main contribution here is the technical contribution, anyway, so weaker results on one of the datasets tested is not a deal-breaker to me. Assuming the technical contributions pass muster (which, as I said, I don*t really feel qualified to judge), I*m satisfied with this paper as it is now. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Major caveat: I have published in the area of adversarial attacks on NLP models, but the specifics of the methods presented in this paper are quite outside of my expertise, and I do not have time to become familiar with them for this review. I hope there are other reviewers that are more qualified than I am to check the specifics of the methods. This paper presents a new technique for generating adversarial examples, by first learning the data manifold in an embedding space, then finding an adversarial example that lies on the manifold. I like this idea, it intuitively seems like a promising method for obtaining semantically meaningful adversarial examples. As I said above, I do not feel qualified to review whether the method should _theoretically_ accomplish its goals, so my judgment of this paper is on the intuition behind the idea (which I like), and the results that I can see (which are less promising). In order to have a *semantics preserving* attack, the method needs to (1) remain on the data manifold, and (2) not change the label a human would give to the input. For (1), this appears to have been accomplished on most datasets, though it seems pretty hard to argue that the artifacts seen in the MNIST examples shown are on the data manifold - there are no such artifacts in any of the inputs, or in the clean reconstruction. How do the authors claim that this actually did a reasonable job of staying in the data manifold? For (2), most of the images do indeed look like they should retain their human labels, which is good (but also not hard for adversarial images). Almost all of the textual examples, however, have correct predictions from the model after the adversarial change to the input. You can*t really argue that these are *semantics preserving*, or even *successful attacks*, as they change the expected input label. This is why semantics-preserving attacks are so hard in NLP, and I don*t think that this method has accomplished its goal here at all, at least for text. The authors should consult with experts in NLP before making claims about successfully constructing semantics preserving attacks on NLP models. I*m pretty on the fence about this paper, as I like the intuition, and the method appears to work reasonably well for vision. It does not work as claimed for text, however, and that should be fixed before this paper is published (either with softened claims or with better results). Hopefully people from other perspectives can pipe in and give a more clear picture on this paper. EDIT: See discussion below for my justification for reducing my score from a 3 to a 1. EDIT 11/14: The authors* revisions have satisfied my concerns about how the NLP attacks are described. I*m a little bit nervous about how the examples were changed - it seems that nothing changed about the method itself, so the authors probably cherry-picked better examples - but that*s not sufficiently worrying to me to justify rejection. The pilot study is also quite weak, as the number of inputs that were evaluated was only 20, and the questions presented don*t appear to ask about changes in the label. I don*t know how you could get 100% on that given the examples that I saw in the previous version of the paper. This is all to say that I don*t think the NLP attacks are actively problematic anymore, as they were previously, now they are just weak. The main contribution here is the technical contribution, anyway, so weaker results on one of the datasets tested is not a deal-breaker to me. Assuming the technical contributions pass muster (which, as I said, I don*t really feel qualified to judge), I*m satisfied with this paper as it is now.\n",
      "      - Review:###The paper paper proposes a mutual information maximization objective for discovering unsupervised robotic manipulation skills. The paper assumes that the state space can be divided into two parts - the state of the robot (“context states”) which is controllable via actions and the state of an object (“states of interest”) which must be manipulated by the robot. Given these two categories of states, the proposed algorithm maximizes a lower bound on the mutual information between the two categories of states such that a policy is learnt that is able to manipulate the object with the robot meaningfully. I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks. My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others. The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption. It doesn’t seem like a surprising discovery that maximizing the mutual information between the robot state and object state will lead to skills that actually make the robot move the object. Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper. Can the authors elaborate on why this choice should intuitively be better than the proposed method alone? The paper does not talk about how these skills can be used as primitive actions by a higher level controller (in a hierarchical RL setup), which would help in demonstrating the usefulness of these skills - e.g.: are these skills sequentially composable such that they can solve a complex task? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper paper proposes a mutual information maximization objective for discovering unsupervised robotic manipulation skills. The paper assumes that the state space can be divided into two parts - the state of the robot (“context states”) which is controllable via actions and the state of an object (“states of interest”) which must be manipulated by the robot. Given these two categories of states, the proposed algorithm maximizes a lower bound on the mutual information between the two categories of states such that a policy is learnt that is able to manipulate the object with the robot meaningfully. I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks. My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others. The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption. It doesn’t seem like a surprising discovery that maximizing the mutual information between the robot state and object state will lead to skills that actually make the robot move the object. Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper. Can the authors elaborate on why this choice should intuitively be better than the proposed method alone? The paper does not talk about how these skills can be used as primitive actions by a higher level controller (in a hierarchical RL setup), which would help in demonstrating the usefulness of these skills - e.g.: are these skills sequentially composable such that they can solve a complex task?\n",
      "      - Review:###This paper introduces a neural controller architecture for learning abstract algorithmic solutions to search and planning problems. By combining abstract and domain-specific components, the model is able to mimic two classical algorithms quite closely across several domains. The precision of the learning is very high; verified by generalization to substantially larger problem sizes and different domains. One notable conclusion is that Evolutionary Strategies is able to learn algorithmic solutions whose precision is on par with deterministic algorithms. The method of triggering learning based on curriculum level performance is a notable feature that nicely couples generalization progress with learning, and yields insightful learning curves. Although the paper shows that learning abstract algorithmic solutions is possible, it is not clear that the framework is able to learn such solutions when the training signals have not already been broken down into their smallest constituent parts. In other words, it is not clear what this framework could be used for, since it appears the experimenter must already possess a complete specification of the target algorithm and breakdown of the domain. Is there a setting where this framework could be used to learn something the experimenter is not already aware of? Or is the main point that it is technically possible to get an NN to learn this behavior? Although it is clear that models are achieved that satisfy R1-R3, it is not clear exactly what problem formulation is being considered. It would be very helpful if the paper included a formal problem definition so that the purpose of each framework component and differences w.r.t. prior work are clear. Similarly, the motivation for each of the data dependent modules is not clear. Is there something fundamental about this particular decomposition into modules? Or are these just the modules that were necessary given the specifics of the algorithms that were learned in experiments? How does this framework generalize to other kinds of algorithms? Are the comparison methods (DNC and stackRNN) unable to generalize to larger problem sizes? Including the full comparisons on generalization would give a more complete picture. Similarly, the figures are missing the comparisons for Learning to Plan for DNC and stackRNN. Is the comparison w.r.t. training time in Figure 3c fair, since the proposed framework pretrains the submodules? Is there a fundamental problem of DNC being addressed here? E.g., are there some critical types of submodules where making them differentiable is not an option? Is the algorithm limited to cases where the number of actions at each state is equal? I.e., could it be applied to algorithms like shortest path in the DNC paper? Finally, the last line talks about intriguing applications to the real world, but the running example in the paper is sorting. Is there some hypothetical but concrete example of how this framework could help in the real world, and do something better than a hard-coded classical algorithm? Or discover a new algorithm? ---------- After the rebuttal and discussion, I am convinced the paper is a useful contribution, and have increased my rating. However, I still think the presentation can be much improved to improve the clarity of the contribution. I would hope to see the following addressed in the final version: 1. More detailed and precise discussion of how the approach relates to prior work. As is, this discussion is informal and scattered throughout the paper. Enumerating the distinctions in one place would make the contribution much more clear. 2. The above would also help make the explicit contribution more clear. E.g., the *Contribution* paragraph currently does not contain the fact that the machine has only been applied to *planning* problems. This should be included to avoid making the contribution seem overly general. 3. More formal description of what each module does. Right now, they are described informally. Actually seeing the equations of what each computes would make it easier to understand how and why they all fit together. I still see the main contribution as *It is technically possible to train the abstract controller of a neural computer for planning using NES, so that R1-R3 are satisfied.* Ideally, the above clarifications would make it more clear that this is the main contribution, or that there are some other key contributions beyond this. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper introduces a neural controller architecture for learning abstract algorithmic solutions to search and planning problems. By combining abstract and domain-specific components, the model is able to mimic two classical algorithms quite closely across several domains. The precision of the learning is very high; verified by generalization to substantially larger problem sizes and different domains. One notable conclusion is that Evolutionary Strategies is able to learn algorithmic solutions whose precision is on par with deterministic algorithms. The method of triggering learning based on curriculum level performance is a notable feature that nicely couples generalization progress with learning, and yields insightful learning curves. Although the paper shows that learning abstract algorithmic solutions is possible, it is not clear that the framework is able to learn such solutions when the training signals have not already been broken down into their smallest constituent parts. In other words, it is not clear what this framework could be used for, since it appears the experimenter must already possess a complete specification of the target algorithm and breakdown of the domain. Is there a setting where this framework could be used to learn something the experimenter is not already aware of? Or is the main point that it is technically possible to get an NN to learn this behavior? Although it is clear that models are achieved that satisfy R1-R3, it is not clear exactly what problem formulation is being considered. It would be very helpful if the paper included a formal problem definition so that the purpose of each framework component and differences w.r.t. prior work are clear. Similarly, the motivation for each of the data dependent modules is not clear. Is there something fundamental about this particular decomposition into modules? Or are these just the modules that were necessary given the specifics of the algorithms that were learned in experiments? How does this framework generalize to other kinds of algorithms? Are the comparison methods (DNC and stackRNN) unable to generalize to larger problem sizes? Including the full comparisons on generalization would give a more complete picture. Similarly, the figures are missing the comparisons for Learning to Plan for DNC and stackRNN. Is the comparison w.r.t. training time in Figure 3c fair, since the proposed framework pretrains the submodules? Is there a fundamental problem of DNC being addressed here? E.g., are there some critical types of submodules where making them differentiable is not an option? Is the algorithm limited to cases where the number of actions at each state is equal? I.e., could it be applied to algorithms like shortest path in the DNC paper? Finally, the last line talks about intriguing applications to the real world, but the running example in the paper is sorting. Is there some hypothetical but concrete example of how this framework could help in the real world, and do something better than a hard-coded classical algorithm? Or discover a new algorithm? ---------- After the rebuttal and discussion, I am convinced the paper is a useful contribution, and have increased my rating. However, I still think the presentation can be much improved to improve the clarity of the contribution. I would hope to see the following addressed in the final version: 1. More detailed and precise discussion of how the approach relates to prior work. As is, this discussion is informal and scattered throughout the paper. Enumerating the distinctions in one place would make the contribution much more clear. 2. The above would also help make the explicit contribution more clear. E.g., the *Contribution* paragraph currently does not contain the fact that the machine has only been applied to *planning* problems. This should be included to avoid making the contribution seem overly general. 3. More formal description of what each module does. Right now, they are described informally. Actually seeing the equations of what each computes would make it easier to understand how and why they all fit together. I still see the main contribution as *It is technically possible to train the abstract controller of a neural computer for planning using NES, so that R1-R3 are satisfied.* Ideally, the above clarifications would make it more clear that this is the main contribution, or that there are some other key contributions beyond this.\n",
      "      - Review:###This paper offers through analysis on instability of NAS training that the system degenerates and reaches to trivial solutions, e.g., more skip-connect operators, as training goes longer. Motivated by this issue, this paper proposes an approach to stabilize NAS training. Strength: [1] Theoretical analysis [2] The paper is well written [3] This paper is trying to solve a very interesting and important problem Weakness: [1] Lack of ablation study. It would be better to show learning curve like Fig.1 to illustrate how the proposed approach helps to stabilize training and how about >500 epoch, any unstable? [2]The proposed approach offers comparable results with SOTA (early stopping). I agree that it opens a direction of stable NAS training, but the contribution so far is limited. I expect to see quality gains due to improved training technology (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper offers through analysis on instability of NAS training that the system degenerates and reaches to trivial solutions, e.g., more skip-connect operators, as training goes longer. Motivated by this issue, this paper proposes an approach to stabilize NAS training. Strength: [1] Theoretical analysis [2] The paper is well written [3] This paper is trying to solve a very interesting and important problem Weakness: [1] Lack of ablation study. It would be better to show learning curve like Fig.1 to illustrate how the proposed approach helps to stabilize training and how about >500 epoch, any unstable? [2]The proposed approach offers comparable results with SOTA (early stopping). I agree that it opens a direction of stable NAS training, but the contribution so far is limited. I expect to see quality gains due to improved training technology\n",
      "      - Review:###This paper proposes a new encoder-decoder framework that combines prior knowledge-based regularization and constrained reconstruction for unsupervised and weakly-supervised classification in structure rich scenarios. This framework injects prior knowledge in the form of relaxed constraints that act as regularization during the training of the encoder network. Some of the constraints concern sets of training examples. In this case, the paper proposes corresponding sampling schemes. Three experiments demonstrate the efficacy of the model. The first is a synthetically created 4x4 Sudoku made of overlaid MNIST digits. The other two are based on predicting crystal structures from x-ray diffraction measurements. Here, the first experiment is on simulated data for the Al-Li-Fe oxide system, while the other is performed on real measurements for the Bi-Cu-V oxide system. Overall, I believe that the proposed framework could be a significant contribution to the fields of representation learning and constrained optimization. However, the paper exhibits serious shortcomings, which require revision. First, the positive aspects of the paper: • The framework is simple yet ingenious. It makes intelligent use of constraints in the form of regularization to guide the training of the encoder. Furthermore, it enables the direct design of the latent representation through the use of (pre-trained) generative models for constrained reconstruction of data points. • The proposed entropy-based method for relaxation of discrete constraints is intuitive and potentially adaptable for further constraints. • The experiments presented in this paper are well chosen. They demonstrate the contribution of the model to both general CV data as well as a specialized domain, where it can solve both simulated and real scenarios. • The paper provides an extensive literature survey, which makes it easy to embed the presented work in the proper context. However, I propose to remove the paragraph titled “Other less closely related work” as the connection to the current work is not clear, and the space could be used more effectively (see below). Unfortunately, this paper has a couple of major flaws: • The results for DRNets (Generalization) on the MNIST Sudoku are compromised because the model trained on the test set for 25 epochs after being trained on the training set. Honestly, I was baffled to read the following sentence in the appendix: “Note that, during the test, instead of predicting the overlapping digits directly as other networks, we further optimize DRNets on the test set for 25 epochs to achieve a better result.” What is more, the main paper does not even mention this fact! • Although this paper relies on empirical verification of its proposition, the experimental results are almost impossible to interpret with just the information provided in the paper. Both experiments are poorly described, and even after reading the appendix several times, some serious detective work was necessary to piece together what happened in the experiments. The XRD experiments are especially hard to decipher, even with a physics background. Many vital components remain shrouded in mystery: What is a composition graph, and how are the paths sampled from it? How does the restart method, which is part of the results, work? Why are only six phases shown in the phase concentration visualizations if there were 159 possible phases? Are these the first six, a random subset, or were the other phases not realized? • The paper introduces the constraint-aware SGD algorithm to incorporate batching rules into the training of the encoder. On pages 2 and 6 and in Algorithm 1, I found the statement that the weights for each constrained are updated dynamically. However, that is where the information on the dynamic update method ends. Nowhere in the paper or the appendix did I find an explanation of how this is done. As this mechanism is a critical component of the proposed framework, the absence of an explanation is a significant oversight. Other remarks: • In the context of global constraints, the paper talks about a constraint graph. If I understand the creation of this graph correctly, this graph has several connected components in which every element connects to every other element. As such, this seems to be a collection of sets rather than a real graph. This is especially confusing in the case of XRD, where all data points are in the same global constraint, leading to a fully connected “graph”. • Although I appreciate the reference to Kahneman’s model of the mind, I suggest to remove the first two paragraphs from the introduction and use the space to motivate the de-mixing problem instead. While it is a compelling (but not novel) observation, the analogy to system 1 and system 2 does not benefit the proposed work in the slightest. • In general, I fail to see the connection between reasoning and the proposed work. The model itself is an encoder-decoder network that cannot reason. It does not discover any new rules during training. All the reasoning has to be done manually beforehand to be then incorporated in the form of constraints. To clarify, I do believe that there is value in the presented work, but not necessarily in the way, it is advertised. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a new encoder-decoder framework that combines prior knowledge-based regularization and constrained reconstruction for unsupervised and weakly-supervised classification in structure rich scenarios. This framework injects prior knowledge in the form of relaxed constraints that act as regularization during the training of the encoder network. Some of the constraints concern sets of training examples. In this case, the paper proposes corresponding sampling schemes. Three experiments demonstrate the efficacy of the model. The first is a synthetically created 4x4 Sudoku made of overlaid MNIST digits. The other two are based on predicting crystal structures from x-ray diffraction measurements. Here, the first experiment is on simulated data for the Al-Li-Fe oxide system, while the other is performed on real measurements for the Bi-Cu-V oxide system. Overall, I believe that the proposed framework could be a significant contribution to the fields of representation learning and constrained optimization. However, the paper exhibits serious shortcomings, which require revision. First, the positive aspects of the paper: • The framework is simple yet ingenious. It makes intelligent use of constraints in the form of regularization to guide the training of the encoder. Furthermore, it enables the direct design of the latent representation through the use of (pre-trained) generative models for constrained reconstruction of data points. • The proposed entropy-based method for relaxation of discrete constraints is intuitive and potentially adaptable for further constraints. • The experiments presented in this paper are well chosen. They demonstrate the contribution of the model to both general CV data as well as a specialized domain, where it can solve both simulated and real scenarios. • The paper provides an extensive literature survey, which makes it easy to embed the presented work in the proper context. However, I propose to remove the paragraph titled “Other less closely related work” as the connection to the current work is not clear, and the space could be used more effectively (see below). Unfortunately, this paper has a couple of major flaws: • The results for DRNets (Generalization) on the MNIST Sudoku are compromised because the model trained on the test set for 25 epochs after being trained on the training set. Honestly, I was baffled to read the following sentence in the appendix: “Note that, during the test, instead of predicting the overlapping digits directly as other networks, we further optimize DRNets on the test set for 25 epochs to achieve a better result.” What is more, the main paper does not even mention this fact! • Although this paper relies on empirical verification of its proposition, the experimental results are almost impossible to interpret with just the information provided in the paper. Both experiments are poorly described, and even after reading the appendix several times, some serious detective work was necessary to piece together what happened in the experiments. The XRD experiments are especially hard to decipher, even with a physics background. Many vital components remain shrouded in mystery: What is a composition graph, and how are the paths sampled from it? How does the restart method, which is part of the results, work? Why are only six phases shown in the phase concentration visualizations if there were 159 possible phases? Are these the first six, a random subset, or were the other phases not realized? • The paper introduces the constraint-aware SGD algorithm to incorporate batching rules into the training of the encoder. On pages 2 and 6 and in Algorithm 1, I found the statement that the weights for each constrained are updated dynamically. However, that is where the information on the dynamic update method ends. Nowhere in the paper or the appendix did I find an explanation of how this is done. As this mechanism is a critical component of the proposed framework, the absence of an explanation is a significant oversight. Other remarks: • In the context of global constraints, the paper talks about a constraint graph. If I understand the creation of this graph correctly, this graph has several connected components in which every element connects to every other element. As such, this seems to be a collection of sets rather than a real graph. This is especially confusing in the case of XRD, where all data points are in the same global constraint, leading to a fully connected “graph”. • Although I appreciate the reference to Kahneman’s model of the mind, I suggest to remove the first two paragraphs from the introduction and use the space to motivate the de-mixing problem instead. While it is a compelling (but not novel) observation, the analogy to system 1 and system 2 does not benefit the proposed work in the slightest. • In general, I fail to see the connection between reasoning and the proposed work. The model itself is an encoder-decoder network that cannot reason. It does not discover any new rules during training. All the reasoning has to be done manually beforehand to be then incorporated in the form of constraints. To clarify, I do believe that there is value in the presented work, but not necessarily in the way, it is advertised.\n",
      "      - Review:###This paper proposes an interesting heuristic of batch construction from samples. Instead of the usual random sampling, the authors to sample based on some measures of the ``uncertainty”. To be specific, the uncertainty is measured as a normalized entropy estimated from a window of historical predictions. I like the idea of designing more sophisticated ways to encourage more exploration over the samples that the model is not good at. The thought is similar as active learning. It is interesting to see how similar thought can be used to improve the performance of the algorithm in the general batch gradient descent setting. On the other hand I am not quite convinced the proposed way is truly better. The main concern is the experiments do not quite show the state-of-the-art result at all. It is not even close on MNIST, CIFAR-10 and CIFAR-100. Also those datasets are relatively small one. Can authors add results on larger datasets such as tiny image net? Besides this main concern I also have some worries about the design of the algorithm. I listed them below: 1. The vanilla stochastic gradient descent can be roughly justified since the expectation of the stochastic gradient is the true gradient of the loss. Now with the proposed heuristic will this still be true? 2. Is there any guarantee the algorithm can converge? It is not clear to me as the optimization proceeds the ``uncertainty” may oscillate. Is there any condition when the convergence is guaranteed? 3. As the number of classes grows the estimation of the entropy itself is a tough problem. Is there any way to mitigate this issue other than increase the window size? Another minor comment: Could the authors add more explanation on equation (4)? For example, is related to the maximum entropy led by a uniform distribution, and the summation term in (4) is related to the empirical entropy. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes an interesting heuristic of batch construction from samples. Instead of the usual random sampling, the authors to sample based on some measures of the ``uncertainty”. To be specific, the uncertainty is measured as a normalized entropy estimated from a window of historical predictions. I like the idea of designing more sophisticated ways to encourage more exploration over the samples that the model is not good at. The thought is similar as active learning. It is interesting to see how similar thought can be used to improve the performance of the algorithm in the general batch gradient descent setting. On the other hand I am not quite convinced the proposed way is truly better. The main concern is the experiments do not quite show the state-of-the-art result at all. It is not even close on MNIST, CIFAR-10 and CIFAR-100. Also those datasets are relatively small one. Can authors add results on larger datasets such as tiny image net? Besides this main concern I also have some worries about the design of the algorithm. I listed them below: 1. The vanilla stochastic gradient descent can be roughly justified since the expectation of the stochastic gradient is the true gradient of the loss. Now with the proposed heuristic will this still be true? 2. Is there any guarantee the algorithm can converge? It is not clear to me as the optimization proceeds the ``uncertainty” may oscillate. Is there any condition when the convergence is guaranteed? 3. As the number of classes grows the estimation of the entropy itself is a tough problem. Is there any way to mitigate this issue other than increase the window size? Another minor comment: Could the authors add more explanation on equation (4)? For example, is related to the maximum entropy led by a uniform distribution, and the summation term in (4) is related to the empirical entropy.\n",
      "    - Theoretical Justification, Guarantees, and Algorithmic Details (L1, 2)\n",
      "      -  Summary: In ES the goal is to find a distribution pi_theta(x) such that the expected value of f(x) under this distribution is high. This can be optimized with REINFORCE or with more sophisticated methods based on the natural gradient. The functional form of pi_theta is almost always a Gaussian, but this isn*t sufficiently flexible (e.g. multi-modal) to provide a good optimization algorithm. In response, the authors advocate for using a flexible family of generative neural networks for pi_theta. Using NICE as a generative model is desirable because it maintains volumes. This means that we can adjust volumes in latent space and this directly corresponds to volumes in x space. Doing so is useful to be able to tune how concentrated the search distribution is and to explicitly reason about the mode of the search distribution. Overall, I found that there were a number of technical details that were well motivated, such as how to leverage the *mode preservation* of NICE, how to use importance sampling to be able to use samples from multiple rounds of optimization when updating theta and the fact that any existing ES algorithm can be used to do the optimization in the latent space. *We found that the PGES algorithm (naive stochastic gradient descent of (8) with the score-function estimator) applied to the NICE distribution suffers from the same limitations as when applied to the Gaussian; it is inable to precisely locate any local minimum.* I don*t understand this. Can*t the Gaussian become very concentrated? You write: *ES implicitly balance the need for exploration and exploitation of the optimization landscape. The exploitation phase consists in updating the search distribution, and exploration happens when samples are drawn from the search distribution’s tails.* This is a weak form of exploration, since there is no explicit mechanism that encourages f(x) to be evaluated at regions that it has never been evaluated on before. The search distribution*s tails will have low probability mass, so exploration unlikely. Your proposed method uses a pi(x) that is flexible enough to represent multi-modal distributions. However, how can you ensure that your search procedure actually uses this flexibility? In other words, how is your proposed method any better at exploration that the baseline ES method? It would be great to have a slightly more detailed alg. box in the main text for your proposed method, instead of having it in the appendix. Some details I found confusing, such as whether you perform one step of alternating optimization per call to f(x) or if you perform many steps of alternating optimization. Your *Mode preserving properties* trick is cool. However, I don*t fully understand how it is used. Surely you need to be able to be able to change the mode of the distribution some time? Do you only use the mode preservation trick at certain optimization steps? Again, incorporating this in the alg box would be helpful. In the results, I was disappointed that you required restart strategies. I thought that one of the key advantages of using NICE was that you could capture a multi-modal search distribution. Can you explain? What do you mean by the *global volume of the distribution?* What is the volume of a distribution? I understand what concept you*re trying to convey, but can you be more precise? (L0, 1)\n",
      "        * Examples:\n",
      "          -  Summary: In ES the goal is to find a distribution pi_theta(x) such that the expected value of f(x) under this distribution is high. This can be optimized with REINFORCE or with more sophisticated methods based on the natural gradient. The functional form of pi_theta is almost always a Gaussian, but this isn*t sufficiently flexible (e.g. multi-modal) to provide a good optimization algorithm. In response, the authors advocate for using a flexible family of generative neural networks for pi_theta. Using NICE as a generative model is desirable because it maintains volumes. This means that we can adjust volumes in latent space and this directly corresponds to volumes in x space. Doing so is useful to be able to tune how concentrated the search distribution is and to explicitly reason about the mode of the search distribution. Overall, I found that there were a number of technical details that were well motivated, such as how to leverage the *mode preservation* of NICE, how to use importance sampling to be able to use samples from multiple rounds of optimization when updating theta and the fact that any existing ES algorithm can be used to do the optimization in the latent space. *We found that the PGES algorithm (naive stochastic gradient descent of (8) with the score-function estimator) applied to the NICE distribution suffers from the same limitations as when applied to the Gaussian; it is inable to precisely locate any local minimum.* I don*t understand this. Can*t the Gaussian become very concentrated? You write: *ES implicitly balance the need for exploration and exploitation of the optimization landscape. The exploitation phase consists in updating the search distribution, and exploration happens when samples are drawn from the search distribution’s tails.* This is a weak form of exploration, since there is no explicit mechanism that encourages f(x) to be evaluated at regions that it has never been evaluated on before. The search distribution*s tails will have low probability mass, so exploration unlikely. Your proposed method uses a pi(x) that is flexible enough to represent multi-modal distributions. However, how can you ensure that your search procedure actually uses this flexibility? In other words, how is your proposed method any better at exploration that the baseline ES method? It would be great to have a slightly more detailed alg. box in the main text for your proposed method, instead of having it in the appendix. Some details I found confusing, such as whether you perform one step of alternating optimization per call to f(x) or if you perform many steps of alternating optimization. Your *Mode preserving properties* trick is cool. However, I don*t fully understand how it is used. Surely you need to be able to be able to change the mode of the distribution some time? Do you only use the mode preservation trick at certain optimization steps? Again, incorporating this in the alg box would be helpful. In the results, I was disappointed that you required restart strategies. I thought that one of the key advantages of using NICE was that you could capture a multi-modal search distribution. Can you explain? What do you mean by the *global volume of the distribution?* What is the volume of a distribution? I understand what concept you*re trying to convey, but can you be more precise?\n",
      "      - Review:###Summary: The authors propose to learn reusable options to make use of prior information and claim to do so with minimal information from the user (such as # of options needed to solve the task, which options etc). The claim is that the agent is first able to learn a near-optimal policy for a small # of problems and then is able to solve a large # of tasks by such a learned policy. The authors build on the idea that minimizing the number of decisions made by the agent results in discovering reusable options. The options are learned offline by learning to solve a small number of tasks. Their algorithm introduces one option at a time until introducing a new option doesn’t improve the objective further. The ideas are interesting, However, the paper as it stands is lacking in thorough evaluation. Detailed comments: The proposed approach offers two key contributions: -an objective function to minimize the decision states -incrementally constructing an option set that can be reused later, without the a priori specification of the # of options needed. The introduction is well written, however, given the intuitions behind the objective function; in some sense, the idea here is to minimize the decisions or terminations intuitively relates to terminating only at critical or bottleneck states. It would be useful to provide such motivation in the introduction. Intuitively the objective criterion is interesting. With a cursory look at the proofs, they seem fine, although I have to admit I have not looked in detail into the proofs. Paper writing could be significantly improved. Several points are not clear and need further clarification: -The term near-optimal is mentioned several times, but it is not clear the policies are near-optimal with respect to what? The task or a set of tasks? -How does the proposed approach ensures that they are near-optimal? Please clarify. -“We can obtain the estimate for equation 1 by averaging over a set of near-optimal trajectories” The aim as states is to learn options that are capable of generating near-optimal trajectories (by using a small # of terminations). The authors then say that “given a set of options, a policy over options, a near-optimal sample trajectory, we can calculate..” Where does the near-optimal sample trajectory come from? Please provide clarifications. In experiments: FR rooms experiments are interesting, in the visualization of the option policies, do the figures here show the flattened policy of the options? What do the termination functions look like? Atari experiments are limited in nature in that they show only two games. Moreover, It is a bit confusing as to what is multi-task in the ATARI experiments. The authors mention the training of options and then talk about the results in the plots (4) show the training curves. However, they do not mention what are “novel tasks for Breakout/Amidar” in this context. Considering the proposed approach is closely related to the idea of selective terminations of options, it is natural to expect a comparison with Harb, 2018 and Hartyuanm 2019. The work could benefit by comparing with the aforementioned baselines. In particular, the visualization in 4b showing options learned in Amidar does not show much improvement from what was observed before in Harb, 2018. With the motivation of this paper, I am unable to convince myself about options being “reusable” for multi-task here. It would be very useful for the reader to clarify what “novel tasks” are here to appreciate what is learned. Looking deeper into the appendix, I understand that the authors “first learned a good performing policy with A3C for each game and sample 12 trajectories for training.” This is not at all clear in the main paper. Besides, what does it mean by a *good* policy? If we already have that, it is unclear what gains do we get from the proposed method. One obvious limitation here is that they also have a hard imposed constraint here is that the options cannot run for more than 20 time-steps in total, to make the objective function a suitable choice. Overall: An interesting objective function, Learn not only option set but also the number of options needed and incrementally learn new options. Paper writing does not convey clearly what are novel tasks and could be significantly improved. Since the paper claims multi-task and mentions several lifelong learning works like [1], I was expecting rigorous baselines showing performance over multiple tasks. The experiments are lacking in that evidence except for four rooms domain, which is much simpler a domain. Near-optimal property is very much lacking the clarity to the best of my knowledge. [1]Ammar, Haitham Bou, et al. *Online multi-task learning for policy gradient methods.* International Conference on Machine Learning. 2014. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Summary: The authors propose to learn reusable options to make use of prior information and claim to do so with minimal information from the user (such as # of options needed to solve the task, which options etc). The claim is that the agent is first able to learn a near-optimal policy for a small # of problems and then is able to solve a large # of tasks by such a learned policy. The authors build on the idea that minimizing the number of decisions made by the agent results in discovering reusable options. The options are learned offline by learning to solve a small number of tasks. Their algorithm introduces one option at a time until introducing a new option doesn’t improve the objective further. The ideas are interesting, However, the paper as it stands is lacking in thorough evaluation. Detailed comments: The proposed approach offers two key contributions: -an objective function to minimize the decision states -incrementally constructing an option set that can be reused later, without the a priori specification of the # of options needed. The introduction is well written, however, given the intuitions behind the objective function; in some sense, the idea here is to minimize the decisions or terminations intuitively relates to terminating only at critical or bottleneck states. It would be useful to provide such motivation in the introduction. Intuitively the objective criterion is interesting. With a cursory look at the proofs, they seem fine, although I have to admit I have not looked in detail into the proofs. Paper writing could be significantly improved. Several points are not clear and need further clarification: -The term near-optimal is mentioned several times, but it is not clear the policies are near-optimal with respect to what? The task or a set of tasks? -How does the proposed approach ensures that they are near-optimal? Please clarify. -“We can obtain the estimate for equation 1 by averaging over a set of near-optimal trajectories” The aim as states is to learn options that are capable of generating near-optimal trajectories (by using a small # of terminations). The authors then say that “given a set of options, a policy over options, a near-optimal sample trajectory, we can calculate..” Where does the near-optimal sample trajectory come from? Please provide clarifications. In experiments: FR rooms experiments are interesting, in the visualization of the option policies, do the figures here show the flattened policy of the options? What do the termination functions look like? Atari experiments are limited in nature in that they show only two games. Moreover, It is a bit confusing as to what is multi-task in the ATARI experiments. The authors mention the training of options and then talk about the results in the plots (4) show the training curves. However, they do not mention what are “novel tasks for Breakout/Amidar” in this context. Considering the proposed approach is closely related to the idea of selective terminations of options, it is natural to expect a comparison with Harb, 2018 and Hartyuanm 2019. The work could benefit by comparing with the aforementioned baselines. In particular, the visualization in 4b showing options learned in Amidar does not show much improvement from what was observed before in Harb, 2018. With the motivation of this paper, I am unable to convince myself about options being “reusable” for multi-task here. It would be very useful for the reader to clarify what “novel tasks” are here to appreciate what is learned. Looking deeper into the appendix, I understand that the authors “first learned a good performing policy with A3C for each game and sample 12 trajectories for training.” This is not at all clear in the main paper. Besides, what does it mean by a *good* policy? If we already have that, it is unclear what gains do we get from the proposed method. One obvious limitation here is that they also have a hard imposed constraint here is that the options cannot run for more than 20 time-steps in total, to make the objective function a suitable choice. Overall: An interesting objective function, Learn not only option set but also the number of options needed and incrementally learn new options. Paper writing does not convey clearly what are novel tasks and could be significantly improved. Since the paper claims multi-task and mentions several lifelong learning works like [1], I was expecting rigorous baselines showing performance over multiple tasks. The experiments are lacking in that evidence except for four rooms domain, which is much simpler a domain. Near-optimal property is very much lacking the clarity to the best of my knowledge. [1]Ammar, Haitham Bou, et al. *Online multi-task learning for policy gradient methods.* International Conference on Machine Learning. 2014.\n",
      "    - Unclear Problem Formalization, Methodological Justification, or Conceptual Errors (L1, 6)\n",
      "      -  Summary: - key problem: address *class mismatch* in adversarial learning methods for unsupervised domain adaptation (UDA); - contributions: 1) extension of the domain adversarial learning objective to leverage class prototypes (exponential moving average of features weighted by predicted class probabilities) in addition to pseudo-labels and intermediate representations (cf. eqs.5-11), 2) state-of-the-art results on several UDA tasks (Office-Home, ImageCLEF-DA, sim2real on Cityscapes). Recommendation: weak accept (with some reservations below). Key reason: interesting and effective use of prototypes for UDA. - The formulation of the prototypes and additional learning objectives for UDA are clear and seem novel, although I would like to see a discussion of additional related works: -- *Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results*, Tarvainen and Valpola, NeurIPS*17; -- *Unsupervised Domain Adaptation with Similarity Learning*, Pinheiro, CVPR*18; -- *Transferable Prototypical Networks for Unsupervised Domain Adaptation*, Pan et al, CVPR*19. - The effectiveness of the contributions is validated on multiple UDA tasks, and the ablative analysis supports the claims (that prototype-level alignment and within-class compactness helps). Main reservation: the specific problem is not clearly formalized. - What is the often mentioned but not clearly described *class mismatch* problem in UDA? To the best of my knowledge, this not a standard problem (could not find any mention in the previous literature, no citations or definitions in the submission). Is it that the target label space is different than the source label space (e.g., different ontologies)? In this case, what is the information on the target label space that enables unsupervised adaptation from the source one? What is the inductive bias / prior / assumptions? - Alternatively, is the tackled problem only the noise in the pseudo-labels? - In any case, the submission would greatly benefit from a clearer mathematical formalism and experimental characterization of the specific problem tackled here, especially in light of claims like *conditioning the alignment on pseudo labels can not well address the mismatch problem. Compared with the pseudo labels, the class prototypes are more robust and reliable in terms of representing the distribution of different semantic classes.* Additional Feedback: - missing references on sim2real UDA: *DADA: Depth-aware Domain Adaptation in Semantic Segmentation* (Vu et al, ICCV*19), *SPIGAN: Privileged Adversarial Learning from Simulation* (Lee et al, ICLR*19) ## Post rebuttal update I would like to thank the authors for replying to our questions. The clarifications with respect to related works and missing references is helpful, although a bit high-level (i.e. not necessarily describing the relative advantages of the proposed method). Nonetheless, the expected benefits of prototypes is still not entirely clear enough here, for instance regarding the main statistical assumptions that the method needs to make to get robust prototypes (e.g., in the presence of outliers or specific forms of *inaccuracies* in the pseudo-labels or *domain misalignment*). Therefore, due to the overall lack of mathematical clarity in the text and rebuttal, my main reservation remains, and I will change my *weak accept / borderline* score to weak reject. I encourage the authors to formalize the problem in a clearer, non-ambiguous way, discussing more explicitly the limitations of the proposed method. (L0, 1)\n",
      "        * Examples:\n",
      "          -  Summary: - key problem: address *class mismatch* in adversarial learning methods for unsupervised domain adaptation (UDA); - contributions: 1) extension of the domain adversarial learning objective to leverage class prototypes (exponential moving average of features weighted by predicted class probabilities) in addition to pseudo-labels and intermediate representations (cf. eqs.5-11), 2) state-of-the-art results on several UDA tasks (Office-Home, ImageCLEF-DA, sim2real on Cityscapes). Recommendation: weak accept (with some reservations below). Key reason: interesting and effective use of prototypes for UDA. - The formulation of the prototypes and additional learning objectives for UDA are clear and seem novel, although I would like to see a discussion of additional related works: -- *Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results*, Tarvainen and Valpola, NeurIPS*17; -- *Unsupervised Domain Adaptation with Similarity Learning*, Pinheiro, CVPR*18; -- *Transferable Prototypical Networks for Unsupervised Domain Adaptation*, Pan et al, CVPR*19. - The effectiveness of the contributions is validated on multiple UDA tasks, and the ablative analysis supports the claims (that prototype-level alignment and within-class compactness helps). Main reservation: the specific problem is not clearly formalized. - What is the often mentioned but not clearly described *class mismatch* problem in UDA? To the best of my knowledge, this not a standard problem (could not find any mention in the previous literature, no citations or definitions in the submission). Is it that the target label space is different than the source label space (e.g., different ontologies)? In this case, what is the information on the target label space that enables unsupervised adaptation from the source one? What is the inductive bias / prior / assumptions? - Alternatively, is the tackled problem only the noise in the pseudo-labels? - In any case, the submission would greatly benefit from a clearer mathematical formalism and experimental characterization of the specific problem tackled here, especially in light of claims like *conditioning the alignment on pseudo labels can not well address the mismatch problem. Compared with the pseudo labels, the class prototypes are more robust and reliable in terms of representing the distribution of different semantic classes.* Additional Feedback: - missing references on sim2real UDA: *DADA: Depth-aware Domain Adaptation in Semantic Segmentation* (Vu et al, ICCV*19), *SPIGAN: Privileged Adversarial Learning from Simulation* (Lee et al, ICLR*19) ## Post rebuttal update I would like to thank the authors for replying to our questions. The clarifications with respect to related works and missing references is helpful, although a bit high-level (i.e. not necessarily describing the relative advantages of the proposed method). Nonetheless, the expected benefits of prototypes is still not entirely clear enough here, for instance regarding the main statistical assumptions that the method needs to make to get robust prototypes (e.g., in the presence of outliers or specific forms of *inaccuracies* in the pseudo-labels or *domain misalignment*). Therefore, due to the overall lack of mathematical clarity in the text and rebuttal, my main reservation remains, and I will change my *weak accept / borderline* score to weak reject. I encourage the authors to formalize the problem in a clearer, non-ambiguous way, discussing more explicitly the limitations of the proposed method.\n",
      "      - Review:########### Rebuttal Response: Thanks for the thorough response. Q2: The title still hasn’t changed on the current draft Q4: To be more precise: ‘a novel data-based approach for analyzing the stability of the closed-loop system is proposed by constructing a Lyapunov function parameterized by deep neural network’ - this alone is not novel, you would need to specify how your method of doing this is new ‘a practical learning algorithm is designed to search the stability guaranteed controller’ - this is a natural consequence of contribution 1, some further justification is needed as to why this could be viewed as an interesting contribution (i.e. the Lagrangian approach, if this is novel) ‘ the learned controller is able to stabilize the system when interfered by uncertainties such as unseen disturbance and system parameters variations of certain extent’ - this is not a contribution, but an experimental result. Q5: The review believes that model-free control and stability-guarantees are fundamentally orthogonal ideas, rather than just under-studied work as the authors have been suggesting in the script and rebuttal. Given that discrete-time Lyapunov stability is defined through expressions along the lines of L(f(x)) - L(x) < 0, for Lyapunov function L and closed-loop dynamics f, claiming that stability is being ‘analyzed’ without f is disingenuous. Instead, by making the value function a Lyapunov function, the goal is that the *converged* value function should produce a stable policy, and still, this is surely only assured within the space of samples. Moreover, the use of the discount factor gamma, popular in MFRL, essentially acts as a time horizon, so I’m not convinced a Lyapunov function learned with a gamma < 1 can be called stable in the pure infinite-horizon sense. With this in mind, I think the work would benefit from a revised central claim: that the use of Lyapunov value functions (as an inductive bias) provides more *robust* model-free controllers. I believe this message highlights the value of this work for MFRL, without making false assertions. This, in particular, would highlight the fact that many MFRL algorithms are benchmarked on deterministic environments, and therefore incredible brittle as the experimental results suggest. Q9: This remark was aimed at earlier in the paper, either the introduction or main section, rather than the experimental section. The fact that a value function can be viewed as a Lyapunov function makes sense but I’m not sure it is a well-known fact in the wider community. Basically, an introduction to the intersection of Lyapunov stability and optimal control would improve the paper. Q:10 The fact that the clipping of the multiplier corresponds to unstable policies during learning demonstrates that this pitfall needs to be expressed explicitly. Whether the stability guarantees apply to the converged policy or also intermediate policies is not clear on the initial reading of the paper. For me, this highlights another weakness in the paper. This initial theorems talk of L(s), which relates to the critic L_c by L(s) = E_{usimpi(s)} [L_c(s,u)], however in the subsequent objectives (eg Eq 2), this marginalization never occurs, therefore I don’t feel like you can say Theorem 2 applies to your resultant algorithm. Moreover, with the alpha_3 c term in Equation 3, c should be c(s, a) with a marginalized, which it doesn’t appear to be, and the hyperparameter alpha_3 is never discussed nor tuning explained. Assuming this not done in the code, the experiments need to be re-evaluated with Theorem 2 properly enforced through a sample approximation of the marginalization. Q11/Q12: Thank you for the Markov jump experiments. I’m not sure I understand why LAC is able to learn the task while SAC cannot. To me, this suggests perhaps a lack of hyperparameter tuning for SAC or further investigation. Moreover, there are typos in captions Fig 1, e and f. A note of figures: Please ensure all axes should be labeled and should be of sufficient size. Many are too small and unreadable. Figure 2 looks like it could be 1 plot (though perhaps requires normalization). Given that the strength of this method is the added robustness upon convergence, I think it would be valuable to focus less on time-domain results (Figure 3) (these can be added to the appendix for clarity), but instead show how each parameter/noise variation affects the mean and variance of the episodic return. I would expect that, while LAC provides significant robustness, it is still limited. The results don’t demonstrate this. It would also be interesting to know which hyperparameter controls this limit. I imagine there is a robustness/performance tradeoff. In conclusion, while I appreciate the efforts the authors put into the rebuttal, the extended discussions made me rethink my rating and I have decreased my rating to reject. I believe fixing the issues highlighted above and redrafting the central message must be done before this paper is ready for publication. ######## Review: This paper investigates the use of Lyanpunov theory as an inductive bias for improving the stability / robustness of policies in a model-free actor-critic reinforcement learning setting. Through viewing the Critic as a Lyapunov function, optimizing the policy with a Lyapunov-based constraint is meant to ensure the stability of the policy through a ‘cost stability’ metric.. Experimental results show that Lyapunov-based Soft-Actor Critic (LAC) is more robust than SAC on some linear and nonlinear environments. The reviewer believes that the study of intersections between Control Theory and RL to be immensely valuable and the authors outline a principled formulation. However, the implementation, experiments and general manuscript suggest that paper requires further work before it is conference-ready. As the author understands it, the current state of the literature of Lyapunov methods for Deep Reinforcement Learning can be summarized as: Richards et al, 2018, Classify stable region and learn neural Lyapunov function for a safe exploration strategy Berkenkamp et al, 2018: Classify the stable region via GP, move there for exploration Chow et al 2018 Constrained MDPs for discrete gridworld environments Chow et al 2019 Constrained MDPs for continuous environments through a projection on the policy This work: Actor-Critic constrained policy optimization with a lyapunov-based value function critic In the introduction and the related work, too much emphasis is put on explaining stability and discussing methods like Model Predictive Control (MPC) which do not benefit the rest of the paper. Additionally, the three contributions listed do not seem particularly novel given the past literature. The premise of the formulation also presents several unquestioned assumptions and design decisions: Why model-free RL, as the authors also state that many samples are required to validate stability? How does the requirement of stability inform the search strategy in this work? Especially as SAC uses a maximum entropy stochastic policy to aid exploration. Do you really get ‘guarantees’ with sample-based methods? I would expect bounds based on the number of samples The cost-based measure of stability seems open to abuse - i.e. for the half-cheetah environment only the centre-of-mass horizontal velocity in covered in the cost function, the stability of the embodiment (joint angles and velocities) are ignored. For the Fetch Reacher, a cost function in cartesian space ignores instabilities from kinematic singularities in joint space. I would image the cost function needs to be a measure on the entire dynamic state. The notion of a Value function as a Lyapunov function is very interesting, and since it was the basis of the work, would have benefitted from more discussion, i.e. for which cost/reward function families the equivalence is valid for, and how it compares to other Lyapunov candidate functions. With the RL formulation, the requirement of clipping with the lagrangians is suspicious, as it suggests the objective and/or its numerics are not well posed. With the choice of experiments, they do not seem to question the central problem outlined by the paper. Rather than show environments SAC returns unstable trajectories during learning, the experiments aim to demonstrate instead a general robustness. The reviewer appreciates that stability is difficult to assess; however, while stability is heavily linked to robustness, a paper title promising stability guarantees should demonstrate some strong empirical evidence stability. Additionally, the choice of environments do not seem to be ideal test beds for stability - i.e, the half-cheetah is stabilized via interactions with the ground. The reviewer would prefer to see simpler nonlinear environments, such as Markov Jump Processes / Switching Linear Dynamics, where SAC clearly demonstrates instability during learning which LAC is sufficiently regularized against. Additionally, while the `repressilator’ is an interesting application to the domain of bioengineering, its addition does not seem to be especially motivated by the central goal of the paper, so just adds to confuse the reader with unnecessary theoretical content. Moreover, a brief literature review uncovered some relevant earlier work which was not cited: Construction of neural network based Lyapunov functions, Petridis et al, 2006 Generation of Lyapunov functions by neural networks, Noroozi et al, 2008 Lyapunov Design for Safe Reinforcement Learning, Perkins et al, 2002 Some of the references also appear incorrectly formatted or incorrect, i.e. the reference for Spencer et al, 2018 should be the CoRL 2018 version rather than arxiv. Also, the general use of grammar in the manuscript would benefit from another draft. In particular, the title could be improved, i.e. Model-free Control of Nonlinear Stochastic Systems with Stability Guarantees (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:########### Rebuttal Response: Thanks for the thorough response. Q2: The title still hasn’t changed on the current draft Q4: To be more precise: ‘a novel data-based approach for analyzing the stability of the closed-loop system is proposed by constructing a Lyapunov function parameterized by deep neural network’ - this alone is not novel, you would need to specify how your method of doing this is new ‘a practical learning algorithm is designed to search the stability guaranteed controller’ - this is a natural consequence of contribution 1, some further justification is needed as to why this could be viewed as an interesting contribution (i.e. the Lagrangian approach, if this is novel) ‘ the learned controller is able to stabilize the system when interfered by uncertainties such as unseen disturbance and system parameters variations of certain extent’ - this is not a contribution, but an experimental result. Q5: The review believes that model-free control and stability-guarantees are fundamentally orthogonal ideas, rather than just under-studied work as the authors have been suggesting in the script and rebuttal. Given that discrete-time Lyapunov stability is defined through expressions along the lines of L(f(x)) - L(x) < 0, for Lyapunov function L and closed-loop dynamics f, claiming that stability is being ‘analyzed’ without f is disingenuous. Instead, by making the value function a Lyapunov function, the goal is that the *converged* value function should produce a stable policy, and still, this is surely only assured within the space of samples. Moreover, the use of the discount factor gamma, popular in MFRL, essentially acts as a time horizon, so I’m not convinced a Lyapunov function learned with a gamma < 1 can be called stable in the pure infinite-horizon sense. With this in mind, I think the work would benefit from a revised central claim: that the use of Lyapunov value functions (as an inductive bias) provides more *robust* model-free controllers. I believe this message highlights the value of this work for MFRL, without making false assertions. This, in particular, would highlight the fact that many MFRL algorithms are benchmarked on deterministic environments, and therefore incredible brittle as the experimental results suggest. Q9: This remark was aimed at earlier in the paper, either the introduction or main section, rather than the experimental section. The fact that a value function can be viewed as a Lyapunov function makes sense but I’m not sure it is a well-known fact in the wider community. Basically, an introduction to the intersection of Lyapunov stability and optimal control would improve the paper. Q:10 The fact that the clipping of the multiplier corresponds to unstable policies during learning demonstrates that this pitfall needs to be expressed explicitly. Whether the stability guarantees apply to the converged policy or also intermediate policies is not clear on the initial reading of the paper. For me, this highlights another weakness in the paper. This initial theorems talk of L(s), which relates to the critic L_c by L(s) = E_{usimpi(s)} [L_c(s,u)], however in the subsequent objectives (eg Eq 2), this marginalization never occurs, therefore I don’t feel like you can say Theorem 2 applies to your resultant algorithm. Moreover, with the alpha_3 c term in Equation 3, c should be c(s, a) with a marginalized, which it doesn’t appear to be, and the hyperparameter alpha_3 is never discussed nor tuning explained. Assuming this not done in the code, the experiments need to be re-evaluated with Theorem 2 properly enforced through a sample approximation of the marginalization. Q11/Q12: Thank you for the Markov jump experiments. I’m not sure I understand why LAC is able to learn the task while SAC cannot. To me, this suggests perhaps a lack of hyperparameter tuning for SAC or further investigation. Moreover, there are typos in captions Fig 1, e and f. A note of figures: Please ensure all axes should be labeled and should be of sufficient size. Many are too small and unreadable. Figure 2 looks like it could be 1 plot (though perhaps requires normalization). Given that the strength of this method is the added robustness upon convergence, I think it would be valuable to focus less on time-domain results (Figure 3) (these can be added to the appendix for clarity), but instead show how each parameter/noise variation affects the mean and variance of the episodic return. I would expect that, while LAC provides significant robustness, it is still limited. The results don’t demonstrate this. It would also be interesting to know which hyperparameter controls this limit. I imagine there is a robustness/performance tradeoff. In conclusion, while I appreciate the efforts the authors put into the rebuttal, the extended discussions made me rethink my rating and I have decreased my rating to reject. I believe fixing the issues highlighted above and redrafting the central message must be done before this paper is ready for publication. ######## Review: This paper investigates the use of Lyanpunov theory as an inductive bias for improving the stability / robustness of policies in a model-free actor-critic reinforcement learning setting. Through viewing the Critic as a Lyapunov function, optimizing the policy with a Lyapunov-based constraint is meant to ensure the stability of the policy through a ‘cost stability’ metric.. Experimental results show that Lyapunov-based Soft-Actor Critic (LAC) is more robust than SAC on some linear and nonlinear environments. The reviewer believes that the study of intersections between Control Theory and RL to be immensely valuable and the authors outline a principled formulation. However, the implementation, experiments and general manuscript suggest that paper requires further work before it is conference-ready. As the author understands it, the current state of the literature of Lyapunov methods for Deep Reinforcement Learning can be summarized as: Richards et al, 2018, Classify stable region and learn neural Lyapunov function for a safe exploration strategy Berkenkamp et al, 2018: Classify the stable region via GP, move there for exploration Chow et al 2018 Constrained MDPs for discrete gridworld environments Chow et al 2019 Constrained MDPs for continuous environments through a projection on the policy This work: Actor-Critic constrained policy optimization with a lyapunov-based value function critic In the introduction and the related work, too much emphasis is put on explaining stability and discussing methods like Model Predictive Control (MPC) which do not benefit the rest of the paper. Additionally, the three contributions listed do not seem particularly novel given the past literature. The premise of the formulation also presents several unquestioned assumptions and design decisions: Why model-free RL, as the authors also state that many samples are required to validate stability? How does the requirement of stability inform the search strategy in this work? Especially as SAC uses a maximum entropy stochastic policy to aid exploration. Do you really get ‘guarantees’ with sample-based methods? I would expect bounds based on the number of samples The cost-based measure of stability seems open to abuse - i.e. for the half-cheetah environment only the centre-of-mass horizontal velocity in covered in the cost function, the stability of the embodiment (joint angles and velocities) are ignored. For the Fetch Reacher, a cost function in cartesian space ignores instabilities from kinematic singularities in joint space. I would image the cost function needs to be a measure on the entire dynamic state. The notion of a Value function as a Lyapunov function is very interesting, and since it was the basis of the work, would have benefitted from more discussion, i.e. for which cost/reward function families the equivalence is valid for, and how it compares to other Lyapunov candidate functions. With the RL formulation, the requirement of clipping with the lagrangians is suspicious, as it suggests the objective and/or its numerics are not well posed. With the choice of experiments, they do not seem to question the central problem outlined by the paper. Rather than show environments SAC returns unstable trajectories during learning, the experiments aim to demonstrate instead a general robustness. The reviewer appreciates that stability is difficult to assess; however, while stability is heavily linked to robustness, a paper title promising stability guarantees should demonstrate some strong empirical evidence stability. Additionally, the choice of environments do not seem to be ideal test beds for stability - i.e, the half-cheetah is stabilized via interactions with the ground. The reviewer would prefer to see simpler nonlinear environments, such as Markov Jump Processes / Switching Linear Dynamics, where SAC clearly demonstrates instability during learning which LAC is sufficiently regularized against. Additionally, while the `repressilator’ is an interesting application to the domain of bioengineering, its addition does not seem to be especially motivated by the central goal of the paper, so just adds to confuse the reader with unnecessary theoretical content. Moreover, a brief literature review uncovered some relevant earlier work which was not cited: Construction of neural network based Lyapunov functions, Petridis et al, 2006 Generation of Lyapunov functions by neural networks, Noroozi et al, 2008 Lyapunov Design for Safe Reinforcement Learning, Perkins et al, 2002 Some of the references also appear incorrectly formatted or incorrect, i.e. the reference for Spencer et al, 2018 should be the CoRL 2018 version rather than arxiv. Also, the general use of grammar in the manuscript would benefit from another draft. In particular, the title could be improved, i.e. Model-free Control of Nonlinear Stochastic Systems with Stability Guarantees\n",
      "      - Review:###I am not from this area and don*t know much about reinforcement learning. The paper discusses zero shot generalization (adaptation) into new environments. The authors propose an approach and then show results on Grid-World, Super Mario Bros, and 3D Robotics. In the training environment E1 = (S, A, p) the algorithm sees a bank of exploratory trajectories \tau_i = {(s_t, a_t)}_{t=1}^{T} but not rewards. The authors then say that algorithm is tested on the test environment E2. They * propose to only inform the new task per trajectory terminal evaluation r(? ) in E1* to give the training signal (where r is the reward). I am a bit confused by this setting. The model never sees any rewards for E1 but it does see rewards for E2? How is this zero shot? The authors then propose their approach, I wish some of it had been described more rigorously with math (e.g. the loss etc.) so it was easier to understand for people not in the domain and familiar with some of the terminology. Empirically the authors show results for 3 datasets and this seems thorough. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###I am not from this area and don*t know much about reinforcement learning. The paper discusses zero shot generalization (adaptation) into new environments. The authors propose an approach and then show results on Grid-World, Super Mario Bros, and 3D Robotics. In the training environment E1 = (S, A, p) the algorithm sees a bank of exploratory trajectories \tau_i = {(s_t, a_t)}_{t=1}^{T} but not rewards. The authors then say that algorithm is tested on the test environment E2. They * propose to only inform the new task per trajectory terminal evaluation r(? ) in E1* to give the training signal (where r is the reward). I am a bit confused by this setting. The model never sees any rewards for E1 but it does see rewards for E2? How is this zero shot? The authors then propose their approach, I wish some of it had been described more rigorously with math (e.g. the loss etc.) so it was easier to understand for people not in the domain and familiar with some of the terminology. Empirically the authors show results for 3 datasets and this seems thorough.\n",
      "      - Review:###In this paper, the authors aim to learn a locally interpretable model via the reinforcement learning approach, to address the fundamental challenge which is that the previous locally interpretable model has smaller representation capacity than black-box models, and causes under-fitting with conventional distillation techniques. Overall speaking, the paper is well organized, and the proposed approach is well tested, but in my opinion, there is a conceptual error. You claimed your method is REINFORCEMENT LEARNING based, but the REINFORCEMENT LEARNING definition for your task is weird, or wrong. In section 3, you didn*t give an explicit explanation for the state transition. With your given RL-like objective function, it seems that the state transition is from features to features. However, there is no specific correlated explanation in your paper on why you make such an assumption. Besides, the state transition in RL relies on decision making at each time step, while it has not reflected in your paper and code, namely, the state-transition independents on the decision making. To sum up, I don’t think the proposed method is RL-based, it would be more appropriate to define it as a MAB problem, and this paper should solve this problem before publishing. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###In this paper, the authors aim to learn a locally interpretable model via the reinforcement learning approach, to address the fundamental challenge which is that the previous locally interpretable model has smaller representation capacity than black-box models, and causes under-fitting with conventional distillation techniques. Overall speaking, the paper is well organized, and the proposed approach is well tested, but in my opinion, there is a conceptual error. You claimed your method is REINFORCEMENT LEARNING based, but the REINFORCEMENT LEARNING definition for your task is weird, or wrong. In section 3, you didn*t give an explicit explanation for the state transition. With your given RL-like objective function, it seems that the state transition is from features to features. However, there is no specific correlated explanation in your paper on why you make such an assumption. Besides, the state transition in RL relies on decision making at each time step, while it has not reflected in your paper and code, namely, the state-transition independents on the decision making. To sum up, I don’t think the proposed method is RL-based, it would be more appropriate to define it as a MAB problem, and this paper should solve this problem before publishing.\n",
      "      - Review:###In this paper, the authors introduce an algorithm to learn a stable controller using deep NN actor-critic method. They define the stability in the mean cost criteria, which is used to constrain the critic network as a Lyapunov function. In addition, the semi-positive definiteness of the Lyapunov function is enforced by constructing the critic. The problem is important to control with deep RL. The paper is written clearly. The reviewer has the following questions regarding the stability of the learned policy. - How is the stability in the mean cost related to the stability of stochastic systems? See, for example, the Lyapunov stability of stochastic systems (survey in [1])? - The authors enforce semi-positive definiteness using the construction of the value function approximation as the quadratic function bases. Then the semi-negative definiteness is enforced using penalty on the Lyapunov stability of the critic. Then the target network is trained to minimize the difference between the target and the critic. The question is, how is the stability of the target ensured by minimizing the difference with a Lyapunov critic? Is it possible to have an unstable target function that happens to minimize the distance? [1] H. J. Kushner, “A partial history of the early development of continuous-time nonlinear stochastic systems theory,” Automatica, vol. 50, no. 2, pp. 303–334, 2014. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###In this paper, the authors introduce an algorithm to learn a stable controller using deep NN actor-critic method. They define the stability in the mean cost criteria, which is used to constrain the critic network as a Lyapunov function. In addition, the semi-positive definiteness of the Lyapunov function is enforced by constructing the critic. The problem is important to control with deep RL. The paper is written clearly. The reviewer has the following questions regarding the stability of the learned policy. - How is the stability in the mean cost related to the stability of stochastic systems? See, for example, the Lyapunov stability of stochastic systems (survey in [1])? - The authors enforce semi-positive definiteness using the construction of the value function approximation as the quadratic function bases. Then the semi-negative definiteness is enforced using penalty on the Lyapunov stability of the critic. Then the target network is trained to minimize the difference between the target and the critic. The question is, how is the stability of the target ensured by minimizing the difference with a Lyapunov critic? Is it possible to have an unstable target function that happens to minimize the distance? [1] H. J. Kushner, “A partial history of the early development of continuous-time nonlinear stochastic systems theory,” Automatica, vol. 50, no. 2, pp. 303–334, 2014.\n",
      "      - Review:###In this work the authors studied the model-free RL approach for learning a policy with stability guarantees. Leveraging the Lyapunov stochastic stability criterion, instead if minimizing the cumulative cost (plus a soft entropy), they propose optimizing an objective function with a specific Lyapunov critic, which is a specific critic function that satisfies the Lyapunov criterion to guarantee stability. They also show in several Cartpole, Mujoco, and Repressilator experiments that this approach is more robust to perturbations (such as sinusoids), where the agent are more robust to dynamic uncertainties and disturbances. In general, the topic of guaranteeing stability is a topic in safe RL, and I find this work of enforcing stability in model-free RL interesting. Through the specific parameterization of quadratic Lyapunov function (in the latent space), the authors proposed learning a new critic function that is a value function but at the same time (almost) satisfies the Lyapunov constraints. While this is an interesting idea, and the experimental results look promising, I do have several questions. First, regarding the learning problem of Lyapunov function, how does the proposed way of learning L differ from the one in Richard*18: The lyapunov neural network: Adaptive stability certification for safe learning of dynamic systems, where the problem is formulated as a classification (while in here it is a regression problem)? Second, while this approach is intuitive, since the approach is penalty-based (Lagrangian based), I do not see how the Lyapunov criteria in Theorem 1 is guaranteed, in this case is stability guaranteed by the policy learning algorithm? If not, what do the authors do to enforce that? Third, if one formulates the immediate constraint cost of the CMDP to be the distance of the state to the equilibrium point, then the (undiscounted, shortest-path type) CMDP total cost constraint should guarantee stability (because the total distance cumulative cost is bounded, meaning that the distance cost converges to zero). Then, one can use the Lyapunov approach by Chow*19 (in modulo to their setting in discounted MDPs) to enforce stability (which is a specific notion of safety in this case). How does the proposed method compare with this approach? Can the authors provide numerical comparisons with the method proposed by Chow*19 as well? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###In this work the authors studied the model-free RL approach for learning a policy with stability guarantees. Leveraging the Lyapunov stochastic stability criterion, instead if minimizing the cumulative cost (plus a soft entropy), they propose optimizing an objective function with a specific Lyapunov critic, which is a specific critic function that satisfies the Lyapunov criterion to guarantee stability. They also show in several Cartpole, Mujoco, and Repressilator experiments that this approach is more robust to perturbations (such as sinusoids), where the agent are more robust to dynamic uncertainties and disturbances. In general, the topic of guaranteeing stability is a topic in safe RL, and I find this work of enforcing stability in model-free RL interesting. Through the specific parameterization of quadratic Lyapunov function (in the latent space), the authors proposed learning a new critic function that is a value function but at the same time (almost) satisfies the Lyapunov constraints. While this is an interesting idea, and the experimental results look promising, I do have several questions. First, regarding the learning problem of Lyapunov function, how does the proposed way of learning L differ from the one in Richard*18: The lyapunov neural network: Adaptive stability certification for safe learning of dynamic systems, where the problem is formulated as a classification (while in here it is a regression problem)? Second, while this approach is intuitive, since the approach is penalty-based (Lagrangian based), I do not see how the Lyapunov criteria in Theorem 1 is guaranteed, in this case is stability guaranteed by the policy learning algorithm? If not, what do the authors do to enforce that? Third, if one formulates the immediate constraint cost of the CMDP to be the distance of the state to the equilibrium point, then the (undiscounted, shortest-path type) CMDP total cost constraint should guarantee stability (because the total distance cumulative cost is bounded, meaning that the distance cost converges to zero). Then, one can use the Lyapunov approach by Chow*19 (in modulo to their setting in discounted MDPs) to enforce stability (which is a specific notion of safety in this case). How does the proposed method compare with this approach? Can the authors provide numerical comparisons with the method proposed by Chow*19 as well?\n",
      "  - Clarity, Presentation, and Exposition Issues (L2, 41)\n",
      "    - Clarity and Quality of Exposition (L1, 2)\n",
      "      - Review:###Summary of the Paper: The authors claim the that i.i.d hypothesis that is often used in the prior when looking for the equivalence between neural networks and GP is not valid. Then, they propose a new interpretation of neural networks as Gibbs distributions (in the case of fully connected layers) and a MRF in the case of convolutional layers. Some simulations are done to verify this. Detailed comments: Overall I believe that the writing of the paper is very sloppy and difficult to read and follow. It is not clear that the GP interpretation cannot be valid simply because the activations and weights are not i.i.d. I.I.D is a sufficient condition but not required in the central limit theorem. For example, the sum of correlated Gaussian variables also tends to a Gaussian distribution. The same for non-Gaussian variables. Therefore, I do not think that the GP interpretation of the NN is wrong simply for that. This simply shows that the i.i.d. prior may be suboptimal. Summing up, I think that this paper needs more work. Currently, I do not think I can extract anything useful from it. The prior is subjective and can be chosen by the user. So if an i.i.d. prior is actually chosen, the corresponding Bayesian neural network will converge to a GP. An i.i.d. prior may be sub-optimal. However, it can be used to interpret neural networks as GP. There is no problem with that. It is well known that the sum of random variables can also converge to a Gaussian distribution even though they are not independent. This questions the claims of the paper. The witting of the paper needs to be improved. There are several expressions that do not sound well. E.g., *, GP with i.i.d.* Z*y in Eq. (9) should depend on l. It is not clear what is the distribution of a hidden layer (activations weights etc..). *...and f Y is an estimation of the true distribution P (Y |X)* what do you mean by that? Eq. (8) seems to be a prob. distribution for the random variable Fy. However, the authors give an expression for f_yl, which does not make sense. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Summary of the Paper: The authors claim the that i.i.d hypothesis that is often used in the prior when looking for the equivalence between neural networks and GP is not valid. Then, they propose a new interpretation of neural networks as Gibbs distributions (in the case of fully connected layers) and a MRF in the case of convolutional layers. Some simulations are done to verify this. Detailed comments: Overall I believe that the writing of the paper is very sloppy and difficult to read and follow. It is not clear that the GP interpretation cannot be valid simply because the activations and weights are not i.i.d. I.I.D is a sufficient condition but not required in the central limit theorem. For example, the sum of correlated Gaussian variables also tends to a Gaussian distribution. The same for non-Gaussian variables. Therefore, I do not think that the GP interpretation of the NN is wrong simply for that. This simply shows that the i.i.d. prior may be suboptimal. Summing up, I think that this paper needs more work. Currently, I do not think I can extract anything useful from it. The prior is subjective and can be chosen by the user. So if an i.i.d. prior is actually chosen, the corresponding Bayesian neural network will converge to a GP. An i.i.d. prior may be sub-optimal. However, it can be used to interpret neural networks as GP. There is no problem with that. It is well known that the sum of random variables can also converge to a Gaussian distribution even though they are not independent. This questions the claims of the paper. The witting of the paper needs to be improved. There are several expressions that do not sound well. E.g., *, GP with i.i.d.* Z*y in Eq. (9) should depend on l. It is not clear what is the distribution of a hidden layer (activations weights etc..). *...and f Y is an estimation of the true distribution P (Y |X)* what do you mean by that? Eq. (8) seems to be a prob. distribution for the random variable Fy. However, the authors give an expression for f_yl, which does not make sense.\n",
      "      - Review:###This paper presents visual imitation with reinforcement learning (VIRL), an algorithm for learning to imitate expert trajectories based solely on visual observations, and without access to the expert’s actions. The algorithm is similar in form to GAIL and its extensions, learning a reward function which captures the similarity between an observed behavior and the expert*s demonstrations, while simultaneously using reinforcement learning to find a policy maximizing this reward, such that the learned policy will replicate the demonstrated behavior as well as possible. A key feature of this method is that the learned reward function is defined by a learned distance metric, which evaluates the similarity between the agent*s current trajectory, and the nearest demonstrated expert trajectory. The network describing the distance metric is recurrent, such that the distance is defined between trajectories rather than individual states. The distance function network is trained via a negative sampling approach, where expert trajectories are randomly reordered to produce examples that dissimilar to the expert trajectories. The distance network also defines a variational autoencoder, and the reconstruction of the target trajectories is treated as an auxiliary task to help train better representations of the trajectory space. While previous work has considered the problem of visual imitation learning, the approach taken here is novel in its architecture and loss function, and significantly outperforms the baselines in terms of the similarity between the resulting behavior and the expert behavior. The clarity of the technical presentation could be improved, however. In particular, it would be helpful for the reader if the definitions of the negative sampling loss and the autoencoder losses were given before the combined loss, and if we saw the form of the loss for both positive and negative sequence pairs. Equation 4 could also be made explicit, with the full summation term included. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper presents visual imitation with reinforcement learning (VIRL), an algorithm for learning to imitate expert trajectories based solely on visual observations, and without access to the expert’s actions. The algorithm is similar in form to GAIL and its extensions, learning a reward function which captures the similarity between an observed behavior and the expert*s demonstrations, while simultaneously using reinforcement learning to find a policy maximizing this reward, such that the learned policy will replicate the demonstrated behavior as well as possible. A key feature of this method is that the learned reward function is defined by a learned distance metric, which evaluates the similarity between the agent*s current trajectory, and the nearest demonstrated expert trajectory. The network describing the distance metric is recurrent, such that the distance is defined between trajectories rather than individual states. The distance function network is trained via a negative sampling approach, where expert trajectories are randomly reordered to produce examples that dissimilar to the expert trajectories. The distance network also defines a variational autoencoder, and the reconstruction of the target trajectories is treated as an auxiliary task to help train better representations of the trajectory space. While previous work has considered the problem of visual imitation learning, the approach taken here is novel in its architecture and loss function, and significantly outperforms the baselines in terms of the similarity between the resulting behavior and the expert behavior. The clarity of the technical presentation could be improved, however. In particular, it would be helpful for the reader if the definitions of the negative sampling loss and the autoencoder losses were given before the combined loss, and if we saw the form of the loss for both positive and negative sequence pairs. Equation 4 could also be made explicit, with the full summation term included.\n",
      "    - Clarity of explanation, notation, or writing (L1, 3)\n",
      "      - Review:###I thank the authors for their detailed response. Some of my questions have been addressed in the rebuttal, but as far as I can tell very few modifications have been made to the paper: mainly an additional super-resolution experiment in the appendix, which goes in a good direction, but currently comes across as quite preliminary. So I think the paper still has most of the weaknesses I mentioned and thus it is not quite fit for publication. But I do encourage the authors to strengthen the experiments (for instance by addressing the points I raised, or by some other means) and resubmit elsewhere. --- The paper proposes an approach to analyzing the properties of *perceptual metrics* used in deep learning image generation methods. *Perceptual metrics* are computed by measuring distances between images not in the pixel space, but i na feature space of a pre-trained cNN. The proposed analysis method, inspired by studies of human perception, is based on measuring the response of these features to sinusoidal gratings of varying frequency or orientation. Based on these responses, the paper proposes a *Perceptual Efficacy Score* that should measure the importance of certain feature in the feature maps for the performance of a perceptual metric. Experiments show that indeed distances measured between features with high score better correlate with human judgement of image similarity than distances between features with a lower score. I find the paper quite interesting, but lean towards rejection at this point. This i smainly because the experiments seem somewhat anecdotal and incomplete, see below for further details. Pros: 1) Application of methods from psychology/neuroscience to artificial neural networks is an interesting avenue of work. Moreover, better unsderstanding of *perceptual metrics* is of wide interest for various image processing applications. 2) The proposed score seems to indeed correlate quite well with the importance of features for human judgement of image similarity. 3) Presentation is mainly clear. Cons: 1) Experiments are not very exhaustive and at times a bit confusing. For instance: 1a) Results are sometimes presented in a confusing way. In Figure 4 first of all it is not quite clear what points correspond to I guess each point is an image) and, second, it is not very obvious that the correlation is higher i none of the plots. In tables 1 and 2 it is confusing that different percentiles for H and L are used for different networks/layers. Is this based on some tuning? Then the tuning process should be clearly explained. Moreover, it might be useful to report the full curves of performance as a function of the percentage of features used. 1b) There are no baselines and there is not much justification of computing the *Perceptual Efficacy* score the way it is computed. What if one uses only the orientation-based score? Or only the frequency-based? What if one selects the most relevant features in a data-driven way (based on correlation on a training set)? What if one selects subsets of features randomly? 1c) While the method is inspired by methods used for studying natural vision systems, there is no connection to human experiments. It would be interesting to see a comparison of frequency and orientation tuning of features in a CNN to human cells (as I understand, the latter should be available in prior works?). 1d) It would be great to see the selected features be used not only for offline image similarity assessment, but also for training image processing models - in the end, this has been the main use of *perceptual metrics*. Do they lead to improved results? 1e) Since the paper is about (subjective) image quality, it might be useful to show some qualitative results, potentially in the appendix if space is an issue. 2) There are some issues with the presentation: 2a) I had a hard time understanding what exactly *Contrast Sensitivity Function* and *contrast masking* are. 2b) Minor issues: - In the abstract: *trained object detection deep CNNs* - I guess image classification is meant - Beginning of Section 2: *Section. 2*, *convolution layer as collection channels* - Section 4: *corresponds the the peak: - Section 5.2 *Berkeley-Adobpe* (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###I thank the authors for their detailed response. Some of my questions have been addressed in the rebuttal, but as far as I can tell very few modifications have been made to the paper: mainly an additional super-resolution experiment in the appendix, which goes in a good direction, but currently comes across as quite preliminary. So I think the paper still has most of the weaknesses I mentioned and thus it is not quite fit for publication. But I do encourage the authors to strengthen the experiments (for instance by addressing the points I raised, or by some other means) and resubmit elsewhere. --- The paper proposes an approach to analyzing the properties of *perceptual metrics* used in deep learning image generation methods. *Perceptual metrics* are computed by measuring distances between images not in the pixel space, but i na feature space of a pre-trained cNN. The proposed analysis method, inspired by studies of human perception, is based on measuring the response of these features to sinusoidal gratings of varying frequency or orientation. Based on these responses, the paper proposes a *Perceptual Efficacy Score* that should measure the importance of certain feature in the feature maps for the performance of a perceptual metric. Experiments show that indeed distances measured between features with high score better correlate with human judgement of image similarity than distances between features with a lower score. I find the paper quite interesting, but lean towards rejection at this point. This i smainly because the experiments seem somewhat anecdotal and incomplete, see below for further details. Pros: 1) Application of methods from psychology/neuroscience to artificial neural networks is an interesting avenue of work. Moreover, better unsderstanding of *perceptual metrics* is of wide interest for various image processing applications. 2) The proposed score seems to indeed correlate quite well with the importance of features for human judgement of image similarity. 3) Presentation is mainly clear. Cons: 1) Experiments are not very exhaustive and at times a bit confusing. For instance: 1a) Results are sometimes presented in a confusing way. In Figure 4 first of all it is not quite clear what points correspond to I guess each point is an image) and, second, it is not very obvious that the correlation is higher i none of the plots. In tables 1 and 2 it is confusing that different percentiles for H and L are used for different networks/layers. Is this based on some tuning? Then the tuning process should be clearly explained. Moreover, it might be useful to report the full curves of performance as a function of the percentage of features used. 1b) There are no baselines and there is not much justification of computing the *Perceptual Efficacy* score the way it is computed. What if one uses only the orientation-based score? Or only the frequency-based? What if one selects the most relevant features in a data-driven way (based on correlation on a training set)? What if one selects subsets of features randomly? 1c) While the method is inspired by methods used for studying natural vision systems, there is no connection to human experiments. It would be interesting to see a comparison of frequency and orientation tuning of features in a CNN to human cells (as I understand, the latter should be available in prior works?). 1d) It would be great to see the selected features be used not only for offline image similarity assessment, but also for training image processing models - in the end, this has been the main use of *perceptual metrics*. Do they lead to improved results? 1e) Since the paper is about (subjective) image quality, it might be useful to show some qualitative results, potentially in the appendix if space is an issue. 2) There are some issues with the presentation: 2a) I had a hard time understanding what exactly *Contrast Sensitivity Function* and *contrast masking* are. 2b) Minor issues: - In the abstract: *trained object detection deep CNNs* - I guess image classification is meant - Beginning of Section 2: *Section. 2*, *convolution layer as collection channels* - Section 4: *corresponds the the peak: - Section 5.2 *Berkeley-Adobpe*\n",
      "      - Review:###In the article the authors propose to measure quality of CNN-features by quantifying the orientation tuning and spatial frequency sensitivity of the features. The underlying hypothesis is that properties of features in the human visual cortex are also indicators for quality in CNNs. The authors devise an experiment similar to experiments performed on mammals to check which features are active under which types of basic patterns. Afterwards, a loss-function is devised that uses proportions of the best or worst features according to the metrics and it is shown that features that have high values on the metrics also lead to good performance. ----------------------------------------------------------------------------------------- I have a problem understanding some of the metrics used. In the introduction, the following claim is made: *The first attribute is sensitivity to spatial frequencies at which there is minimal contrast masking in human visual perception*. To my understanding, the metric to measure this is (2) in 4.2. a_m^k is to my understanding the average response of the feature when given an image with orientation-frequency f. therefore, the derivative should be *the change of activation under change of frequency*. I feel unable to connect this with the initial hypothesis, as it does not mention change of frequencies. i would have expected the correlation between CSF(f) and a^k_m(f), did you have a reason why you did not chose correlation? Similarly, for mu_2 in (3) you are using the maximum value. Is there a reason not to use the mean? in that case mu_2 would be the variance, which would be a natural measure for orientation selectivity. ------------------ Is there a way to make Table 1 more pleasing for the human eye wrt the discussion of the results? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###In the article the authors propose to measure quality of CNN-features by quantifying the orientation tuning and spatial frequency sensitivity of the features. The underlying hypothesis is that properties of features in the human visual cortex are also indicators for quality in CNNs. The authors devise an experiment similar to experiments performed on mammals to check which features are active under which types of basic patterns. Afterwards, a loss-function is devised that uses proportions of the best or worst features according to the metrics and it is shown that features that have high values on the metrics also lead to good performance. ----------------------------------------------------------------------------------------- I have a problem understanding some of the metrics used. In the introduction, the following claim is made: *The first attribute is sensitivity to spatial frequencies at which there is minimal contrast masking in human visual perception*. To my understanding, the metric to measure this is (2) in 4.2. a_m^k is to my understanding the average response of the feature when given an image with orientation-frequency f. therefore, the derivative should be *the change of activation under change of frequency*. I feel unable to connect this with the initial hypothesis, as it does not mention change of frequencies. i would have expected the correlation between CSF(f) and a^k_m(f), did you have a reason why you did not chose correlation? Similarly, for mu_2 in (3) you are using the maximum value. Is there a reason not to use the mean? in that case mu_2 would be the variance, which would be a natural measure for orientation selectivity. ------------------ Is there a way to make Table 1 more pleasing for the human eye wrt the discussion of the results?\n",
      "      - Review:###This paper proposes an analysis of convolutional neural networks (CNNs) features the basis for making perceptual quality comparisons. The analysis is based on the proposed Perceptual Efficacy (PE) Score that measures spatial frequency and orientation selectivity of CNN features. The hypothesis put forward by the authors is that a CNN features with high PE score can be used to formulate a perceptual loss (Eq. 1) that correlates well with human image quality judgement. The authors use a dataset of human image quality judgements to assess their hypothesis. One issue I see with the hypothesis as stated is that in the definition of frequency selective features, the authors make use of the Contrast Sensitivity Function (CSF) which quantifies the dependency of human perceptual characteristics on frequency. So in the definition of the PE score we have embedded knowledge of human perceptual sensitivity. Is it therefore surprising that we see correlation between the high PE features and human judgements of quality? Experimental results: The scatterplot presented in Figure 4 does not say to me what the authors claim it should. I do not see a significant difference between the low-PE features and the high-PE features in terms of their correlation with human image quality judgement (as measures in this case by the DMOS). I also find the large table of number in Table 1 to be rather impenetrable. I would recommend an alternative method of presentation to make the desired point. Clarity: There are many undefined terms and acronyms (eg. SISR, HVS are not defined, while DMOS and SROCC are not described). Also, the description of visual masking in Sec. 4.3 was confusing and difficult to follow. Otherwise the writing was reasonably clear. Impact and significance: Overall, the findings of the paper are not terribly surprising and as discuss above, given the use of the CSF (quantifying human perceptual characteristics) in the definition of the CNN Perceptual Efficacy (PE) score, it would seem rather surprising that a correlations would not be found. As a result of this as well as the rather narrow nature of the study involved, I am inclined to think that the impact potential of this paper would be rather low. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes an analysis of convolutional neural networks (CNNs) features the basis for making perceptual quality comparisons. The analysis is based on the proposed Perceptual Efficacy (PE) Score that measures spatial frequency and orientation selectivity of CNN features. The hypothesis put forward by the authors is that a CNN features with high PE score can be used to formulate a perceptual loss (Eq. 1) that correlates well with human image quality judgement. The authors use a dataset of human image quality judgements to assess their hypothesis. One issue I see with the hypothesis as stated is that in the definition of frequency selective features, the authors make use of the Contrast Sensitivity Function (CSF) which quantifies the dependency of human perceptual characteristics on frequency. So in the definition of the PE score we have embedded knowledge of human perceptual sensitivity. Is it therefore surprising that we see correlation between the high PE features and human judgements of quality? Experimental results: The scatterplot presented in Figure 4 does not say to me what the authors claim it should. I do not see a significant difference between the low-PE features and the high-PE features in terms of their correlation with human image quality judgement (as measures in this case by the DMOS). I also find the large table of number in Table 1 to be rather impenetrable. I would recommend an alternative method of presentation to make the desired point. Clarity: There are many undefined terms and acronyms (eg. SISR, HVS are not defined, while DMOS and SROCC are not described). Also, the description of visual masking in Sec. 4.3 was confusing and difficult to follow. Otherwise the writing was reasonably clear. Impact and significance: Overall, the findings of the paper are not terribly surprising and as discuss above, given the use of the CSF (quantifying human perceptual characteristics) in the definition of the CNN Perceptual Efficacy (PE) score, it would seem rather surprising that a correlations would not be found. As a result of this as well as the rather narrow nature of the study involved, I am inclined to think that the impact potential of this paper would be rather low.\n",
      "    - Clarity, Exposition, or Presentation Issues (L1, 3)\n",
      "      - Review:###1. Summary of the paper This paper describes the decision boundaries of a certain class of neural networks (piecewise linear, non-linear activation functions) through the lens of tropical geometry. An earlier result by Zhang et al. (2018) is extended to multi-class classification problems (technically, only the result for a binary classification is given in the main text, though). Similar to this earlier work, the network is shown to be represented as a tropical rational function. The dual subdivision of this function is shown to be represented as the convex hull of two zonotopes. This characterisation is used to explain different phenomena of neural network training, viz. the *lottery ticket hypothesis*, network pruning, and adversarial attacks. 2. Summary of the review This is a highly interesting paper with a very relevant subject. I think that the perspective of tropical geometry leads to valuable insights. My background is *not* in tropical geometry, so this paper required several passes to fully grasp. I like the novel insights that this paper creates; it is very interesting to observe known phenomena via tropical geometry. I appreciate the thorough description of all concepts in this paper. This is to some extent both *boon and bane*: on the one hand, the paper contains a lot of information and concepts that need to be understood; on the other hand, the experiments go *wide* but not *deep*. I suggest to accept the paper, but to fully endorse it, I would recommend to work on the following issues: - Clarity & exposition: In some places, the paper could build intuition for non-experts (such as myself) better. This is closely tied to the second point. - Focus: I would maybe pick *one* or *two* of the experimental areas and use the remaining space to explain all concepts in more detail, build some intuition, and provide a more in-depth setup. Nothing has to removed of course; it could still be put in the appendix. This paper has the potential to be a very strong insightful contribution to to our community; the authors are to be commended! 3. Detailed comments (clarity) The paper describes its concepts well and has a high information density. At times, there is the risk that readers are provided with too much information in the main text, leaving the necessary intuition somewhat lacking (unless they are already experts in the subject matter, in which case a lot of the information can be skipped). I realise that writing a paper based on methods that are not yet well-established is no small feat; the authors are to be commended for that! Here are some suggestions from someone with a background in differential topology: - The introduction and contributions are somewhat repetitive; I would suggest merging the *Contributions.* paragraph with the one preceding it - Even though it *should* be a well-known definition, I would briefly explain that the semiring lacks an additive inverse - Add an explanation of the tropical quotient to Definition 1; I find the current phrasing of Definition 3 to be confusing at first glance - What are *upper faces*? Faces with a specific coordinate fixed? - A definition of is required. It is my understanding that is a projection function that *drops* the last coordinate. Is this correct? If so, it should be briefly mentioned on p. 3; else, the discussion about the bias-free case on p. 4 cannot be understood. - Theorem 2 lacks an *intuitive* formulation; the results are stated in a terse mathematical fashion, but it would be helpful (also in light of the subsequent discussion) to briefly comment on their *meaning*. For example, the first result could be restated as *the decision boundary is a subset of a tropical hypersurface of the polynomial *. The paragraph *Theorem 2 bridges the gap...* could maybe also be moved to _precede_ the theorem statement. - The paper refers to as a super-set, but in my understanding, it is a *level set* because Definition 4 uses an equality, not an inequality. Am I misunderstanding this? One potential misinterpretation of my part could be that superset refers to the fact that ; so not a superset in the sense of level set analysis, but rather a superset in terms of set theory. If this is the case, maybe rephrase the sentence above to something like *boundaries through their superset according to the first statement of Theorem 2*. - What does the colour map in Figure 2 depict? The number of iterations? Moreover, I find the polytope though to understand at first glance; how is it related to the decision boundaries that are shown in the leftmost figure? - I would suggest to place Figure 1 after stating Theorem 2, since it is only referenced later on. Furthermore, the red structures are somewhat confusing. According to Theorem 2, the decision boundary is a subset of the hypersurface, right? What is the relation of the red structures in the convex hull visualisation? The caption states that they are normals, but as far as I can tell, this has not been formalised anywhere in the paper (it is used later on, though). - To what extent is the existence of the functions described by Theorem 2 unique? On p. 5, in Section 4, the paper alludes to *not* using the functional form of the network directly because it does not seem to be unique. I would like this to be explained a in more details, as I found the justification of why the dual subdivision is used quite hard to follow. - In Section 4, how many experiments of the sort were performed? I find this a highly instructive view so I would love to see more experiments of this sort. Do these claims hold over multiple repetitions and for (slightly) larger architectures as well? Please also see my comments on Figure 2 above. - The claim that orientations are preserved should be formalised. I immediately understand the intuition behind this concept, but if possible, I would like a quantification of this. Might it be possible to *measure* changes in orientation with respect to an original polytope? If so, it should be possible to provide more experiments about these effects and summarise them accordingly. Maybe it would also be interesting to investigate whether other initialisations can be compared in terms of their orientations? - In Section 5, I would give a brief link to the appendix for the definition of a Minkowski sum. - Section 5 has (in contrast to the other sections) a lot of details containing the experimental setup, but it is missing a description of the competitor methods. Adding to what I wrote above, I feel that the paper should rather pick *one* area in which experiments are performed; the pruning (together with the lottery ticket hypothesis explanation, which could be seen as a motivating example) strikes me as a good candidate for this. I really like this concept of tropical pruning, by the way---it is an elegant, principled description! - The plots in Figure 4 should summarise multiple pruning runs, if possible. Why not show a standard deviation as well? Given the stochasticity of training, I would think this highly necessary. - In Section 6, I find the comment on normals generating a superset to the decision boundaries hard to understand. - The perspective of perturbing the network such that the decision boundaries change is really interesting, but I am missing a *take-away message* or a discussion of the insights. Currently, this section seems more like a feasibility study: it appears to be possible to use the tropical description to find new parameters that misclassify a given input. I would propose a discussion of the implications of these findings. - Concerning future extensions of this work, are there some promising results or directions for CNNs or GCNs? If so, it would strengthen the conclusion if they were mentioned. 4. Minor style issues The paper is well-written. I found some minor style issues: - *piece-wise* --> *piecewise* (occur multiple times) - *recently demonstrated* --> *demonstrated* - *Thereafter, tropical hypersurfaces divide* --> *Tropical hypersurfaces divide* - *If set --> *Letting B* - I am not sure if I would call adversarial attacks a *nuisance*; maybe rather a *problem*? - Use *operatorname* or *mathrm* to typeset the loss in Eq. 5 5. Update after rebuttal The authors addressed all my important comments; their efforts in rewriting and revising the paper in such a short time period are to be commended. I am very happy to raise my score. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###1. Summary of the paper This paper describes the decision boundaries of a certain class of neural networks (piecewise linear, non-linear activation functions) through the lens of tropical geometry. An earlier result by Zhang et al. (2018) is extended to multi-class classification problems (technically, only the result for a binary classification is given in the main text, though). Similar to this earlier work, the network is shown to be represented as a tropical rational function. The dual subdivision of this function is shown to be represented as the convex hull of two zonotopes. This characterisation is used to explain different phenomena of neural network training, viz. the *lottery ticket hypothesis*, network pruning, and adversarial attacks. 2. Summary of the review This is a highly interesting paper with a very relevant subject. I think that the perspective of tropical geometry leads to valuable insights. My background is *not* in tropical geometry, so this paper required several passes to fully grasp. I like the novel insights that this paper creates; it is very interesting to observe known phenomena via tropical geometry. I appreciate the thorough description of all concepts in this paper. This is to some extent both *boon and bane*: on the one hand, the paper contains a lot of information and concepts that need to be understood; on the other hand, the experiments go *wide* but not *deep*. I suggest to accept the paper, but to fully endorse it, I would recommend to work on the following issues: - Clarity & exposition: In some places, the paper could build intuition for non-experts (such as myself) better. This is closely tied to the second point. - Focus: I would maybe pick *one* or *two* of the experimental areas and use the remaining space to explain all concepts in more detail, build some intuition, and provide a more in-depth setup. Nothing has to removed of course; it could still be put in the appendix. This paper has the potential to be a very strong insightful contribution to to our community; the authors are to be commended! 3. Detailed comments (clarity) The paper describes its concepts well and has a high information density. At times, there is the risk that readers are provided with too much information in the main text, leaving the necessary intuition somewhat lacking (unless they are already experts in the subject matter, in which case a lot of the information can be skipped). I realise that writing a paper based on methods that are not yet well-established is no small feat; the authors are to be commended for that! Here are some suggestions from someone with a background in differential topology: - The introduction and contributions are somewhat repetitive; I would suggest merging the *Contributions.* paragraph with the one preceding it - Even though it *should* be a well-known definition, I would briefly explain that the semiring lacks an additive inverse - Add an explanation of the tropical quotient to Definition 1; I find the current phrasing of Definition 3 to be confusing at first glance - What are *upper faces*? Faces with a specific coordinate fixed? - A definition of is required. It is my understanding that is a projection function that *drops* the last coordinate. Is this correct? If so, it should be briefly mentioned on p. 3; else, the discussion about the bias-free case on p. 4 cannot be understood. - Theorem 2 lacks an *intuitive* formulation; the results are stated in a terse mathematical fashion, but it would be helpful (also in light of the subsequent discussion) to briefly comment on their *meaning*. For example, the first result could be restated as *the decision boundary is a subset of a tropical hypersurface of the polynomial *. The paragraph *Theorem 2 bridges the gap...* could maybe also be moved to _precede_ the theorem statement. - The paper refers to as a super-set, but in my understanding, it is a *level set* because Definition 4 uses an equality, not an inequality. Am I misunderstanding this? One potential misinterpretation of my part could be that superset refers to the fact that ; so not a superset in the sense of level set analysis, but rather a superset in terms of set theory. If this is the case, maybe rephrase the sentence above to something like *boundaries through their superset according to the first statement of Theorem 2*. - What does the colour map in Figure 2 depict? The number of iterations? Moreover, I find the polytope though to understand at first glance; how is it related to the decision boundaries that are shown in the leftmost figure? - I would suggest to place Figure 1 after stating Theorem 2, since it is only referenced later on. Furthermore, the red structures are somewhat confusing. According to Theorem 2, the decision boundary is a subset of the hypersurface, right? What is the relation of the red structures in the convex hull visualisation? The caption states that they are normals, but as far as I can tell, this has not been formalised anywhere in the paper (it is used later on, though). - To what extent is the existence of the functions described by Theorem 2 unique? On p. 5, in Section 4, the paper alludes to *not* using the functional form of the network directly because it does not seem to be unique. I would like this to be explained a in more details, as I found the justification of why the dual subdivision is used quite hard to follow. - In Section 4, how many experiments of the sort were performed? I find this a highly instructive view so I would love to see more experiments of this sort. Do these claims hold over multiple repetitions and for (slightly) larger architectures as well? Please also see my comments on Figure 2 above. - The claim that orientations are preserved should be formalised. I immediately understand the intuition behind this concept, but if possible, I would like a quantification of this. Might it be possible to *measure* changes in orientation with respect to an original polytope? If so, it should be possible to provide more experiments about these effects and summarise them accordingly. Maybe it would also be interesting to investigate whether other initialisations can be compared in terms of their orientations? - In Section 5, I would give a brief link to the appendix for the definition of a Minkowski sum. - Section 5 has (in contrast to the other sections) a lot of details containing the experimental setup, but it is missing a description of the competitor methods. Adding to what I wrote above, I feel that the paper should rather pick *one* area in which experiments are performed; the pruning (together with the lottery ticket hypothesis explanation, which could be seen as a motivating example) strikes me as a good candidate for this. I really like this concept of tropical pruning, by the way---it is an elegant, principled description! - The plots in Figure 4 should summarise multiple pruning runs, if possible. Why not show a standard deviation as well? Given the stochasticity of training, I would think this highly necessary. - In Section 6, I find the comment on normals generating a superset to the decision boundaries hard to understand. - The perspective of perturbing the network such that the decision boundaries change is really interesting, but I am missing a *take-away message* or a discussion of the insights. Currently, this section seems more like a feasibility study: it appears to be possible to use the tropical description to find new parameters that misclassify a given input. I would propose a discussion of the implications of these findings. - Concerning future extensions of this work, are there some promising results or directions for CNNs or GCNs? If so, it would strengthen the conclusion if they were mentioned. 4. Minor style issues The paper is well-written. I found some minor style issues: - *piece-wise* --> *piecewise* (occur multiple times) - *recently demonstrated* --> *demonstrated* - *Thereafter, tropical hypersurfaces divide* --> *Tropical hypersurfaces divide* - *If set --> *Letting B* - I am not sure if I would call adversarial attacks a *nuisance*; maybe rather a *problem*? - Use *operatorname* or *mathrm* to typeset the loss in Eq. 5 5. Update after rebuttal The authors addressed all my important comments; their efforts in rewriting and revising the paper in such a short time period are to be commended. I am very happy to raise my score.\n",
      "      - Review:###Summary: The paper proposes a method for improving the scalability of communication-based cooperative multi-agent reinforcement learning. While existing approaches assume a fixed underlying network topology over which agents communicate, in the proposed method, this network topology is dynamic (changes at each time step) and learnable (by assigning a weight to each node and *rewiring* nodes in a particular way based on these weights). Authors highlight the importance of having a topology that is roughly similar to a collection of star-topologies. The center of stars (central nodes) further form a complete graph. They argue that such a topology can achieve global cooperation while reducing the number of messages exchanged as compared to the case where all agents can communicate with each other. To learn a dynamically changing topology, the method assigns a weight (an integer between 0 and 4) to each agent based on its local observation. An existing method (CBRP) is then used to establish connections between agents based on weights assigned to them. A graph neural network (GNN) is used for computing the messages that are exchanged among agents. Communication uses the following 3 steps: (i) agents talk to the central agent(s) to which they are connected, (ii) central agents exchange information among themselves, and (iii) central agents transmit information to the agents that are connected to them. Each agent uses a deep Q-network - the parameters of this network are shared across agents. This Q-network receives rewards from the environment. Gradients flowing through Q-network are also used to update GNN. Since CBRP is non-differentiable, the parameters for network that computes weights for all the agents are also updated using a Q-network that gets the same reward from the environment as the first Q-network. Experiments done on MAgent environment demonstrate that: (i) communication is useful and (ii) method scales well as number of agents increases. Additional qualitative studies have also been performed to understand the content of messages and the learned strategies. Authors have also experimented with the StarCraftII environment. Comments: The paper deals with an interesting problem, however, the presentation can be significantly improved as there are multiple grammatical mistakes in the manuscript. Unfortunately, the work does not position very well with the existing literature. The motivation and the impact of the contributions are not very clear. I would rate contributions as marginal. It is not clear what POSs* terms in Algorithm 1 mean. It would be interesting to see which agents become central agents over time. As central agents form a complete graph, if there are many central agents then the approach will be inefficient. Under the message visualization heading on p9, it is not clear how one decides whether a message was a *move* message or an *attack* message. Questions to the Authors: 1. On p2, it is written that when concatenation or mean operation is used for aggregation, then inter-relationship between agents are not captured. What does this mean? Why does this problem not apply to GNN based solution which may also use mean for aggregation? 2. On p10, first line, it is written that all methods were made to react to the same initial state. How was this state chosen? A few questions are also embedded in the comments above. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Summary: The paper proposes a method for improving the scalability of communication-based cooperative multi-agent reinforcement learning. While existing approaches assume a fixed underlying network topology over which agents communicate, in the proposed method, this network topology is dynamic (changes at each time step) and learnable (by assigning a weight to each node and *rewiring* nodes in a particular way based on these weights). Authors highlight the importance of having a topology that is roughly similar to a collection of star-topologies. The center of stars (central nodes) further form a complete graph. They argue that such a topology can achieve global cooperation while reducing the number of messages exchanged as compared to the case where all agents can communicate with each other. To learn a dynamically changing topology, the method assigns a weight (an integer between 0 and 4) to each agent based on its local observation. An existing method (CBRP) is then used to establish connections between agents based on weights assigned to them. A graph neural network (GNN) is used for computing the messages that are exchanged among agents. Communication uses the following 3 steps: (i) agents talk to the central agent(s) to which they are connected, (ii) central agents exchange information among themselves, and (iii) central agents transmit information to the agents that are connected to them. Each agent uses a deep Q-network - the parameters of this network are shared across agents. This Q-network receives rewards from the environment. Gradients flowing through Q-network are also used to update GNN. Since CBRP is non-differentiable, the parameters for network that computes weights for all the agents are also updated using a Q-network that gets the same reward from the environment as the first Q-network. Experiments done on MAgent environment demonstrate that: (i) communication is useful and (ii) method scales well as number of agents increases. Additional qualitative studies have also been performed to understand the content of messages and the learned strategies. Authors have also experimented with the StarCraftII environment. Comments: The paper deals with an interesting problem, however, the presentation can be significantly improved as there are multiple grammatical mistakes in the manuscript. Unfortunately, the work does not position very well with the existing literature. The motivation and the impact of the contributions are not very clear. I would rate contributions as marginal. It is not clear what POSs* terms in Algorithm 1 mean. It would be interesting to see which agents become central agents over time. As central agents form a complete graph, if there are many central agents then the approach will be inefficient. Under the message visualization heading on p9, it is not clear how one decides whether a message was a *move* message or an *attack* message. Questions to the Authors: 1. On p2, it is written that when concatenation or mean operation is used for aggregation, then inter-relationship between agents are not captured. What does this mean? Why does this problem not apply to GNN based solution which may also use mean for aggregation? 2. On p10, first line, it is written that all methods were made to react to the same initial state. How was this state chosen? A few questions are also embedded in the comments above.\n",
      "      - Review:###The paper attempts to develop a counterpart of the well-known Nesterov accelerated gradient method for gradient flows on the space of probability measures equipped with an information metric. This is an important problem which is useful for optimization on probability spaces. The accelerated gradient flow is developed by leveraging a damping Hamiltonian flow. The paper focuses mainly on the case with the Wasserstein metric and provides a convergence analysis. Practical considerations such as discretizing the accelerated flow and bandwidth selection are developed for the use of the method in practical problems. Although the paper has some important merit, I find the paper extremely hard to follow, partly because of its writing style. There is not enough motivation and explanation for the ideas presented. Some discussions and sentences either do not make much sense to me or read badly. For example, this sentence *For the Wasserstein gradient, many classical methods such as Markov Chain Monte Carlo .... are based on this framework...* doesn*t make sense, as the development of MCMC is never based on Wasserstein gradient. Or the sentence right before it *For the Fisher-Rao gradient, classical results including Adam ... and K-FAC .. demonstrate its effectiveness in ...*: it*s not clear what the authors are trying to say here. Adam is not relevant to the Fisher-Rao natural gradient while K-FAC is just an approximation method and isn*t a good reference for demonstrating the effectiveness of the natural gradient. Also, there are many English typos and grammar errors. I didn*t read the proof carefully due to the time constraint, so I cannot judge on the theoretical part of the paper. The numerical experiment is quite limited as it considers very simple problems (a toy example, a single Gaussian distribution and a logistic regression problem). As such, I think there isn*t enough evidence to judge the usefulness of the proposed method in practice. Having said that, I believe this paper can be an important contribution if the authors invest more time on refining its presentation and if more thorough experimental studies are conducted. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper attempts to develop a counterpart of the well-known Nesterov accelerated gradient method for gradient flows on the space of probability measures equipped with an information metric. This is an important problem which is useful for optimization on probability spaces. The accelerated gradient flow is developed by leveraging a damping Hamiltonian flow. The paper focuses mainly on the case with the Wasserstein metric and provides a convergence analysis. Practical considerations such as discretizing the accelerated flow and bandwidth selection are developed for the use of the method in practical problems. Although the paper has some important merit, I find the paper extremely hard to follow, partly because of its writing style. There is not enough motivation and explanation for the ideas presented. Some discussions and sentences either do not make much sense to me or read badly. For example, this sentence *For the Wasserstein gradient, many classical methods such as Markov Chain Monte Carlo .... are based on this framework...* doesn*t make sense, as the development of MCMC is never based on Wasserstein gradient. Or the sentence right before it *For the Fisher-Rao gradient, classical results including Adam ... and K-FAC .. demonstrate its effectiveness in ...*: it*s not clear what the authors are trying to say here. Adam is not relevant to the Fisher-Rao natural gradient while K-FAC is just an approximation method and isn*t a good reference for demonstrating the effectiveness of the natural gradient. Also, there are many English typos and grammar errors. I didn*t read the proof carefully due to the time constraint, so I cannot judge on the theoretical part of the paper. The numerical experiment is quite limited as it considers very simple problems (a toy example, a single Gaussian distribution and a logistic regression problem). As such, I think there isn*t enough evidence to judge the usefulness of the proposed method in practice. Having said that, I believe this paper can be an important contribution if the authors invest more time on refining its presentation and if more thorough experimental studies are conducted.\n",
      "    - Dataset, problem, or evaluation protocol clarity (application/dataset clarity or too easy) (L1, 1)\n",
      "      -  While this paper tackles an interesting problem. The technical approach is unfortunately too outdated and obvious and not quite the level of ICLR. The dataset is likely too easy given the high accuracy. (L0, 1)\n",
      "        * Examples:\n",
      "          -  While this paper tackles an interesting problem. The technical approach is unfortunately too outdated and obvious and not quite the level of ICLR. The dataset is likely too easy given the high accuracy.\n",
      "    - Issues with Clarity, Explanation, and Notation (L1, 1)\n",
      "      - Review:###This paper is reporting an unsupervised approach to learn node embeddings and communities simultaneously by minimizing the mincut loss function. This approach programs the data through encoder to generate membership likehood matrix H, and then generates corresponding membership matrix P using node selection. The matrix P and adjacency matrix are coupled to generate community adjacency matrix to minimize the cutting. However, despite such attractive points, the novelty and strength of this study is not outstanding enough for publication in ICLR. Details comments are as follows, 1. Similar work such as the methods of transferring matrix E to H then to P have already been published , which reduce the novelty of this study. 2. Though there is a schematic, the learning embedding process is still not described clearly. More details of the algorithms need to be discussed to such as how to get the node embedding matrix E. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper is reporting an unsupervised approach to learn node embeddings and communities simultaneously by minimizing the mincut loss function. This approach programs the data through encoder to generate membership likehood matrix H, and then generates corresponding membership matrix P using node selection. The matrix P and adjacency matrix are coupled to generate community adjacency matrix to minimize the cutting. However, despite such attractive points, the novelty and strength of this study is not outstanding enough for publication in ICLR. Details comments are as follows, 1. Similar work such as the methods of transferring matrix E to H then to P have already been published , which reduce the novelty of this study. 2. Though there is a schematic, the learning embedding process is still not described clearly. More details of the algorithms need to be discussed to such as how to get the node embedding matrix E.\n",
      "    - Paper Quality/Presentation Issues (Writing, Formatting, Anonymity) (L1, 2)\n",
      "      -  - This paper simply proposes to use UNet for the segmentation of stagnant zones in X-ray CTs. While the applicability of this model may represent an advance in the particular field of the authors, the technical contribution of this paper is far from the level expected in this conference. - As the paper reads, the main contribution of the paper is the modified version of UNet *proposed* by the authors, which major modification consists on replacing SGD by Adam. Nevertheless, this cannot be considered a contribution, as changing the optimizer in a deep model is a marginal change, from a methodological point of view. - Overall, the quality of the paper is below the standards of ICLR (content, technical contribution, length). - The submission is not anonymized (authors included their names and affiliations). (L0, 1)\n",
      "        * Examples:\n",
      "          -  - This paper simply proposes to use UNet for the segmentation of stagnant zones in X-ray CTs. While the applicability of this model may represent an advance in the particular field of the authors, the technical contribution of this paper is far from the level expected in this conference. - As the paper reads, the main contribution of the paper is the modified version of UNet *proposed* by the authors, which major modification consists on replacing SGD by Adam. Nevertheless, this cannot be considered a contribution, as changing the optimizer in a deep model is a marginal change, from a methodological point of view. - Overall, the quality of the paper is below the standards of ICLR (content, technical contribution, length). - The submission is not anonymized (authors included their names and affiliations).\n",
      "      - Review:###This paper propose a modified U-net architecture to segment the stagnant zone during silo discharging process. It lacks novelty and the improvement is marginal. More importantly than all of that, this paper violates the double blind review rule and is same with [1]. So I think this paper is not suitable for acception. [1]Waktola S, Grudzien K, Babout L. Stagnant zone segmentation with U-net[C]//2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering (AIKE). IEEE, 2019: 277-280. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper propose a modified U-net architecture to segment the stagnant zone during silo discharging process. It lacks novelty and the improvement is marginal. More importantly than all of that, this paper violates the double blind review rule and is same with [1]. So I think this paper is not suitable for acception. [1]Waktola S, Grudzien K, Babout L. Stagnant zone segmentation with U-net[C]//2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering (AIKE). IEEE, 2019: 277-280.\n",
      "    - Presentation and Clarity Issues (L1, 2)\n",
      "      - Review:######Summary### This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ### Novelty ### The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty. ###Clarity### Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ###Pros### 1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. 2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. 3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ###Cons### 1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset: DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/ Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/ 2) The organization and presentation of this paper should be polished. Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:######Summary### This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ### Novelty ### The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty. ###Clarity### Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ###Pros### 1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. 2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. 3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ###Cons### 1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset: DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/ Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/ 2) The organization and presentation of this paper should be polished. Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.\n",
      "      - Review:###Parameter sharing (PS) is an important approach to speed up neural network search (NAS), which further allows the development of differential architecture search (e.g., DARTS) methods. However, PS also deteriorates the performance of learning models on the validation/testing set. This paper first changes the search space from a DAG (micro+marco) in e.g., DARTS to a stacked one based on MBConv; and then, propose to use several tricks to train the super-net well. Finally, a search method is constructed for the supernet to find the desired architectures. Overall, the paper is too experimental. The method is an ensemble of existing approaches, i.e., every single component in the paper has been visited in the literature. Expect for experimental results, I do not see many general lessons we can learn from the paper. Finally, why the proposed method can be better than others is not well-explained and clarified. Please see the questions below: Q1. Is NAS a method only for ImageNet? Can the method generalize to more applications/datasets? - While ImageNet is a good dataset for CV experiments, I think NAS should be a method for deriving architectures with certain requirements. - So, with so many tricks proposed in the paper, I wish authors can carry on experiments on other data sets as well, e.g., CIFAR and MNIST, which can still be preferred. Q2. On motivation, could authors explain more about the difficulties of combining all these techniques? - Each method is brought from some other paper, what motivate authors to combine them together? What makes them believe this is possible? Q3. On presentation, could authors draw a figure of the search space in the main text and give an overall algorithm for Section *3.2 COARSE-TO-FINE ARCHITECTURE SELECTION*. It is hard for a reader to see novelties there. Q4. *We also use the swish activation (Ramachandran et al., 2017) and fixed AutoAugment V0 policy* - are all other compared methods using swish activation and AutoAugment V0 policy? Q5. How about the search efficiency of the proposed method? Only the accuracy is reported in the paper. Q6. Could authors give STD (i.e., gray area to represent STD) in Figure 4, 5 and 7? Some curves are too close, I am not sure they are statistically different. Q7. How is the performance of the super-net? Q8. Could the authors add an ablation study on this point? - *The motivation is to improve all child models in our search space simultaneously, by pushing up both the performance lower bound (the smallest child model) and the performance upper bound (the biggest child model) across all child models.* - It is important to avoid fine-tune - From the paper, I am not sure whether the problem is solved by changing the space or the proposed training method (See Q1). (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Parameter sharing (PS) is an important approach to speed up neural network search (NAS), which further allows the development of differential architecture search (e.g., DARTS) methods. However, PS also deteriorates the performance of learning models on the validation/testing set. This paper first changes the search space from a DAG (micro+marco) in e.g., DARTS to a stacked one based on MBConv; and then, propose to use several tricks to train the super-net well. Finally, a search method is constructed for the supernet to find the desired architectures. Overall, the paper is too experimental. The method is an ensemble of existing approaches, i.e., every single component in the paper has been visited in the literature. Expect for experimental results, I do not see many general lessons we can learn from the paper. Finally, why the proposed method can be better than others is not well-explained and clarified. Please see the questions below: Q1. Is NAS a method only for ImageNet? Can the method generalize to more applications/datasets? - While ImageNet is a good dataset for CV experiments, I think NAS should be a method for deriving architectures with certain requirements. - So, with so many tricks proposed in the paper, I wish authors can carry on experiments on other data sets as well, e.g., CIFAR and MNIST, which can still be preferred. Q2. On motivation, could authors explain more about the difficulties of combining all these techniques? - Each method is brought from some other paper, what motivate authors to combine them together? What makes them believe this is possible? Q3. On presentation, could authors draw a figure of the search space in the main text and give an overall algorithm for Section *3.2 COARSE-TO-FINE ARCHITECTURE SELECTION*. It is hard for a reader to see novelties there. Q4. *We also use the swish activation (Ramachandran et al., 2017) and fixed AutoAugment V0 policy* - are all other compared methods using swish activation and AutoAugment V0 policy? Q5. How about the search efficiency of the proposed method? Only the accuracy is reported in the paper. Q6. Could authors give STD (i.e., gray area to represent STD) in Figure 4, 5 and 7? Some curves are too close, I am not sure they are statistically different. Q7. How is the performance of the super-net? Q8. Could the authors add an ablation study on this point? - *The motivation is to improve all child models in our search space simultaneously, by pushing up both the performance lower bound (the smallest child model) and the performance upper bound (the biggest child model) across all child models.* - It is important to avoid fine-tune - From the paper, I am not sure whether the problem is solved by changing the space or the proposed training method (See Q1).\n",
      "    - Problems with Clarity, Explanations, or Writing (L1, 3)\n",
      "      -  This paper proposes modifications and modular extensions to the differential neural computer (DNC). The approach is nicely modular, decoupling the data modules from algorithmic modules. This enables the authors to pretrain the data modules with supervised learning and to train the small algorithmic modules with neural evolution strategies (NES). NES is a global optimization method (which may be understood as policy gradients where the parameters of the neural policy are the actions) and consequently this enables the authors to use discrete selection mechanisms instead of the soft attention mechanisms of DNC. The authors correctly argue for generalization to different and unseen tasks (R1), independence of data representation (R2) and independence of task environment (R3). These are important points, and the authors are able to achieve these by using a modular approach where the interface modules are pretrained. While what can be achieved with modularity is important, the modules themselves are rather simple and leave open the question of scalability. NES clearly does not scale to higher dimensional spaces, and the complexity or real say vision modules is the problem for many people --- one does not just learn the vision module and freeze it. The idea that modularity can be used to attain greater generality and domain independence has already been explored at ICLR to some extent. In fact, some authors have shown theoretically provable generalisation via verification approaches: https://openreview.net/forum?id=BkbY4psgg In summary, this paper demonstrated the advantages of modular neural systems, but fails to address the important issue of making sure the modules scale to real problems. (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper proposes modifications and modular extensions to the differential neural computer (DNC). The approach is nicely modular, decoupling the data modules from algorithmic modules. This enables the authors to pretrain the data modules with supervised learning and to train the small algorithmic modules with neural evolution strategies (NES). NES is a global optimization method (which may be understood as policy gradients where the parameters of the neural policy are the actions) and consequently this enables the authors to use discrete selection mechanisms instead of the soft attention mechanisms of DNC. The authors correctly argue for generalization to different and unseen tasks (R1), independence of data representation (R2) and independence of task environment (R3). These are important points, and the authors are able to achieve these by using a modular approach where the interface modules are pretrained. While what can be achieved with modularity is important, the modules themselves are rather simple and leave open the question of scalability. NES clearly does not scale to higher dimensional spaces, and the complexity or real say vision modules is the problem for many people --- one does not just learn the vision module and freeze it. The idea that modularity can be used to attain greater generality and domain independence has already been explored at ICLR to some extent. In fact, some authors have shown theoretically provable generalisation via verification approaches: https://openreview.net/forum?id=BkbY4psgg In summary, this paper demonstrated the advantages of modular neural systems, but fails to address the important issue of making sure the modules scale to real problems.\n",
      "      - Review:###The paper studies active learning in graph representations. To decide which nodes in a graph to label during the active labeling, the paper proposes two approaches. First it argues that one should consider region-based measures rather than single nodes. Second, it proposes to adapt the page rank algorithm (APR) to determine which nodes are far away from labeled nodes. Overall it remains unclear *how* to select the right strategy (before seeing the results for a dataset) i.e. which of the proposed approaches or variants should one select for a new dataset. Strength: - One of the ideas of the paper, using region entropy over single node entropy makes sense to me. - The paper evaluates on 6 datasets and compares different variants as well to related work on 2 datasets. Weaknesses: 1. The paper contains several confusing and contradicting statements or claims which are not supported by the experimental results: For example: 1.1. “APR outperforms all other methods at low sampling fractions”. This is supported neither in Table 1 nor Table 2, where APR is frequently not highest performing 1.2. “We have here shown that the accuracy of AL when uncertainty is computed regionally is much higher than when either local uncertainty or representative nodes are used”, this is not the case on CiteSeer in Table 1 1.2.1. Also e.g. “Region Margin” is worse than random on 5% Email-EU; or “Region Margin AE” on 3% SubeljCora (Table 2) [It is unclear how to select with or without AE] 1.3. “We outperform all existing methods in the Cora dataset, and get very similar results to the best accuracy obtained by Chang et al methods:” 1.3.1. The difference to Cai et al. on Cora is very small (improvement by only 0.002), while on Citeseer the performance is comparatively bigger (Cai et al. is by 0.016 better) 1.3.2. It should be “Cai et al” 2. Clarity: I found the paper rather difficult to understand and follow: Some specifics: 2.1. The introduction could be more concisely discussing the motivation, the main idea of the paper, as well as contributions. 2.2. Figure 1: according to the caption, APR should point to node 15, but in the figure it points to node 14. From the example it makes much more sense to label node 14 to me. 2.3. Page 6 mentions twice the “ratio between APR and PR”, is this is this used/evaluated in the results? 2.4. The decision what is bold and what is not is not consistent throughout the table 2. 2.5. “Thus, hybrid techniques, combining several approaches, outperform using only one approach have been proposed.” It is not clear what this refers to and where the hybrid techniques have been evaluated. Minor: The paper contains many minor writing issues, e.g. - missing spaces, e.g. “distribution,and” (page 2) - Table 1: incomplete sentence: “?? scores for smaller budget, since it was the” - Table 2: unclear: “accuracy without content” The paper’s incorrect claims (weakness 1) are highly concerning and strongly suggest rejecting the paper. Furthermore, the clarity of the paper should be improved to follow the author arguments and make the paper easier to read. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper studies active learning in graph representations. To decide which nodes in a graph to label during the active labeling, the paper proposes two approaches. First it argues that one should consider region-based measures rather than single nodes. Second, it proposes to adapt the page rank algorithm (APR) to determine which nodes are far away from labeled nodes. Overall it remains unclear *how* to select the right strategy (before seeing the results for a dataset) i.e. which of the proposed approaches or variants should one select for a new dataset. Strength: - One of the ideas of the paper, using region entropy over single node entropy makes sense to me. - The paper evaluates on 6 datasets and compares different variants as well to related work on 2 datasets. Weaknesses: 1. The paper contains several confusing and contradicting statements or claims which are not supported by the experimental results: For example: 1.1. “APR outperforms all other methods at low sampling fractions”. This is supported neither in Table 1 nor Table 2, where APR is frequently not highest performing 1.2. “We have here shown that the accuracy of AL when uncertainty is computed regionally is much higher than when either local uncertainty or representative nodes are used”, this is not the case on CiteSeer in Table 1 1.2.1. Also e.g. “Region Margin” is worse than random on 5% Email-EU; or “Region Margin AE” on 3% SubeljCora (Table 2) [It is unclear how to select with or without AE] 1.3. “We outperform all existing methods in the Cora dataset, and get very similar results to the best accuracy obtained by Chang et al methods:” 1.3.1. The difference to Cai et al. on Cora is very small (improvement by only 0.002), while on Citeseer the performance is comparatively bigger (Cai et al. is by 0.016 better) 1.3.2. It should be “Cai et al” 2. Clarity: I found the paper rather difficult to understand and follow: Some specifics: 2.1. The introduction could be more concisely discussing the motivation, the main idea of the paper, as well as contributions. 2.2. Figure 1: according to the caption, APR should point to node 15, but in the figure it points to node 14. From the example it makes much more sense to label node 14 to me. 2.3. Page 6 mentions twice the “ratio between APR and PR”, is this is this used/evaluated in the results? 2.4. The decision what is bold and what is not is not consistent throughout the table 2. 2.5. “Thus, hybrid techniques, combining several approaches, outperform using only one approach have been proposed.” It is not clear what this refers to and where the hybrid techniques have been evaluated. Minor: The paper contains many minor writing issues, e.g. - missing spaces, e.g. “distribution,and” (page 2) - Table 1: incomplete sentence: “?? scores for smaller budget, since it was the” - Table 2: unclear: “accuracy without content” The paper’s incorrect claims (weakness 1) are highly concerning and strongly suggest rejecting the paper. Furthermore, the clarity of the paper should be improved to follow the author arguments and make the paper easier to read.\n",
      "      - Review:###This work design a framework to predict valence and arousal values in the emotional impact. Although its performance is much better than previous works, I have some questions about the current submission: -- The writing is not clear enough. I have to guess the technical details based on the context. For example, in Equation 1, how to estimate y_{i}? And why put 1/m behind sum? Is there any particular reason for this? It is a classification problem here, why use MSE loss rather than cross-entropy loss? -- *If modality i plays an important role, it would push the mapping to complete the f_{i-1} towards f_i to get better performance.* Are you suggesting that sometimes H() will be about zero? -- Figure 2 is not clear enough for me. I still can not fully understand what ``progressive* means here. I will appreciate it if the authors can make this more clear. -- What concerns me the most is the novelty of this work. It seems solid and effective. However, based on the current content, I can not fully appreciate its novelty. Using deep learning models to extract features from different modalities has been used before. Using deep learning to consider inter-clip and intra-clip relations also has been conducted. Using LSTM for fusion is not new, either. The authors claimed they propose a progressive training strategy for effectively training and information fusion. However, given the current version, I can not fully appreciate it. I can change my point if the authors would provide more details and explanations later, which can help me to understand the novelty of this work fully. Thanks. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This work design a framework to predict valence and arousal values in the emotional impact. Although its performance is much better than previous works, I have some questions about the current submission: -- The writing is not clear enough. I have to guess the technical details based on the context. For example, in Equation 1, how to estimate y_{i}? And why put 1/m behind sum? Is there any particular reason for this? It is a classification problem here, why use MSE loss rather than cross-entropy loss? -- *If modality i plays an important role, it would push the mapping to complete the f_{i-1} towards f_i to get better performance.* Are you suggesting that sometimes H() will be about zero? -- Figure 2 is not clear enough for me. I still can not fully understand what ``progressive* means here. I will appreciate it if the authors can make this more clear. -- What concerns me the most is the novelty of this work. It seems solid and effective. However, based on the current content, I can not fully appreciate its novelty. Using deep learning models to extract features from different modalities has been used before. Using deep learning to consider inter-clip and intra-clip relations also has been conducted. Using LSTM for fusion is not new, either. The authors claimed they propose a progressive training strategy for effectively training and information fusion. However, given the current version, I can not fully appreciate it. I can change my point if the authors would provide more details and explanations later, which can help me to understand the novelty of this work fully. Thanks.\n",
      "    - Problems with Clarity, Writing Quality, or Presentation (L1, 7)\n",
      "      -  ## Overview The paper tackles object detection without predefined anchors (sliding windows). The paper is well motivated and existing detection methods often rely on anchors, which may limits their potential. So this paper is solving an interesting problem and seems novel. The paper provides experiments to support the proposed architecture empirically. ## Summary of the contribution: 1. The paper proposed an object detection approach called FoveaBox that does not rely on anchors (sliding widows). 2. The paper shows that FoveaBox outperforms some existing object detection methods. 3. The paper shows that FoveaBox can also be used for object proposals by changing the classification target to class-agnostic head. ## My feedback: I think the paper is well motivated and has value to the object detection community. So I am leaning positive to accept this paper. The major reasons are: 1. The paper studies an interesting architecture which does not rely on sliding windows and shows its effectiveness. The problem seems not very well studied in the existing object detection literature. 2. The paper provides sufficient ablation study to analyze and understand the proposed methods. However, I believe the paper could be significantly improved especially in the writing. So my position is not strong. ## Improvements 1. The writing can be significantly improved. The paper reads a bit confusing and unclear especially at the technical part. The paper could benefit from a clear overview figure about the proposed approach. Figure 4 seems to be doing that illustration but giving only the tensor shape seems quite confusing. I also listed some questions below about the actual algorithm. 2. The paper could be improved with experiments and ablation on another dataset. Current ablation shows that eta should be 2.0 for best performance but without a second dataset it is hard to say this value is general. So one may have to tune this parameters in different datasets. 3. The scale is still discretized. So that is essentially anchors in the scale space. I wonder how could the approach applies to scale to? ## Questions 1. How would the proposed approach address the issue when there are multiple bounding boxes around the same pixels? It seems the current approach is predicting a box per pixel? 2. What is the inference computation cost? And do you have a comparison with existing methods on the compute cost? (L0, 1)\n",
      "        * Examples:\n",
      "          -  ## Overview The paper tackles object detection without predefined anchors (sliding windows). The paper is well motivated and existing detection methods often rely on anchors, which may limits their potential. So this paper is solving an interesting problem and seems novel. The paper provides experiments to support the proposed architecture empirically. ## Summary of the contribution: 1. The paper proposed an object detection approach called FoveaBox that does not rely on anchors (sliding widows). 2. The paper shows that FoveaBox outperforms some existing object detection methods. 3. The paper shows that FoveaBox can also be used for object proposals by changing the classification target to class-agnostic head. ## My feedback: I think the paper is well motivated and has value to the object detection community. So I am leaning positive to accept this paper. The major reasons are: 1. The paper studies an interesting architecture which does not rely on sliding windows and shows its effectiveness. The problem seems not very well studied in the existing object detection literature. 2. The paper provides sufficient ablation study to analyze and understand the proposed methods. However, I believe the paper could be significantly improved especially in the writing. So my position is not strong. ## Improvements 1. The writing can be significantly improved. The paper reads a bit confusing and unclear especially at the technical part. The paper could benefit from a clear overview figure about the proposed approach. Figure 4 seems to be doing that illustration but giving only the tensor shape seems quite confusing. I also listed some questions below about the actual algorithm. 2. The paper could be improved with experiments and ablation on another dataset. Current ablation shows that eta should be 2.0 for best performance but without a second dataset it is hard to say this value is general. So one may have to tune this parameters in different datasets. 3. The scale is still discretized. So that is essentially anchors in the scale space. I wonder how could the approach applies to scale to? ## Questions 1. How would the proposed approach address the issue when there are multiple bounding boxes around the same pixels? It seems the current approach is predicting a box per pixel? 2. What is the inference computation cost? And do you have a comparison with existing methods on the compute cost?\n",
      "      -  This paper introduces deep audio prior (DAP), which uses CNN*S prior to perform classical tasks in audio processing: source separation, denoising, texture synthesis, co-separation. This paper gets inspiration from deep image prior and adapts it to audio, by introducing a lot of insights in the audio domain, which I believe is a good amount of contribution. I have the following questions on the paper: (1) The source generation part is a bit confusing. When testing on real-world examples, do you need to generate sources? I want more explanations on the ablations studies in Section 4, temporal/dynamic sources. What are the input/output, how is the noise generated, how is the model trained. (2) In general, I expect more details in the paper, like the model architecture (does that affect performance), how training is performed (iteration, convergence, etc). (3) Some notations are missing, e.g. equation (4). (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper introduces deep audio prior (DAP), which uses CNN*S prior to perform classical tasks in audio processing: source separation, denoising, texture synthesis, co-separation. This paper gets inspiration from deep image prior and adapts it to audio, by introducing a lot of insights in the audio domain, which I believe is a good amount of contribution. I have the following questions on the paper: (1) The source generation part is a bit confusing. When testing on real-world examples, do you need to generate sources? I want more explanations on the ablations studies in Section 4, temporal/dynamic sources. What are the input/output, how is the noise generated, how is the model trained. (2) In general, I expect more details in the paper, like the model architecture (does that affect performance), how training is performed (iteration, convergence, etc). (3) Some notations are missing, e.g. equation (4).\n",
      "      -  This paper is quite difficult to read. The figures are pixelated. There is almost no organization to the text of the work. Descriptions are imprecise and lax: for example, *We apply basic preprocessing on Amharic News Comments. These include normalization of Amharic script symbols, tokenization, stop word removal, punctuation mark removal and so on. Amharic writing system is expressed using only consonants. To handle the features of the language is very challenging. We require conversion of Amharic scripts to consonant-vowel form. Particularly, before performing negation handling and stemming, the algorithm converts each Amharic word to its consonant vowel form.* What kind of normalization? What kind of tokenization? Did you use outside tools? If so, cite them or refer to the code. If not, then please explain your method more thoroughly. Don*t assume all your readers know what consonant-vowel form is. *and so on* is not appropriate. You should explain what the so on is in a paper submitted to a conference. Don*t bother telling us the features of the language are challenging. Just tell us how you did it. Switching between words like *cue* and *clue* Even the font of the text changes part way through the paper. Figure 2 is almost entirely uninformative. The algorithms are poorly displayed and nearly unreadable. Experiments are too limited. (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper is quite difficult to read. The figures are pixelated. There is almost no organization to the text of the work. Descriptions are imprecise and lax: for example, *We apply basic preprocessing on Amharic News Comments. These include normalization of Amharic script symbols, tokenization, stop word removal, punctuation mark removal and so on. Amharic writing system is expressed using only consonants. To handle the features of the language is very challenging. We require conversion of Amharic scripts to consonant-vowel form. Particularly, before performing negation handling and stemming, the algorithm converts each Amharic word to its consonant vowel form.* What kind of normalization? What kind of tokenization? Did you use outside tools? If so, cite them or refer to the code. If not, then please explain your method more thoroughly. Don*t assume all your readers know what consonant-vowel form is. *and so on* is not appropriate. You should explain what the so on is in a paper submitted to a conference. Don*t bother telling us the features of the language are challenging. Just tell us how you did it. Switching between words like *cue* and *clue* Even the font of the text changes part way through the paper. Figure 2 is almost entirely uninformative. The algorithms are poorly displayed and nearly unreadable. Experiments are too limited.\n",
      "      - Review:###1. Summary The authors learn structured communication patterns between multiple RL agents. Their framework uses a Structured Communication Network Module and Communication-based Policy Module. These use a hierarchical decomposition of the multi-agent system and a graph neural network that operates over the resulting abstract agent (groups). The authors evaluate on two environments, where this approach outperforms other ways to communication protocols. 2. Decision (accept or reject) with one or two key reasons for this choice. Weak accept. 3. Supporting arguments Scalable communication with many agents will require a (learned) trade-off between structural priors and learning representations of communicaton (protocols). This work seems like an interesting step in analyzing how to implement this. 4. Additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. 5. Questions (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###1. Summary The authors learn structured communication patterns between multiple RL agents. Their framework uses a Structured Communication Network Module and Communication-based Policy Module. These use a hierarchical decomposition of the multi-agent system and a graph neural network that operates over the resulting abstract agent (groups). The authors evaluate on two environments, where this approach outperforms other ways to communication protocols. 2. Decision (accept or reject) with one or two key reasons for this choice. Weak accept. 3. Supporting arguments Scalable communication with many agents will require a (learned) trade-off between structural priors and learning representations of communicaton (protocols). This work seems like an interesting step in analyzing how to implement this. 4. Additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. 5. Questions\n",
      "      - Review:###I acknowledge reading the rebuttal of the authors. Thank you for your clarifications and explanation. My point was this paper would make a good submission to ICLR if it was better motivated presented and explained to a wider audience. Unfortunately in its current form it can only reach a limited audience. #### Summary of the paper: The paper proposes accelerated information flows under Wasserstein or Fisher rao metric . i.e a method for solving minimization of probability functional , where the probability space is either endowed with the Wasserstein or the Fisher distance. Gradient descent methods in euclidian spaces can be accelerated using a type of momentum , this paper extends this to gradient flows, using similar formalism of Hamiltonian that appeals to the dynamic of a the particles and the velocity (or momentum, ). Writing down the lagrangian one obtains two co-evolving PDE one of the dynamic of the density and one for the potential . The PDEs are specified for both Fisher Rao, and Wasserstein distance. The hamiltonian for example for the Wasserstein distance . and PDEs amounts a continuity equation for the density evolving with drift , the evolution of the momentum is also given by a PDE. Proposition 2 of the paper gives the particles differential equation corresponding to the system of PDEs. For the energy being the KL divergence an explicit expression is given , this expression remains difficult in practice since it needs the knowledge of the density . Authors propose in the application section to use gaussian approximation , or using a kernel density estimators. The Bandwidth of the kernel is then choosen using a heuristic proposed in the paper. The paper then focuses on deriving expression for flows when the densities are centered gaussians, and this amounts to an ODE on the covariance , the ODE is discretized in Appendix D.2 to lead to computational method. Then convergence of the flow is analyzed for the wasserstein accelerated flows, under eta- convextiy* in the wasserstein sense of the functional. Some experiments of the particle based method are shown on synthetic experiments and in bayesian logistic regression. Review: Contribution/ Clarity: The main contribution of the paper is in deriving the accelerated gradient flow for the wasserstein distance this was also addressed in a recent paper [Taghevia and Mettha 2019]. The technical contribution is interesting but given that this field of flows in probability space is still not very well spread in the ML community, I wonder if ICLR is the best fit for this type of work. I support good theoretical work, but I think the authors could have done a better job in exposing the ideas how they extend form euclidean space, to manifolds, to probability spaces gradually. Simple derivations of euclidean space Hamiltonian will help the reader that is not exposed to such literature. I think the paper will benefit from a less technical writing in introducing the ideas coming from euclidean space and in conveying the intuitions. Comments: - In the proof of Proposition 2 you give the expression of evolution of by conservation of the momentum. Could you please elaborate more how you obtain this expression, and where you proved the conservation of momentum? - In term of damping if ones uses the Wasserstein Fisher Rao flows , one obtain also accelerartion , maybe you can comment on that ? since you analyze both flows , would be interesting to discuss the relation to Global convergence of neuron birth-death dynamics, that shows that an acceleration is obtained via WFR flows, since it will introduce a damping as well. - since MCMC and BM method lead to similar result what is the advantage of the wasserstein accelerated flow? one could also implement also an accelerated langevin dynamic (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###I acknowledge reading the rebuttal of the authors. Thank you for your clarifications and explanation. My point was this paper would make a good submission to ICLR if it was better motivated presented and explained to a wider audience. Unfortunately in its current form it can only reach a limited audience. #### Summary of the paper: The paper proposes accelerated information flows under Wasserstein or Fisher rao metric . i.e a method for solving minimization of probability functional , where the probability space is either endowed with the Wasserstein or the Fisher distance. Gradient descent methods in euclidian spaces can be accelerated using a type of momentum , this paper extends this to gradient flows, using similar formalism of Hamiltonian that appeals to the dynamic of a the particles and the velocity (or momentum, ). Writing down the lagrangian one obtains two co-evolving PDE one of the dynamic of the density and one for the potential . The PDEs are specified for both Fisher Rao, and Wasserstein distance. The hamiltonian for example for the Wasserstein distance . and PDEs amounts a continuity equation for the density evolving with drift , the evolution of the momentum is also given by a PDE. Proposition 2 of the paper gives the particles differential equation corresponding to the system of PDEs. For the energy being the KL divergence an explicit expression is given , this expression remains difficult in practice since it needs the knowledge of the density . Authors propose in the application section to use gaussian approximation , or using a kernel density estimators. The Bandwidth of the kernel is then choosen using a heuristic proposed in the paper. The paper then focuses on deriving expression for flows when the densities are centered gaussians, and this amounts to an ODE on the covariance , the ODE is discretized in Appendix D.2 to lead to computational method. Then convergence of the flow is analyzed for the wasserstein accelerated flows, under eta- convextiy* in the wasserstein sense of the functional. Some experiments of the particle based method are shown on synthetic experiments and in bayesian logistic regression. Review: Contribution/ Clarity: The main contribution of the paper is in deriving the accelerated gradient flow for the wasserstein distance this was also addressed in a recent paper [Taghevia and Mettha 2019]. The technical contribution is interesting but given that this field of flows in probability space is still not very well spread in the ML community, I wonder if ICLR is the best fit for this type of work. I support good theoretical work, but I think the authors could have done a better job in exposing the ideas how they extend form euclidean space, to manifolds, to probability spaces gradually. Simple derivations of euclidean space Hamiltonian will help the reader that is not exposed to such literature. I think the paper will benefit from a less technical writing in introducing the ideas coming from euclidean space and in conveying the intuitions. Comments: - In the proof of Proposition 2 you give the expression of evolution of by conservation of the momentum. Could you please elaborate more how you obtain this expression, and where you proved the conservation of momentum? - In term of damping if ones uses the Wasserstein Fisher Rao flows , one obtain also accelerartion , maybe you can comment on that ? since you analyze both flows , would be interesting to discuss the relation to Global convergence of neuron birth-death dynamics, that shows that an acceleration is obtained via WFR flows, since it will introduce a damping as well. - since MCMC and BM method lead to similar result what is the advantage of the wasserstein accelerated flow? one could also implement also an accelerated langevin dynamic\n",
      "      - Review:###The paper aims to improve injury prediction and modeling in the National Football League (NFL) using machine learning. Unfortunately, the lack of clarity in the paper and poor writing prevents me from writing a thorough review. Given that the authors seem to apply off-the-shelf machine learning algorithms to NFL data, and spend a significant amount of time discussing low-level details of data storage, preprocessing, and underlying hardware/software pipelines, this paper does not appear to be a good fit for ICLR and would be better served at either a data science conference, or alternatively a sports medicine/science conference. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper aims to improve injury prediction and modeling in the National Football League (NFL) using machine learning. Unfortunately, the lack of clarity in the paper and poor writing prevents me from writing a thorough review. Given that the authors seem to apply off-the-shelf machine learning algorithms to NFL data, and spend a significant amount of time discussing low-level details of data storage, preprocessing, and underlying hardware/software pipelines, this paper does not appear to be a good fit for ICLR and would be better served at either a data science conference, or alternatively a sports medicine/science conference.\n",
      "      - Review:###This paper introduces a machine learning pipeline for injury prediction in NFL events. The paper discusses several system settings on data streaming and processing, along with model selection and other hyper-parameter tuning details. The problem itself is very important. However, there are several disadvantages of the current status of the paper. The writing of this paper needs to be largely improved. The description is very redundant and the texts are very hard to read. Therefore, it makes the paper much less clear to the reader. Second, the proposed system is not related to the focus of ICLR and it lacks the novelty. Both the data processing and model selection methods are already well-known practicies. Experiments are also not established well enough to demonstrate the advantage of the proposed solution to this problem. Due to the above issues, I think the paper is not ready for publication. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper introduces a machine learning pipeline for injury prediction in NFL events. The paper discusses several system settings on data streaming and processing, along with model selection and other hyper-parameter tuning details. The problem itself is very important. However, there are several disadvantages of the current status of the paper. The writing of this paper needs to be largely improved. The description is very redundant and the texts are very hard to read. Therefore, it makes the paper much less clear to the reader. Second, the proposed system is not related to the focus of ICLR and it lacks the novelty. Both the data processing and model selection methods are already well-known practicies. Experiments are also not established well enough to demonstrate the advantage of the proposed solution to this problem. Due to the above issues, I think the paper is not ready for publication.\n",
      "    - Unclear Presentation or Ambiguity in Explanation (L1, 15)\n",
      "      -  This paper extends the neural coreference resolution model in Lee et al. (2018) by 1) introducing an additional mention-level feature (grammatical numbers), and 2) letting the mention/pair scoring functions attend over multiple mention-level features. The proposed model achieves marginal improvement (0.2 avg. F1 points) over Lee et al., 2018, on the CoNLL 2012 English test set. I recommend rejection for this paper due to the following reasons: - The technical contribution is very incremental (introducing one more features, and adding an attention layer over the feature vectors). - The experiment results aren*t strong enough. And the experiments are done on only one dataset. - I am not convinced that adding the grammatical numbers features and the attention mechanism makes the model more context-aware. Other suggestions: - The citation format seems weird through out the paper. - Table 1 and 3 look like ablation results. It might be less confusing if it*s presented as *Full system: xx%; without pairwise FA: yy%; without grammatical numbers zz% ...*. - Equation 8 - 10 are quite confusing. What is f(x)? How large is V? What is u? etc. - Please define/explain the *grammatical numbers* feature when it*s introduced in Section 2.2. (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper extends the neural coreference resolution model in Lee et al. (2018) by 1) introducing an additional mention-level feature (grammatical numbers), and 2) letting the mention/pair scoring functions attend over multiple mention-level features. The proposed model achieves marginal improvement (0.2 avg. F1 points) over Lee et al., 2018, on the CoNLL 2012 English test set. I recommend rejection for this paper due to the following reasons: - The technical contribution is very incremental (introducing one more features, and adding an attention layer over the feature vectors). - The experiment results aren*t strong enough. And the experiments are done on only one dataset. - I am not convinced that adding the grammatical numbers features and the attention mechanism makes the model more context-aware. Other suggestions: - The citation format seems weird through out the paper. - Table 1 and 3 look like ablation results. It might be less confusing if it*s presented as *Full system: xx%; without pairwise FA: yy%; without grammatical numbers zz% ...*. - Equation 8 - 10 are quite confusing. What is f(x)? How large is V? What is u? etc. - Please define/explain the *grammatical numbers* feature when it*s introduced in Section 2.2.\n",
      "      -  This paper proposed to adopt data augmentation over explicit regularization (weight decay and dropout), supported by a comparative study of 3 ConvNets (all-conv, wide-resnet and densenet) on three image classification benchmarks (ImageNet, CIFAR-10 and CIFAR-100), as well as variants of experiments such as training with smaller training set. Most of the observations presented here are largely known to the community, though I think at least a few of them are worth emphasizing: 1. One major downside of explicit regularization the requirement to tune hyper-parameters on each different scenarios. For example, the paper shows that the default hyper-parameter becomes suboptimal when the architecture becomes either deeper or shallower. 2. Given strong enough data augmentation, sometimes turning off weight decay / dropout could give better test performance (i.e. the optimal hyper-parameters are zeros in those cases). That being said, I think this the paper at the current state does not contain sufficient message to stand as a full paper. I list a few areas that I think could potentially improve the paper if properly addressed: 1. Proper definition of explicit / implicit regularization: one goal this paper is trying to achieve is an unambiguous definition of explicit / implicit regularization. However, the current definition given by the paper is not any less ambiguous than prevision conventions. If I understand correctly, the paper defines explicit regularization as mechanism that is explicitly designed to reduce the model capacity, and implicit one as anything else (that happens to improve generalization). There is still a lot ambiguity here: for example, one technique, say, random left-right flip of the inputs, is considered implicit regularization because it improves CIFAR-10 classification performance. However, applying it to wrong data, e.g. speech recognition, would potentially hurt the test performance. Would this technique sometimes be an implicit regularization and sometimes not? Another question is regarding dropout, which is classified as explicit regularization by the paper. Yet, if dropout is applied not to some intermediate layer, but to the input layer, does it suddenly become more similar to data augmentation (think of, e.g. cutout augmentation), and therefore an *implicit* regularization? I think a if clear, mathematical and formally verifiable definition of different types of regularization, if possible, will definite make the paper stronger. 2. Theoretical analysis: currently the paper shows the definition of the Rademacher complexity and some handwavy discussions. It does not provide any additional insights apart from what the definition says: explicit regularization constrains the capacity and data augmentation increase the number of training examples. The discussion mentioned that the augmented data are non i.i.d. which would be an interesting topic for providing theoretical insights, but it is ruled *out of the scope of the paper*. Besides, the definition of the Rademacher complexity captures the capacity of the hypothesis via the mathematical sup operator, and is completely independent of the underlying algorithm used to compute the sup. Therefore, data augmentation, which is part of the training procedure, does not change the Rademacher complexity, unless we completely re-define the hypothesis space to be a complicated notion that captures something like *all the functions achievable by SGD with this augmentation and that hyper-parameters, etc.*. But this will turn the *implicit regularization* nature of *data augmentation* into the regime of *explicit regularization*, because it is now confining the hypothesis space into a subset (so reducing the capacity). In summary, data augmentation clearly improves the generalization performance, but to formally characterize it, one needs an alternative approach from the default Rademacher complexity way. It would make the paper much stronger if a viable approach is proposed, and even stronger if the non-i.i.d. nature of augmented data points could be discussed under the proposed framework. 3. Empirical studies: I think the paper could still be strong even if without any theoretical characterization, if it contains strong supporting experiments. However, currently the paper only studies image classification tasks and a few convolutional neural networks. Moreover, even within those domain and tasks, there are many well known techniques, such as the cutout augmentation, that are not studied. It would be of a much useful paper to the community if the paper could provide a comprehensive survey of existing explicit / implicit regularization techniques across multiple tasks and domains (e.g. the paper mentioned vision, speech recognition, NLP, etc.). Ideally, pros and cons of each regularization technique could be discussed and if the message that *data augmentation is better than explicit regularization* could hold across multiple domains and tasks, then this will definitely be delivering a strong message. Even if that is not the case, this paper could still serve as practical guides to practitioners for choosing between different approaches of regularization techniques. Other potential improvements are: a. hyper-parameter tuning for each regularization technique. The paper acknowledged that the hyper-parameters are suboptimal when they change settings for explicit regularization. This is demonstrating a good point that explicit regularization is inconvenient. However, it would be better if in parallel with the default hyper-parameter, we could learn what would be the best results if we re-tune the best hyper-parameters. Because sometimes people would like to achieve the best performance in practice with all the resources and hammers they could get their hands on. So, for example, with strong data augmentation, would the (re-tuned) optimal dropout rate actually perform even better? (if true, this is also contradicting with the main message of the paper) b. to some extent data augmentation also have *hyper-parameters*, which control how to augment the data. It would be interesting to see, for example, controlled experiments on the effects on test performance when the *wrong* type or magnitudes of augmentation is applied to the data. minor things: * all bars in the plots are red / purplish colors. Maybe more diverse colors could be used to make it easier to distinguish which is which? * Why is there no DenseNet results on ImageNet? (e.g. Figure 1 missing one row?) * As a paper that studies augmentation, it would be great to provide full details on all the details (including the hyper parameters on the magnitudes of all perturbations) of the data augmentation used, especially for the *heavier augmentation* variant. Finally, I*m not sure if this violate the policy of anonymous submission --- the acknowledgements section mentioned explicit names when thanking for feedback. (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper proposed to adopt data augmentation over explicit regularization (weight decay and dropout), supported by a comparative study of 3 ConvNets (all-conv, wide-resnet and densenet) on three image classification benchmarks (ImageNet, CIFAR-10 and CIFAR-100), as well as variants of experiments such as training with smaller training set. Most of the observations presented here are largely known to the community, though I think at least a few of them are worth emphasizing: 1. One major downside of explicit regularization the requirement to tune hyper-parameters on each different scenarios. For example, the paper shows that the default hyper-parameter becomes suboptimal when the architecture becomes either deeper or shallower. 2. Given strong enough data augmentation, sometimes turning off weight decay / dropout could give better test performance (i.e. the optimal hyper-parameters are zeros in those cases). That being said, I think this the paper at the current state does not contain sufficient message to stand as a full paper. I list a few areas that I think could potentially improve the paper if properly addressed: 1. Proper definition of explicit / implicit regularization: one goal this paper is trying to achieve is an unambiguous definition of explicit / implicit regularization. However, the current definition given by the paper is not any less ambiguous than prevision conventions. If I understand correctly, the paper defines explicit regularization as mechanism that is explicitly designed to reduce the model capacity, and implicit one as anything else (that happens to improve generalization). There is still a lot ambiguity here: for example, one technique, say, random left-right flip of the inputs, is considered implicit regularization because it improves CIFAR-10 classification performance. However, applying it to wrong data, e.g. speech recognition, would potentially hurt the test performance. Would this technique sometimes be an implicit regularization and sometimes not? Another question is regarding dropout, which is classified as explicit regularization by the paper. Yet, if dropout is applied not to some intermediate layer, but to the input layer, does it suddenly become more similar to data augmentation (think of, e.g. cutout augmentation), and therefore an *implicit* regularization? I think a if clear, mathematical and formally verifiable definition of different types of regularization, if possible, will definite make the paper stronger. 2. Theoretical analysis: currently the paper shows the definition of the Rademacher complexity and some handwavy discussions. It does not provide any additional insights apart from what the definition says: explicit regularization constrains the capacity and data augmentation increase the number of training examples. The discussion mentioned that the augmented data are non i.i.d. which would be an interesting topic for providing theoretical insights, but it is ruled *out of the scope of the paper*. Besides, the definition of the Rademacher complexity captures the capacity of the hypothesis via the mathematical sup operator, and is completely independent of the underlying algorithm used to compute the sup. Therefore, data augmentation, which is part of the training procedure, does not change the Rademacher complexity, unless we completely re-define the hypothesis space to be a complicated notion that captures something like *all the functions achievable by SGD with this augmentation and that hyper-parameters, etc.*. But this will turn the *implicit regularization* nature of *data augmentation* into the regime of *explicit regularization*, because it is now confining the hypothesis space into a subset (so reducing the capacity). In summary, data augmentation clearly improves the generalization performance, but to formally characterize it, one needs an alternative approach from the default Rademacher complexity way. It would make the paper much stronger if a viable approach is proposed, and even stronger if the non-i.i.d. nature of augmented data points could be discussed under the proposed framework. 3. Empirical studies: I think the paper could still be strong even if without any theoretical characterization, if it contains strong supporting experiments. However, currently the paper only studies image classification tasks and a few convolutional neural networks. Moreover, even within those domain and tasks, there are many well known techniques, such as the cutout augmentation, that are not studied. It would be of a much useful paper to the community if the paper could provide a comprehensive survey of existing explicit / implicit regularization techniques across multiple tasks and domains (e.g. the paper mentioned vision, speech recognition, NLP, etc.). Ideally, pros and cons of each regularization technique could be discussed and if the message that *data augmentation is better than explicit regularization* could hold across multiple domains and tasks, then this will definitely be delivering a strong message. Even if that is not the case, this paper could still serve as practical guides to practitioners for choosing between different approaches of regularization techniques. Other potential improvements are: a. hyper-parameter tuning for each regularization technique. The paper acknowledged that the hyper-parameters are suboptimal when they change settings for explicit regularization. This is demonstrating a good point that explicit regularization is inconvenient. However, it would be better if in parallel with the default hyper-parameter, we could learn what would be the best results if we re-tune the best hyper-parameters. Because sometimes people would like to achieve the best performance in practice with all the resources and hammers they could get their hands on. So, for example, with strong data augmentation, would the (re-tuned) optimal dropout rate actually perform even better? (if true, this is also contradicting with the main message of the paper) b. to some extent data augmentation also have *hyper-parameters*, which control how to augment the data. It would be interesting to see, for example, controlled experiments on the effects on test performance when the *wrong* type or magnitudes of augmentation is applied to the data. minor things: * all bars in the plots are red / purplish colors. Maybe more diverse colors could be used to make it easier to distinguish which is which? * Why is there no DenseNet results on ImageNet? (e.g. Figure 1 missing one row?) * As a paper that studies augmentation, it would be great to provide full details on all the details (including the hyper parameters on the magnitudes of all perturbations) of the data augmentation used, especially for the *heavier augmentation* variant. Finally, I*m not sure if this violate the policy of anonymous submission --- the acknowledgements section mentioned explicit names when thanking for feedback.\n",
      "      -  This paper proposes a label correction approach based on a likelihood ratio test, for robust training of deep neural networks against label noise. First, this paper introduces the LRT-Correction procedure, which is the main component of the proposed label correction approach. LRT-Correction uses the current model prediction to run a likelihood ratio test and flip labels when they are rejected. The decision is made by comparing the likelihood test results with a predefined value Delta. Then they introduce the full algorithm, AdaCorr, where the LRT-Correction procedure serves as an inner loop for the label correction. Lastly, there are experiments done on four datasets to conclude the superior performance of the proposed AdaCorr in contrast to several existing methods. Overall, this paper proposes a new label correction approach based on a likelihood ratio test. Standard experiments show that the proposed AdaCorr is superior to several existing methods. The following questions are expected to be addressed during rebuttal: 1. The LRT-Correction procedure introduces additional computation costs. What is the difference in computation costs between Standard and the proposed label correction approach? Is the extra computation cost significant? 2. The LRT-Correction procedure is introduced based on a binary setting. The main theorem (Theorem 1) also only supports the binary setting. There is a statement in Corollary 1 that “LRT-Correction can be generalized to multiclass classification tasks, by flipping \tilde{y} to be the best prediction of f when the null hypothesis is rejected. Theorem 1 can be generalized to multiclass classification tasks, by considering all pairs of class values.” How exactly did you do for that? Please provide more details. 3. This paper uses the ablation study to demonstrate that the proposed AdaCorr is robust to several important hyper-parameters, e.g. the number of epochs m for the burn-in stage, the predefined value Delta for likelihood ratio test. How did you exactly choose the optimal value for these hyper-parameters? These is a statement “We choose m=20 in this data set (CIFAR10) and “similarly” in other datasets.” Did you use the same m(=20) for all datasets? Does this also hold for the hyper-parameter Delta ? 4. The experiments are too standard. Any results on real-world datasets, e.g. Clothing1M [1]? Minor comments: 1. The summarization of the existing related work is not consistent throughout the paper. In Introduction section, this paper believes that the existing methods mainly follow two directions, i.e. probabilistic reasoning and data selecting; while in Related Work section, they are classified into three categories. Please clarify your arguments. 2. Page 5: In Corollary 1, “LRT-Correctioncan” -> “LRT-Correction can” 3. Page 7: In Table 2, MINIST -> MNIST. 4. Page 7: In Experiment Setup, please provide more training details, e.g. learning rate. [1] Xiao, Tong, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. *Learning from massive noisy labeled data for image classification.* In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2691-2699. 2015. (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper proposes a label correction approach based on a likelihood ratio test, for robust training of deep neural networks against label noise. First, this paper introduces the LRT-Correction procedure, which is the main component of the proposed label correction approach. LRT-Correction uses the current model prediction to run a likelihood ratio test and flip labels when they are rejected. The decision is made by comparing the likelihood test results with a predefined value Delta. Then they introduce the full algorithm, AdaCorr, where the LRT-Correction procedure serves as an inner loop for the label correction. Lastly, there are experiments done on four datasets to conclude the superior performance of the proposed AdaCorr in contrast to several existing methods. Overall, this paper proposes a new label correction approach based on a likelihood ratio test. Standard experiments show that the proposed AdaCorr is superior to several existing methods. The following questions are expected to be addressed during rebuttal: 1. The LRT-Correction procedure introduces additional computation costs. What is the difference in computation costs between Standard and the proposed label correction approach? Is the extra computation cost significant? 2. The LRT-Correction procedure is introduced based on a binary setting. The main theorem (Theorem 1) also only supports the binary setting. There is a statement in Corollary 1 that “LRT-Correction can be generalized to multiclass classification tasks, by flipping \tilde{y} to be the best prediction of f when the null hypothesis is rejected. Theorem 1 can be generalized to multiclass classification tasks, by considering all pairs of class values.” How exactly did you do for that? Please provide more details. 3. This paper uses the ablation study to demonstrate that the proposed AdaCorr is robust to several important hyper-parameters, e.g. the number of epochs m for the burn-in stage, the predefined value Delta for likelihood ratio test. How did you exactly choose the optimal value for these hyper-parameters? These is a statement “We choose m=20 in this data set (CIFAR10) and “similarly” in other datasets.” Did you use the same m(=20) for all datasets? Does this also hold for the hyper-parameter Delta ? 4. The experiments are too standard. Any results on real-world datasets, e.g. Clothing1M [1]? Minor comments: 1. The summarization of the existing related work is not consistent throughout the paper. In Introduction section, this paper believes that the existing methods mainly follow two directions, i.e. probabilistic reasoning and data selecting; while in Related Work section, they are classified into three categories. Please clarify your arguments. 2. Page 5: In Corollary 1, “LRT-Correctioncan” -> “LRT-Correction can” 3. Page 7: In Table 2, MINIST -> MNIST. 4. Page 7: In Experiment Setup, please provide more training details, e.g. learning rate. [1] Xiao, Tong, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. *Learning from massive noisy labeled data for image classification.* In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2691-2699. 2015.\n",
      "      -  This paper proposes a new method for correcting training label noise, using a likelihood ratio test on the predicted probability of the classifier trained on the noisy labels, and then proposes an algorithm for iteratively cleaning training labels and retraining a model. The paper also provides a theoretical guarantee for the probability of correctly re-labeling the training set, and provides empirical results showing that the proposed method significantly outperforms existing approaches for handling noisy training labels. Overall, I think the empirical results appear very strong, but think this paper is below the acceptance threshold due to three factors, in ranked order: - (1) The theoretical guarantee, which is positioned as a core contribution of the paper (and in fact claims it as *the first to correct labels with theoretical guarantees*, which is not true), is based on assumptions that seem overly strong; these are somewhat relaxed in a *Remark*, but this seems unproven and is a confusing presentation regardless. - (2) The theoretical bound itself is somewhat vacuous as it contains several constant factors that seem very material to the bound, but totally opaque to the reader. - (3) The experiments are very strong overall, which is a major plus for the paper; however, there are some questions about the hyperparameter tuning and some other points where more clarity could improve the strength of the empirical results Regarding (1): As a reader, my first natural reaction was to worry about circularity/degeneracy in the proposed method: basically, we are using the confidence (as a ratio of predicted cond. probabilities) of the model trained on the corrupted labels to correct those labels... if the labels are so corrupted that the model is also way off, then intuitively, this method should not work. I was wondering about how this situation would be bounded / handled. It turns out in Thm 1 that an incredibly strong assumption is made, namely that the model trained on the corrupted labels, f, is a linear function of the true model, with constants a, b known to some small degree of error epsilon (note that the theorem statement says that these constants are unknown- but it then assumes that Delta, which is set based on a and b, is known up to epsilon error). This seems like an incredibly strong assumption- and no context / motivation is given about why it should be taken as reasonable. Then, immediately after the Theorem, *Remark 2* states that this condition is not actually needed at all- but (a) then why not just strike it from Thm 1 statement, and (b) there does not seem to be any proof of this Remark in the appendix (where the proof of the main theorem itself is closer to a sketch than a standard proof...). Regarding (2): The bound produced in Thm 1 seems somewhat vacuous: letting \tau_{01} = \tau_{10}, then the probability of the label correction being erroneous is bounded by 8C(O(epsilon))^lambda. This quantity is presumably in (0,1/2], so it*s a small range to start... but it seems hard to get anything from this bound without some idea of what the constant factors (C, and those hidden in O(epsilon)) and lambda are. In particular, as presented, it seems implausible that epsilon- the error in specifying the Delta threshold- gets that small, in which case these constants become very important to know! Another way of phrasing this remark: many theoretical bounds have lots of unknown constant factors, but are ultimately just trying to expose some scaling with respect to one parameter, e.g. number of data points, and therefore the constants don*t need to be known that well for the statement to have some value. This doesn*t exactly seem to be the case here- therefore it seems hard to extract something from this statement (even ignoring the strong assumptions it is predicated on). Regarding (3): Overall, I think the empirical performance reported in Table 2, and overall thoroughness of the ablation in Section 4, are major strong points for the paper- the performance is very impressive! However I have a few questions, clarification of which would be very helpful in my mind: - (i) A major issue that seems to be raised in the earlier sections is that there is a hyperparameter Delta--the threshold for the likelihood ratio test--that everything depends on, and must be chosen empirically. Table 5 shows that the effect of choosing it is not crazy, but also clearly not insignificant. My question is: how is it chosen? On the validation dataset? And is this validation dataset also corrupted in the same way as the training dataset? If not, that seems like a major whole in the setup. - (ii) I also have a high level question for understanding: how is it possible for the various approaches to do so well with 0.6 and 0.8 noise level of uniform flipping? In the p=0.8 noise model, for example, the probability of a data point getting flipped to *any individual wrong label* is *greater than that of it being the correct label*. How is it possible to learn a model based on such a dataset? Was there some kind of pre-training? Was the validation set not corrupted? I don*t conceptually understand how the results shown are possible...? Overall, I think points in (1) and (3) could be helped with additional clarification and contextualization, and possibly (2) as well. (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper proposes a new method for correcting training label noise, using a likelihood ratio test on the predicted probability of the classifier trained on the noisy labels, and then proposes an algorithm for iteratively cleaning training labels and retraining a model. The paper also provides a theoretical guarantee for the probability of correctly re-labeling the training set, and provides empirical results showing that the proposed method significantly outperforms existing approaches for handling noisy training labels. Overall, I think the empirical results appear very strong, but think this paper is below the acceptance threshold due to three factors, in ranked order: - (1) The theoretical guarantee, which is positioned as a core contribution of the paper (and in fact claims it as *the first to correct labels with theoretical guarantees*, which is not true), is based on assumptions that seem overly strong; these are somewhat relaxed in a *Remark*, but this seems unproven and is a confusing presentation regardless. - (2) The theoretical bound itself is somewhat vacuous as it contains several constant factors that seem very material to the bound, but totally opaque to the reader. - (3) The experiments are very strong overall, which is a major plus for the paper; however, there are some questions about the hyperparameter tuning and some other points where more clarity could improve the strength of the empirical results Regarding (1): As a reader, my first natural reaction was to worry about circularity/degeneracy in the proposed method: basically, we are using the confidence (as a ratio of predicted cond. probabilities) of the model trained on the corrupted labels to correct those labels... if the labels are so corrupted that the model is also way off, then intuitively, this method should not work. I was wondering about how this situation would be bounded / handled. It turns out in Thm 1 that an incredibly strong assumption is made, namely that the model trained on the corrupted labels, f, is a linear function of the true model, with constants a, b known to some small degree of error epsilon (note that the theorem statement says that these constants are unknown- but it then assumes that Delta, which is set based on a and b, is known up to epsilon error). This seems like an incredibly strong assumption- and no context / motivation is given about why it should be taken as reasonable. Then, immediately after the Theorem, *Remark 2* states that this condition is not actually needed at all- but (a) then why not just strike it from Thm 1 statement, and (b) there does not seem to be any proof of this Remark in the appendix (where the proof of the main theorem itself is closer to a sketch than a standard proof...). Regarding (2): The bound produced in Thm 1 seems somewhat vacuous: letting \tau_{01} = \tau_{10}, then the probability of the label correction being erroneous is bounded by 8C(O(epsilon))^lambda. This quantity is presumably in (0,1/2], so it*s a small range to start... but it seems hard to get anything from this bound without some idea of what the constant factors (C, and those hidden in O(epsilon)) and lambda are. In particular, as presented, it seems implausible that epsilon- the error in specifying the Delta threshold- gets that small, in which case these constants become very important to know! Another way of phrasing this remark: many theoretical bounds have lots of unknown constant factors, but are ultimately just trying to expose some scaling with respect to one parameter, e.g. number of data points, and therefore the constants don*t need to be known that well for the statement to have some value. This doesn*t exactly seem to be the case here- therefore it seems hard to extract something from this statement (even ignoring the strong assumptions it is predicated on). Regarding (3): Overall, I think the empirical performance reported in Table 2, and overall thoroughness of the ablation in Section 4, are major strong points for the paper- the performance is very impressive! However I have a few questions, clarification of which would be very helpful in my mind: - (i) A major issue that seems to be raised in the earlier sections is that there is a hyperparameter Delta--the threshold for the likelihood ratio test--that everything depends on, and must be chosen empirically. Table 5 shows that the effect of choosing it is not crazy, but also clearly not insignificant. My question is: how is it chosen? On the validation dataset? And is this validation dataset also corrupted in the same way as the training dataset? If not, that seems like a major whole in the setup. - (ii) I also have a high level question for understanding: how is it possible for the various approaches to do so well with 0.6 and 0.8 noise level of uniform flipping? In the p=0.8 noise model, for example, the probability of a data point getting flipped to *any individual wrong label* is *greater than that of it being the correct label*. How is it possible to learn a model based on such a dataset? Was there some kind of pre-training? Was the validation set not corrupted? I don*t conceptually understand how the results shown are possible...? Overall, I think points in (1) and (3) could be helped with additional clarification and contextualization, and possibly (2) as well.\n",
      "      - Review:###*an information theoretic principle, information bottleneck principle* in the abstract is quite redundant with the use of *principle* twice **great, great* and *great, thought provoking*. They have the same level of sparsity.* What kind of sparsity are you referring to with this example? Why can*t sparsity reduce semantic redundancy? Please explain further. *However, the first explanation has a large MI with the input document where *great* occurs a lot.* What example input document are you referring to? You should save the explanation of how your method in Equation 2 differs from the original information bottleneck of Equation 1 until after you have actually written out Equation 2. As it is now, you are referencing Equation 2 before it has been seen. I find Equation 2 confusing. Is it possible to make the dependence of the expression on z more explicit. It isn*t clear from the equation itself how p(z|x) influences either quantity in Equation 2. Perhaps you should wait to introduce this equation until you have first explained how z relates to x and t. As it is, z is not clearly defined. I can gather information about it from the figure, from how you describe the difference in your method from the original information bottleneck, but the relationship is not clear enough by reading only the text of the paper before Equation 2 is presented. Can you explain briefly how your *hierarchical LSTM* works in the main text of the paper? Its an unusual enough term that I would want to see a citation or brief explanation right away rather than having it deferred to the Appendix. Why not use a state-of-the-art model for IMDb? Are you not using the standard splits for IMDb? the In Appendix B.1 *output vector is averaged and followed by log-softmax calculation. The final layer is formed to return a log-probability indicating which cognitive chunks should be taken as an input to the approximator* The single output vector of the biLSTM is averaged and followed by log-softmax? the final layer is formed? What does this mean? I find the phrasing of *Negative Sentiment if any negative words* and the corresponding title for positive in Fig 2 confusing. What do you mean by *if any*? The phrasing makes it sound like the prediction of the model somehow depends on a logical step based on whether there are any negative/positive words found. I find the lack of a comparison to some kind of attentional method somewhat glaring in the IMDb example, since I would expect that many classifiers with attention would simply attend to the same words. What does your method give us that attention would not? The same can be said for the MNIST example regarding an attention map. *by the human intelligences* sounds quite robotic Can you provide some sense of inter annotator agreement for labeling the images and sentences? It does seem that there is key information like the definition of approximator fidelity in the appendices which is crucial to actually understanding the paper. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###*an information theoretic principle, information bottleneck principle* in the abstract is quite redundant with the use of *principle* twice **great, great* and *great, thought provoking*. They have the same level of sparsity.* What kind of sparsity are you referring to with this example? Why can*t sparsity reduce semantic redundancy? Please explain further. *However, the first explanation has a large MI with the input document where *great* occurs a lot.* What example input document are you referring to? You should save the explanation of how your method in Equation 2 differs from the original information bottleneck of Equation 1 until after you have actually written out Equation 2. As it is now, you are referencing Equation 2 before it has been seen. I find Equation 2 confusing. Is it possible to make the dependence of the expression on z more explicit. It isn*t clear from the equation itself how p(z|x) influences either quantity in Equation 2. Perhaps you should wait to introduce this equation until you have first explained how z relates to x and t. As it is, z is not clearly defined. I can gather information about it from the figure, from how you describe the difference in your method from the original information bottleneck, but the relationship is not clear enough by reading only the text of the paper before Equation 2 is presented. Can you explain briefly how your *hierarchical LSTM* works in the main text of the paper? Its an unusual enough term that I would want to see a citation or brief explanation right away rather than having it deferred to the Appendix. Why not use a state-of-the-art model for IMDb? Are you not using the standard splits for IMDb? the In Appendix B.1 *output vector is averaged and followed by log-softmax calculation. The final layer is formed to return a log-probability indicating which cognitive chunks should be taken as an input to the approximator* The single output vector of the biLSTM is averaged and followed by log-softmax? the final layer is formed? What does this mean? I find the phrasing of *Negative Sentiment if any negative words* and the corresponding title for positive in Fig 2 confusing. What do you mean by *if any*? The phrasing makes it sound like the prediction of the model somehow depends on a logical step based on whether there are any negative/positive words found. I find the lack of a comparison to some kind of attentional method somewhat glaring in the IMDb example, since I would expect that many classifiers with attention would simply attend to the same words. What does your method give us that attention would not? The same can be said for the MNIST example regarding an attention map. *by the human intelligences* sounds quite robotic Can you provide some sense of inter annotator agreement for labeling the images and sentences? It does seem that there is key information like the definition of approximator fidelity in the appendices which is crucial to actually understanding the paper.\n",
      "      - Review:###Motivated by group action theory, this paper proposes a method to obtain ‘equivariant’ neural nets given trained one, where ‘equivariant’ refers to a network that gives identical output if certain symmetry of the dataset is performed on the input (for example, if we rotate a sample the predicted class should not change). After reading the paper, I don’t understand the experiments section. In particular, it is not clear to me how the proposed method differs from regular data augmentation, as to my understanding, the input to conv1 is copied 4 times and performed rotation for 0, 90, 180 and 270 degrees and the 4 times increased number of parameters (in-depth) of conv1 are shared. Furthermore, the same rotations are performed to the input to the second layer-conv 2: as the augmentation is cyclic I don’t understand why the authors perform this operation second time. Could the authors elaborate on this? After reading this section, I don’t understand the proposed fine-tuning procedure (pre-train, finetune and test): (1) what is the accuracy of the pre-trained network that was started from? (2) how is the initial network fine-tuned and modified? (as the authors mention that during training the samples are not rotated). Also, I am confused with the first sentence on page 8: ‘the complexity of the constructed network does not grow in terms of the number of parameters’. It would be useful if the results in Fig. 4 are more clearly illustrated. Is the order or increased computation 4x4x4? It would be useful to compare the method (computation & performance) with a baseline where the dataset is enlarged with data augmentation. The authors mention in the introduction that this increases training overhead, whereas the proposed practical method increases the computation at inference as well as the memory footprint of the model and the forward pass. It would be useful if the authors compared empirically with baselines with (1) data augmentation (2) network with an increased number of parameters (same as the proposed one). In summary, the idea of using group action theory seems interesting. However after reading the paper, it is not clear to me how the idea is carried out, and although the authors provide theoretical justification, it is not clear how this connects with the practical proposed method and whether it outperforms standard data augmentation (see above). Moreover, I find the writing of the paper quite unclear (see discussion above and examples below). - If digits 6 or 9 are rotated the label changes, how does the proposed method handle this? - page 8, conclusion: The authors claim that the proposed approach yields a ‘significant reduction in the design and training complexity’. I don’t understand relative to what this comparison refers to, as the regular data augmentation approach is more straightforward in my opinion. Also, given that this is pointed as an important contribution, in my opinion, an empirical comparison must be done with such baselines (see above). - Page 1: it is mentioned that ‘the number of parameters can be the same as the original network’ but the experiments do not include such architecture. After reading the paper I don’t understand how such a network can be implemented and whether it works. — Minor — - Page 1 & 1par-Pg2: I don’t understand what the authors mean by ‘uniformly *across layers* of NN? - Page 2: In these existing works, … I don’t understand this sentence - Page 2: our .. method use -> uses - Page 2: map over the orbits. I don’t understand this - Page 2: the first truly equivariant NN. After reading Sec 5 I don’t understand this point. - Sec. 4: how to equivarifying -> equivarify - Page 4: ‘pick a generator’, would recommend elaborating this term or only mentioning g as an element of G for clarity for readers unfamiliar with group theory - What is the testing accuracy if rotated for different angles than trained (e.g. 45 degrees)? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Motivated by group action theory, this paper proposes a method to obtain ‘equivariant’ neural nets given trained one, where ‘equivariant’ refers to a network that gives identical output if certain symmetry of the dataset is performed on the input (for example, if we rotate a sample the predicted class should not change). After reading the paper, I don’t understand the experiments section. In particular, it is not clear to me how the proposed method differs from regular data augmentation, as to my understanding, the input to conv1 is copied 4 times and performed rotation for 0, 90, 180 and 270 degrees and the 4 times increased number of parameters (in-depth) of conv1 are shared. Furthermore, the same rotations are performed to the input to the second layer-conv 2: as the augmentation is cyclic I don’t understand why the authors perform this operation second time. Could the authors elaborate on this? After reading this section, I don’t understand the proposed fine-tuning procedure (pre-train, finetune and test): (1) what is the accuracy of the pre-trained network that was started from? (2) how is the initial network fine-tuned and modified? (as the authors mention that during training the samples are not rotated). Also, I am confused with the first sentence on page 8: ‘the complexity of the constructed network does not grow in terms of the number of parameters’. It would be useful if the results in Fig. 4 are more clearly illustrated. Is the order or increased computation 4x4x4? It would be useful to compare the method (computation & performance) with a baseline where the dataset is enlarged with data augmentation. The authors mention in the introduction that this increases training overhead, whereas the proposed practical method increases the computation at inference as well as the memory footprint of the model and the forward pass. It would be useful if the authors compared empirically with baselines with (1) data augmentation (2) network with an increased number of parameters (same as the proposed one). In summary, the idea of using group action theory seems interesting. However after reading the paper, it is not clear to me how the idea is carried out, and although the authors provide theoretical justification, it is not clear how this connects with the practical proposed method and whether it outperforms standard data augmentation (see above). Moreover, I find the writing of the paper quite unclear (see discussion above and examples below). - If digits 6 or 9 are rotated the label changes, how does the proposed method handle this? - page 8, conclusion: The authors claim that the proposed approach yields a ‘significant reduction in the design and training complexity’. I don’t understand relative to what this comparison refers to, as the regular data augmentation approach is more straightforward in my opinion. Also, given that this is pointed as an important contribution, in my opinion, an empirical comparison must be done with such baselines (see above). - Page 1: it is mentioned that ‘the number of parameters can be the same as the original network’ but the experiments do not include such architecture. After reading the paper I don’t understand how such a network can be implemented and whether it works. — Minor — - Page 1 & 1par-Pg2: I don’t understand what the authors mean by ‘uniformly *across layers* of NN? - Page 2: In these existing works, … I don’t understand this sentence - Page 2: our .. method use -> uses - Page 2: map over the orbits. I don’t understand this - Page 2: the first truly equivariant NN. After reading Sec 5 I don’t understand this point. - Sec. 4: how to equivarifying -> equivarify - Page 4: ‘pick a generator’, would recommend elaborating this term or only mentioning g as an element of G for clarity for readers unfamiliar with group theory - What is the testing accuracy if rotated for different angles than trained (e.g. 45 degrees)?\n",
      "      - Review:###Summary: The paper proposes a method for noise robustness based on scaling gradients of examples. By choosing the proper scaling parameters (alpha and beta), the method recovers standard losses such as CCE, MAE, and GCE, while also recovering other losses. The method is strongly related to reweighting training examples, where alpha and beta define the shape of this weighting as a function of the model*s prediction (i.e., p_i). Experiments show that the proposed method achieves competitive results on several standard benchmarks for noisy-labelled data. Comments: - The main drawback of the proposed approach is that there is no clear way of choosing alpha and beta, other than a hyper-parameter search, which is not very practical and can lead to overfitting the test set. - The paper in general is easy to follow, but the paper is not very rigorous or clear on some important concepts. For example: * No clear mathematical definition of emphasis focus and spread * The term *semantically abnormal examples* should be defined in the main text. * It is not so clear what it means to *babyset* emphasis focus and spread. * I don*t understand what Eq. 6 is supposed to tell. * What are the dots in equation - The experiments are very thorough and the results are very good, but I have few clarifying questions: * The procedure for choosing beta/gamma is not clear, and I see that for every experiment those values change. * It would be nice if the CIFAR-10 Table 4 results are performed using the exact same setting to prior work to make sure the comparison is fair. For example, GCE results in (Zhang & Sabuncu 2018) are much that the reported ones. While it seems that you*re using GoogLeNet V1 architecture similar to Jiang et al. 2018, it*s not clear which experimental setting you are comparing against. * Can you be more specific what do you mean by *with a little effort for optimizing beta and gamma* in caption of Table 5? Minor: * Grammer mistake: *what training examples...focused *on*...* * Citations should be done with parentheses (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Summary: The paper proposes a method for noise robustness based on scaling gradients of examples. By choosing the proper scaling parameters (alpha and beta), the method recovers standard losses such as CCE, MAE, and GCE, while also recovering other losses. The method is strongly related to reweighting training examples, where alpha and beta define the shape of this weighting as a function of the model*s prediction (i.e., p_i). Experiments show that the proposed method achieves competitive results on several standard benchmarks for noisy-labelled data. Comments: - The main drawback of the proposed approach is that there is no clear way of choosing alpha and beta, other than a hyper-parameter search, which is not very practical and can lead to overfitting the test set. - The paper in general is easy to follow, but the paper is not very rigorous or clear on some important concepts. For example: * No clear mathematical definition of emphasis focus and spread * The term *semantically abnormal examples* should be defined in the main text. * It is not so clear what it means to *babyset* emphasis focus and spread. * I don*t understand what Eq. 6 is supposed to tell. * What are the dots in equation - The experiments are very thorough and the results are very good, but I have few clarifying questions: * The procedure for choosing beta/gamma is not clear, and I see that for every experiment those values change. * It would be nice if the CIFAR-10 Table 4 results are performed using the exact same setting to prior work to make sure the comparison is fair. For example, GCE results in (Zhang & Sabuncu 2018) are much that the reported ones. While it seems that you*re using GoogLeNet V1 architecture similar to Jiang et al. 2018, it*s not clear which experimental setting you are comparing against. * Can you be more specific what do you mean by *with a little effort for optimizing beta and gamma* in caption of Table 5? Minor: * Grammer mistake: *what training examples...focused *on*...* * Citations should be done with parentheses\n",
      "      - Review:###Summary: This paper gives a new MLP formulation and architecture that is ostensibly suited for data where the label*s dependence on the input feature has form of sparsity. The paper reports the performance of this architecture on a few standard datasets and outperforms other baselines. Review: The main contribution arguably in the *Group select* matrix which selects features that each Group-FC focusses on. While it is an interesting idea, it is not hugely novel and requires a lot more demonstration. Without the Group-select the architecture in the paper can simply viewed as an MLP with a block diagonal sparsity structure enforced on the weights and max-out pooling as non-linearity. Based on the above reasoning, I would rate the architectural/theoretical contribution as not significant. The empirical results also do not look particularly convincing, as there is just a few percentage improvement over MLP (except CIFAR10). Questions for authors: 1. In the modified CIFAR10 dtaset, is the same permutation of pixels applied to all images or is it a different random permutation for every? If it is different, the entire rationale of Group-select breaks down, so I am assuming it is the same, but this must be clarified. 2. What were the groups selected in CIFAR10? Did the groups selected correspond to nearby pixels or some other meaningful way? A better understanding of why this improvement happened might help a lot. 3. In figure 5, learning curves are plotted for different m. But what about k? Is it kept constant? If so what constant? Similarly for figure 6. 4. The visualisation for MNIST Group-select seems very limited compared to what is there in the architecture. I there any way a selected group can be shown and visualised in a meaningful way? 5. The synthetic example is interesting, but this can easily be extended to much larger scales. Do the results hold up in such cases too? Typos/errors: 1. In equation 4, the i+k/2 part makes sense only for the l=0 layer. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Summary: This paper gives a new MLP formulation and architecture that is ostensibly suited for data where the label*s dependence on the input feature has form of sparsity. The paper reports the performance of this architecture on a few standard datasets and outperforms other baselines. Review: The main contribution arguably in the *Group select* matrix which selects features that each Group-FC focusses on. While it is an interesting idea, it is not hugely novel and requires a lot more demonstration. Without the Group-select the architecture in the paper can simply viewed as an MLP with a block diagonal sparsity structure enforced on the weights and max-out pooling as non-linearity. Based on the above reasoning, I would rate the architectural/theoretical contribution as not significant. The empirical results also do not look particularly convincing, as there is just a few percentage improvement over MLP (except CIFAR10). Questions for authors: 1. In the modified CIFAR10 dtaset, is the same permutation of pixels applied to all images or is it a different random permutation for every? If it is different, the entire rationale of Group-select breaks down, so I am assuming it is the same, but this must be clarified. 2. What were the groups selected in CIFAR10? Did the groups selected correspond to nearby pixels or some other meaningful way? A better understanding of why this improvement happened might help a lot. 3. In figure 5, learning curves are plotted for different m. But what about k? Is it kept constant? If so what constant? Similarly for figure 6. 4. The visualisation for MNIST Group-select seems very limited compared to what is there in the architecture. I there any way a selected group can be shown and visualised in a meaningful way? 5. The synthetic example is interesting, but this can easily be extended to much larger scales. Do the results hold up in such cases too? Typos/errors: 1. In equation 4, the i+k/2 part makes sense only for the l=0 layer.\n",
      "      - Review:###The paper considers random projection forests for similarity measurements (which have been proposed earlier) and proposes to accelerate them by reusing projections. Tree levels up-to level-X use distinct random transformations, and subsequent levels cycle through existing projections (X of them). As this kind of reuse reduces the quality of trees, the paper proposes to (greatly) increase the number of trees in the forest. The paper also introduces a sensible *beta-similarity* which is based on average tree-distance between leafs into which the two data-points fall, rather than fraction of trees in which they fall into the same leaf-node. I recommend to reject the paper -- as the contribution in my opinion is incremental, not principled (more of an engineering trick) and not very convincing. Furthermore, the paper has a number of issues with presentation, language, and experimental results. RP trees have been introduced over a decade ago (Dasgupta and Freund, 2008), and the authors cite a reference to RP forests from (Yan et al, 2019, IEEE Big data). Level-wise use of the same projection within a tree (that the authors call *layer-by-layer RP Trees*) has been discussed (and shown to be effective) in the original Dasgupta and Freund paper. The paper makes it very hard to understand what is a contribution, and what is borrowed from existing papers -- as the authors say *we introduce* both for well-known concepts (like RP trees), and for what I understand to be their contribution. I would ask to clearly state what is existing work, and what is new, and what are the key contributions. Other comments. 1. Experiments: you cite a paper on RP-forests (Yan et al, 2019) -- so ensembles of RP trees have already been proposed. Why in the experiments you still compare only to a single RP-tree? You report speed gains ~ 2x or 3x w.r.t standard RP Trees -- even when you use very many (hundreds or thousands of trees in an X-forest). Are these layer-by-layer RP trees, or do you use new random projection for each node? Are these your implementations of RP trees, or did you use existing code? The experiments do not provide enough details on the implementation to judge their significance -- e.g. 2x gain of speed could be achieved by better software implementation of the same algorithm. Is it standard to measure clustering quality by taking a classification problem and assuming that clusters should correspond to class labels? There are various other measures of cluster quality that do not rely on labels (e.g. rand-index, homogeneity, ratio of within-cluster to inter-cluster distances e.t.c.). 2. In figure 8, (a),(b),(c) -- it looks like choice of beta can have a dramatic impact on accuracy, and can either be monotone, or peak at intermediate values. How do you propose to chose beta in practice -- don*t you need supervision? What is the cost to compute beta-similarity compared to naive 0-1 similarity? 3. The claim that similarity measures should be independent from prior knowledge -- can be controversial -- and is not a widely accepted truth. The field of metric learning tries to find better similarity matrices based on additional prior information. It*s hard to imagine that a single similarity matrix will be suitable for all problem domains. I agree that it*s useful to have a generic default similarity measure to try first, but clearly if prior data is available it should be used. 4. In addition to *mathematical distance-based similarity* and tree-based similarities -- there is also considerable work on similarity based on non-linear embeddings -- e.g. T-SNE or UMAP, provide an embedding, and then a simple cosine similarity can be used after the embedding. Overall -- this is a complex nonlinear similarity measure, not captured by your two classes. 5. A well known paper *Extremely randomized trees* also proposes kernels (similarity measurements) based on a forest created with (nearly) random splits, and should be cited, and used in comparisons. There is even a scikit-learn implementation -- called random trees embedding. 6. The introduction of the paper: *Similarity measurement is to measure similarity...* conveys no information. The entire first paragraph does not have much content, and should be rewritten or skipped. Bad word choice: Unsupervised clustering is *to classify*, *Exalted speed*, dimension of original dataset is *degraded*... If you say *we introduce RP Trees* -- it sounds like you propose them in this paper -- which is clearly not the case, as you cite Dasgupta and Freund*s paper. You can instead say -- we consider RP trees introduced by (ABC)... (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper considers random projection forests for similarity measurements (which have been proposed earlier) and proposes to accelerate them by reusing projections. Tree levels up-to level-X use distinct random transformations, and subsequent levels cycle through existing projections (X of them). As this kind of reuse reduces the quality of trees, the paper proposes to (greatly) increase the number of trees in the forest. The paper also introduces a sensible *beta-similarity* which is based on average tree-distance between leafs into which the two data-points fall, rather than fraction of trees in which they fall into the same leaf-node. I recommend to reject the paper -- as the contribution in my opinion is incremental, not principled (more of an engineering trick) and not very convincing. Furthermore, the paper has a number of issues with presentation, language, and experimental results. RP trees have been introduced over a decade ago (Dasgupta and Freund, 2008), and the authors cite a reference to RP forests from (Yan et al, 2019, IEEE Big data). Level-wise use of the same projection within a tree (that the authors call *layer-by-layer RP Trees*) has been discussed (and shown to be effective) in the original Dasgupta and Freund paper. The paper makes it very hard to understand what is a contribution, and what is borrowed from existing papers -- as the authors say *we introduce* both for well-known concepts (like RP trees), and for what I understand to be their contribution. I would ask to clearly state what is existing work, and what is new, and what are the key contributions. Other comments. 1. Experiments: you cite a paper on RP-forests (Yan et al, 2019) -- so ensembles of RP trees have already been proposed. Why in the experiments you still compare only to a single RP-tree? You report speed gains ~ 2x or 3x w.r.t standard RP Trees -- even when you use very many (hundreds or thousands of trees in an X-forest). Are these layer-by-layer RP trees, or do you use new random projection for each node? Are these your implementations of RP trees, or did you use existing code? The experiments do not provide enough details on the implementation to judge their significance -- e.g. 2x gain of speed could be achieved by better software implementation of the same algorithm. Is it standard to measure clustering quality by taking a classification problem and assuming that clusters should correspond to class labels? There are various other measures of cluster quality that do not rely on labels (e.g. rand-index, homogeneity, ratio of within-cluster to inter-cluster distances e.t.c.). 2. In figure 8, (a),(b),(c) -- it looks like choice of beta can have a dramatic impact on accuracy, and can either be monotone, or peak at intermediate values. How do you propose to chose beta in practice -- don*t you need supervision? What is the cost to compute beta-similarity compared to naive 0-1 similarity? 3. The claim that similarity measures should be independent from prior knowledge -- can be controversial -- and is not a widely accepted truth. The field of metric learning tries to find better similarity matrices based on additional prior information. It*s hard to imagine that a single similarity matrix will be suitable for all problem domains. I agree that it*s useful to have a generic default similarity measure to try first, but clearly if prior data is available it should be used. 4. In addition to *mathematical distance-based similarity* and tree-based similarities -- there is also considerable work on similarity based on non-linear embeddings -- e.g. T-SNE or UMAP, provide an embedding, and then a simple cosine similarity can be used after the embedding. Overall -- this is a complex nonlinear similarity measure, not captured by your two classes. 5. A well known paper *Extremely randomized trees* also proposes kernels (similarity measurements) based on a forest created with (nearly) random splits, and should be cited, and used in comparisons. There is even a scikit-learn implementation -- called random trees embedding. 6. The introduction of the paper: *Similarity measurement is to measure similarity...* conveys no information. The entire first paragraph does not have much content, and should be rewritten or skipped. Bad word choice: Unsupervised clustering is *to classify*, *Exalted speed*, dimension of original dataset is *degraded*... If you say *we introduce RP Trees* -- it sounds like you propose them in this paper -- which is clearly not the case, as you cite Dasgupta and Freund*s paper. You can instead say -- we consider RP trees introduced by (ABC)...\n",
      "      - Review:###The paper considers the impact of initialization bias on test error in strongly overparameterized neural networks. The study uses tools from recent literature on the generalization of overparameterized neural networks, i.e. neural tangent kernels and interpolating kernel method, to provide useful insights on how the variance of weights initialization affects the test error. I have a few questions about theoretical results, but the paper has a convincing experiment that supports its theoretical claims. Addressing the following points will improve the exposition of the paper. 1. Please provide a little hint on how Lemma 2 rewrites the equation (13) for linearized function for easier readability without referring to the Appendix. 2. In the case of cross-entropy error, would the effect be similar? Could this be verified with a similar experiment as for MSE? 3. To what extent this result is observed in not as strongly overparameterized settings? In other words, it would be interesting to see what happens if you fix the architectural choice while increasing the number of training parameters, how long does the test error effect persist? Minor remark: - a few typos are present on pages 4, 5, 7, 8 (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper considers the impact of initialization bias on test error in strongly overparameterized neural networks. The study uses tools from recent literature on the generalization of overparameterized neural networks, i.e. neural tangent kernels and interpolating kernel method, to provide useful insights on how the variance of weights initialization affects the test error. I have a few questions about theoretical results, but the paper has a convincing experiment that supports its theoretical claims. Addressing the following points will improve the exposition of the paper. 1. Please provide a little hint on how Lemma 2 rewrites the equation (13) for linearized function for easier readability without referring to the Appendix. 2. In the case of cross-entropy error, would the effect be similar? Could this be verified with a similar experiment as for MSE? 3. To what extent this result is observed in not as strongly overparameterized settings? In other words, it would be interesting to see what happens if you fix the architectural choice while increasing the number of training parameters, how long does the test error effect persist? Minor remark: - a few typos are present on pages 4, 5, 7, 8\n",
      "      - Review:###This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance. Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN*s and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis. There are several parts that are confusing/missing in the paper: - In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this. - On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples? - Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch. - I think the statement in the first paragraph in Section 4.4 that *MSE loss term would act as a denoising loss smoothing noisy missing pixels* could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear. I think the paper would benefit if the authors could explain/show: - Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance. Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN*s and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis. There are several parts that are confusing/missing in the paper: - In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this. - On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples? - Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch. - I think the statement in the first paragraph in Section 4.4 that *MSE loss term would act as a denoising loss smoothing noisy missing pixels* could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear. I think the paper would benefit if the authors could explain/show: - Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?\n",
      "      - Review:###This paper proposes a new method for measuring pairwise similarity between data points. The idea is to define the similarity between two data points to be the probability (over the randomness in constructing the trees) that they are close in an RP tree. More concretely, the proposed method constructs a collection of RP trees (albeit with some modifications), and takes the similarity to be the average over different RP trees of a strictly decreasing function of the distance between the leaf nodes containing the data points in each RP tree. The key modification to the RP tree is to limit the number of projection vectors used in an RP tree and re-use previous projection vectors. I believe the method is more or less equivalent to Euclidean distance, for the following reason. Two data points would have the highest similarity under the proposed similarity measure if they are in the same leaf node. For this to happen, both points must be on the same side of the dividing hyperplane corresponding to each of the ancestor nodes. Because the threshold along the projection vector is chosen randomly at uniform, this means that for the two data points to have high similarity consistently, the distance between them along the projection vector must be small (so that the probability of splitting them is small). Because the projection vectors themselves are chosen randomly on the unit sphere, this implies that this must hold along most projection vectors for similarity to be high consistently, which means that the Euclidean distance between the two points must be low. If true, this raises several questions: 1) Why does the proposed method work better than distance similarity (which I assume means Euclidean distance) in the experiments? Are there situations when the proposed method would yield a high similarity consistently whereas Euclidean distance wouldn’t? Are the results in Fig. 6 just for a single run of the proposed method? If so, many more runs need to be performed since the decisions of the RP tree should vary significantly depending on the projection vectors and thresholds. Both the mean and standard deviation should be reported. 2) In Sect. 1.2, the authors critiqued distance-based similarity because it often does not correspond to intuitive notions of similarity/perceptual similarity. However, it does not appear that the proposed method would correspond to perceptual similarity either. For example, consider a dataset where some coordinates are more perceptually important than others (this is the case for example for the wavelet coefficients of a natural image - the lower frequencies are typically more perceptually important than higher frequencies). A more perceptually meaningful distance than Euclidean distance would be a Mahalanobis distance (which can essentially weight different coordinates differently), but the random projections use standard inner products and so are unable to capture the appropriate weighting of the different coordinates. So, why would one expect the proposed method to be more perceptually meaningful? 3) In Sect. 1.2, the authors critiqued multi-partition-based similarity because it does depend on the data distribution and cited the elimination of “prior knowledge dependence” in Sect. 1.3 as one of the benefits of the proposed method. This appears to be at odds with the goal of devising a similarity measure that is perceptually aligned, because such a similarity measure must depend on the representation of the data (e.g.: if the data is represented in the wavelet domain, one needs to know which order the different dimensions are arranged, i.e. from lowest frequency to highest frequency or the other way around). Overall, it is unclear if the desiderata makes sense, and if the proposed method achieves the objectives. Other questions: 4) For the X-Projection tree (which re-uses projection vectors), it seems to be equivalent to a layer-by-layer RP tree with larger branching factor. If so, the presentation of the method should be changed to this, because a layer-by-layer RP tree with larger branching factor is both conceptually clearer and simpler to analyze. If not, the proposed method should be compared to a layer-by-layer RP tree with larger branching factor, to justify the increased conceptual complexity of the X-Projection tree. 5) For the experiments, comparisons should also be made to multi-partition-based similarity, like Multiple RP+EM and RF similarity. 6) In addition, the proposed method should be compared to two simpler baselines that computes the average and the minimum distance of the two points along multiple random projection vectors, in order to justify the increased conceptual complexity of RP trees. 7) In Sect. 1.3, the paper claims that “it is well known that in an RP Tree, data points that are closely distributed, indicating their high level of similarity in space, are always partitioned into the same subset”. This is not true, since hyperplane could divide a cluster down the middle for example. 8) One of the claimed contributions in the abstract that “we introduce randomness into partition to eliminate its reliance on prior knowledge”. Note that just by introducing randomness, prior knowledge is not necessarily eliminated. For example, the way in which random projection is performed (i.e. standard inner product vs. other inner products) assumes knowledge of the distance metric, which is induced from the inner product. 9) In Fig. 6, what is the distance metric used for the baseline? Minor issues: pg. 2: “project all data points into one random vector” -> “project all data points along one random vector” pg. 3: “leading to unsatisfied results“ -> “leading to unsatisfactory results” pg. 3: “nearest neighbours finding” -> “nearest neighbour search” pg. 3: “pattern discovering” -> “pattern discovery” pg. 4: “similarly data points” -> “similar data points” pg. 4: “01 matrix” -> “0-1 matrix” (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a new method for measuring pairwise similarity between data points. The idea is to define the similarity between two data points to be the probability (over the randomness in constructing the trees) that they are close in an RP tree. More concretely, the proposed method constructs a collection of RP trees (albeit with some modifications), and takes the similarity to be the average over different RP trees of a strictly decreasing function of the distance between the leaf nodes containing the data points in each RP tree. The key modification to the RP tree is to limit the number of projection vectors used in an RP tree and re-use previous projection vectors. I believe the method is more or less equivalent to Euclidean distance, for the following reason. Two data points would have the highest similarity under the proposed similarity measure if they are in the same leaf node. For this to happen, both points must be on the same side of the dividing hyperplane corresponding to each of the ancestor nodes. Because the threshold along the projection vector is chosen randomly at uniform, this means that for the two data points to have high similarity consistently, the distance between them along the projection vector must be small (so that the probability of splitting them is small). Because the projection vectors themselves are chosen randomly on the unit sphere, this implies that this must hold along most projection vectors for similarity to be high consistently, which means that the Euclidean distance between the two points must be low. If true, this raises several questions: 1) Why does the proposed method work better than distance similarity (which I assume means Euclidean distance) in the experiments? Are there situations when the proposed method would yield a high similarity consistently whereas Euclidean distance wouldn’t? Are the results in Fig. 6 just for a single run of the proposed method? If so, many more runs need to be performed since the decisions of the RP tree should vary significantly depending on the projection vectors and thresholds. Both the mean and standard deviation should be reported. 2) In Sect. 1.2, the authors critiqued distance-based similarity because it often does not correspond to intuitive notions of similarity/perceptual similarity. However, it does not appear that the proposed method would correspond to perceptual similarity either. For example, consider a dataset where some coordinates are more perceptually important than others (this is the case for example for the wavelet coefficients of a natural image - the lower frequencies are typically more perceptually important than higher frequencies). A more perceptually meaningful distance than Euclidean distance would be a Mahalanobis distance (which can essentially weight different coordinates differently), but the random projections use standard inner products and so are unable to capture the appropriate weighting of the different coordinates. So, why would one expect the proposed method to be more perceptually meaningful? 3) In Sect. 1.2, the authors critiqued multi-partition-based similarity because it does depend on the data distribution and cited the elimination of “prior knowledge dependence” in Sect. 1.3 as one of the benefits of the proposed method. This appears to be at odds with the goal of devising a similarity measure that is perceptually aligned, because such a similarity measure must depend on the representation of the data (e.g.: if the data is represented in the wavelet domain, one needs to know which order the different dimensions are arranged, i.e. from lowest frequency to highest frequency or the other way around). Overall, it is unclear if the desiderata makes sense, and if the proposed method achieves the objectives. Other questions: 4) For the X-Projection tree (which re-uses projection vectors), it seems to be equivalent to a layer-by-layer RP tree with larger branching factor. If so, the presentation of the method should be changed to this, because a layer-by-layer RP tree with larger branching factor is both conceptually clearer and simpler to analyze. If not, the proposed method should be compared to a layer-by-layer RP tree with larger branching factor, to justify the increased conceptual complexity of the X-Projection tree. 5) For the experiments, comparisons should also be made to multi-partition-based similarity, like Multiple RP+EM and RF similarity. 6) In addition, the proposed method should be compared to two simpler baselines that computes the average and the minimum distance of the two points along multiple random projection vectors, in order to justify the increased conceptual complexity of RP trees. 7) In Sect. 1.3, the paper claims that “it is well known that in an RP Tree, data points that are closely distributed, indicating their high level of similarity in space, are always partitioned into the same subset”. This is not true, since hyperplane could divide a cluster down the middle for example. 8) One of the claimed contributions in the abstract that “we introduce randomness into partition to eliminate its reliance on prior knowledge”. Note that just by introducing randomness, prior knowledge is not necessarily eliminated. For example, the way in which random projection is performed (i.e. standard inner product vs. other inner products) assumes knowledge of the distance metric, which is induced from the inner product. 9) In Fig. 6, what is the distance metric used for the baseline? Minor issues: pg. 2: “project all data points into one random vector” -> “project all data points along one random vector” pg. 3: “leading to unsatisfied results“ -> “leading to unsatisfactory results” pg. 3: “nearest neighbours finding” -> “nearest neighbour search” pg. 3: “pattern discovering” -> “pattern discovery” pg. 4: “similarly data points” -> “similar data points” pg. 4: “01 matrix” -> “0-1 matrix”\n",
      "      - Review:###This paper proposes the similarity measure called *beta-similarity* generated by an ensemble of Random projection trees (RP trees) by Dasgupta & Freund (2008). To reduce the computational costs for building many RP trees, the paper develops an efficient approximate version called X-Projection trees by first generating X independent random projection directions, and then by sharing them at layers in turns. X-Forest is a set of X-Projection trees with X different random projections, and the proposed *beta similarity* between x and y is defined by distances between a leaf region having x and one having y in PR trees. Experimental evaluations demonstrated that the use of beta similarity improves the clustering accuracy using it within three types of methods (kernel k-means, DBSCAN, Spectral clustering). The paper*s idea of defining a similarity using many RP Trees with different randomization is quite interesting and sounds promising given that the recently reported performance of tree ensembles such as XGBoost and LightGBM is very good. Partition-based trees plus randomizations are known to have very nice properties particularly in high dimensions, theoretically speaking. However, this paper also has several problems 1) novelty and 2) confusing and imprecise statements. 1) novelty The novelty of the paper is basically (a) a simple computation savings of X-forest and (b) a definition of beta similarity. For (a), the novelty is rather small, and how much this computation savings have a practical impact is questionable since reported computation timings in Figure.9 are in ms. Furthermore, trivial parallelization would be possible because individual computations of RP trees in the ensemble are independent. Also, there exists a highly cited paper by Yan et al KDD*09 proposed a fast clustering method based on RP trees as *fast approximate spectral clustering* in their title. On the other hand, (b) would be novel but any consideration about alternatives is not given, and the definition sounds quite heuristic and less convincing. RP trees are existing spatial structures (proposed by Dasgupta & Freund) extending widely used k-d trees. It naturally defines the spatial closeness of data points, and thus the use of RP trees to define the similarity, and applying them to clustering (kernel k-means, DBSCAN, spectral) is not new. RP trees were proposed as an alternative of k-d trees, and the primary applications would be for nearest neighbor search or data compression like vector quantization. 2) confusing and imprecise statements Many confusing and imprecise statements exist. 2a) The proposed *beta similarity* eq (2) seems to lack the definition of DIS_i(X, Y). It would be something like the path length between node X and Y, or steps to the LCA (the lowest common ancestor). Also, the number m (the number of trees?) is also undefined. 2b) The three goals are set: accuracy, efficiency, and independence from prior knowledge. But when we use RP trees, 1st and 3rd are considered as resolved and sounds like the only remaining problem is *efficiency* for their computations. Also, the third goal *independence from prior knowledge* is quite vaguely explained, and hard to understand. For example, the affinity matrix in spectral clustering or kernel matrix with an RBF kernel is the case? It would be a kind of hyperparameters but not like *dependence on prior knowledge*. 2c) Also, *accuracy* of *similarity measurements* is quite ambiguous. The use of kernel distance with RBF kernel is less accurate than the use of RP trees?? The *similarity measurement* sounds like the problem of definition, and it cannot be accurate or inaccurate. The distance-based similarities themselves have no problems, and even when RP trees are used, we need some distance metric (i.e. Euclidean distance) in a space. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes the similarity measure called *beta-similarity* generated by an ensemble of Random projection trees (RP trees) by Dasgupta & Freund (2008). To reduce the computational costs for building many RP trees, the paper develops an efficient approximate version called X-Projection trees by first generating X independent random projection directions, and then by sharing them at layers in turns. X-Forest is a set of X-Projection trees with X different random projections, and the proposed *beta similarity* between x and y is defined by distances between a leaf region having x and one having y in PR trees. Experimental evaluations demonstrated that the use of beta similarity improves the clustering accuracy using it within three types of methods (kernel k-means, DBSCAN, Spectral clustering). The paper*s idea of defining a similarity using many RP Trees with different randomization is quite interesting and sounds promising given that the recently reported performance of tree ensembles such as XGBoost and LightGBM is very good. Partition-based trees plus randomizations are known to have very nice properties particularly in high dimensions, theoretically speaking. However, this paper also has several problems 1) novelty and 2) confusing and imprecise statements. 1) novelty The novelty of the paper is basically (a) a simple computation savings of X-forest and (b) a definition of beta similarity. For (a), the novelty is rather small, and how much this computation savings have a practical impact is questionable since reported computation timings in Figure.9 are in ms. Furthermore, trivial parallelization would be possible because individual computations of RP trees in the ensemble are independent. Also, there exists a highly cited paper by Yan et al KDD*09 proposed a fast clustering method based on RP trees as *fast approximate spectral clustering* in their title. On the other hand, (b) would be novel but any consideration about alternatives is not given, and the definition sounds quite heuristic and less convincing. RP trees are existing spatial structures (proposed by Dasgupta & Freund) extending widely used k-d trees. It naturally defines the spatial closeness of data points, and thus the use of RP trees to define the similarity, and applying them to clustering (kernel k-means, DBSCAN, spectral) is not new. RP trees were proposed as an alternative of k-d trees, and the primary applications would be for nearest neighbor search or data compression like vector quantization. 2) confusing and imprecise statements Many confusing and imprecise statements exist. 2a) The proposed *beta similarity* eq (2) seems to lack the definition of DIS_i(X, Y). It would be something like the path length between node X and Y, or steps to the LCA (the lowest common ancestor). Also, the number m (the number of trees?) is also undefined. 2b) The three goals are set: accuracy, efficiency, and independence from prior knowledge. But when we use RP trees, 1st and 3rd are considered as resolved and sounds like the only remaining problem is *efficiency* for their computations. Also, the third goal *independence from prior knowledge* is quite vaguely explained, and hard to understand. For example, the affinity matrix in spectral clustering or kernel matrix with an RBF kernel is the case? It would be a kind of hyperparameters but not like *dependence on prior knowledge*. 2c) Also, *accuracy* of *similarity measurements* is quite ambiguous. The use of kernel distance with RBF kernel is less accurate than the use of RP trees?? The *similarity measurement* sounds like the problem of definition, and it cannot be accurate or inaccurate. The distance-based similarities themselves have no problems, and even when RP trees are used, we need some distance metric (i.e. Euclidean distance) in a space.\n",
      "      - Review:###This paper proposes to use an extra feature (grammatical number) for context-aware coreference resolution and an attention-based weighting mechanism. The approach proposed is built on top of a recent well performing model by Lee et al. The improvement is rather minor in my view: 72.64 to 72.84 in the test set. There is not much in the paper to review. I don*t think the one extra feature warrants a paper at a top conference. The weighting mechanism over the features is also unclear to me why it benefits from attention. Couldn*t we just learn the weights using another layer? It could be context dependent if desired. It is also incorrect to criticise Lee et al. (2018) that they would give the same representation to the same mention every time. Their model is context dependent as they use a BiLSTM over the sentence. Of course the same mentions are likely to get similar representations, but this is desirable. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes to use an extra feature (grammatical number) for context-aware coreference resolution and an attention-based weighting mechanism. The approach proposed is built on top of a recent well performing model by Lee et al. The improvement is rather minor in my view: 72.64 to 72.84 in the test set. There is not much in the paper to review. I don*t think the one extra feature warrants a paper at a top conference. The weighting mechanism over the features is also unclear to me why it benefits from attention. Couldn*t we just learn the weights using another layer? It could be context dependent if desired. It is also incorrect to criticise Lee et al. (2018) that they would give the same representation to the same mention every time. Their model is context dependent as they use a BiLSTM over the sentence. Of course the same mentions are likely to get similar representations, but this is desirable.\n",
      "      - Review:###[Due to the rebuttal, my score was raised from a weak reject to a weak accept] Summary The paper addresses the problem of interpreting predictions/decisions of a black-box classifier/regressor by masking the parts of the input that were most relevant. The proposed approach consists of, first, manually designing “cognitive chunks” of input data, e.g. individual words for sentiment classification or fixed-size image-patches for image-classification. Then, a variational IB framework is used to infer which of these chunks are relevant for the classifier’s decision. Additionally, there is a (hard) constraint, making sure that only a fixed (small) number of chunks is used. The bottleneck variable, in this case, is a sparse-chunk representation of the data. The latter is obviously a more compressed representation of the data, but importantly it is a more compressed representation that contains the largest possible amount of relevant information about the decision (because of properties of the information-bottleneck objective). Both factors together, according to the paper, constitute a “good” (i.e. brief but comprehensive) explanation which allows for interpretability and attribution of the black-box system’s decision. The method is evaluated on three tasks (sentiment prediction, image classification, TCR to epitope binding prediction) and performance is reported to be on-par or better than state-of-the-art methods. Contributions -) Application of the IB-method for generating summaries of decision-relevant input-data, which are good candidates for interpretability. The theoretical properties of the IB objective are appealing for producing interpretable data-summaries. -) Adaptation of the variational IB framework, using bits and pieces reported in the literature such that the bottleneck variable is a sparse, binary vector over “cognitive chunks”. -) Experimental evaluation, where human judges rate the “interpretability“ of various state-of-the-art attribution methods. Quality, Clarity, Novelty, Impact The paper addresses a timely and important problem, particularly the IB framework could add some solid theoretical footing (the “theory of relevant information”) to the field of interpetability methods. The paper is well written (though it needs another pass for typos, etc.), related methods and literature are discussed and compared against, and the specific variational IB objective is introduced nicely. Large parts of the method (deep variational IB, VI with categorical variables) have been published before, but these parts are combined in a novel and original way. My main issues with the current paper are (I) interpretability and comprehensiveness are not necessarily the same as maximum compression of maximally relevant information, (II) the method (in theory) depends strongly on the quality of the approximator, this is currently not mentioned and not explored, (III) the experimental section is currently not very strong, in particular the MNIST experiment. See more details for the main issues below. Overall, I personally think that the main idea of the paper is interesting, mature and fleshed out enough for a publication, however, the experimental section is somewhat lacking and (II) is missing from the current manuscript. While the method has theoretical advantages, empirically it seems to perform more or less equal to L2X (but the chunks produced seem qualitatively different which is interesting). I am therefore slightly leaning towards suggesting a major revision of the paper, but I am happy to be convinced otherwise by the other reviewers and the authors during discussion/rebuttal. (I would rate the paper as *borderline*, but it seems that this year*s review system only allows for *weak reject* or *weak accept*, so I*ll go for *weak reject* for now). Improvements / major issues (I) Good compression of highly relevant information is not (always) the same as good interpretability/comprehensiveness. In the limit, the bottleneck variable captures a minimal sufficient statistic, i.e. a maximally compressed version of all relevant information - for finite beta, the bottleneck approximates such a minimal sufficient statistic. From a theoretical point of view this is very appealing, since it is guaranteed to cover a maximal amount of information (given a certain level of compression). But the way this information is represented matters a lot for interpretability - any reversible mapping of the bottleneck variable does not change its information content but can have substantial effects on interpretability, e.g. consider encrypting or randomly perturbing elements of the explanation (i.e. the selected cognitive chunks). This is a major open problem, and some theoretical grounding in the IB framework helps by talking about this problem in very concrete terms. While I would not expect the paper to solve the problem in full generality, some discussion, and perhaps adding a “shortcomings” section would be nice. (II) The relevance of information is measured via I(t;y), which ultimately boils down to the approximation q(y|t). The quality of this approximation is crucial, which can of course be seen by how it influences the tightness of the bound. While I appreciate that the paper investigates the quality of the approximation to some degree (by inspecting the approximator fidelity), I would highly appreciate a thorough discussion of this issue (because ultimately the method will produce cognitive chunks that are relevant for q(y|t), not p(y|x) - the interpretations can be trusted only if q matches fairly well). It would be very interesting to see how quickly interpretability degrades with lower-quality q(y|t) - the latter would of course require more experiments with human “interpreters” which I would not expect to be easily feasible within the rebuttal period. Another interesting experiment to test the match between q(y|t) and p(y|x) would be to “minimally intervene” on the input-chunks suggested by the method and see whether that actually affects the predictions of the black-box models. E.g. do small random perturbations to the selected cognitive chunks in the MNIST digits change the prediction of the black-box classifier? Compare this against small random perturbations in arbitrary chunks of the input. (III) Experimental section: I’m fairly happy with the IMDB experiment, and the TCR to Epitope binding is a nice non-standard application but I find the quality and significance of the results a bit hard to judge. My main concern though is the MNIST experiment: what I would have expected was the following: cognitive chunks are shown to participants and they need to guess the correct number (just like in the IMDB experiment). In the experiment reported in the paper, I’m afraid that there’s a certain bias for judges favoring explanations that lie on the digits rather than off digits. It remains unclear whether they simply prefer the chunks selected by VIBI over other methods, or whether they have actually gained more understanding of how the black-box makes decisions. (IV) Table 2 can easily be misleading because entries with highest mean-accuracy are marked in bold, regardless of whether confidence intervals overlap with other entries or not. Please fix this by either only marking entries in bold where the error bars don’t overlap with an entry in the same row, or marking all entries in bold that lie within the error bars of the best-performing entry. Particularly for “Approximate Fidelity” VIBI often does not perform significantly better than L2X but performs roughly equally well. Of course it would also help to run more repetitions to potentially shrink confidence intervals. (V) Please state the (parametric form) of the prior r(z*) used for the experiments. Also state the analytical expression for the KL-term in the final objective that this prior leads to. Minor comments a) Please add some discussion on how the method depends on hand-crafting cognitive chunks, and how hard/easy this might be for different domains. b) Rather than fixing r(z), other papers have proposed to optimize the prior as well (typically in the context of VAEs / VIB) which is well justified from an IB perspective. It might be interesting to explore these possibilities for VIBI as well in the future. [1] Fixing a broken ELBO. Alemi et al. 2017 [2] The beta-VAE’s Implicit Prior. Hoffman et al. 2017 c) Instead of fixing the number of cognitive chunks in advance, it could also be interesting to infer that number as well (as a future extension of the method). This could either be achieved via a sparsity-inducing prior r(z), or perhaps by borrowing some ideas from the Deterministic IB [3], and its variational version. [3] The deterministic information bottleneck. Strouse and Schwab. 2016 (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###[Due to the rebuttal, my score was raised from a weak reject to a weak accept] Summary The paper addresses the problem of interpreting predictions/decisions of a black-box classifier/regressor by masking the parts of the input that were most relevant. The proposed approach consists of, first, manually designing “cognitive chunks” of input data, e.g. individual words for sentiment classification or fixed-size image-patches for image-classification. Then, a variational IB framework is used to infer which of these chunks are relevant for the classifier’s decision. Additionally, there is a (hard) constraint, making sure that only a fixed (small) number of chunks is used. The bottleneck variable, in this case, is a sparse-chunk representation of the data. The latter is obviously a more compressed representation of the data, but importantly it is a more compressed representation that contains the largest possible amount of relevant information about the decision (because of properties of the information-bottleneck objective). Both factors together, according to the paper, constitute a “good” (i.e. brief but comprehensive) explanation which allows for interpretability and attribution of the black-box system’s decision. The method is evaluated on three tasks (sentiment prediction, image classification, TCR to epitope binding prediction) and performance is reported to be on-par or better than state-of-the-art methods. Contributions -) Application of the IB-method for generating summaries of decision-relevant input-data, which are good candidates for interpretability. The theoretical properties of the IB objective are appealing for producing interpretable data-summaries. -) Adaptation of the variational IB framework, using bits and pieces reported in the literature such that the bottleneck variable is a sparse, binary vector over “cognitive chunks”. -) Experimental evaluation, where human judges rate the “interpretability“ of various state-of-the-art attribution methods. Quality, Clarity, Novelty, Impact The paper addresses a timely and important problem, particularly the IB framework could add some solid theoretical footing (the “theory of relevant information”) to the field of interpetability methods. The paper is well written (though it needs another pass for typos, etc.), related methods and literature are discussed and compared against, and the specific variational IB objective is introduced nicely. Large parts of the method (deep variational IB, VI with categorical variables) have been published before, but these parts are combined in a novel and original way. My main issues with the current paper are (I) interpretability and comprehensiveness are not necessarily the same as maximum compression of maximally relevant information, (II) the method (in theory) depends strongly on the quality of the approximator, this is currently not mentioned and not explored, (III) the experimental section is currently not very strong, in particular the MNIST experiment. See more details for the main issues below. Overall, I personally think that the main idea of the paper is interesting, mature and fleshed out enough for a publication, however, the experimental section is somewhat lacking and (II) is missing from the current manuscript. While the method has theoretical advantages, empirically it seems to perform more or less equal to L2X (but the chunks produced seem qualitatively different which is interesting). I am therefore slightly leaning towards suggesting a major revision of the paper, but I am happy to be convinced otherwise by the other reviewers and the authors during discussion/rebuttal. (I would rate the paper as *borderline*, but it seems that this year*s review system only allows for *weak reject* or *weak accept*, so I*ll go for *weak reject* for now). Improvements / major issues (I) Good compression of highly relevant information is not (always) the same as good interpretability/comprehensiveness. In the limit, the bottleneck variable captures a minimal sufficient statistic, i.e. a maximally compressed version of all relevant information - for finite beta, the bottleneck approximates such a minimal sufficient statistic. From a theoretical point of view this is very appealing, since it is guaranteed to cover a maximal amount of information (given a certain level of compression). But the way this information is represented matters a lot for interpretability - any reversible mapping of the bottleneck variable does not change its information content but can have substantial effects on interpretability, e.g. consider encrypting or randomly perturbing elements of the explanation (i.e. the selected cognitive chunks). This is a major open problem, and some theoretical grounding in the IB framework helps by talking about this problem in very concrete terms. While I would not expect the paper to solve the problem in full generality, some discussion, and perhaps adding a “shortcomings” section would be nice. (II) The relevance of information is measured via I(t;y), which ultimately boils down to the approximation q(y|t). The quality of this approximation is crucial, which can of course be seen by how it influences the tightness of the bound. While I appreciate that the paper investigates the quality of the approximation to some degree (by inspecting the approximator fidelity), I would highly appreciate a thorough discussion of this issue (because ultimately the method will produce cognitive chunks that are relevant for q(y|t), not p(y|x) - the interpretations can be trusted only if q matches fairly well). It would be very interesting to see how quickly interpretability degrades with lower-quality q(y|t) - the latter would of course require more experiments with human “interpreters” which I would not expect to be easily feasible within the rebuttal period. Another interesting experiment to test the match between q(y|t) and p(y|x) would be to “minimally intervene” on the input-chunks suggested by the method and see whether that actually affects the predictions of the black-box models. E.g. do small random perturbations to the selected cognitive chunks in the MNIST digits change the prediction of the black-box classifier? Compare this against small random perturbations in arbitrary chunks of the input. (III) Experimental section: I’m fairly happy with the IMDB experiment, and the TCR to Epitope binding is a nice non-standard application but I find the quality and significance of the results a bit hard to judge. My main concern though is the MNIST experiment: what I would have expected was the following: cognitive chunks are shown to participants and they need to guess the correct number (just like in the IMDB experiment). In the experiment reported in the paper, I’m afraid that there’s a certain bias for judges favoring explanations that lie on the digits rather than off digits. It remains unclear whether they simply prefer the chunks selected by VIBI over other methods, or whether they have actually gained more understanding of how the black-box makes decisions. (IV) Table 2 can easily be misleading because entries with highest mean-accuracy are marked in bold, regardless of whether confidence intervals overlap with other entries or not. Please fix this by either only marking entries in bold where the error bars don’t overlap with an entry in the same row, or marking all entries in bold that lie within the error bars of the best-performing entry. Particularly for “Approximate Fidelity” VIBI often does not perform significantly better than L2X but performs roughly equally well. Of course it would also help to run more repetitions to potentially shrink confidence intervals. (V) Please state the (parametric form) of the prior r(z*) used for the experiments. Also state the analytical expression for the KL-term in the final objective that this prior leads to. Minor comments a) Please add some discussion on how the method depends on hand-crafting cognitive chunks, and how hard/easy this might be for different domains. b) Rather than fixing r(z), other papers have proposed to optimize the prior as well (typically in the context of VAEs / VIB) which is well justified from an IB perspective. It might be interesting to explore these possibilities for VIBI as well in the future. [1] Fixing a broken ELBO. Alemi et al. 2017 [2] The beta-VAE’s Implicit Prior. Hoffman et al. 2017 c) Instead of fixing the number of cognitive chunks in advance, it could also be interesting to infer that number as well (as a future extension of the method). This could either be achieved via a sparsity-inducing prior r(z), or perhaps by borrowing some ideas from the Deterministic IB [3], and its variational version. [3] The deterministic information bottleneck. Strouse and Schwab. 2016\n",
      "    - Writing and Organization Issues (L1, 2)\n",
      "      - Review:###The paper is out of my research area. I could only provide little recommendation. I have tried to read this paper, but it was rather tedious with heavy notations. It would be more friendly to represent the models in visible way for example using diagrams as I can see that the model is a sequence matrix operators with non-linear transformations after that. The paper states that the proposed DrGCNs can improve the stability of GCN models via mean field theory. The experiments were conducted on benchmark datasets and the proposed method was compared to several GCN variations. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper is out of my research area. I could only provide little recommendation. I have tried to read this paper, but it was rather tedious with heavy notations. It would be more friendly to represent the models in visible way for example using diagrams as I can see that the model is a sequence matrix operators with non-linear transformations after that. The paper states that the proposed DrGCNs can improve the stability of GCN models via mean field theory. The experiments were conducted on benchmark datasets and the proposed method was compared to several GCN variations.\n",
      "      - Review:###This work proposes a neural netowrk approach to minimize mincutloss, thus achieving embedding nodes and find communities at the same time. However, it is difficult for me to understand the paper and I feel that it is not clearly written. 1. The algorithm is not written in a box as in Algorithm 1. At first I thought algorithm 1 is the main method, but only after reading it I realized that it is one step of the algorithm. I would appreciate it if the complete algorithm (including input, output, parameters) can be summerized clearly. 2. I am confused about the claim *spectral approach underperforms significantly on the bibliographic datasets as it only uses the structure information in the graph.* I thought the input of all methods are the adjencency matrix A. 3. In table 2 and table 4, why does the paper compare different methods with different measures? Is it possible to compare all methods using all measures? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This work proposes a neural netowrk approach to minimize mincutloss, thus achieving embedding nodes and find communities at the same time. However, it is difficult for me to understand the paper and I feel that it is not clearly written. 1. The algorithm is not written in a box as in Algorithm 1. At first I thought algorithm 1 is the main method, but only after reading it I realized that it is one step of the algorithm. I would appreciate it if the complete algorithm (including input, output, parameters) can be summerized clearly. 2. I am confused about the claim *spectral approach underperforms significantly on the bibliographic datasets as it only uses the structure information in the graph.* I thought the input of all methods are the adjencency matrix A. 3. In table 2 and table 4, why does the paper compare different methods with different measures? Is it possible to compare all methods using all measures?\n",
      "- Empirical Evaluation, Validation, and Experimental Comparison (L3, 68)\n",
      "  - Empirical Evaluation and Experimental Comparison Issues (L2, 24)\n",
      "    - Experimental sufficiency and missing/unclear baselines (L1, 2)\n",
      "      - Review:###In this paper, the authors study an important recommendation problem, i.e., sequential recommendation, and design a novel and improved model called SSE-PT (Stochastic Shared Embedding - Personalized Transformer). Specifically, the authors mainly follow the previous works of the Transformer model and the stochastic shared embedding (SSE) regularization technique. For the part of the personalized transfer (PT), the authors introduce the user embedding for each user , i.e., , shown in Eq.(2) and illustrated in Figure 1. For the part of regularization, the authors find that the SSE technique works well in terms of avoid overfiting in context of other regularization techniques. Extensive empirical studies on five datasets show the effectiveness of the proposed approach compared with other related methods. Overall, the paper is very well presented, in particular of the introduction and discussion about the related works, and the analysis of the experimental results. My major concern is that the technical novelty is somehow limited in terms of the two closely related works of Transformer and stochastic shared embedding (SSE). I thus recommend weak acceptance. Some suggestion: Some important baseline methods may be included to make the results more convincing, e.g., Fossil, MARank, and/or BERT4Rec. Some minors: Typo: in the paragraph below Eq.(3): user -> user Typo: FPMF, PFMC in different places (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###In this paper, the authors study an important recommendation problem, i.e., sequential recommendation, and design a novel and improved model called SSE-PT (Stochastic Shared Embedding - Personalized Transformer). Specifically, the authors mainly follow the previous works of the Transformer model and the stochastic shared embedding (SSE) regularization technique. For the part of the personalized transfer (PT), the authors introduce the user embedding for each user , i.e., , shown in Eq.(2) and illustrated in Figure 1. For the part of regularization, the authors find that the SSE technique works well in terms of avoid overfiting in context of other regularization techniques. Extensive empirical studies on five datasets show the effectiveness of the proposed approach compared with other related methods. Overall, the paper is very well presented, in particular of the introduction and discussion about the related works, and the analysis of the experimental results. My major concern is that the technical novelty is somehow limited in terms of the two closely related works of Transformer and stochastic shared embedding (SSE). I thus recommend weak acceptance. Some suggestion: Some important baseline methods may be included to make the results more convincing, e.g., Fossil, MARank, and/or BERT4Rec. Some minors: Typo: in the paragraph below Eq.(3): user -> user Typo: FPMF, PFMC in different places\n",
      "      - Review:###The submission aims to analyze deep neural network (DNN) features in terms of how well they measure the perceptual severity of image distortions. It proposes to characterize each DNN feature in terms of two well known properties of the human visual system: a) sensitivity to changes in visual frequency and b) orientation selectivity. Both properties are evaluated with respect to the known human Contrast Sensitivity Function (CSF) and measured empirically from the feature’s response to (oriented) sinusoidal gratings. The results are quantified by a composite score termed Perceptual Efficacy (PE). In a set of comprehensive experiments (several pre-trained DNNs, several layers per DNN and two different datasets of distorted images with human perceptual quality annotations) it is demonstrated that feature representation consisting of a layer’s features with high PE better agree with human perceptual quality judgments than low PE feature representations from the same layer. I believe the submission convincingly demonstrates a statistical association between the proposed PE score of a DNN feature and human perceptual quality assessments. Though, it remains unclear whether the characteristics captured by the PE score are necessary or sufficient to explain the success of DNNs to guide image generation tasks by providing a perceptual loss function. For that I believe it is necessary to demonstrate that the present empirical results can be used to improve results of an image generation task, e.g. super-resolution. Furthermore the limits of the PE score could be explored by hand-crafting image representations with maximal PE score and comparing their usefulness in guiding e.g. a super-resolution task compared to a pre-trained DNN. Thus, overall I believe the submission reports interesting initial results but falls short of showing that they capture general properties that can be transferred to improving perceptual loss functions. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The submission aims to analyze deep neural network (DNN) features in terms of how well they measure the perceptual severity of image distortions. It proposes to characterize each DNN feature in terms of two well known properties of the human visual system: a) sensitivity to changes in visual frequency and b) orientation selectivity. Both properties are evaluated with respect to the known human Contrast Sensitivity Function (CSF) and measured empirically from the feature’s response to (oriented) sinusoidal gratings. The results are quantified by a composite score termed Perceptual Efficacy (PE). In a set of comprehensive experiments (several pre-trained DNNs, several layers per DNN and two different datasets of distorted images with human perceptual quality annotations) it is demonstrated that feature representation consisting of a layer’s features with high PE better agree with human perceptual quality judgments than low PE feature representations from the same layer. I believe the submission convincingly demonstrates a statistical association between the proposed PE score of a DNN feature and human perceptual quality assessments. Though, it remains unclear whether the characteristics captured by the PE score are necessary or sufficient to explain the success of DNNs to guide image generation tasks by providing a perceptual loss function. For that I believe it is necessary to demonstrate that the present empirical results can be used to improve results of an image generation task, e.g. super-resolution. Furthermore the limits of the PE score could be explored by hand-crafting image representations with maximal PE score and comparing their usefulness in guiding e.g. a super-resolution task compared to a pre-trained DNN. Thus, overall I believe the submission reports interesting initial results but falls short of showing that they capture general properties that can be transferred to improving perceptual loss functions.\n",
      "    - Incorrect, Incomplete, or Unconvincing Comparison/Baselines (L1, 2)\n",
      "      -  The authors proposed a novel method to find the Maximum Common Subgraph (MCS) of two graphs. I am familiar with the quadratic assignment problem (QAP) based graph matching and I am not very familiar with the MCS problem. The authors adopt Graph Matching Networks (GMN) for feature embedding, and then similarity matrix X can be generated by computing the similarities between the embeddings. The similarity matrix is then normalized using a similar way as the sinkhorn procedure in [1-3]. The Assignment matrix then can be given from X. Then a novel procedure, named Guided subgraph Extraction (GSE, which is considered as the main contribution of this paper), is used to get an MCS from assignment matrix. Here the authors may consider a simple baseline, which is to use QAP to give the assignment matrix, and then run GSE to obtain the MCS. Overall the paper is well written, and the experiment is good and solid. Some suggestions: The GCN based GMN might not be the best choice for graph embedding. The authors may consider stronger Graph Neural Networks such as DGCNN (used in [3]) or Message Passing Neural Network (used in [4] and [5]) as the graph embedding module in the future work. [1] Deep Learning of Graph Matching, CVPR18 [2] Learning Combinatorial Embedding Networks for Deep Graph Matching. ICCV19 [3] Deep Closest Point: Learning Representations for Point Cloud Registration. ICCV19 [4] Deep Graphical Feature Learning for the Feature Matching Problem, ICCV19 [5] Neural Message Passing for Quantum Chemistry, ICML17 (L0, 1)\n",
      "        * Examples:\n",
      "          -  The authors proposed a novel method to find the Maximum Common Subgraph (MCS) of two graphs. I am familiar with the quadratic assignment problem (QAP) based graph matching and I am not very familiar with the MCS problem. The authors adopt Graph Matching Networks (GMN) for feature embedding, and then similarity matrix X can be generated by computing the similarities between the embeddings. The similarity matrix is then normalized using a similar way as the sinkhorn procedure in [1-3]. The Assignment matrix then can be given from X. Then a novel procedure, named Guided subgraph Extraction (GSE, which is considered as the main contribution of this paper), is used to get an MCS from assignment matrix. Here the authors may consider a simple baseline, which is to use QAP to give the assignment matrix, and then run GSE to obtain the MCS. Overall the paper is well written, and the experiment is good and solid. Some suggestions: The GCN based GMN might not be the best choice for graph embedding. The authors may consider stronger Graph Neural Networks such as DGCNN (used in [3]) or Message Passing Neural Network (used in [4] and [5]) as the graph embedding module in the future work. [1] Deep Learning of Graph Matching, CVPR18 [2] Learning Combinatorial Embedding Networks for Deep Graph Matching. ICCV19 [3] Deep Closest Point: Learning Representations for Point Cloud Registration. ICCV19 [4] Deep Graphical Feature Learning for the Feature Matching Problem, ICCV19 [5] Neural Message Passing for Quantum Chemistry, ICML17\n",
      "      -  This paper proposed a graph net based approach for subgraph matching. The general idea is based on the graph matching network (Li et.al, ICML 2019) that computes the node embeddings of two graphs with co-attentions. The training requires the supervision of ground truth matching. During inference an iterative method with heuristic stopping criteria is used. Experiments on tiny graphs show better results than learning based baselines, but worse results than MCS solver. Overall the paper is well motivated. However there are several major concerns with the paper: 1. Since it relies on the solver to provide training data, it might be hard to train on large graphs as there would be no cheap supervision. Also it seems that getting slightly faster but much worse results than the solver on small graphs is not that exciting. 2. It seems there*s a mismatch between training and inference. The inference method is done iteratively, where the Eq (6) is somewhat not clear to me: as this ||w1-w2|| criteria is not trained during training, it seems quite heuristic by doing so. 3. I’m not sure why the two stop conditions are needed. One can easily check (incrementally) whether the added nodes are isomorphic. 4. The graphs used in experiments are too small. Some other minor issues: It would be better to define Y with Eq (1) and Eq (2) in the paper. There seems to be no explicit definition of Y. (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper proposed a graph net based approach for subgraph matching. The general idea is based on the graph matching network (Li et.al, ICML 2019) that computes the node embeddings of two graphs with co-attentions. The training requires the supervision of ground truth matching. During inference an iterative method with heuristic stopping criteria is used. Experiments on tiny graphs show better results than learning based baselines, but worse results than MCS solver. Overall the paper is well motivated. However there are several major concerns with the paper: 1. Since it relies on the solver to provide training data, it might be hard to train on large graphs as there would be no cheap supervision. Also it seems that getting slightly faster but much worse results than the solver on small graphs is not that exciting. 2. It seems there*s a mismatch between training and inference. The inference method is done iteratively, where the Eq (6) is somewhat not clear to me: as this ||w1-w2|| criteria is not trained during training, it seems quite heuristic by doing so. 3. I’m not sure why the two stop conditions are needed. One can easily check (incrementally) whether the added nodes are isomorphic. 4. The graphs used in experiments are too small. Some other minor issues: It would be better to define Y with Eq (1) and Eq (2) in the paper. There seems to be no explicit definition of Y.\n",
      "    - Insufficient or Unconvincing Experimental Evaluation (L1, 2)\n",
      "      -  Summary: As the title of the paper states, this paper tries to improve evolution strategies (ES) using a generative neural network. In the standard ES candidate solution is generated from a multivariate normal distribution, where the parameters of the distribution are adapted during the optimization process. The authors claim that the gaussian distribution, i.e., the ellipsoidal shape of the sampling distribution, is not adequate for the objective functions such as multimodal functions or functions with curved ridge levelsets such as the well-known Rosenbrock functions. The motivation is clearly stated. The technique is interesting and non-trivial. However, the experimental results are not very convincing to conclude that the proposed approach achieves the stated goal. Moreover, this paper may fit more to optimization conferences such as GECCO. Because of the empirical results, I would rate this paper as the border line (around 5), but due to the slightly annoying rating system the rate appears as 6. Comments: P2: *Efficient Natural Evolutionary Strategies (xNES) (Sun et al., 2009) has been shown to reach state- of-the-art performances on a large ES benchmark.* This algorithm is *eNES* and this algorithm is not competitive with the state-of-the-art ES such as CMA-ES. The authors might want to refer to exponential NES, which is xNES, proposed by Glasmachers et al 2010. P5: *Indeed, other bijective GNN models like the Real-NVP (Dinh et al., 2016) introduce non-volume preserving transformations, which can easily overfit and lead to premature concentration and convergence.* Has it been reported in a reference? If so provide the reference. If not, the authors should state that it has been observed the authors preliminary study. In any case, I think it depends how the model is used or trained, and this statement itself is not universally true. P7: *By using f? instead of g? as the push-forward map of the NICE model, we ensure that the flexibility brought by the GNN only impacts the tails of the search distribution. As detailed in an ablation study presented in Appendix F, this additional tool turns out to be essential in order to use GNNs for ES.* I barely understood this point. Please make is clearer. P8: Experimental results are not very convincing. The experiments are limited to dimension 2, 5, 10 and only a few functions are selected from the BBOB test function suite. How about on 20D? What happens if the target is 1e-8, which is the default setting in BBOB? Figure 3 looks interesting, and this is what the authors are trying to achieve. Therefore, it looks like the authors reached the stated objective. However, this is only 2D. No results are provided to convince that the proposed strategy achieved the stated objective. Figure 4 simply looks that the proposed algorithm failed to reach the *flexibility* stated in Section 2: *Another limitation of classical search distribution is their inability to follow multiple hypothesis, that is to explore at the same time different local minima. Even if mixture models can show such flexibility, hyper-parameters like the number of mixtures have optimal values that are impossible to guess a priori.* From these results, I am not convinced that the proposed strategy really achieved more flexible distribution than the classical methods, and whether the flexibility contributes to improve the performance. Another critical point to be discussed is its usefulness. Since this algorithm is proposed to *improve evolution strategy* as a black-box optimizer (not for specific tasks), I expect to improve the state-of-the-art performance. Are the reported results outperform the CMA-ES? Based on Glasmachers et al (2010), xNES tends to require more objective function evaluations than CMA-ES, especially for higher dimensional cases. I am curious to know if the proposed approach outperforms the CMA-ES on Rosenbrock functions. (L0, 1)\n",
      "        * Examples:\n",
      "          -  Summary: As the title of the paper states, this paper tries to improve evolution strategies (ES) using a generative neural network. In the standard ES candidate solution is generated from a multivariate normal distribution, where the parameters of the distribution are adapted during the optimization process. The authors claim that the gaussian distribution, i.e., the ellipsoidal shape of the sampling distribution, is not adequate for the objective functions such as multimodal functions or functions with curved ridge levelsets such as the well-known Rosenbrock functions. The motivation is clearly stated. The technique is interesting and non-trivial. However, the experimental results are not very convincing to conclude that the proposed approach achieves the stated goal. Moreover, this paper may fit more to optimization conferences such as GECCO. Because of the empirical results, I would rate this paper as the border line (around 5), but due to the slightly annoying rating system the rate appears as 6. Comments: P2: *Efficient Natural Evolutionary Strategies (xNES) (Sun et al., 2009) has been shown to reach state- of-the-art performances on a large ES benchmark.* This algorithm is *eNES* and this algorithm is not competitive with the state-of-the-art ES such as CMA-ES. The authors might want to refer to exponential NES, which is xNES, proposed by Glasmachers et al 2010. P5: *Indeed, other bijective GNN models like the Real-NVP (Dinh et al., 2016) introduce non-volume preserving transformations, which can easily overfit and lead to premature concentration and convergence.* Has it been reported in a reference? If so provide the reference. If not, the authors should state that it has been observed the authors preliminary study. In any case, I think it depends how the model is used or trained, and this statement itself is not universally true. P7: *By using f? instead of g? as the push-forward map of the NICE model, we ensure that the flexibility brought by the GNN only impacts the tails of the search distribution. As detailed in an ablation study presented in Appendix F, this additional tool turns out to be essential in order to use GNNs for ES.* I barely understood this point. Please make is clearer. P8: Experimental results are not very convincing. The experiments are limited to dimension 2, 5, 10 and only a few functions are selected from the BBOB test function suite. How about on 20D? What happens if the target is 1e-8, which is the default setting in BBOB? Figure 3 looks interesting, and this is what the authors are trying to achieve. Therefore, it looks like the authors reached the stated objective. However, this is only 2D. No results are provided to convince that the proposed strategy achieved the stated objective. Figure 4 simply looks that the proposed algorithm failed to reach the *flexibility* stated in Section 2: *Another limitation of classical search distribution is their inability to follow multiple hypothesis, that is to explore at the same time different local minima. Even if mixture models can show such flexibility, hyper-parameters like the number of mixtures have optimal values that are impossible to guess a priori.* From these results, I am not convinced that the proposed strategy really achieved more flexible distribution than the classical methods, and whether the flexibility contributes to improve the performance. Another critical point to be discussed is its usefulness. Since this algorithm is proposed to *improve evolution strategy* as a black-box optimizer (not for specific tasks), I expect to improve the state-of-the-art performance. Are the reported results outperform the CMA-ES? Based on Glasmachers et al (2010), xNES tends to require more objective function evaluations than CMA-ES, especially for higher dimensional cases. I am curious to know if the proposed approach outperforms the CMA-ES on Rosenbrock functions.\n",
      "      -  This very well written and executed paper synthesizes several ideas recently published in the field of deep reinforcement learning-based goal-driven navigation. It elegantly combines these ideas together by presenting a neural agent architecture that consists of: * a perception module (e.g. a convnet) that extracts coarse visual feature maps s_t from an RGBD image * a differentiable map canvas M_t that is rotated at each step based on affine egocentric velocity (dx_t, dy_y, d phi_t) * differentiable inverse projection mapping, which uses known camera parameters, projective geometry and the depth channel of the image to project the visual feature vectors s_t onto a 2D map and add it to the existing canvas M_t * a recurrent module (GRU) for update a state h_t that is used for computing the policy distribution and value function * additional inputs to the policy and value function, that include a global map read r_t, as well as a query q_t (produced by the policy head) based retrieval of features from the map * position indexing of features retrieved from the map The algorithm is trained end-to-end, without extra supervision, using Advantage Actor-Critic (A2C) RL. Based on the strong inductive biases regarding the map, namely affine transforms of the map given information about relative movement, and projective geometry transformations of visual features in the map frame, it seems that the question of where to write is solved, and that the network only needs to learn what to write in the differentiable map. Evaluation is done on 3 games in VizDoom: finding the exit of the Labyrinth, object retrieval and find and return / Minotaur. Criticism: The authors could justify better the choice of using the projective geometry inductive prior. They use sentences like *We argue that projective geometry is a strong law imposed on any vision system working from egocentric observations* (not quite related to grid and place cells, despite being in that section) and *this inverse mapping operation is second nature to many organisms* without giving any reference. Several papers have been published in the last two years, focusing on differential memory architectures with a 2D map structure, projective geometry. This paper goes further by building and iteratively updating a 2D occupancy map using visual features and image geometry, just like RGBD-SLAM (which would merit a citation, e.g., [1] and [2]). This paper essentially combines existing ideas (see table 1): projective geometry, reward-based learning of M_t, RL, multitask navigation, semantic features. While this is not novel, seeing all this combined in a single technique does have merit. What is disappointing, given that this is a combination paper, is that the environment is so simply, and that photorealistic environments were not tested. For example, the VizDoom environment uses 2D sprites for objects, making the visual feature extraction from objects much simpler. Would the method work equally well with the objects in DeepMind Lab, which are seen from multiple view points? And what in an environment like AdobeIndoorNav? [1] Henry et al (2010) *RGB-D mapping: Using depth cameras for dense 3D modeling of indoor environments* [2] Izadi et al (2011) *KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera* (L0, 1)\n",
      "        * Examples:\n",
      "          -  This very well written and executed paper synthesizes several ideas recently published in the field of deep reinforcement learning-based goal-driven navigation. It elegantly combines these ideas together by presenting a neural agent architecture that consists of: * a perception module (e.g. a convnet) that extracts coarse visual feature maps s_t from an RGBD image * a differentiable map canvas M_t that is rotated at each step based on affine egocentric velocity (dx_t, dy_y, d phi_t) * differentiable inverse projection mapping, which uses known camera parameters, projective geometry and the depth channel of the image to project the visual feature vectors s_t onto a 2D map and add it to the existing canvas M_t * a recurrent module (GRU) for update a state h_t that is used for computing the policy distribution and value function * additional inputs to the policy and value function, that include a global map read r_t, as well as a query q_t (produced by the policy head) based retrieval of features from the map * position indexing of features retrieved from the map The algorithm is trained end-to-end, without extra supervision, using Advantage Actor-Critic (A2C) RL. Based on the strong inductive biases regarding the map, namely affine transforms of the map given information about relative movement, and projective geometry transformations of visual features in the map frame, it seems that the question of where to write is solved, and that the network only needs to learn what to write in the differentiable map. Evaluation is done on 3 games in VizDoom: finding the exit of the Labyrinth, object retrieval and find and return / Minotaur. Criticism: The authors could justify better the choice of using the projective geometry inductive prior. They use sentences like *We argue that projective geometry is a strong law imposed on any vision system working from egocentric observations* (not quite related to grid and place cells, despite being in that section) and *this inverse mapping operation is second nature to many organisms* without giving any reference. Several papers have been published in the last two years, focusing on differential memory architectures with a 2D map structure, projective geometry. This paper goes further by building and iteratively updating a 2D occupancy map using visual features and image geometry, just like RGBD-SLAM (which would merit a citation, e.g., [1] and [2]). This paper essentially combines existing ideas (see table 1): projective geometry, reward-based learning of M_t, RL, multitask navigation, semantic features. While this is not novel, seeing all this combined in a single technique does have merit. What is disappointing, given that this is a combination paper, is that the environment is so simply, and that photorealistic environments were not tested. For example, the VizDoom environment uses 2D sprites for objects, making the visual feature extraction from objects much simpler. Would the method work equally well with the objects in DeepMind Lab, which are seen from multiple view points? And what in an environment like AdobeIndoorNav? [1] Henry et al (2010) *RGB-D mapping: Using depth cameras for dense 3D modeling of indoor environments* [2] Izadi et al (2011) *KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera*\n",
      "    - Insufficient or Unconvincing Experimental Evaluation and Analysis (L1, 7)\n",
      "      -  I take issue with the usage of the phrase *skill discovery*. In prior work (e.g. VIC, DIAYN), this meant learning a skill-conditional policy. Here, there is only a single (unconditioned) policy, and the different *skills* come from modifications of the environment -- the number of skills is tied to the number of environments. This is not to say that this way of doing things is wrong, but rather that it is misleading in the context of prior work. Skill discovery in this context implies being able to have a single agent execute a variety of learned skills, rather than having one agent per environment with each environment designed to elicit a specific skill. Rather than *skill discovery*, I suggest the authors position MISC relative to earlier work on empowerment, wherein a single policy was used to maximize mutual information of the form I(a; s_t | s_{t-1}). Modifying the objective to incorporate domain knowledge (as done in your DIAYN baseline) yields I(a; s_i | s_{t-1}) and is amenable to maximization by either of the lower bounds considered here. Indeed, your DIAYN baseline with skill length set to 1 and the number of skills equal to the number of actions (or same parameterization in the case of continuous actions) should recover this approach. I believe this would be a much more appropriate baseline, and I*d be curious to hear the intuition for why I(s_c ; s_i) should be superior. Apart from this missing baseline, the experimental results seem convincing. However, it is unclear whether or not VIME and PER were modified to incorporate domain knowledge (i.e. s_i/s_c distinction). Indeed, an appendix would be greatly appreciated, as many experimental details were omitted. Ideally, an experimental setup with previously published results (e.g. control suite for DIAYN, Seaquest for DISCERN) would be considered, but I can understand why this wasn*t done as incorporating domain knowledge is the main contribution of the paper. That said, the claims should be weakened to reflect this gap, and domain knowledge should be mentioned more prominently (e.g. states of interest vs context are given, not learned). Rebuttal EDIT: The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking. Needing new environment variations to obtain new skills is a large step backwards from things like DIAYN (the MISC/DIAYN combination needs more evidence to be considered a possible solution), and the s_i/s_c distinction is non-trivial to specify or learn for harder problems (e.g. pixel observations). That said, in the sort of settings under consideration (low dimensional state variables and environmental variations are simple to create) MISC does appear to be superior to prior work. The empowerment baseline is much appreciated, and while modifications of PER and VIME that incorporate prior knowledge would*ve also been nice, the experimental results pass the bar for acceptance in my view. (L0, 1)\n",
      "        * Examples:\n",
      "          -  I take issue with the usage of the phrase *skill discovery*. In prior work (e.g. VIC, DIAYN), this meant learning a skill-conditional policy. Here, there is only a single (unconditioned) policy, and the different *skills* come from modifications of the environment -- the number of skills is tied to the number of environments. This is not to say that this way of doing things is wrong, but rather that it is misleading in the context of prior work. Skill discovery in this context implies being able to have a single agent execute a variety of learned skills, rather than having one agent per environment with each environment designed to elicit a specific skill. Rather than *skill discovery*, I suggest the authors position MISC relative to earlier work on empowerment, wherein a single policy was used to maximize mutual information of the form I(a; s_t | s_{t-1}). Modifying the objective to incorporate domain knowledge (as done in your DIAYN baseline) yields I(a; s_i | s_{t-1}) and is amenable to maximization by either of the lower bounds considered here. Indeed, your DIAYN baseline with skill length set to 1 and the number of skills equal to the number of actions (or same parameterization in the case of continuous actions) should recover this approach. I believe this would be a much more appropriate baseline, and I*d be curious to hear the intuition for why I(s_c ; s_i) should be superior. Apart from this missing baseline, the experimental results seem convincing. However, it is unclear whether or not VIME and PER were modified to incorporate domain knowledge (i.e. s_i/s_c distinction). Indeed, an appendix would be greatly appreciated, as many experimental details were omitted. Ideally, an experimental setup with previously published results (e.g. control suite for DIAYN, Seaquest for DISCERN) would be considered, but I can understand why this wasn*t done as incorporating domain knowledge is the main contribution of the paper. That said, the claims should be weakened to reflect this gap, and domain knowledge should be mentioned more prominently (e.g. states of interest vs context are given, not learned). Rebuttal EDIT: The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking. Needing new environment variations to obtain new skills is a large step backwards from things like DIAYN (the MISC/DIAYN combination needs more evidence to be considered a possible solution), and the s_i/s_c distinction is non-trivial to specify or learn for harder problems (e.g. pixel observations). That said, in the sort of settings under consideration (low dimensional state variables and environmental variations are simple to create) MISC does appear to be superior to prior work. The empowerment baseline is much appreciated, and while modifications of PER and VIME that incorporate prior knowledge would*ve also been nice, the experimental results pass the bar for acceptance in my view.\n",
      "      -  In this paper, the authors performed an empirical study on the importance of neural network weights and to which extent they need to be updated. Some observations are obtained such as from the third epoch on, a large proportion of weights do not need to be updated and the performance of the network is not significantly affected. Overall speaking, the qualitative result in the paper has already been discovered in many previous work, although the quantitative results seem to be new. However, there is large room to improve regarding the experimental design and the comprehensiveness of the experiments. Just name a few as follows: 1) For different models and different tasks, the quantitative results are different. There is no deep discussion on the intrinsic reason for this, and what is the most important factor that influences the redundancy of weight updates. The authors came to the conclusion that from the third epoch on, no need to update most of the weights. “3” seems to be a magic number to me. Why is it? No solid experiments were done regarding this, and no convincing analysis was made. 2) The datasets used in the experiments are not diverse enough and are not of large scale. For example, the CIFA-10 and MNIST datasets are relatively of small scale. What if the datasets are much larger like ImageNet. In such more complicated case, will the weight updates still be unnecessary? Will the ratio and the epoch number change? What is the underlying factor determining these? For another example, there are many NLP datasets for language understanding and machine translation, which are of large scale. Why choosing an image captioning dataset (which I do not agree to be real-life experiments when compared with language understanding and machine translation)? Can the observations generalizable to more complicated tasks and datasets? 3) The models studied in the paper are also a little simple, especially for the text task. Why just using a single-layer LSTM? Why not popularly used Transformer? As a summary, for an empirical study to be convincing, the tasks, datasets, scales, model structures, detailed settings, and discussions are the critical aspects. However, as explained above, this paper has not done a good job on these aspects. Significantly more work needs to be done in order to make it an impactful work. *I read the author rebuttal, but would like to keep my rating unchanged. (L0, 1)\n",
      "        * Examples:\n",
      "          -  In this paper, the authors performed an empirical study on the importance of neural network weights and to which extent they need to be updated. Some observations are obtained such as from the third epoch on, a large proportion of weights do not need to be updated and the performance of the network is not significantly affected. Overall speaking, the qualitative result in the paper has already been discovered in many previous work, although the quantitative results seem to be new. However, there is large room to improve regarding the experimental design and the comprehensiveness of the experiments. Just name a few as follows: 1) For different models and different tasks, the quantitative results are different. There is no deep discussion on the intrinsic reason for this, and what is the most important factor that influences the redundancy of weight updates. The authors came to the conclusion that from the third epoch on, no need to update most of the weights. “3” seems to be a magic number to me. Why is it? No solid experiments were done regarding this, and no convincing analysis was made. 2) The datasets used in the experiments are not diverse enough and are not of large scale. For example, the CIFA-10 and MNIST datasets are relatively of small scale. What if the datasets are much larger like ImageNet. In such more complicated case, will the weight updates still be unnecessary? Will the ratio and the epoch number change? What is the underlying factor determining these? For another example, there are many NLP datasets for language understanding and machine translation, which are of large scale. Why choosing an image captioning dataset (which I do not agree to be real-life experiments when compared with language understanding and machine translation)? Can the observations generalizable to more complicated tasks and datasets? 3) The models studied in the paper are also a little simple, especially for the text task. Why just using a single-layer LSTM? Why not popularly used Transformer? As a summary, for an empirical study to be convincing, the tasks, datasets, scales, model structures, detailed settings, and discussions are the critical aspects. However, as explained above, this paper has not done a good job on these aspects. Significantly more work needs to be done in order to make it an impactful work. *I read the author rebuttal, but would like to keep my rating unchanged.\n",
      "      -  This paper studies how to build semantic spatial maps for the purpose of navigation in 3D environments. The paper presents a differentiable policy network that pastes together semantic map predictions into a spatial map. Information is read out from this map using a global read operation (that looks at the entire map) and a self-attention read operation. This information is used to produce actions. The paper presents experimental results in 3D VizDoom scenarios and reports improvements over a vanilla LSTM, and another spatial memory based method (Neural Map). Strengths: 1. I very much like the proposed formulation for tackling navigation problems. Using learning to leverage semantic reasoning, and structuring the computation spatially makes a lot of sense. 2. In my view, the proposed formulation advances current models in the following ways: a. Maintaining and updating allocentric maps, and reading off egocentric maps. This alleviates need for repeated rotations of the map, and thus prevents aliasing. 3. The paper provides ablations for the various parts of the system and provides qualitative analysis of the learned spatial representations. 4. Very good placement of work in current literature. I really like Table 1. Shortcomings: 1. The central contribution of the paper is the design of the egocentric spatial memory, how to build and maintain it over time, and its use in deep RL. The paper does this by using components from previous papers and presents a very nice summary of this in Table 1. Unfortunately, modulo the component described above (that of maintaining allocentric maps and reading off egocentric maps as and when needed), all other components are borrowed from existing papers, as can be seen in Table 1 already. The paper lists its contributions in Introduction on page 2, and each of those contributions has been studied in previous papers (though I will that admit no single paper does all these things together). Thus, I believe the paper falls short in terms of technical contributions. 2. Following on from point above, putting everything together and showing that it works, could also be a reasonable contribution, though it would warrant more extensive and systematic experiments for the different design choices, possibly in more realistic environments. For example, a) is the projective projection important, or could that have been learned, b) do repeated rotations indeed lead to blurred representations, c) what is critical to get such models work with RL, that past models that used imitation learning couldn*t, d) other claimed differences from past works in this space. 3. Experiments and analysis: a. The paper compares against NeuralMap, and reports improvements, but doesn*t give a reason as to why this happens. b. Past works have demonstrated these ideas in visually realistic environments (similar to those in Gibson / Habitat, see semantic tasks in CogMap). Current paper only investigates proposed ideas in VizDoom environments. Thus, while I like the direction of research and the fact that the paper presents an architectures that uses latest techniques in the area, I believe the paper doesn*t have enough technical contribution of its own, and experiments are limited to synthetic VizDoom environments. (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper studies how to build semantic spatial maps for the purpose of navigation in 3D environments. The paper presents a differentiable policy network that pastes together semantic map predictions into a spatial map. Information is read out from this map using a global read operation (that looks at the entire map) and a self-attention read operation. This information is used to produce actions. The paper presents experimental results in 3D VizDoom scenarios and reports improvements over a vanilla LSTM, and another spatial memory based method (Neural Map). Strengths: 1. I very much like the proposed formulation for tackling navigation problems. Using learning to leverage semantic reasoning, and structuring the computation spatially makes a lot of sense. 2. In my view, the proposed formulation advances current models in the following ways: a. Maintaining and updating allocentric maps, and reading off egocentric maps. This alleviates need for repeated rotations of the map, and thus prevents aliasing. 3. The paper provides ablations for the various parts of the system and provides qualitative analysis of the learned spatial representations. 4. Very good placement of work in current literature. I really like Table 1. Shortcomings: 1. The central contribution of the paper is the design of the egocentric spatial memory, how to build and maintain it over time, and its use in deep RL. The paper does this by using components from previous papers and presents a very nice summary of this in Table 1. Unfortunately, modulo the component described above (that of maintaining allocentric maps and reading off egocentric maps as and when needed), all other components are borrowed from existing papers, as can be seen in Table 1 already. The paper lists its contributions in Introduction on page 2, and each of those contributions has been studied in previous papers (though I will that admit no single paper does all these things together). Thus, I believe the paper falls short in terms of technical contributions. 2. Following on from point above, putting everything together and showing that it works, could also be a reasonable contribution, though it would warrant more extensive and systematic experiments for the different design choices, possibly in more realistic environments. For example, a) is the projective projection important, or could that have been learned, b) do repeated rotations indeed lead to blurred representations, c) what is critical to get such models work with RL, that past models that used imitation learning couldn*t, d) other claimed differences from past works in this space. 3. Experiments and analysis: a. The paper compares against NeuralMap, and reports improvements, but doesn*t give a reason as to why this happens. b. Past works have demonstrated these ideas in visually realistic environments (similar to those in Gibson / Habitat, see semantic tasks in CogMap). Current paper only investigates proposed ideas in VizDoom environments. Thus, while I like the direction of research and the fact that the paper presents an architectures that uses latest techniques in the area, I believe the paper doesn*t have enough technical contribution of its own, and experiments are limited to synthetic VizDoom environments.\n",
      "      - Review:###The paper proposes a novel architecture for spatially structured memory. The main idea is to incorporate inductive bias/invariance derived from projective geometry arguments. The experiments seem to clearly show that this new architecture improves previous approaches to tasks which require spatial reasoning and memory, and the ablations studies and visualizations provide useful insights into the workings of the agent. One thing I*m missing is an experiment showing that this inductive bias also doesn*t degrade performance on tasks where spatial reasoning is not necessary (as compared to vanilla GRU/LSTM). (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper proposes a novel architecture for spatially structured memory. The main idea is to incorporate inductive bias/invariance derived from projective geometry arguments. The experiments seem to clearly show that this new architecture improves previous approaches to tasks which require spatial reasoning and memory, and the ablations studies and visualizations provide useful insights into the workings of the agent. One thing I*m missing is an experiment showing that this inductive bias also doesn*t degrade performance on tasks where spatial reasoning is not necessary (as compared to vanilla GRU/LSTM).\n",
      "      - Review:###This paper introduces a large-scale knowledge graph database called TechKG, which is constructed from a massive repository of academic papers in Chinese. The authors have described in details the process and heuristics in use for constructing such knowledge base, and also reported important statistics and characteristics of the database, including duplicate name, imbalance issue etc. The main contribution is to provide a KG in Chinese, which wasn’t widely available compared to popular languages such as English. The paper does not clearly demonstrate experiment showing how this new knowledge graph can be complementary to or benefit existing realm of machine learning tasks. How much does it help improve KG-dependent task in Chinese language, or in multi-lingual setting? Without these experiments, it’s hard to estimate precisely the contribution of the KG database to the machine learning community. In Section 3.1, the authors should specify more clearly the source of the journals collected & the representativeness of the repository. In Section 3.3, how is the threshold defined to determine the hierarchical relation from k1 to k2? The writing can be polished further. Things to improve the paper that did not impact the score: Page 1: Great progress have been -> great progress has been Page 1: most of existing -> many existing Section 2, paragraph 4: lack space between `WordNet` and `Miller`, also need adding space between `YAGO` and `suchanek` (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper introduces a large-scale knowledge graph database called TechKG, which is constructed from a massive repository of academic papers in Chinese. The authors have described in details the process and heuristics in use for constructing such knowledge base, and also reported important statistics and characteristics of the database, including duplicate name, imbalance issue etc. The main contribution is to provide a KG in Chinese, which wasn’t widely available compared to popular languages such as English. The paper does not clearly demonstrate experiment showing how this new knowledge graph can be complementary to or benefit existing realm of machine learning tasks. How much does it help improve KG-dependent task in Chinese language, or in multi-lingual setting? Without these experiments, it’s hard to estimate precisely the contribution of the KG database to the machine learning community. In Section 3.1, the authors should specify more clearly the source of the journals collected & the representativeness of the repository. In Section 3.3, how is the threshold defined to determine the hierarchical relation from k1 to k2? The writing can be polished further. Things to improve the paper that did not impact the score: Page 1: Great progress have been -> great progress has been Page 1: most of existing -> many existing Section 2, paragraph 4: lack space between `WordNet` and `Miller`, also need adding space between `YAGO` and `suchanek`\n",
      "      - Review:###This paper introduces a method combining the lexicon-based feature and character ngram model for handling negation in Amharic sentiment classification. The algorithmic contribution is demonstrated in a Amharic dataset. My rating for this paper is Reject because there is no novelty in the approach, it introduces some rules to build a FSA plus a logistic regression model with character-level ngram feature. It is also lack of argument/motivation why the proposed model is special and specific to the problem. The experiment only compares with two baselines in single dataset, which is also not convincing. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper introduces a method combining the lexicon-based feature and character ngram model for handling negation in Amharic sentiment classification. The algorithmic contribution is demonstrated in a Amharic dataset. My rating for this paper is Reject because there is no novelty in the approach, it introduces some rules to build a FSA plus a logistic regression model with character-level ngram feature. It is also lack of argument/motivation why the proposed model is special and specific to the problem. The experiment only compares with two baselines in single dataset, which is also not convincing.\n",
      "      - Review:###This paper proposes a noise regularization method which adds noise on both x and y for conditional density estimation problem (e.g., regression and classification). The writing is good and the whole paper is easy to follow. However, I vote for reject, since the novelty is somehow limited, the claims made in the paper is not well supported and experiments are not very convincing. 1. Adding noise on x (e.g., [1]), y (e.g., [2]) is not new. Though it is claimed that this paper extends previous results on classification/regression to conditional density estimation which is a more general case. This claim is not well supported. Experiments are still evaluated in classification/regression tasks. 2. Theorem 1 & 2 in Sec 4.2 only show the asymptotic case, which are quite obvious and seems helpless in understanding the advantage of adding noise regularization in conditional density estimation. 3. Sec 4.1. The explanation that Page 5, ```*The second term in (6) penalizes large negative second derivatives of the conditional log density estimate...*. It is hard for me to understand. Large positive second derivatives also lead to poor smoothness. [1] Learning with Marginalized Corrupted Features, ICML 2013 [2] Learning with Noisy Labels, NIPS 2013 (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a noise regularization method which adds noise on both x and y for conditional density estimation problem (e.g., regression and classification). The writing is good and the whole paper is easy to follow. However, I vote for reject, since the novelty is somehow limited, the claims made in the paper is not well supported and experiments are not very convincing. 1. Adding noise on x (e.g., [1]), y (e.g., [2]) is not new. Though it is claimed that this paper extends previous results on classification/regression to conditional density estimation which is a more general case. This claim is not well supported. Experiments are still evaluated in classification/regression tasks. 2. Theorem 1 & 2 in Sec 4.2 only show the asymptotic case, which are quite obvious and seems helpless in understanding the advantage of adding noise regularization in conditional density estimation. 3. Sec 4.1. The explanation that Page 5, ```*The second term in (6) penalizes large negative second derivatives of the conditional log density estimate...*. It is hard for me to understand. Large positive second derivatives also lead to poor smoothness. [1] Learning with Marginalized Corrupted Features, ICML 2013 [2] Learning with Noisy Labels, NIPS 2013\n",
      "    - Lack of Comprehensive Experiments/Empirical Evaluation (L1, 5)\n",
      "      - Review:###The manuscript proposes SSE-PT, a sequential recommendation model based on transformer and stochastic shared embedding (SST). Experiments on several datasets show that SSE-PT outperforms a number of baseline methods. Some analytical results are also provided. Overall, I think this work is not suitable for ICLR due to following reasons. The novelty of this work is limited. This work is based on SASREC [W Kang, ICDM2018] and uses transformer to encode user-item interactions in sequential manner. The difference is that this work adds user embedding in bottom layer and utilizes SSE for regularization as well as designs SSE-PT++ by sampling. To me, there is little extension or novelty. The experiment results are not convincing. Most of results are copied from [W Kang, ICDM2018] except HGN in Table 1. Table 1 shows SASREC is much better than HGN [C Ma, KDD2019]. However, I checked the results in HGN paper and found HGN is much better than SASREC. Even though datasets are different, most of them are from Amazon data. I was not convinced by this result due to the large difference. In addition, I did not understand why the authors change evaluation metrics in Table 3, i.e., from NDCG/Recall@10 to NDCG/Recall@5. I found SSE-PT without regularization and with different regularizations are much worse than the best result, which makes me concern about the effectiveness of personalized transformer. I did not see ablation study or discussion about this. Update: I have considered author rebuttal. I appreciate the extensive hyper-parameter sensitivity and ablation study in the paper, while these cannot be a key factor in evaluating paper as most of them can be done easily. I main concerns still lie in the novelty and experimental results. I still think this work is not suitable for ICLR and I keep my score. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The manuscript proposes SSE-PT, a sequential recommendation model based on transformer and stochastic shared embedding (SST). Experiments on several datasets show that SSE-PT outperforms a number of baseline methods. Some analytical results are also provided. Overall, I think this work is not suitable for ICLR due to following reasons. The novelty of this work is limited. This work is based on SASREC [W Kang, ICDM2018] and uses transformer to encode user-item interactions in sequential manner. The difference is that this work adds user embedding in bottom layer and utilizes SSE for regularization as well as designs SSE-PT++ by sampling. To me, there is little extension or novelty. The experiment results are not convincing. Most of results are copied from [W Kang, ICDM2018] except HGN in Table 1. Table 1 shows SASREC is much better than HGN [C Ma, KDD2019]. However, I checked the results in HGN paper and found HGN is much better than SASREC. Even though datasets are different, most of them are from Amazon data. I was not convinced by this result due to the large difference. In addition, I did not understand why the authors change evaluation metrics in Table 3, i.e., from NDCG/Recall@10 to NDCG/Recall@5. I found SSE-PT without regularization and with different regularizations are much worse than the best result, which makes me concern about the effectiveness of personalized transformer. I did not see ablation study or discussion about this. Update: I have considered author rebuttal. I appreciate the extensive hyper-parameter sensitivity and ablation study in the paper, while these cannot be a key factor in evaluating paper as most of them can be done easily. I main concerns still lie in the novelty and experimental results. I still think this work is not suitable for ICLR and I keep my score.\n",
      "      - Review:###This paper adapts Bayesian linear regression to the setting of a limited memory replay buffer. The idea is to calibrate the prior mean and variance when the neural representation of context is updated. Overall the paper is well written and explained clearly. Some experiments are provided to show that the proposed method is able to achieve a performance competitive to Bayesian linear regression with infinite memory. The result of this paper is interesting. But I am not sure if the current experimental results are convincing enough to justify the significance of the proposed method. 1. The results in Section 4.2 seem to be following the setting in Riquelme 2018. These datasets are all in a supervised learning setting. It is a bit disappointing that the proposed method is not tested on RL datasets. 2. No other baseline is provided in the experiments for comparison. a. There are other methods in the literature to overcome catastrophic forgetting of neural networks, e.g regularizing the update of the network. How would that be compared to the proposed method? b. What about other methods, like [1]? 3. Most of the experiment details are missing. For example, how is the reward defined in section 4.3? What is the overhead in computation in practice, especially for the SDP? Other comments: 1. Why would solving a SDP require only O(g^{0.5}) in section 3.1? 2. In the discussion in section 3, even if equation (5) and (6) can be exactly solved, how does the heavy tailed problem mentioned in section 2 been solved? [1] Elmachtoub, Adam N., et al. *A practical method for solving contextual bandit problems using decision trees.* arXiv preprint arXiv:1706.04687 (2017). (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper adapts Bayesian linear regression to the setting of a limited memory replay buffer. The idea is to calibrate the prior mean and variance when the neural representation of context is updated. Overall the paper is well written and explained clearly. Some experiments are provided to show that the proposed method is able to achieve a performance competitive to Bayesian linear regression with infinite memory. The result of this paper is interesting. But I am not sure if the current experimental results are convincing enough to justify the significance of the proposed method. 1. The results in Section 4.2 seem to be following the setting in Riquelme 2018. These datasets are all in a supervised learning setting. It is a bit disappointing that the proposed method is not tested on RL datasets. 2. No other baseline is provided in the experiments for comparison. a. There are other methods in the literature to overcome catastrophic forgetting of neural networks, e.g regularizing the update of the network. How would that be compared to the proposed method? b. What about other methods, like [1]? 3. Most of the experiment details are missing. For example, how is the reward defined in section 4.3? What is the overhead in computation in practice, especially for the SDP? Other comments: 1. Why would solving a SDP require only O(g^{0.5}) in section 3.1? 2. In the discussion in section 3, even if equation (5) and (6) can be exactly solved, how does the heavy tailed problem mentioned in section 2 been solved? [1] Elmachtoub, Adam N., et al. *A practical method for solving contextual bandit problems using decision trees.* arXiv preprint arXiv:1706.04687 (2017).\n",
      "      - Review:###This paper proposes a method for assigning values to each datum. For example, data with incorrect labels, data of low quality, or data from off-the-target distributions should be assigned low values. The main method involves training a neural network to predict the value for each training datum. The reward is based on performance on a small validation set. To make gradient flow through data sampling, REINFORCE is used. The method is evaluated on multiple datasets. The results show that the proposed method outperforms a number of existing approaches. I think the proposed method is reasonable, and the results look promising. However, I*m concerned that there*s limited ablation study provided to show how each design choice impacts the performance. (After all, the proposed has many differences from existing methods.) Without proper ablation study, it*s hard for the community to learn conclusively from the proposed techniques. In addition, as pointed out by the comments by Abubakar Abid, there is a model that is trained on the clean validation data used during training. But this is not discussed in paper. How does it impact performance? Also, all the image datasets studied in this paper are small, and this paper only considers fine-tuning the final layer from an ImageNet-pre-trained model. It*ll be more convincing to show results on more relevant datasets or tasks in the community. Overall I think this paper is slightly below the bar for publication in its current form, and will benefit from additional experiments. -------after rebuttal-------- Thanks for providing additional results and explanations. I found the new ablations in C.6 helpful for understanding the impact of each of the design choices. The rebuttal also addresses my concerns regarding datasets, and my concerns regarding implementation details of ‘y_train_hat’, as now it*s included in sec 4. Overall, after rebuttal, I*d like to recommend *weak accept* for this paper. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a method for assigning values to each datum. For example, data with incorrect labels, data of low quality, or data from off-the-target distributions should be assigned low values. The main method involves training a neural network to predict the value for each training datum. The reward is based on performance on a small validation set. To make gradient flow through data sampling, REINFORCE is used. The method is evaluated on multiple datasets. The results show that the proposed method outperforms a number of existing approaches. I think the proposed method is reasonable, and the results look promising. However, I*m concerned that there*s limited ablation study provided to show how each design choice impacts the performance. (After all, the proposed has many differences from existing methods.) Without proper ablation study, it*s hard for the community to learn conclusively from the proposed techniques. In addition, as pointed out by the comments by Abubakar Abid, there is a model that is trained on the clean validation data used during training. But this is not discussed in paper. How does it impact performance? Also, all the image datasets studied in this paper are small, and this paper only considers fine-tuning the final layer from an ImageNet-pre-trained model. It*ll be more convincing to show results on more relevant datasets or tasks in the community. Overall I think this paper is slightly below the bar for publication in its current form, and will benefit from additional experiments. -------after rebuttal-------- Thanks for providing additional results and explanations. I found the new ablations in C.6 helpful for understanding the impact of each of the design choices. The rebuttal also addresses my concerns regarding datasets, and my concerns regarding implementation details of ‘y_train_hat’, as now it*s included in sec 4. Overall, after rebuttal, I*d like to recommend *weak accept* for this paper.\n",
      "      - Review:###This paper proposes a simple and one-shot approach on neural architecture search for the number of channels to achieve better accuracy. Rather than training a lot of network samples, the proposed method trains a single slimmable network to approximate the network accuracy of different channel configurations. The experimental results show that the proposed method achieves better performance than the existing baseline methods. - It would be better to provide the search cost of the proposed method and the other baseline methods because that is the important metric for neural architecture search methods. As this paper points out that NAS methods are computationally expensive, it would be better to make the efficiency of the proposed method clear. - According to this paper, the notable difference between the proposed method and the existing pruning methods is that the pruning methods are grounded on the importance of trained weights, but the proposed method focuses more on the importance of channel numbers. It is unclear to me why such a difference is caused by the proposed method, that is, which part of the proposed method causes the difference? And how does the difference affect the final performance? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a simple and one-shot approach on neural architecture search for the number of channels to achieve better accuracy. Rather than training a lot of network samples, the proposed method trains a single slimmable network to approximate the network accuracy of different channel configurations. The experimental results show that the proposed method achieves better performance than the existing baseline methods. - It would be better to provide the search cost of the proposed method and the other baseline methods because that is the important metric for neural architecture search methods. As this paper points out that NAS methods are computationally expensive, it would be better to make the efficiency of the proposed method clear. - According to this paper, the notable difference between the proposed method and the existing pruning methods is that the pruning methods are grounded on the importance of trained weights, but the proposed method focuses more on the importance of channel numbers. It is unclear to me why such a difference is caused by the proposed method, that is, which part of the proposed method causes the difference? And how does the difference affect the final performance?\n",
      "      - Review:###This paper proposes a training strategy for robustness against label noise. The training strategy is simple and straightforward. The neural network will first be trained on the entire dataset with all the noisy labels. After obtaining the network with lowest validation error, the network will be used to make a prediciton on the original training set and select a subset of it to construct a maximal safe set. Finally, the network will be findtuned on this maximal safe set. The training strategy is very similar to tradictional self-training in semi-superivsed learning and co-training for domain adaptation ([Co-training for domain adaptation, NIPS 2011]), except that the proposed prestopping only iterate the procedure once. The paper discusses two important questions for the method: (1) when to early stop the training; (2) how to constuct a maximal safe set. The authors* responses to these questions are very natual but less interesting. Using the lowest validation error to early stop the training could be suboptimal, since the small validation set can not fully capture the data distribution and could make the network empirically overfit to this validation set. The criterion to contruct a maxial safe set is also conventional, and is similar to what a number of papers are doing, for example, [1] Co-training for domain adaptation, NIPS 2011 [2] Self-ensembling for visual domain adaptation, ICLR 2018 [3] A dirt-t approach to unsupervised domain adaptation, ICLR 2018 [4] Iterative learning with open-set noisy labels, CVPR 2018 In experiments, the results are not very surprising. There are some baselines that adopt a similar (iterative) pipeline (learning the network - selecting a subset of the training samples - re-learning the network): [1] Iterative Learning with Open-set Noisy Labels, CVPR 2018 [2] Dimensionality-Driven Learning with Noisy Labels, ICML 2018 [3] Symmetric Cross Entropy for Robust Learning with Noisy Labels, ICCV 2019 The authors can consider to compare to some of these baselines, especially [1] and [2]. The difference between the paper and [1,2] is basically the criterion to construct the maximal safe subset. Besides, I suggest the authors to conduct large-scale experiments on ImageNet or even a subset of ImageNet, since the difficulty of detecting label noise is much higher when the resolution of images become bigger. CIFAR-10 and CIFAR-100 only contain 32x32 images, which is far less challenging. Overall, I think the paper is well written, the idea is clearly presented, and the experiments also seem convinceing. However, the contribution of this paper is very incremental. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a training strategy for robustness against label noise. The training strategy is simple and straightforward. The neural network will first be trained on the entire dataset with all the noisy labels. After obtaining the network with lowest validation error, the network will be used to make a prediciton on the original training set and select a subset of it to construct a maximal safe set. Finally, the network will be findtuned on this maximal safe set. The training strategy is very similar to tradictional self-training in semi-superivsed learning and co-training for domain adaptation ([Co-training for domain adaptation, NIPS 2011]), except that the proposed prestopping only iterate the procedure once. The paper discusses two important questions for the method: (1) when to early stop the training; (2) how to constuct a maximal safe set. The authors* responses to these questions are very natual but less interesting. Using the lowest validation error to early stop the training could be suboptimal, since the small validation set can not fully capture the data distribution and could make the network empirically overfit to this validation set. The criterion to contruct a maxial safe set is also conventional, and is similar to what a number of papers are doing, for example, [1] Co-training for domain adaptation, NIPS 2011 [2] Self-ensembling for visual domain adaptation, ICLR 2018 [3] A dirt-t approach to unsupervised domain adaptation, ICLR 2018 [4] Iterative learning with open-set noisy labels, CVPR 2018 In experiments, the results are not very surprising. There are some baselines that adopt a similar (iterative) pipeline (learning the network - selecting a subset of the training samples - re-learning the network): [1] Iterative Learning with Open-set Noisy Labels, CVPR 2018 [2] Dimensionality-Driven Learning with Noisy Labels, ICML 2018 [3] Symmetric Cross Entropy for Robust Learning with Noisy Labels, ICCV 2019 The authors can consider to compare to some of these baselines, especially [1] and [2]. The difference between the paper and [1,2] is basically the criterion to construct the maximal safe subset. Besides, I suggest the authors to conduct large-scale experiments on ImageNet or even a subset of ImageNet, since the difficulty of detecting label noise is much higher when the resolution of images become bigger. CIFAR-10 and CIFAR-100 only contain 32x32 images, which is far less challenging. Overall, I think the paper is well written, the idea is clearly presented, and the experiments also seem convinceing. However, the contribution of this paper is very incremental.\n",
      "    - Unfair/Incomparable or Misleading Experimental Comparisons (L1, 2)\n",
      "      - Review:###====== Updates ====== I appreciate the authors* time and effort in the response. I have read the rebuttal, but I am not convinced by the authors* argument on using L2 (or L_infty) constraints. No matter whether L2 or L_infty constraint is used, the authors* method is not directly comparable to methods in Song et al. (2018), making the results in Table 2 and Table 3 meaningless and confusing. - Song et al. (2018) indeed constraints the search region of latent code to be within a small L2 ball of a randomly sampled anchor latent code. However, this anchor latent code is not directly related to any given image in the dataset, and therefore the generated adversarial examples are not close to any existing image. In contrast, the authors* attack is still basically a norm-bounded attack, which is not directly comparable to the unrestricted attack in Song et al. (2018). - Song et al. (2018) is a white box attack, while the attack in this paper is black box. ====== Original review ======= This paper proposes to generate semantic preserving adversarial examples by first learning a manifold and then perturbing data along the manifold. In this way the generated adversarial examples can be semantically close to the original clean examples, and the perturbations can be hopefully more natural. For manifold learning, the authors propose to use a similar approach to that proposed in Pu et al. (2017), which uses SVGD to train a VAE. After the VAE is trained, the authors use GBSM to train a model to produce semantic adversarial examples efficiently. I have many concerns for this paper: - The approach is not well motivated. It is unclear why using a fully Bayesian framework and employing SVGD to learn the VAE model is preferred for conducting semantic adversarial attacks. Many choices in the algorithm seem to be arbitrary, and there are many approximations in the method whose accuracies have no guarantees. For example, the recognition networks are used to approximate the updated parameters of the encoder from SVGD. Sampling from the posterior distribution of z is approximated by first doing Monte Carlo over Theta. For *manifold alignment* another recognition network is used to approximate the updates from SVGD. It is hard to predict how those approximation errors accumulate when all pieces are combined together to form a very complicated algorithm. - In Equation (6) the authors hard-constrain the generated adversarial example such that they cannot differ from the original data by some pre-specified l_2-norm. This leads to many unfair comparisons in the experiments: 1. The authors compare their approach to other attacking methods on the success rates of attacking Madry*s model and Kolter & Wong*s certified model. However, both Madry and Kolter & Wong*s model are for attacks using the l_infinity norm. It is unfair that the authors* attack uses l_2 norm. In fact, it is known that models robust to l_infinity norm attacks are generally not robust to attacks using other norms. 2. The authors also compare their approach to methods in Song et al. (2018) and Zhao et al. (2018a). However, the two previous approaches did not directly constrain the distance between generated adversarial examples and the corresponding clean inputs. Therefore, when using human evaluation to assess the image quality of generated adversarial examples, the two previous methods are naturally at a huge disadvantage. In stark contrast, the authors* adversarial images are constrained to be close to the corresponding unperturbed images under a small l_2 norm, which naturally have higher image quality. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###====== Updates ====== I appreciate the authors* time and effort in the response. I have read the rebuttal, but I am not convinced by the authors* argument on using L2 (or L_infty) constraints. No matter whether L2 or L_infty constraint is used, the authors* method is not directly comparable to methods in Song et al. (2018), making the results in Table 2 and Table 3 meaningless and confusing. - Song et al. (2018) indeed constraints the search region of latent code to be within a small L2 ball of a randomly sampled anchor latent code. However, this anchor latent code is not directly related to any given image in the dataset, and therefore the generated adversarial examples are not close to any existing image. In contrast, the authors* attack is still basically a norm-bounded attack, which is not directly comparable to the unrestricted attack in Song et al. (2018). - Song et al. (2018) is a white box attack, while the attack in this paper is black box. ====== Original review ======= This paper proposes to generate semantic preserving adversarial examples by first learning a manifold and then perturbing data along the manifold. In this way the generated adversarial examples can be semantically close to the original clean examples, and the perturbations can be hopefully more natural. For manifold learning, the authors propose to use a similar approach to that proposed in Pu et al. (2017), which uses SVGD to train a VAE. After the VAE is trained, the authors use GBSM to train a model to produce semantic adversarial examples efficiently. I have many concerns for this paper: - The approach is not well motivated. It is unclear why using a fully Bayesian framework and employing SVGD to learn the VAE model is preferred for conducting semantic adversarial attacks. Many choices in the algorithm seem to be arbitrary, and there are many approximations in the method whose accuracies have no guarantees. For example, the recognition networks are used to approximate the updated parameters of the encoder from SVGD. Sampling from the posterior distribution of z is approximated by first doing Monte Carlo over Theta. For *manifold alignment* another recognition network is used to approximate the updates from SVGD. It is hard to predict how those approximation errors accumulate when all pieces are combined together to form a very complicated algorithm. - In Equation (6) the authors hard-constrain the generated adversarial example such that they cannot differ from the original data by some pre-specified l_2-norm. This leads to many unfair comparisons in the experiments: 1. The authors compare their approach to other attacking methods on the success rates of attacking Madry*s model and Kolter & Wong*s certified model. However, both Madry and Kolter & Wong*s model are for attacks using the l_infinity norm. It is unfair that the authors* attack uses l_2 norm. In fact, it is known that models robust to l_infinity norm attacks are generally not robust to attacks using other norms. 2. The authors also compare their approach to methods in Song et al. (2018) and Zhao et al. (2018a). However, the two previous approaches did not directly constrain the distance between generated adversarial examples and the corresponding clean inputs. Therefore, when using human evaluation to assess the image quality of generated adversarial examples, the two previous methods are naturally at a huge disadvantage. In stark contrast, the authors* adversarial images are constrained to be close to the corresponding unperturbed images under a small l_2 norm, which naturally have higher image quality.\n",
      "      - Review:###This paper proposed a framework based on a mathematical tool of tropical geometry to characterize the decision boundary of neural networks. The analysis is applied to network pruning, lottery ticket hypothesis and adversarial attacks. I have some questions: Q1: What benefit does introducing tropical geometry brings in terms of theoretical analysis? Does using tropical geometry give us the theoretical results that traditional analysis can not give us? If so, what is it? I am trying to understand why the authors use this tool. The authors should be explicit in their motivation so that the readers are clear about the contribution of this paper. More specifically, from my perspective, tropical semiring, tropical polynomials and tropical rational functions all can be represented with the standard mathematical tools. Here they are just redefining several concepts. Q2: In “Experiments on Tropical Pruning”, the authors mentioned “we compare our tropical pruning approach against Class Blind (CB), Class Uniform (CU), and Class Distribution (CD) methods Han et al. (2015)”. What is Class Blind, Class Uniform and Class Distribution? There seems to be an error here “Figure 5 shows the pruning comparison between our tropical approach ...”, i think Figure 5 should be Figure 4. Q3: In the adversarial attack part, is the authors proposing a new attack method? If so, then the authors should report the test accuracy under attack. Also, the experimental results should not be restricted to MNIST dataset. I am also not sure about the attack settings here, the authors said “Instead of designing a sample noise ? such that (x0 + ?) belongs to a new decision region, one can instead fix x0 and perturb the network parameters to move the decision boundaries in a way that x0 appears in a new classification region.”. Why use this setting? Are there any intuitions? Since this is different from traditional adversarial attack terminology, the authors should stop using adversarial attacks as in “tropical adversarial attacks” because it is really misleading. =================================================================== Thanks the authors for the response. I still have two questions: Q1: The authors say that this theory provides a deeper understanding to Lottery Ticket Hypothesis (LTH). Then another paper “Rethinking the Value of Network Pruning” [1] suggests something different than LTH. [1] suggests that we do not need the initialization of large networks to train the pruned network from scratch to achieve high accuracy. Since the authors claim that their theory is related to LTH, then what would the proposed theory say about [1]? Q2: Since you redesign the task of adversarial attacks, I am still not convinced why this setting is interesting? The reason why people are interested in adversarial attacks is because it could happen during test time. What is the application of this setting? Why this new setting is important and worth studying? They are not clear to me. Also, as i wrote in my initial review, “the authors should stop using “adversarial attacks” as in “tropical adversarial attacks” because it is really misleading.”. I hope the authors can address this concern, or otherwise new readers may also find this part difficult to understand. [1] Rethinking the Value of Network Pruning. Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell. ICLR 2019. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposed a framework based on a mathematical tool of tropical geometry to characterize the decision boundary of neural networks. The analysis is applied to network pruning, lottery ticket hypothesis and adversarial attacks. I have some questions: Q1: What benefit does introducing tropical geometry brings in terms of theoretical analysis? Does using tropical geometry give us the theoretical results that traditional analysis can not give us? If so, what is it? I am trying to understand why the authors use this tool. The authors should be explicit in their motivation so that the readers are clear about the contribution of this paper. More specifically, from my perspective, tropical semiring, tropical polynomials and tropical rational functions all can be represented with the standard mathematical tools. Here they are just redefining several concepts. Q2: In “Experiments on Tropical Pruning”, the authors mentioned “we compare our tropical pruning approach against Class Blind (CB), Class Uniform (CU), and Class Distribution (CD) methods Han et al. (2015)”. What is Class Blind, Class Uniform and Class Distribution? There seems to be an error here “Figure 5 shows the pruning comparison between our tropical approach ...”, i think Figure 5 should be Figure 4. Q3: In the adversarial attack part, is the authors proposing a new attack method? If so, then the authors should report the test accuracy under attack. Also, the experimental results should not be restricted to MNIST dataset. I am also not sure about the attack settings here, the authors said “Instead of designing a sample noise ? such that (x0 + ?) belongs to a new decision region, one can instead fix x0 and perturb the network parameters to move the decision boundaries in a way that x0 appears in a new classification region.”. Why use this setting? Are there any intuitions? Since this is different from traditional adversarial attack terminology, the authors should stop using adversarial attacks as in “tropical adversarial attacks” because it is really misleading. =================================================================== Thanks the authors for the response. I still have two questions: Q1: The authors say that this theory provides a deeper understanding to Lottery Ticket Hypothesis (LTH). Then another paper “Rethinking the Value of Network Pruning” [1] suggests something different than LTH. [1] suggests that we do not need the initialization of large networks to train the pruned network from scratch to achieve high accuracy. Since the authors claim that their theory is related to LTH, then what would the proposed theory say about [1]? Q2: Since you redesign the task of adversarial attacks, I am still not convinced why this setting is interesting? The reason why people are interested in adversarial attacks is because it could happen during test time. What is the application of this setting? Why this new setting is important and worth studying? They are not clear to me. Also, as i wrote in my initial review, “the authors should stop using “adversarial attacks” as in “tropical adversarial attacks” because it is really misleading.”. I hope the authors can address this concern, or otherwise new readers may also find this part difficult to understand. [1] Rethinking the Value of Network Pruning. Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell. ICLR 2019.\n",
      "    - Weak or Unconvincing Empirical Validation/Experiments (L1, 4)\n",
      "      - Review:###This paper addresses the problem of learning expressive feature combinations in order to improve learning for domains where there is no known structure between features. These settings would normally lead to the use of fully-connected MLP networks, which unfortunately have problems with efficient training and generalization after a few layers of depth. The main idea is to use grouping at first, in combination with smaller fully-connected layers for each group, as well as pooling pairs of groups together as the layers go on. Results are shown as comparisons on 5 real-world datasets, and intuitive visualizations on two other datasets. Related work covered MLPs, regularization techniques, sparse networks, random forest models, and other feature grouping. The paper is well written and easy to read. This work did a good job with giving implementation details as well as performing hyperparameter searches and giving the baselines a good effort. My current decision is a weak reject, for a well-written paper, but some concerns as follows: -The results do not show much improvement (i.e., < 0.3% improvement for 3 of the datasets, and < 1% for another one), aside from CIFAR-10. Considering that the premise of the paper is that MLP’s are not good enough when dealing with data in which the relationships between features are unknown, it seems like these are definitely not good datasets on which to demonstrate this notion of “there has been little progress in deep reinforcement learning for domains without a known structure between features.” -The MNIST visualization of group-select felt informative, but the XOR example for grouping visualizations seemed too easy. It would’ve been good to see visualizations or intuitions regarding grouping for harder datasets, in order to be convinced of the need for more expressive feature representations than standard MLP’s. -I’m not an expert on causality, but it seems like citations from that area are required for this problem statement of dealing with features where the connections between them are unknown but potentially very important. Less major: -It would have been nice to include related work on other ways to encourage inter-feature interactions, such as perhaps taking the outer product of the input with itself. -It seems like different sizes per group would be a more realistic expectation, and that perhaps this should be worked into the algorithm. Similarly, pooling only 2 groups together (from pre-specified positions) seems like it would be limiting as well. It also seems like the algorithm should account for being able to use a high-level feature from one layer as part of multiple groups in the future (i.e. reuse). Even if any of these options don’t make a difference, it would be good to check/evaluate. Minor: -Equation 8 did not fully make sense to me. -Why were “random horizontal flips” used as preprocessing for the permutation-invariant CIFAR-10 dataset? This shouldn’t make a difference at all if the pixels become randomly shuffled anyway. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper addresses the problem of learning expressive feature combinations in order to improve learning for domains where there is no known structure between features. These settings would normally lead to the use of fully-connected MLP networks, which unfortunately have problems with efficient training and generalization after a few layers of depth. The main idea is to use grouping at first, in combination with smaller fully-connected layers for each group, as well as pooling pairs of groups together as the layers go on. Results are shown as comparisons on 5 real-world datasets, and intuitive visualizations on two other datasets. Related work covered MLPs, regularization techniques, sparse networks, random forest models, and other feature grouping. The paper is well written and easy to read. This work did a good job with giving implementation details as well as performing hyperparameter searches and giving the baselines a good effort. My current decision is a weak reject, for a well-written paper, but some concerns as follows: -The results do not show much improvement (i.e., < 0.3% improvement for 3 of the datasets, and < 1% for another one), aside from CIFAR-10. Considering that the premise of the paper is that MLP’s are not good enough when dealing with data in which the relationships between features are unknown, it seems like these are definitely not good datasets on which to demonstrate this notion of “there has been little progress in deep reinforcement learning for domains without a known structure between features.” -The MNIST visualization of group-select felt informative, but the XOR example for grouping visualizations seemed too easy. It would’ve been good to see visualizations or intuitions regarding grouping for harder datasets, in order to be convinced of the need for more expressive feature representations than standard MLP’s. -I’m not an expert on causality, but it seems like citations from that area are required for this problem statement of dealing with features where the connections between them are unknown but potentially very important. Less major: -It would have been nice to include related work on other ways to encourage inter-feature interactions, such as perhaps taking the outer product of the input with itself. -It seems like different sizes per group would be a more realistic expectation, and that perhaps this should be worked into the algorithm. Similarly, pooling only 2 groups together (from pre-specified positions) seems like it would be limiting as well. It also seems like the algorithm should account for being able to use a high-level feature from one layer as part of multiple groups in the future (i.e. reuse). Even if any of these options don’t make a difference, it would be good to check/evaluate. Minor: -Equation 8 did not fully make sense to me. -Why were “random horizontal flips” used as preprocessing for the permutation-invariant CIFAR-10 dataset? This shouldn’t make a difference at all if the pixels become randomly shuffled anyway.\n",
      "      - Review:###This paper demonstrates that for regularization, data augmentation usually works better than explicit regularization methods such as weight decay and dropout. The experiments are detailed and also includes theory explanation of why data augmentation works. Data augmentation (or increasing the size of training data) and explicit regularization are standard methods to overcome overfitting. From my understanding, they are two different methods to tune the model quality, and they can combined to further improve the model performance, as discussed in [1]. So this paper is not well motivated. [1] DeVries, Terrance, and Graham W. Taylor. *Improved regularization of convolutional neural networks with cutout.* arXiv preprint arXiv:1708.04552 (2017). (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper demonstrates that for regularization, data augmentation usually works better than explicit regularization methods such as weight decay and dropout. The experiments are detailed and also includes theory explanation of why data augmentation works. Data augmentation (or increasing the size of training data) and explicit regularization are standard methods to overcome overfitting. From my understanding, they are two different methods to tune the model quality, and they can combined to further improve the model performance, as discussed in [1]. So this paper is not well motivated. [1] DeVries, Terrance, and Graham W. Taylor. *Improved regularization of convolutional neural networks with cutout.* arXiv preprint arXiv:1708.04552 (2017).\n",
      "      - Review:###This paper presents a method for architecture search in deep neural networks in order to identify scaled-down networks that can operate on resource limited hardware. The approach taken in this paper is different from other approaches, which train a single big model then fine tune smaller models for specific hardware, or use distillation to train progressively smaller models. Here, a single model is trained in a manner that allows subsequent slicing to smaller models without additional training required. The authors employ a variety of strategies to get this to work well, including specific initialization techniques, regularization methods, learning schedules, and a coarse-to-fine optimization method to obtain the smaller models. The authors demonstrate SotA performance on ImageNet relative to other techniques. Overall, this was a well-written paper, and the results appear convincing. I would have liked a little bit more explanation about the implementation details though. As someone knowledgeable about deep ANNs, but not an expert in NAS for efficiency, I was not very clear on a couple of items. Specifically, I think it would be good to clarify the following: 1) The authors say that they use a masking strategy to implement slicing during training. So, do I take it that they effectively do a separate pass through the network with different masks to implement the different sized models? If so, do you then simply accumulate gradients across all masks to do the updates? 2) When the authors say they perform a fine grained grid search by varying the configurations, what is meant by this exactly? Do you mean that you do a search through different slicings of the big model to discover the best smaller architecture? What does it mean to do a grid search through binary masks? Or, is there a continuous hyperparameter that determines the masking which you do the grid search on? Maybe I’m just not understanding well at all, but even still, in that case, it indicates that this could be clarified in the manuscript. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper presents a method for architecture search in deep neural networks in order to identify scaled-down networks that can operate on resource limited hardware. The approach taken in this paper is different from other approaches, which train a single big model then fine tune smaller models for specific hardware, or use distillation to train progressively smaller models. Here, a single model is trained in a manner that allows subsequent slicing to smaller models without additional training required. The authors employ a variety of strategies to get this to work well, including specific initialization techniques, regularization methods, learning schedules, and a coarse-to-fine optimization method to obtain the smaller models. The authors demonstrate SotA performance on ImageNet relative to other techniques. Overall, this was a well-written paper, and the results appear convincing. I would have liked a little bit more explanation about the implementation details though. As someone knowledgeable about deep ANNs, but not an expert in NAS for efficiency, I was not very clear on a couple of items. Specifically, I think it would be good to clarify the following: 1) The authors say that they use a masking strategy to implement slicing during training. So, do I take it that they effectively do a separate pass through the network with different masks to implement the different sized models? If so, do you then simply accumulate gradients across all masks to do the updates? 2) When the authors say they perform a fine grained grid search by varying the configurations, what is meant by this exactly? Do you mean that you do a search through different slicings of the big model to discover the best smaller architecture? What does it mean to do a grid search through binary masks? Or, is there a continuous hyperparameter that determines the masking which you do the grid search on? Maybe I’m just not understanding well at all, but even still, in that case, it indicates that this could be clarified in the manuscript.\n",
      "      - Review:###This paper suggests generating a large news summarization dataset by taking advantage of the fact that in news articles it is often the case that first few sentences contain the most important information. I have the following criticisms of this paper: - the idea is not novel. The XSUM dataset cited had used this to create a large dataset based on BBC articles as the editorial guidelines are such that the first sentence is a summary of the article. The lead1 baseline doesn*t make sense, as it is the actual reference of the dataset. As implemented, it actually picks the second sentence of the original article, and unsurprisingly works worse than the lead-X for the other two datasets. - the filtering based on word overlap between the initial sentences and the rest of the document means that the training dataset will encourage models copying words; good summaries don*t have high word overlap necessarily. - no human evaluation is not conducted; ROUGE indicates small differences, but it can*t be trusted without confirmation by human evaluation - I don*t agree that using positional information is bad for the models. The point is that we need to do better than that, but we should still take it into account (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper suggests generating a large news summarization dataset by taking advantage of the fact that in news articles it is often the case that first few sentences contain the most important information. I have the following criticisms of this paper: - the idea is not novel. The XSUM dataset cited had used this to create a large dataset based on BBC articles as the editorial guidelines are such that the first sentence is a summary of the article. The lead1 baseline doesn*t make sense, as it is the actual reference of the dataset. As implemented, it actually picks the second sentence of the original article, and unsurprisingly works worse than the lead-X for the other two datasets. - the filtering based on word overlap between the initial sentences and the rest of the document means that the training dataset will encourage models copying words; good summaries don*t have high word overlap necessarily. - no human evaluation is not conducted; ROUGE indicates small differences, but it can*t be trusted without confirmation by human evaluation - I don*t agree that using positional information is bad for the models. The point is that we need to do better than that, but we should still take it into account\n",
      "  - Empirical Evaluation, Sample Efficiency, and Comparisons in Meta-learning/Data Valuation Approaches (L2, 2)\n",
      "    - Review:###This paper proposes a meta learning approach based on data valuation for reinforcement learning tasks. The core idea is to train a second network (the data value estimator) in conjunction to a regular predictor network. The predictor is then trained with samples chosen via the data value estimation. The authors motivate this construction with the goal to filter out unreliable and corrupted data. It*s well established that RL poses a difficult learning problem, and as such the goal to improve the RL process is definitely a good one. To the best of my knowledge the approach proposed here is new. The exposition of the paper is also quite clear, and all parts of the approach are explained nicely. In addition, the submission contains a thorough evaluation of the method. A central point for the method seems to be the validation data set which is used to train the data value estimator. The text emphasizes that this data set can be *small* several times, and the discussion and results of section 4.5 try to shed light here. However, Figure 5 indicates that a fairly large fraction of samples is needed to identify, e.g., more than 50% of the corrupted samples. Another cricital aspect for meta-learning approaches such as this one is also the training time. RL is already expensive, so if the meta learning introduces a large factor, the training could quickly become infeasible. Here, the text gives a factor of about 3, which is noticeable, but not overly big. This still seems practical. Potentially, the estimator could also be reused (at least partially) for repeated training runs. The tests in figure 2 are somewhat unituitive at first - I was expecting results of models trained on datasets with different samples being removed beforehand, rather than removing samples based on a trained estimator. However, this test makes sense on second sight, and e.g., the significant drop in performance after removing the most important samples indicates that the estimator was able to correctly identify a certain portion of data that is actually important for successful predictions. In addition to the noisy label and domain adaptation tests, this paints a positive picture. The method seems to yield useful (be it somewhat small) improvements in terms of learning performance. One aspect that could be justified better in my opinion is the choise for a discrete representation for the data value estimator. Shouldn*t the method likewise work with a continuous representation here? The paper explain how the discrete model is trained with quite some detail, but the choice itself could be motivated more clearly. However, overall I think the method is interesting and yields nice performance improvements. It is described and evaluated in detail, so I think the paper could me included in the ICLR program. (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposes a meta learning approach based on data valuation for reinforcement learning tasks. The core idea is to train a second network (the data value estimator) in conjunction to a regular predictor network. The predictor is then trained with samples chosen via the data value estimation. The authors motivate this construction with the goal to filter out unreliable and corrupted data. It*s well established that RL poses a difficult learning problem, and as such the goal to improve the RL process is definitely a good one. To the best of my knowledge the approach proposed here is new. The exposition of the paper is also quite clear, and all parts of the approach are explained nicely. In addition, the submission contains a thorough evaluation of the method. A central point for the method seems to be the validation data set which is used to train the data value estimator. The text emphasizes that this data set can be *small* several times, and the discussion and results of section 4.5 try to shed light here. However, Figure 5 indicates that a fairly large fraction of samples is needed to identify, e.g., more than 50% of the corrupted samples. Another cricital aspect for meta-learning approaches such as this one is also the training time. RL is already expensive, so if the meta learning introduces a large factor, the training could quickly become infeasible. Here, the text gives a factor of about 3, which is noticeable, but not overly big. This still seems practical. Potentially, the estimator could also be reused (at least partially) for repeated training runs. The tests in figure 2 are somewhat unituitive at first - I was expecting results of models trained on datasets with different samples being removed beforehand, rather than removing samples based on a trained estimator. However, this test makes sense on second sight, and e.g., the significant drop in performance after removing the most important samples indicates that the estimator was able to correctly identify a certain portion of data that is actually important for successful predictions. In addition to the noisy label and domain adaptation tests, this paints a positive picture. The method seems to yield useful (be it somewhat small) improvements in terms of learning performance. One aspect that could be justified better in my opinion is the choise for a discrete representation for the data value estimator. Shouldn*t the method likewise work with a continuous representation here? The paper explain how the discrete model is trained with quite some detail, but the choice itself could be motivated more clearly. However, overall I think the method is interesting and yields nice performance improvements. It is described and evaluated in detail, so I think the paper could me included in the ICLR program.\n",
      "    - Review:###This paper studies the problem of interpreting predictions of blackbox models. In particular, they study local interpretable models, which are used to study interpretability at the level of one or a few data points. A key challenge is that in order for local models to be interpretable, they need to be simple in form and therefore lower in capacity (i.e. linear); thus, if they are trained on entire datasets they will underfit. The aim of this work is to address this issue by learning how to select representative samples from a dataset for training local linear models to reproduce predictions of black-boxes. In particular, they propose to use RL to learn to weight instances from datasets; RL is required as they make hard (i.e. non-differentiable) decisions to select a subset of the dataset. This work is closely related to Ren et al. 2018 [1], which proposes to meta-learn how to weight samples in a batch so as to maximize performance on a validation set. By analogy, in this paper, the samples are taken from the entire dataset (i.e. the dataset is subsampled), while the validation loss is effectively an imitation loss formed by treating the black-box model predictions as a target. The data subsampling operation introduces the added complication of non-differentiability. The contribution of this paper is thus the use of RL for meta-learning how to subsample a larger dataset in order to maximize some validation loss. * Pros: * Considers an interesting dataset subsampling variant of the sample weighting meta-learning problem. * Novel application of meta-learning for improving locally-linear models. * Extensive quantitative evaluation shows that the method seems to perform better than baselines, though it might be that a differentiable approximation could do as well while being more sample efficient. * It is a nice result that the l1 penalty actually works well in reducing the number of samples chosen by the * I found the discussion and figures presented in 4.2 to be quite nice and informative. * Cons: * Given the lack of a differentiable approximation baseline, I am not entirely convinced that the use of RL is absolutely necessary/optimal. * I.e. if the weighting function is actually high-entropy, randomly sampling a (large) batch and weighting it might work just as well. * Though there is discussion of the complexity of the overall method it would be nice to see a discussion and figures related to the sample efficiency of REINFORCE? * This would be strongest if given with a comparison to differentiable alternatives (mentioned above) as well. * This would help elucidate whether RL is optimal in this setting: fitting a linear model on more data might be cheaper learning to subsample with REINFORCE. * While the sample weighting function is fast at inference time, most of the overhead comes at training time. This function needs be updated in settings where the underlying dataset changes. * This is a minor issue, but this pushes the burden of interpretability further up to the black-box sample weighting function. While this interpretability problem is less critical, it still exists. * Other comments/requests: * While the use of RL is certainly motivated in order to solve the problem in an unbiased way, it would be nice to see a comparison to a differentiable approximation as a baseline? A few ideas: * Randomly sample a (possibly large batch) and learn to weight it (closely related to the straight through estimator) * Randomly sample a batch and apply [1] * Would be nice to show the sizes of datasets and how many samples end up being used for different values of lambda. * Would be nice to understand which samples are chosen and why. This is probably tricky to analyze, but it would be interesting to see if certain samples are often chosen, or if the weighting distribution has an interesting shape (i.e. is low or high-entropy). I’ve given a weak accept, conditioned on being provided more evidence regarding 1) comparisons to simple differentiable alternatives, 2) sample efficiency of the RL method, and 3) basic analysis of the weighting function. [1] Learning to Reweight Examples for Robust Deep Learning. Mengye Ren, Wenyuan Zeng, Bin Yang, Raquel Urtasun. https://arxiv.org/abs/1803.09050 (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###This paper studies the problem of interpreting predictions of blackbox models. In particular, they study local interpretable models, which are used to study interpretability at the level of one or a few data points. A key challenge is that in order for local models to be interpretable, they need to be simple in form and therefore lower in capacity (i.e. linear); thus, if they are trained on entire datasets they will underfit. The aim of this work is to address this issue by learning how to select representative samples from a dataset for training local linear models to reproduce predictions of black-boxes. In particular, they propose to use RL to learn to weight instances from datasets; RL is required as they make hard (i.e. non-differentiable) decisions to select a subset of the dataset. This work is closely related to Ren et al. 2018 [1], which proposes to meta-learn how to weight samples in a batch so as to maximize performance on a validation set. By analogy, in this paper, the samples are taken from the entire dataset (i.e. the dataset is subsampled), while the validation loss is effectively an imitation loss formed by treating the black-box model predictions as a target. The data subsampling operation introduces the added complication of non-differentiability. The contribution of this paper is thus the use of RL for meta-learning how to subsample a larger dataset in order to maximize some validation loss. * Pros: * Considers an interesting dataset subsampling variant of the sample weighting meta-learning problem. * Novel application of meta-learning for improving locally-linear models. * Extensive quantitative evaluation shows that the method seems to perform better than baselines, though it might be that a differentiable approximation could do as well while being more sample efficient. * It is a nice result that the l1 penalty actually works well in reducing the number of samples chosen by the * I found the discussion and figures presented in 4.2 to be quite nice and informative. * Cons: * Given the lack of a differentiable approximation baseline, I am not entirely convinced that the use of RL is absolutely necessary/optimal. * I.e. if the weighting function is actually high-entropy, randomly sampling a (large) batch and weighting it might work just as well. * Though there is discussion of the complexity of the overall method it would be nice to see a discussion and figures related to the sample efficiency of REINFORCE? * This would be strongest if given with a comparison to differentiable alternatives (mentioned above) as well. * This would help elucidate whether RL is optimal in this setting: fitting a linear model on more data might be cheaper learning to subsample with REINFORCE. * While the sample weighting function is fast at inference time, most of the overhead comes at training time. This function needs be updated in settings where the underlying dataset changes. * This is a minor issue, but this pushes the burden of interpretability further up to the black-box sample weighting function. While this interpretability problem is less critical, it still exists. * Other comments/requests: * While the use of RL is certainly motivated in order to solve the problem in an unbiased way, it would be nice to see a comparison to a differentiable approximation as a baseline? A few ideas: * Randomly sample a (possibly large batch) and learn to weight it (closely related to the straight through estimator) * Randomly sample a batch and apply [1] * Would be nice to show the sizes of datasets and how many samples end up being used for different values of lambda. * Would be nice to understand which samples are chosen and why. This is probably tricky to analyze, but it would be interesting to see if certain samples are often chosen, or if the weighting distribution has an interesting shape (i.e. is low or high-entropy). I’ve given a weak accept, conditioned on being provided more evidence regarding 1) comparisons to simple differentiable alternatives, 2) sample efficiency of the RL method, and 3) basic analysis of the weighting function. [1] Learning to Reweight Examples for Robust Deep Learning. Mengye Ren, Wenyuan Zeng, Bin Yang, Raquel Urtasun. https://arxiv.org/abs/1803.09050\n",
      "  - Empirical Validation and Baseline Comparison Deficiencies (L2, 30)\n",
      "    - Concerns About Scalability, Applicability, or Scope (L1, 2)\n",
      "      -  The authors detail a set of priors for unsupervised decomposition of individual spectrograms into their component parts. The introduce reasonable constraints on temporal coherence (consistency and dynamic shifts) and mask activations (at least one component always activated). They also regularize sources to not overlap spectrotemporally. Decomposition is performed by training the weights of a U-Net on a single spectrogram as in deep image priors. The authors demonstrate quantitative improvements on blind source separation over other data-agnostic techniques, and qualitative use of the model for interactive editing, audio texture synthesis, and audio watermark removal. The work also performs an ablation study to qualitatively demonstrate the importance of each element for the prior. The experiments are performed well and explained clearly. They also introduce a dataset of diverse mixtures for future comparisons. Pros: * Important motivation for why audio has different properties than images (even if it can be represented as a *image* spectrogram). The priors are well-motivated by the dynamics of audio. * Good ablations and quantitative comparisons to baselines. Cons: * Some details could be better demonstrated / explained (even if only in the appendix). For example the paper cites the network architecture, but a local description would be helpful. Similarly, the latent dynamics are carefully regularized, so visualizing them would be helpful to understand the dynamics. * The scaling of the technique is not supported by the current experiments. The authors claim they have extended to 4 sources, but all experiments in the paper seem to only involve two sources. * More motivation could help in terms of the value of non-amortized methods like deep priors, vs. other approaches such as pretraining or self-supervised methods. While it is difficult to get lots of labeled data for a specific task, the argument was not convincingly made that methods like deep priors should outperform methods that use pretrained priors on adjacent tasks (where collecting data is easy). (L0, 1)\n",
      "        * Examples:\n",
      "          -  The authors detail a set of priors for unsupervised decomposition of individual spectrograms into their component parts. The introduce reasonable constraints on temporal coherence (consistency and dynamic shifts) and mask activations (at least one component always activated). They also regularize sources to not overlap spectrotemporally. Decomposition is performed by training the weights of a U-Net on a single spectrogram as in deep image priors. The authors demonstrate quantitative improvements on blind source separation over other data-agnostic techniques, and qualitative use of the model for interactive editing, audio texture synthesis, and audio watermark removal. The work also performs an ablation study to qualitatively demonstrate the importance of each element for the prior. The experiments are performed well and explained clearly. They also introduce a dataset of diverse mixtures for future comparisons. Pros: * Important motivation for why audio has different properties than images (even if it can be represented as a *image* spectrogram). The priors are well-motivated by the dynamics of audio. * Good ablations and quantitative comparisons to baselines. Cons: * Some details could be better demonstrated / explained (even if only in the appendix). For example the paper cites the network architecture, but a local description would be helpful. Similarly, the latent dynamics are carefully regularized, so visualizing them would be helpful to understand the dynamics. * The scaling of the technique is not supported by the current experiments. The authors claim they have extended to 4 sources, but all experiments in the paper seem to only involve two sources. * More motivation could help in terms of the value of non-amortized methods like deep priors, vs. other approaches such as pretraining or self-supervised methods. While it is difficult to get lots of labeled data for a specific task, the argument was not convincingly made that methods like deep priors should outperform methods that use pretrained priors on adjacent tasks (where collecting data is easy).\n",
      "      - Review:###This paper proposes a new option discovery method for multi-task RL to reuse the option learned in previous tasks for better generalization. The authors utilize demonstrations collected beforehand and train an option learning framework offline by minimizing the expected number of terminations while encouraging diverse options by adding a regularization term. During the offline training, they add one option at a time and move onto the next option when the current loss fails to improve over the previous loss, which enables automatically learning the number of options without manually specifying it. Experiments are conducted on the four rooms environment and Atari 2600 games and demonstrate that the proposed method leads to faster learning on new tasks. Overall, this paper gives a novel option learning framework that results in some improvement in multi-task learning. While the paper is technically sound and somewhat supported by experimental evidence, the experiments are limited to low-dimensional state space and discrete action space. I do wonder if the method can scale to high-dimensional space with continuous control. Moreover, the framework requires optimal policies to generate trajectories for offline option learning, which seems to add more supervision signals than prior work such as option-critic. I wonder how the method would perform under sub-optimal demonstrations or even random trajectories generated by some RL policy. Finally, I wonder how this method can be compared to skill embedding learning methods such as [1], which have been shown to be able to compactly represent skills in a latent space and reuse those skills in high-dimensional robotic manipulation tasks. [1] Hausman, Karol, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. *Learning an embedding space for transferable robot skills.* (2018). (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a new option discovery method for multi-task RL to reuse the option learned in previous tasks for better generalization. The authors utilize demonstrations collected beforehand and train an option learning framework offline by minimizing the expected number of terminations while encouraging diverse options by adding a regularization term. During the offline training, they add one option at a time and move onto the next option when the current loss fails to improve over the previous loss, which enables automatically learning the number of options without manually specifying it. Experiments are conducted on the four rooms environment and Atari 2600 games and demonstrate that the proposed method leads to faster learning on new tasks. Overall, this paper gives a novel option learning framework that results in some improvement in multi-task learning. While the paper is technically sound and somewhat supported by experimental evidence, the experiments are limited to low-dimensional state space and discrete action space. I do wonder if the method can scale to high-dimensional space with continuous control. Moreover, the framework requires optimal policies to generate trajectories for offline option learning, which seems to add more supervision signals than prior work such as option-critic. I wonder how the method would perform under sub-optimal demonstrations or even random trajectories generated by some RL policy. Finally, I wonder how this method can be compared to skill embedding learning methods such as [1], which have been shown to be able to compactly represent skills in a latent space and reuse those skills in high-dimensional robotic manipulation tasks. [1] Hausman, Karol, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. *Learning an embedding space for transferable robot skills.* (2018).\n",
      "    - Concerns with Generalizability, Scalability, or Efficiency (L1, 3)\n",
      "      - Review:###Although this paper seems to only combine existing techniques in community detection and node embedding into a co-train process. The idea is simple and easy understood and the paper is well-written. Theoretical analysis is provided for the approximation error for the sampling strategy. However, major concerns are: 1. Experimental results show that co-training node embedding and community detection can improve the performance for node classification. The improvements may result from the assumption that papers with the same class label are associated with the same community in the citation graph. However, in the dataset, there are many cases that there are not dense connections among the same labeled papers. The authors should check the correlation between the detected communities and the original paper labels. 2. No comparison with other community-preserving node embedding methods, such as *Community Preserving Network Embedding* in AAAI17 3. Since this paper aims to combine community detection and node embedding process, a set of baseline should be considered. For example, if considering the downstream node classification of node embedding as an evaluation task, then how about the performance of the following two-step method. We can first detect communities based on the node features then do graph node embedding by considering the communities* membership and node features together (e.g. simply concatenating both community membership features and node features). 4. Efficiency and scalability evaluations are needed. Spectral clustering has a scalability issue when meeting big graphs. Since the spectral process is also applied in the proposed method, efficiency and scalability evaluations are encouraged to provide, especially for big graphs which are not covered in the selected datasets in this paper. 5. In Sec 5.3 and Fig 2, it*s mentioned that trends of the three datasets are different. For the increasing trend, how about the performance for an extreme case where all nodes are considered in one batch. On the other hand, adding more nodes in one minibatch could provide more information, but why there exists a decreasing trend? Though the authors provide a reason in Sec 5.3, it*s better to analyze the reason directly from the datasets. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Although this paper seems to only combine existing techniques in community detection and node embedding into a co-train process. The idea is simple and easy understood and the paper is well-written. Theoretical analysis is provided for the approximation error for the sampling strategy. However, major concerns are: 1. Experimental results show that co-training node embedding and community detection can improve the performance for node classification. The improvements may result from the assumption that papers with the same class label are associated with the same community in the citation graph. However, in the dataset, there are many cases that there are not dense connections among the same labeled papers. The authors should check the correlation between the detected communities and the original paper labels. 2. No comparison with other community-preserving node embedding methods, such as *Community Preserving Network Embedding* in AAAI17 3. Since this paper aims to combine community detection and node embedding process, a set of baseline should be considered. For example, if considering the downstream node classification of node embedding as an evaluation task, then how about the performance of the following two-step method. We can first detect communities based on the node features then do graph node embedding by considering the communities* membership and node features together (e.g. simply concatenating both community membership features and node features). 4. Efficiency and scalability evaluations are needed. Spectral clustering has a scalability issue when meeting big graphs. Since the spectral process is also applied in the proposed method, efficiency and scalability evaluations are encouraged to provide, especially for big graphs which are not covered in the selected datasets in this paper. 5. In Sec 5.3 and Fig 2, it*s mentioned that trends of the three datasets are different. For the increasing trend, how about the performance for an extreme case where all nodes are considered in one batch. On the other hand, adding more nodes in one minibatch could provide more information, but why there exists a decreasing trend? Though the authors provide a reason in Sec 5.3, it*s better to analyze the reason directly from the datasets.\n",
      "      - Review:###Summary: This work provides a memory-efficient nonlinear bandit algorithm based on deep neural networks. More specifically, the algorithm in this work only uses part of history information to save the memory usage. To overcome the catastrophic forgetting problem, the authors provided novel covariance matrix approximation method. Experiment results also suggest that Pros: The writing of this paper is very well. It provides enough introduction of the background of nonlinear bandit problems. The experiment settings and results are convincible. Cons: - The core idea lacks solid theoretical supports. There is no regret bound result in this paper. The reason why I think the authors should add such theoretical proof is that it seems that the idea to construct new prior matrix instead of old one to avoid the catastrophic forgetting is not related to deep neural network at all. Thus, given existing regret analysis for Thompson sampling on linear bandit problems, the authors should also provide a simple analysis on linear case to show that the construction of prior matrix is indeed meaningful. - The experiment part does not show the accuracy the SDP solve needs. As the authors mentioned in Discussion part, below equation 6, it is very crucial to decide the accuracy the SDP solver needs. I suggest the authors add more details about the SDP solver in the experiment part. Minor comments: - The authors used DNN to minimize equation 3. Have the authors tried a regularized MSE instead of (3)? I think to add a regularizer can further improve the results. - At page 13, below equation 8: why the first equality lacks? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Summary: This work provides a memory-efficient nonlinear bandit algorithm based on deep neural networks. More specifically, the algorithm in this work only uses part of history information to save the memory usage. To overcome the catastrophic forgetting problem, the authors provided novel covariance matrix approximation method. Experiment results also suggest that Pros: The writing of this paper is very well. It provides enough introduction of the background of nonlinear bandit problems. The experiment settings and results are convincible. Cons: - The core idea lacks solid theoretical supports. There is no regret bound result in this paper. The reason why I think the authors should add such theoretical proof is that it seems that the idea to construct new prior matrix instead of old one to avoid the catastrophic forgetting is not related to deep neural network at all. Thus, given existing regret analysis for Thompson sampling on linear bandit problems, the authors should also provide a simple analysis on linear case to show that the construction of prior matrix is indeed meaningful. - The experiment part does not show the accuracy the SDP solve needs. As the authors mentioned in Discussion part, below equation 6, it is very crucial to decide the accuracy the SDP solver needs. I suggest the authors add more details about the SDP solver in the experiment part. Minor comments: - The authors used DNN to minimize equation 3. Have the authors tried a regularized MSE instead of (3)? I think to add a regularizer can further improve the results. - At page 13, below equation 8: why the first equality lacks?\n",
      "      - Review:###This paper proposes a neural linear bandits algorithm that is resilient to catastrophic forgetting when using limited memory. The proposed algorithm Alg. 2 is similar to Thompson sampling for linear contextual bandits, Alg. 1, but using the last layer activation vectors as a linear feature, and also a different way of updating noise parameter prior and posterior is used based on Bayesian linear regression Eq. (2). Alg. 2 also works with limited memory of history data, therefore after every time, the memory is refreshed, likelihood matching is used to calculate new Phi to make the likelihood (mean and variance) of reward estimation the same as it for the old feature. For mean matching, minimizing MSE Eq. (3) is used and for variance, solving PSD problem Eq. (6) is used. The complexity of this algorithm is analyzed. And experiments are conducted to show that the proposed method is resilient to catastrophic forgetting and can achieve good cumulative reward results. The proposed method is reasonable and the results look promising. However, I found several weak points as follows. 1. As mentioned Bayesian linear regression Eq. (2) is used to update noise prior and posterior, but this update has no theoretical guarantees as mentioned. As an algorithm mainly works under bandit settings, this is kind of undesirable. 2. This algorithm works with neural network-based features, but it is in nature not scalable as shown in the complexity analysis (linear dependence on action number). The linear feature is just replaced by the last layer activation of NNs. From this perspective, the experimental results just justify again that the NN feature is somehow powerful, which is as expected. 3. The likelihood matching can deal with catastrophic forgetting with limited history memory, which looks good. But the fact that it actually works for linear feature (last layer activation) together with realization assumption weakens this contribution a lot. The authors find using Eq. (3) is better than the exact mean matching Eq. (5), and there is no explanation for this, which kind of shows the proposed likelihood matching probably is not a good way when using full NNs rather than just linear features (last layer). On the other hand, the SDP seems also can only work under linear feature settings, and is not promising to be generalized to fully update for NNs. Overall, this is a reasonable paper. However, on the one hand, as an algorithm mainly works under bandit settings, it is a lack of theoretical support. On the other hand, the linear feature setting weakens the contribution of likelihood matching to deal with catastrophic forgetting with limited memory. There are some questions of the proposed mean matching, and the matching is not able to generalize. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a neural linear bandits algorithm that is resilient to catastrophic forgetting when using limited memory. The proposed algorithm Alg. 2 is similar to Thompson sampling for linear contextual bandits, Alg. 1, but using the last layer activation vectors as a linear feature, and also a different way of updating noise parameter prior and posterior is used based on Bayesian linear regression Eq. (2). Alg. 2 also works with limited memory of history data, therefore after every time, the memory is refreshed, likelihood matching is used to calculate new Phi to make the likelihood (mean and variance) of reward estimation the same as it for the old feature. For mean matching, minimizing MSE Eq. (3) is used and for variance, solving PSD problem Eq. (6) is used. The complexity of this algorithm is analyzed. And experiments are conducted to show that the proposed method is resilient to catastrophic forgetting and can achieve good cumulative reward results. The proposed method is reasonable and the results look promising. However, I found several weak points as follows. 1. As mentioned Bayesian linear regression Eq. (2) is used to update noise prior and posterior, but this update has no theoretical guarantees as mentioned. As an algorithm mainly works under bandit settings, this is kind of undesirable. 2. This algorithm works with neural network-based features, but it is in nature not scalable as shown in the complexity analysis (linear dependence on action number). The linear feature is just replaced by the last layer activation of NNs. From this perspective, the experimental results just justify again that the NN feature is somehow powerful, which is as expected. 3. The likelihood matching can deal with catastrophic forgetting with limited history memory, which looks good. But the fact that it actually works for linear feature (last layer activation) together with realization assumption weakens this contribution a lot. The authors find using Eq. (3) is better than the exact mean matching Eq. (5), and there is no explanation for this, which kind of shows the proposed likelihood matching probably is not a good way when using full NNs rather than just linear features (last layer). On the other hand, the SDP seems also can only work under linear feature settings, and is not promising to be generalized to fully update for NNs. Overall, this is a reasonable paper. However, on the one hand, as an algorithm mainly works under bandit settings, it is a lack of theoretical support. On the other hand, the linear feature setting weakens the contribution of likelihood matching to deal with catastrophic forgetting with limited memory. There are some questions of the proposed mean matching, and the matching is not able to generalize.\n",
      "    - Hyperparameter, tuning, and methodology concerns (L1, 5)\n",
      "      - Review:###*UPDATE* I have read the other reviews, author*s comments and the revised pdf. I maintain my weak accept rating, the paper is borderline but above the bar. The inclusion of experiments that Reviewer1 suggested definitely make the contributions stronger. I believe the paper will be substantially stronger with a careful study of where the empirical improvement is coming from. The theory (that the approximate gradient has an acute angle with the desired gradient) is potentially vulnerable -- this property is certainly desirable when stochastic-optimizing convex functions (with appropriate step sizes) but it*s not trivial that it gives good behavior for optimizing non-convex functions like NAS. Without this careful study, it is not obvious that the proposed method doesn*t suffer from it*s own *gradient traps*. That said, pointing out gradient approximation issues with differentiable NAS may be a valuable enough contribution. The paper studies differentiable approaches to neural architecture search and convincingly points out that existing approximations to the gradient w.r.t. architecture parameters are problematic. A new approximation is proposed, and evaluated to show that degenerate architectures are not getting selected once search has converged (empirically) on standard image classification datasets. The problem with existing approximations (e.g. first-order or second-order DARTS) is explained clearly. It is however unclear whether the proposed solution provides a complete solution, or if there are avenues for further improvement. The key question marks are: is the proposal a tractable approximation? is the empirical improvement indeed arising because of better gradients? Studying these two questions carefully via experiment will make the paper*s contributions stronger. Minor questions (related to writing/exposition): How is Eqn2 different from Eq3? If they are the same, please remove the redundant equation (you already repeat it in the Introduction). Why is Eqn7 a tractable approximation when H is very high-dimensional? I agree that it is more efficient that H^-1, but don*t see how it can be tractable to compute in general. Section 4.1.2: What is *auxiliary loss tower*? Abstract: *obstacles it* (obstacles is an awkward verb, perhaps use *hinders it* instead) Introduction: *was very few covered in previous works* is an awkward phrase. *has not been studied carefully in previous works* Related work: *exhausted search* -> *exhaustive search* (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###*UPDATE* I have read the other reviews, author*s comments and the revised pdf. I maintain my weak accept rating, the paper is borderline but above the bar. The inclusion of experiments that Reviewer1 suggested definitely make the contributions stronger. I believe the paper will be substantially stronger with a careful study of where the empirical improvement is coming from. The theory (that the approximate gradient has an acute angle with the desired gradient) is potentially vulnerable -- this property is certainly desirable when stochastic-optimizing convex functions (with appropriate step sizes) but it*s not trivial that it gives good behavior for optimizing non-convex functions like NAS. Without this careful study, it is not obvious that the proposed method doesn*t suffer from it*s own *gradient traps*. That said, pointing out gradient approximation issues with differentiable NAS may be a valuable enough contribution. The paper studies differentiable approaches to neural architecture search and convincingly points out that existing approximations to the gradient w.r.t. architecture parameters are problematic. A new approximation is proposed, and evaluated to show that degenerate architectures are not getting selected once search has converged (empirically) on standard image classification datasets. The problem with existing approximations (e.g. first-order or second-order DARTS) is explained clearly. It is however unclear whether the proposed solution provides a complete solution, or if there are avenues for further improvement. The key question marks are: is the proposal a tractable approximation? is the empirical improvement indeed arising because of better gradients? Studying these two questions carefully via experiment will make the paper*s contributions stronger. Minor questions (related to writing/exposition): How is Eqn2 different from Eq3? If they are the same, please remove the redundant equation (you already repeat it in the Introduction). Why is Eqn7 a tractable approximation when H is very high-dimensional? I agree that it is more efficient that H^-1, but don*t see how it can be tractable to compute in general. Section 4.1.2: What is *auxiliary loss tower*? Abstract: *obstacles it* (obstacles is an awkward verb, perhaps use *hinders it* instead) Introduction: *was very few covered in previous works* is an awkward phrase. *has not been studied carefully in previous works* Related work: *exhausted search* -> *exhaustive search*\n",
      "      - Review:###Summary This paper presents Gradient Rescaling (GR) for robust learning to combat label noise. They propose to treat each data sample with different significance scores: some samples are important to learning, and some examples are insignificant (or even detrimental) to learning. So they desire to weight each samples according to their significance. They propose the notion of emphasis focus (When learning, whether we should put emphasis on learning “hard” examples or “easy” examples) and emphasis spread (the variance of these significance weights). The authors propose that this “difficulty” of samples are proportional to their network output logit values. The authors examine the analytical forms of the gradients of popular loss functions such as Categorical Cross Entropy, Mean Absolute Error and Generalized Cross Entropy. They find that the formulas for the gradient are of similar family with varying hyperparameters. Authors claim that tweaking these hyperparameters result in tuning the emphasis focus and spread. The authors conduct Experiments on CIFAR10, CIFAR100 with simulated symmetric noise. Also, they conduct experiments on real-world noisy datasets: Clothing 1M dataset and MARS video dataset. The authors claim that the performance of GR exceeds various baselines. Significance/Novelty/Clarity Significance: Low-Medium. The performance increase exhibited in the experiments are a bit underwhelming (when considering the fact that benchmarks of most recent noise-robustness algos such as <Lee et al. 2019 ICML> are missing). Novelty: Medium. The paper is interesting in the sense that the authors integrated (and allegedly generalized) the gradient formulas for several losses into one family, and tried to integrate and tweak their postulation of “Emphasis focus” and “emphasis spread” into the framework. However, the theoretical ground and convincing reasoning for their claim seems a bit lackluster. Clarity: Low. The overall flow of the paper is a bit fuzzy - exhibiting a stream-of-consciousness style flow. Pros and Cons in Detail Pros: 1.The authors try to unify the analytical forms of the gradients of various loss functions into a single family equipped with hyperparameters that control emphasis focus and spread. 2.Conducted experiments show that GR achieved increased performance when compared to the baselines. Cons: My major concern is about tuning newly introduced hyperparameters in practical settings. How can we guarantee to have intact validation set? Can we get any improvement via GR even with corrupted validation set for tuning hyperparameters? 1. The arguments of the authors are grounded in the premise that “difficult” samples will exhibit small logit values, and “easy” samples high logit values. 2. No justifications (both theoretical and experimental) are provided on the claim that controlling emphasis focus/spread will result in more robust learning. 3. This algorithm introduces 2 additional hyperparameters that are correlated with each other. This introduces additional labor. 4. By changing the loss function, the outputs of the network might lose its interpretation as a probability distribution. 5. No confidence intervals are shown except for the CIFAR-100 experiment. 6. Experiments are only conducted on vision tasks. 7. The baseline menagerie also changes when the authors change the target dataset. 8. Additional benchmarks of most recent noise-robustness algos such as <Lee et al. 2019 ICML> are required. Questions 1. Is it always the case that “difficult” samples exhibit small logit values, and “easy” samples high logit values? 2. If not, GR’s emphasis manipulation might result in neglecting samples containing valuable information. 3. Can GR be used simultaneously with other noise-robust learning methods to further boost the performance? 4. Technically, GR aims to rescale the gradients of the logits. How will it interact with optimizers other than SGD such as Adam? 5. Does GR still work well on small datasets(#points < 5000)? Misc. Comments Page 3-> inside L1 norm, no differentiation sign in the denominator. Around eq 2 and 4: missing derivative symbol w.r.t. z (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Summary This paper presents Gradient Rescaling (GR) for robust learning to combat label noise. They propose to treat each data sample with different significance scores: some samples are important to learning, and some examples are insignificant (or even detrimental) to learning. So they desire to weight each samples according to their significance. They propose the notion of emphasis focus (When learning, whether we should put emphasis on learning “hard” examples or “easy” examples) and emphasis spread (the variance of these significance weights). The authors propose that this “difficulty” of samples are proportional to their network output logit values. The authors examine the analytical forms of the gradients of popular loss functions such as Categorical Cross Entropy, Mean Absolute Error and Generalized Cross Entropy. They find that the formulas for the gradient are of similar family with varying hyperparameters. Authors claim that tweaking these hyperparameters result in tuning the emphasis focus and spread. The authors conduct Experiments on CIFAR10, CIFAR100 with simulated symmetric noise. Also, they conduct experiments on real-world noisy datasets: Clothing 1M dataset and MARS video dataset. The authors claim that the performance of GR exceeds various baselines. Significance/Novelty/Clarity Significance: Low-Medium. The performance increase exhibited in the experiments are a bit underwhelming (when considering the fact that benchmarks of most recent noise-robustness algos such as <Lee et al. 2019 ICML> are missing). Novelty: Medium. The paper is interesting in the sense that the authors integrated (and allegedly generalized) the gradient formulas for several losses into one family, and tried to integrate and tweak their postulation of “Emphasis focus” and “emphasis spread” into the framework. However, the theoretical ground and convincing reasoning for their claim seems a bit lackluster. Clarity: Low. The overall flow of the paper is a bit fuzzy - exhibiting a stream-of-consciousness style flow. Pros and Cons in Detail Pros: 1.The authors try to unify the analytical forms of the gradients of various loss functions into a single family equipped with hyperparameters that control emphasis focus and spread. 2.Conducted experiments show that GR achieved increased performance when compared to the baselines. Cons: My major concern is about tuning newly introduced hyperparameters in practical settings. How can we guarantee to have intact validation set? Can we get any improvement via GR even with corrupted validation set for tuning hyperparameters? 1. The arguments of the authors are grounded in the premise that “difficult” samples will exhibit small logit values, and “easy” samples high logit values. 2. No justifications (both theoretical and experimental) are provided on the claim that controlling emphasis focus/spread will result in more robust learning. 3. This algorithm introduces 2 additional hyperparameters that are correlated with each other. This introduces additional labor. 4. By changing the loss function, the outputs of the network might lose its interpretation as a probability distribution. 5. No confidence intervals are shown except for the CIFAR-100 experiment. 6. Experiments are only conducted on vision tasks. 7. The baseline menagerie also changes when the authors change the target dataset. 8. Additional benchmarks of most recent noise-robustness algos such as <Lee et al. 2019 ICML> are required. Questions 1. Is it always the case that “difficult” samples exhibit small logit values, and “easy” samples high logit values? 2. If not, GR’s emphasis manipulation might result in neglecting samples containing valuable information. 3. Can GR be used simultaneously with other noise-robust learning methods to further boost the performance? 4. Technically, GR aims to rescale the gradients of the logits. How will it interact with optimizers other than SGD such as Adam? 5. Does GR still work well on small datasets(#points < 5000)? Misc. Comments Page 3-> inside L1 norm, no differentiation sign in the denominator. Around eq 2 and 4: missing derivative symbol w.r.t. z\n",
      "      - Review:###This paper presents a method for blind source separation relying on randomly initialized networks to decompose an input audio spectrogram into two components. The networks are designed to promote temporal contiguity of spectral energy in the estimated signals, which are modulated (in time) by estimated masks. The proposed method is evaluated on a collection of 150 random mixes of sounds, and performs favorably relative to some standard baseline methods (RPCA, NMF, KAM). This seems like a promising line of work, but at this point I think the weaknesses of the paper outweigh its strengths (as detailed below). Some of these points may be addressed during discussion, but I currently lean toward reject. Strengths of the paper: - The ideas are interesting, and appear to perform well on a simulated and real(ish) data. - The authors investigate several variations and applications of source separation, including interactive editing, co-separation, and texture synthesis. - A small (qualitative) ablation study is included to clarify the importance of different components of the loss function. Weaknesses of the paper: - Much of the presentation is vague or opaque. There is little detail provided about the specific architectural parameters of the model, and the diagram (figure 1) does not appear to match the equations. Specifically, it*s unclear whether S_1^* is a function of S_mixture or not. - The quantitative evaluation focuses entirely on one (new) dataset with unclear characteristics. No details are provided about the evaluation protocol, and in particular, the tuning of hyperparameters for the various methods under comparison. Aggregate statistics are included (mean? SDR, etc), but no notion of variance or error bars are included. It*s not ultimately clear how fair this evaluation is. There should at minimum be a comparison on a standard source separation or speech enhancement dataset, in addition to the new set presented here. - Most of the spectrogram figures appear to be upside-down, which is confusing. The details of the audio processing are omitted: STFT parameters are stated, but not the sampling rate. - There are numerous typos (*grounth*, *spectrogram stokes*, etc), indicative of the authors not running a spell-checker on their submission. Questions for the authors: - The model itself consists of several competing loss functions, but it seems like they may have a trivial, optimal solution at S1 = S_mix (M_1 = 1) and S_2 = 0 (M_2 = 0 or 1). As far as I can tell, this solution would trivially minimize each term of equation 9. (If S_1* does not depend on S_mixture, this may be less of an issue, but the trivial solution may still exist when the driving noise is of sufficiently high dimension and the optimization is run long enough.) Am I misunderstanding the algorithm, or is there some deeper reason why this solution would not be preferred? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper presents a method for blind source separation relying on randomly initialized networks to decompose an input audio spectrogram into two components. The networks are designed to promote temporal contiguity of spectral energy in the estimated signals, which are modulated (in time) by estimated masks. The proposed method is evaluated on a collection of 150 random mixes of sounds, and performs favorably relative to some standard baseline methods (RPCA, NMF, KAM). This seems like a promising line of work, but at this point I think the weaknesses of the paper outweigh its strengths (as detailed below). Some of these points may be addressed during discussion, but I currently lean toward reject. Strengths of the paper: - The ideas are interesting, and appear to perform well on a simulated and real(ish) data. - The authors investigate several variations and applications of source separation, including interactive editing, co-separation, and texture synthesis. - A small (qualitative) ablation study is included to clarify the importance of different components of the loss function. Weaknesses of the paper: - Much of the presentation is vague or opaque. There is little detail provided about the specific architectural parameters of the model, and the diagram (figure 1) does not appear to match the equations. Specifically, it*s unclear whether S_1^* is a function of S_mixture or not. - The quantitative evaluation focuses entirely on one (new) dataset with unclear characteristics. No details are provided about the evaluation protocol, and in particular, the tuning of hyperparameters for the various methods under comparison. Aggregate statistics are included (mean? SDR, etc), but no notion of variance or error bars are included. It*s not ultimately clear how fair this evaluation is. There should at minimum be a comparison on a standard source separation or speech enhancement dataset, in addition to the new set presented here. - Most of the spectrogram figures appear to be upside-down, which is confusing. The details of the audio processing are omitted: STFT parameters are stated, but not the sampling rate. - There are numerous typos (*grounth*, *spectrogram stokes*, etc), indicative of the authors not running a spell-checker on their submission. Questions for the authors: - The model itself consists of several competing loss functions, but it seems like they may have a trivial, optimal solution at S1 = S_mix (M_1 = 1) and S_2 = 0 (M_2 = 0 or 1). As far as I can tell, this solution would trivially minimize each term of equation 9. (If S_1* does not depend on S_mixture, this may be less of an issue, but the trivial solution may still exist when the driving noise is of sufficiently high dimension and the optimization is run long enough.) Am I misunderstanding the algorithm, or is there some deeper reason why this solution would not be preferred?\n",
      "      - Review:###This paper proposes a method to characterize the decision boundary of deep neural networks. The authors take advantage of a new perspective on deep neural network i.e., tropical geometry. They present and prove a theorem connecting the decision boundary of deep neural networks to tropical hyperspheres. Then, they use the theoretical results of the theorem for two applications i.e., network pruning and adversarial examples generation. For the former, they show that using the introduced decision boundary characterization using the tropical geometry, one can dimish the number of parameters in a model with insignificant loss in performance. For the latter, they introduce a new method for adversarial examples generation by altering the decision boundary. Overall, the technical and theoretical contribution of the paper regarding the relation of decision boundary to the tropical geometry is significant and could be useful for further investigation of the decision boundary of deep neural networks. However, there are several caveats in this paper that need further clarification. 1) This paper needs to be placed properly among several important missing references on the decision boundary of deep neural networks [1][2]. In particular, using introduced tropical geometry perspective, how we can obtain the complexity of the decision boundary of a deep neural network? 2) The second part of Theorem 2 should be explained straightforwardly and clearly as it plays an important role in the subsequent results and applications. 3) In the tropical network pruning section, the authors mention that *since fully connected layers in deep neural networks tend to have much higher memory complexity than convolutional layers, we restrict our focus to pruning fully connected layers*. However, convolutional layers of investigated architectures (i.e., VGG16 and AlexNet) have a large number of parameters as well. So, wouldn*t it be necessary to investigate pruning convolutional layers as well if diminishing the number of parameters is the main purpose? Moreover, the size of the last fully connected layer is simply determined by the preceding convolutional architecture, which in fact extracts the salient features (at least for well-known image datasets), while the last fully connected layer just flattens the extracted features to be fed into the subsequent classifier. Hence, it would be interesting and, in my opinion, necessary to investigate pruning convolutional layers as well. Furthermore, the authors mention that a pruned subnetwork has a similar decision boundary to the original network. What does it mean exactly by *similar*? What are the measures capturing this similarity, if any? Also, the authors imply that two networks performing similarly (in terms of the accuracy) on a particular dataset have similar decision boundaries. I am skeptical if this the case and it needs to be validated concretely. 4) In adversarial examples generation, typically for a pre-trained deep neural network model one is interested in generating examples that are misclassified by the model while they resemble real instances. In this setting, we keep the model and thus its decision boundary intact. In this paper, nevertheless, aiming at generating adversarial examples, the decision boundary and thus the (pre-trained) model is altered. By chaining the decision boundary, however, the model*s decisions for original real samples might change as well. Therefore, it is not clear to the reviewer how the introduced method is comparable to the well-established adversarial example generation setting. 5) Two previous papers investigated the decision boundary of the deep neural networks in the presence of adversarial examples [3][4]. Please discuss how the introduced method in this paper is placed among these methods. Minor comments: In the abstract, *We utilize this geometric characterization to shed light and new perspective on three tasks* --> unclear, needs to be revised. Proposition 1, *the zonotope formed be the line segments* --> *the zonotope formed by the line segments* Page 7. Results. *For VGG16, we perform similarly on both SVHN and CIFAR10 CIFAR100.* --> *For VGG16, we perform similarly on both SVHN and CIFAR10.* References: [1] @article{li2018decision, title={On the decision boundary of deep neural networks}, author={Li, Yu and Richtarik, Peter and Ding, Lizhong and Gao, Xin}, journal={arXiv preprint arXiv:1808.05385}, year={2018} } [2] @article{beise2018decision, title={On decision regions of narrow deep neural networks}, author={Beise, Hans-Peter and Da Cruz, Steve Dias and Schr{*o}der, Udo}, journal={arXiv preprint arXiv:1807.01194}, year={2018} } [3] @article{khoury2018geometry, title={On the geometry of adversarial examples}, author={Khoury, Marc and Hadfield-Menell, Dylan}, journal={arXiv preprint arXiv:1811.00525}, year={2018} } [4] @inproceedings{ he2018decision, title={Decision Boundary Analysis of Adversarial Examples}, author={Warren He and Bo Li and Dawn Song}, booktitle={International Conference on Learning Representations}, year={2018}, url={https://openreview.net/forum?id=BkpiPMbA-}, } (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a method to characterize the decision boundary of deep neural networks. The authors take advantage of a new perspective on deep neural network i.e., tropical geometry. They present and prove a theorem connecting the decision boundary of deep neural networks to tropical hyperspheres. Then, they use the theoretical results of the theorem for two applications i.e., network pruning and adversarial examples generation. For the former, they show that using the introduced decision boundary characterization using the tropical geometry, one can dimish the number of parameters in a model with insignificant loss in performance. For the latter, they introduce a new method for adversarial examples generation by altering the decision boundary. Overall, the technical and theoretical contribution of the paper regarding the relation of decision boundary to the tropical geometry is significant and could be useful for further investigation of the decision boundary of deep neural networks. However, there are several caveats in this paper that need further clarification. 1) This paper needs to be placed properly among several important missing references on the decision boundary of deep neural networks [1][2]. In particular, using introduced tropical geometry perspective, how we can obtain the complexity of the decision boundary of a deep neural network? 2) The second part of Theorem 2 should be explained straightforwardly and clearly as it plays an important role in the subsequent results and applications. 3) In the tropical network pruning section, the authors mention that *since fully connected layers in deep neural networks tend to have much higher memory complexity than convolutional layers, we restrict our focus to pruning fully connected layers*. However, convolutional layers of investigated architectures (i.e., VGG16 and AlexNet) have a large number of parameters as well. So, wouldn*t it be necessary to investigate pruning convolutional layers as well if diminishing the number of parameters is the main purpose? Moreover, the size of the last fully connected layer is simply determined by the preceding convolutional architecture, which in fact extracts the salient features (at least for well-known image datasets), while the last fully connected layer just flattens the extracted features to be fed into the subsequent classifier. Hence, it would be interesting and, in my opinion, necessary to investigate pruning convolutional layers as well. Furthermore, the authors mention that a pruned subnetwork has a similar decision boundary to the original network. What does it mean exactly by *similar*? What are the measures capturing this similarity, if any? Also, the authors imply that two networks performing similarly (in terms of the accuracy) on a particular dataset have similar decision boundaries. I am skeptical if this the case and it needs to be validated concretely. 4) In adversarial examples generation, typically for a pre-trained deep neural network model one is interested in generating examples that are misclassified by the model while they resemble real instances. In this setting, we keep the model and thus its decision boundary intact. In this paper, nevertheless, aiming at generating adversarial examples, the decision boundary and thus the (pre-trained) model is altered. By chaining the decision boundary, however, the model*s decisions for original real samples might change as well. Therefore, it is not clear to the reviewer how the introduced method is comparable to the well-established adversarial example generation setting. 5) Two previous papers investigated the decision boundary of the deep neural networks in the presence of adversarial examples [3][4]. Please discuss how the introduced method in this paper is placed among these methods. Minor comments: In the abstract, *We utilize this geometric characterization to shed light and new perspective on three tasks* --> unclear, needs to be revised. Proposition 1, *the zonotope formed be the line segments* --> *the zonotope formed by the line segments* Page 7. Results. *For VGG16, we perform similarly on both SVHN and CIFAR10 CIFAR100.* --> *For VGG16, we perform similarly on both SVHN and CIFAR10.* References: [1] @article{li2018decision, title={On the decision boundary of deep neural networks}, author={Li, Yu and Richtarik, Peter and Ding, Lizhong and Gao, Xin}, journal={arXiv preprint arXiv:1808.05385}, year={2018} } [2] @article{beise2018decision, title={On decision regions of narrow deep neural networks}, author={Beise, Hans-Peter and Da Cruz, Steve Dias and Schr{*o}der, Udo}, journal={arXiv preprint arXiv:1807.01194}, year={2018} } [3] @article{khoury2018geometry, title={On the geometry of adversarial examples}, author={Khoury, Marc and Hadfield-Menell, Dylan}, journal={arXiv preprint arXiv:1811.00525}, year={2018} } [4] @inproceedings{ he2018decision, title={Decision Boundary Analysis of Adversarial Examples}, author={Warren He and Bo Li and Dawn Song}, booktitle={International Conference on Learning Representations}, year={2018}, url={https://openreview.net/forum?id=BkpiPMbA-}, }\n",
      "      - Review:###This paper proposes a new method for empirical composition problems to which the vanilla SGD is not applicable because it has a finite-sum structure inside non-linear loss functions. A proposed method (named SCCG) is a combination of stochastic compositional gradient descent (SCGD) and stochastically controlled stochastic gradient (SCSG). In a theoretical analysis part, a linear convergence rate and a sub-linear convergence rate are derived under the strong convex and non-convex settings, respectively. In experiments, the superior performance of the method to competitors is verified on both strongly convex and non-convex problems. Clarity: The paper is clear and well written. Quality: The work is of good quality and is technically sound. Significance: The problem treated in this paper is important and contains several applications as mentioned in the paper. Hence, developing an efficient method for this problem is important and interesting. Although derived convergence rates are better than existing primal methods, this paper lacks a comparison with the recently proposed primal-dual method by [A.Devraj & J.Chen (2019)]. [A.Devraj & J.Chen (2019)] Stochastic Variance Reduced Primal Dual Algorithms for Empirical Composition Optimization. NeurIPS, 2019. A convergence rate obtained in [A.Devraj & J.Chen (2019)] seems faster than that of SCCG for ill-conditioned strongly convex problems. However, there exists a certain setting (large-scale setting) where SCCG outperforms their method. Thus, the contribution of the paper is not lost, but it is better to compare SCCG with the method in [A.Devraj & J.Chen (2019)], empirically and theoretically. If the authors can show an empirical advantage over their method, it will make the paper stronger. ----- Update: I thank the authors for the response and hard work. I am convinced of the advantage of the proposed method. I would like to keep my score. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a new method for empirical composition problems to which the vanilla SGD is not applicable because it has a finite-sum structure inside non-linear loss functions. A proposed method (named SCCG) is a combination of stochastic compositional gradient descent (SCGD) and stochastically controlled stochastic gradient (SCSG). In a theoretical analysis part, a linear convergence rate and a sub-linear convergence rate are derived under the strong convex and non-convex settings, respectively. In experiments, the superior performance of the method to competitors is verified on both strongly convex and non-convex problems. Clarity: The paper is clear and well written. Quality: The work is of good quality and is technically sound. Significance: The problem treated in this paper is important and contains several applications as mentioned in the paper. Hence, developing an efficient method for this problem is important and interesting. Although derived convergence rates are better than existing primal methods, this paper lacks a comparison with the recently proposed primal-dual method by [A.Devraj & J.Chen (2019)]. [A.Devraj & J.Chen (2019)] Stochastic Variance Reduced Primal Dual Algorithms for Empirical Composition Optimization. NeurIPS, 2019. A convergence rate obtained in [A.Devraj & J.Chen (2019)] seems faster than that of SCCG for ill-conditioned strongly convex problems. However, there exists a certain setting (large-scale setting) where SCCG outperforms their method. Thus, the contribution of the paper is not lost, but it is better to compare SCCG with the method in [A.Devraj & J.Chen (2019)], empirically and theoretically. If the authors can show an empirical advantage over their method, it will make the paper stronger. ----- Update: I thank the authors for the response and hard work. I am convinced of the advantage of the proposed method. I would like to keep my score.\n",
      "    - Insufficiently Convincing Empirical Evidence for Claims (L1, 7)\n",
      "      -  Summary: This paper considers the addition of self-supervised learning techniques in the few-shot learning setting. Extensive experiments are done to show that it can be helpful, including in cases where the labeled data is corrupted. The paper also considers the domain mismatch issue where unlabeled images come from a different domain. Review: This paper is thorough and clearly written. Applying self-supervised learning techniques to the few-shot learning regime is a simple idea, and this paper clearly shows that it can be beneficial. It includes extensive experiments on a wide variety of image datasets, including many additional studies in the appendix. The only possible criticisms of this paper are that it is limited to the image domain (as are most few-shot learning/self-supervised learning studies, so we can probably ignore this) and that it does not produce a huge delta in understanding compared to Gidaris et al. (2019). Gidaris et al. (2019) was posted to arxiv in June, I*m not sure if this work was also on arxiv around the same time, and even if it wasn*t I*m not sure what to consider *concurrent*. However, as the authors note they include additional experimental settings (like the domain-selection idea) that are not in Gidaris et al. (2019), so the works are somewhat complementary. The only other comment I have is that the paper is quite large in scope for a conference submission and as a result there are many details and experiments that are left for the appendix. I could also see the domain selection experiments constituting their own submission. For example the definition of the **distance* between a pair of domains* is only introduced in passing in the midst of Section 4.2 covering domain shift experiments, and the method for training a domain classifier is similarly only mentioned in passing in 4.2. Of course, it is not really valid to criticize a paper for being too exhaustive. Overall, I recommend acceptance. Specific comments: - Truly a minor suggestion but I suggest moving Figure 1 to the top of page 2. - Snell et al. (2017) needs a citep - You ought to cite *S4L: Self-Supervised Semi-Supervised Learning*, which is related to your discussion of connections between self- and semi-supervised learning (though not few-shot). - The paragraph beginning *The focus of most prior work...* in the Related Work section provides a nice framing of your work and so might make more sense in the introduction. - *we consider self-supervised losses based on labeled data ... that can be derived from inputs x alone* All labels (can be derived from in the inputs x alone, given an oracle (or human labeler). I think you mean *that can be derived automatically without any human labeling*. - You state *Our final loss function combines the two losses*. Is there no scalar multiplier on either loss term to trade-off the importance of each? - The gains from the self-supervised auxiliary tasks are over and above any gains from data augmentation alone. More experimental details are in Appendix A.5.* I don*t see any information to substantiate the claim that the self-supervised tasks result in a bigger improvement than data augmentation alone, can you provide those details? - *However, does more unlabeled data always help for a task in hand?* This question actually was addressed somewhat in *Realistic Evaluation of Semi-Supervised Learning Algorithms*, see sections 4.4 and 4.5 therein. (L0, 1)\n",
      "        * Examples:\n",
      "          -  Summary: This paper considers the addition of self-supervised learning techniques in the few-shot learning setting. Extensive experiments are done to show that it can be helpful, including in cases where the labeled data is corrupted. The paper also considers the domain mismatch issue where unlabeled images come from a different domain. Review: This paper is thorough and clearly written. Applying self-supervised learning techniques to the few-shot learning regime is a simple idea, and this paper clearly shows that it can be beneficial. It includes extensive experiments on a wide variety of image datasets, including many additional studies in the appendix. The only possible criticisms of this paper are that it is limited to the image domain (as are most few-shot learning/self-supervised learning studies, so we can probably ignore this) and that it does not produce a huge delta in understanding compared to Gidaris et al. (2019). Gidaris et al. (2019) was posted to arxiv in June, I*m not sure if this work was also on arxiv around the same time, and even if it wasn*t I*m not sure what to consider *concurrent*. However, as the authors note they include additional experimental settings (like the domain-selection idea) that are not in Gidaris et al. (2019), so the works are somewhat complementary. The only other comment I have is that the paper is quite large in scope for a conference submission and as a result there are many details and experiments that are left for the appendix. I could also see the domain selection experiments constituting their own submission. For example the definition of the **distance* between a pair of domains* is only introduced in passing in the midst of Section 4.2 covering domain shift experiments, and the method for training a domain classifier is similarly only mentioned in passing in 4.2. Of course, it is not really valid to criticize a paper for being too exhaustive. Overall, I recommend acceptance. Specific comments: - Truly a minor suggestion but I suggest moving Figure 1 to the top of page 2. - Snell et al. (2017) needs a citep - You ought to cite *S4L: Self-Supervised Semi-Supervised Learning*, which is related to your discussion of connections between self- and semi-supervised learning (though not few-shot). - The paragraph beginning *The focus of most prior work...* in the Related Work section provides a nice framing of your work and so might make more sense in the introduction. - *we consider self-supervised losses based on labeled data ... that can be derived from inputs x alone* All labels (can be derived from in the inputs x alone, given an oracle (or human labeler). I think you mean *that can be derived automatically without any human labeling*. - You state *Our final loss function combines the two losses*. Is there no scalar multiplier on either loss term to trade-off the importance of each? - The gains from the self-supervised auxiliary tasks are over and above any gains from data augmentation alone. More experimental details are in Appendix A.5.* I don*t see any information to substantiate the claim that the self-supervised tasks result in a bigger improvement than data augmentation alone, can you provide those details? - *However, does more unlabeled data always help for a task in hand?* This question actually was addressed somewhat in *Realistic Evaluation of Semi-Supervised Learning Algorithms*, see sections 4.4 and 4.5 therein.\n",
      "      -  This paper proposed a new method for unsupervised domain adaptation. Different from a conventional domain classifier based adaptation, they propose to utilize the loss of autoencoder to extract domain-invariant features. They trained reconstruction network to reconstruct source examples well whereas making reconstruction loss of the target examples large with some margin. Their goal is to stabilize the training of adversarial training for domain adaptation, incorporate pixel-level information, and give interpretable learned feature space. They performed experiments on digits datasets and WiFi Gesture Recognition datasets. Through experiments, they have shown that their method shows better performance than baseline methods and their method is not parameter-sensitive, is stable and provides interpretable adaptation results. I think their method is interesting and motivation is important. However, their experimental results are not convincing enough. First, they did not compare their method with recent state-of-the-art methods. For example, there are classifier*s discrepancy based adversarial learning method, Saito, Kuniaki, et al. *Maximum classifier discrepancy for unsupervised domain adaptation.*. In addition, they did not compare with *A dirt-t approach to unsupervised domain adaptation*, which they cited in the paper. I think their method is for stable and interpretable adversarial learning. So, it does not have to outperform other methods in accuracy. However, they need to show some superiority over these representative adversarial methods. Second, their experiment is only on digits and WIFI datasets. Is the method effective for object recognition datasets, such as Office or OfficeHome? This is an important question to be addressed because the two datasets are benchmark domain adaptation dataset and the behavior on this dataset will show how this method is applicable to various datasets. I would say that the method does not have to outperform state-of-the art methods for these datasets, but they need to show how the method works on this dataset with respect to stability and interpretability. In addition, this method seems to have clear connection with *Zhao, Junbo, Michael Mathieu, and Yann LeCun. *Energy-based generative adversarial network.* arXiv preprint arXiv:1609.03126 (2016)*. They need to add this paper to a reference and explain some connections. To sum up, due to the two questions listed above, I think this paper is marginally below the acceptance threshold. Please respond to the questions. (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper proposed a new method for unsupervised domain adaptation. Different from a conventional domain classifier based adaptation, they propose to utilize the loss of autoencoder to extract domain-invariant features. They trained reconstruction network to reconstruct source examples well whereas making reconstruction loss of the target examples large with some margin. Their goal is to stabilize the training of adversarial training for domain adaptation, incorporate pixel-level information, and give interpretable learned feature space. They performed experiments on digits datasets and WiFi Gesture Recognition datasets. Through experiments, they have shown that their method shows better performance than baseline methods and their method is not parameter-sensitive, is stable and provides interpretable adaptation results. I think their method is interesting and motivation is important. However, their experimental results are not convincing enough. First, they did not compare their method with recent state-of-the-art methods. For example, there are classifier*s discrepancy based adversarial learning method, Saito, Kuniaki, et al. *Maximum classifier discrepancy for unsupervised domain adaptation.*. In addition, they did not compare with *A dirt-t approach to unsupervised domain adaptation*, which they cited in the paper. I think their method is for stable and interpretable adversarial learning. So, it does not have to outperform other methods in accuracy. However, they need to show some superiority over these representative adversarial methods. Second, their experiment is only on digits and WIFI datasets. Is the method effective for object recognition datasets, such as Office or OfficeHome? This is an important question to be addressed because the two datasets are benchmark domain adaptation dataset and the behavior on this dataset will show how this method is applicable to various datasets. I would say that the method does not have to outperform state-of-the art methods for these datasets, but they need to show how the method works on this dataset with respect to stability and interpretability. In addition, this method seems to have clear connection with *Zhao, Junbo, Michael Mathieu, and Yann LeCun. *Energy-based generative adversarial network.* arXiv preprint arXiv:1609.03126 (2016)*. They need to add this paper to a reference and explain some connections. To sum up, due to the two questions listed above, I think this paper is marginally below the acceptance threshold. Please respond to the questions.\n",
      "      - Review:###Summary: In this study, the authors propose a new method for performing artistic style transfer for arbitrary image and styles. The new method employs a cascade/serial architecture for performing the style transfer. The authors test their method using human preference studies. In summary, I found the architecture choice to be minimally explored. More importantly, a vast majority of the results to demonstrate the relative merits of this method were qualitative. The minimal quantitative results were unconvincing and left many unanswered questions about how well one could trust these results. Major Comments: 1. No experiments to explore the architecture hyperparameters. A natural question might be how the quality of the method varies systematically as the number of methods N grows. Presumably, if N=1, this would recover previous methods. 2. Authors are missing an important reference and point of comparison for arbitrary style transfer. Exploring the structure of a real-time, arbitrary neural artistic stylization network Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent Dumoulin, Jonathon Shlens https://arxiv.org/abs/1705.06830 http://goo.gle/2oiDKaT 3. Minimal quantitative analysis. A vast majority of the results (30 of 32 figures) are qualitative comparisons and the paper is sorely lacking an emphasis on quantitative comparisons. This is a large and notable problem in this paper and a quantitative comparison *should* constitute the primary thrust and central result of such a paper to convincingly demonstrate to a reader that the proposed method is indeed to superior to other techniques. I wish the authors dedicated more emphasis in this paper to a detailed quantitative comparison for these methods. As a starter, the analysis presented as the final two appendix figures (31 and 32) should be front and center in the result section of the paper. 4. User study for quantitative comparison is incomplete and unconvincing. Table 1 and Appendix Figure 31 and 32 represent the primary result of this paper as these results and comprise the user studies to quantify how much better this method is to previous methods. These studies however are fairly unconvincing as lots of details are omitted and and I am concerned about the rigor of the human studies including but not limited to: 4a. How long did each human study each image? What controls were added to the study to ensure that all images were equally studied by humans? For instance, were any golden tests employed to ensure user engagement throughout the study? 4b. What was the repeatability of each measurement of preference? If a single human was presented the same image twice, how consistent were there ratings? For that matter, how consistent were the ratings across humans? I presume that some humans preferred some styles over others but how systematic was this? 4c. What types of user testing scenarios were explored to ensure minimal bias in the results? Were multi-choice, paired choice or force choice employed? What about minimal or maximal time limit enforcement? 4d. How can I have confidence that the authors did not cherry pick images and styles that favored their method? For that matter, I would expect that some methods work better on some styles or images. I would expect to see analysis accordingly to break down which styles/images work better on different slices of the data. 4e. The statistical significance of Figure 31 and Figure 32 is not provided. What would an error bar look like with resampling? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Summary: In this study, the authors propose a new method for performing artistic style transfer for arbitrary image and styles. The new method employs a cascade/serial architecture for performing the style transfer. The authors test their method using human preference studies. In summary, I found the architecture choice to be minimally explored. More importantly, a vast majority of the results to demonstrate the relative merits of this method were qualitative. The minimal quantitative results were unconvincing and left many unanswered questions about how well one could trust these results. Major Comments: 1. No experiments to explore the architecture hyperparameters. A natural question might be how the quality of the method varies systematically as the number of methods N grows. Presumably, if N=1, this would recover previous methods. 2. Authors are missing an important reference and point of comparison for arbitrary style transfer. Exploring the structure of a real-time, arbitrary neural artistic stylization network Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent Dumoulin, Jonathon Shlens https://arxiv.org/abs/1705.06830 http://goo.gle/2oiDKaT 3. Minimal quantitative analysis. A vast majority of the results (30 of 32 figures) are qualitative comparisons and the paper is sorely lacking an emphasis on quantitative comparisons. This is a large and notable problem in this paper and a quantitative comparison *should* constitute the primary thrust and central result of such a paper to convincingly demonstrate to a reader that the proposed method is indeed to superior to other techniques. I wish the authors dedicated more emphasis in this paper to a detailed quantitative comparison for these methods. As a starter, the analysis presented as the final two appendix figures (31 and 32) should be front and center in the result section of the paper. 4. User study for quantitative comparison is incomplete and unconvincing. Table 1 and Appendix Figure 31 and 32 represent the primary result of this paper as these results and comprise the user studies to quantify how much better this method is to previous methods. These studies however are fairly unconvincing as lots of details are omitted and and I am concerned about the rigor of the human studies including but not limited to: 4a. How long did each human study each image? What controls were added to the study to ensure that all images were equally studied by humans? For instance, were any golden tests employed to ensure user engagement throughout the study? 4b. What was the repeatability of each measurement of preference? If a single human was presented the same image twice, how consistent were there ratings? For that matter, how consistent were the ratings across humans? I presume that some humans preferred some styles over others but how systematic was this? 4c. What types of user testing scenarios were explored to ensure minimal bias in the results? Were multi-choice, paired choice or force choice employed? What about minimal or maximal time limit enforcement? 4d. How can I have confidence that the authors did not cherry pick images and styles that favored their method? For that matter, I would expect that some methods work better on some styles or images. I would expect to see analysis accordingly to break down which styles/images work better on different slices of the data. 4e. The statistical significance of Figure 31 and Figure 32 is not provided. What would an error bar look like with resampling?\n",
      "      - Review:###The idea of the paper is to learn a distance function between observed and the agent’s behaviors. Once they have the distance function, they can learn the agent’s policy efficiently given a single demonstration of each task. In their formulation, the distance function and the policy are jointly learned. The idea is reasonable and the performance outperforms baselines like GAIL and VAE. However, the paper is not-well written with many relevant equations defined in the supplementary material. The unsupervised data labeling part seems Adhoc with many details in the supplementary material. I wonder if the process stable or not. How many lower than the average performance of the proposed method as shown in F.g 4 are caused by unsupervised data labeling? In Fig. 4b, the manual performance is very strong once converged. Although the proposed method initially reaches high reward, after twice many iterations the manual performance even outperforms the proposed method on average many times. Hence, I am not very convinced about the proposed method will be the best-picked method in practice. Overall, I think the idea is good. But the paper is poorly written and I concern the most about the stability of the unsupervised data labeling process. The experimental results are also not super convincing. Hence, I recommend for weak rejection. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The idea of the paper is to learn a distance function between observed and the agent’s behaviors. Once they have the distance function, they can learn the agent’s policy efficiently given a single demonstration of each task. In their formulation, the distance function and the policy are jointly learned. The idea is reasonable and the performance outperforms baselines like GAIL and VAE. However, the paper is not-well written with many relevant equations defined in the supplementary material. The unsupervised data labeling part seems Adhoc with many details in the supplementary material. I wonder if the process stable or not. How many lower than the average performance of the proposed method as shown in F.g 4 are caused by unsupervised data labeling? In Fig. 4b, the manual performance is very strong once converged. Although the proposed method initially reaches high reward, after twice many iterations the manual performance even outperforms the proposed method on average many times. Hence, I am not very convinced about the proposed method will be the best-picked method in practice. Overall, I think the idea is good. But the paper is poorly written and I concern the most about the stability of the unsupervised data labeling process. The experimental results are also not super convincing. Hence, I recommend for weak rejection.\n",
      "      - Review:###The paper describes a method that aims to learn task-agnostic priors for zero-shot generalization. The main idea is to employ the following modeling approach on top of the model-based RL framework: a local convolution network is used to compute a score for each local state action pair, and then another network is used to aggregate all the scores. While the problem being studied is important and the experimental results seem positive, there are a few concerns. First, the baselines presented in the experiments are relatively weak. In Related Work, the authors discuss the differences between the proposed method and the related methods, but few of the related methods are used as baselines for comparison with the proposed method. Moreover, the experiments are quite insufficient in terms of ablating different components of the proposed methods. Second, essentially the proposed method is trying to solve the zero-shot generalization by parameter initialization; a model is pretrained on related tasks and used as initializations for target tasks. The authors claim that it is different from prior work mainly because of the neural architecture that deals with sparse rewards via score aggregation. While the proposed architecture might be more suitable for solving tasks with sparse rewards, it is not intuitive whether it has something to do with learning zero-shot generalization. And apparently, the method will also rely on the similarity between the pretrained task and the target task, and such a scope constraint is not discussed in the paper. In other words, I*m not quite sure a better architecture is fundamental progress towards zero-shot RL. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper describes a method that aims to learn task-agnostic priors for zero-shot generalization. The main idea is to employ the following modeling approach on top of the model-based RL framework: a local convolution network is used to compute a score for each local state action pair, and then another network is used to aggregate all the scores. While the problem being studied is important and the experimental results seem positive, there are a few concerns. First, the baselines presented in the experiments are relatively weak. In Related Work, the authors discuss the differences between the proposed method and the related methods, but few of the related methods are used as baselines for comparison with the proposed method. Moreover, the experiments are quite insufficient in terms of ablating different components of the proposed methods. Second, essentially the proposed method is trying to solve the zero-shot generalization by parameter initialization; a model is pretrained on related tasks and used as initializations for target tasks. The authors claim that it is different from prior work mainly because of the neural architecture that deals with sparse rewards via score aggregation. While the proposed architecture might be more suitable for solving tasks with sparse rewards, it is not intuitive whether it has something to do with learning zero-shot generalization. And apparently, the method will also rely on the similarity between the pretrained task and the target task, and such a scope constraint is not discussed in the paper. In other words, I*m not quite sure a better architecture is fundamental progress towards zero-shot RL.\n",
      "      - Review:###The paper proposes a framework (Scoring-Aggregating-Planning (SAP)) for learning task-agnostic priors that allow generalization to new tasks without finetuning. The motivation for this is very clear - humans can perform much better than machines in zero-shot conditions because humans have learned priors about objects, semantics, physics, etc. This is achieved by learning a scoring function based on the final reward and a self-supervised learned dynamics model. Overall, the paper is very clear and easy to follow. The presented task is realistic and important, and the paper seems to address it in a reasonable approach. However, the evaluation seems lacking to me - the evaluation convinced me that SAP works, but I am not convinced that it works better than existing approaches (see below), and especially did not convince me that it is better in the zero-shot test environment. The (anonymized) website contains nice videos that support the submission. Questions for the authors: 1. Page 3, 3rd paragraph of Section 3: the paper says that *The proposed formulation requires much less information and thus more realistic and feasible* - I agree that this is more realistic, but is it really more feasible? The requirement of much less information makes the proposed formulation much more sparse. 2. A basic assumption in the SAP framework is that a local region score is a sum of all the sub-regions. As phrased in the paper: *in the physical world, there is usually some level of rotational or transnational invariance*. I*m not sure that this assumption makes sense neither in the Mario case or in other tasks, e.g., robotics. Doesn*t it matter if you have a *turtle* right in front of you (which means that the turtle is going to hit you), or below you (which means that you are going hit the turtle)? 3. A question about the planning phase - page 5 says: *We select the action sequence that gives us the best-aggregated score and execute the first action*. Do you select the entire sequence of actions in the new environment in advance? Can the agent observe the new state after every action, and decide on the next action based on the actual step that the action has reached, rather than on the state that was approximated in advance? In other words - what happens if the first action in the new test environment yields an unexpected state, that was not predicted well by the dynamics model; does the agent continue on the initial planned trajectory (that ignores the *surprise*), or does it compute its next action based on the unexpected state? 4. Experiments: in Gridworld and Mario - are there any stronger baselines in the literature, or reductions of known baselines to the zero-shot scenario? Are the chosen *Human Priors*, BC-random and BC-SAP just strawmen? Since the main goal of this paper is the zero-shot task, what would convince me is a state-of-the-art model that does possibly *better than SAP on the training level*, but *worse than SAP in generalizing to the new level*. Additionally, are there other baselines that specifically address the zero-shot task in the literature? Minor (did not impact score): Page 2, 1st paragraph: *... we show that how an intelligent agent*... Page 3, 3rd paragraph: *... in model-free RL problem* - missing an *a* or *problem*s**? Page 3, 3rd paragraph: *. Model based method ...* - missing an *a* as well? Page 4, 1st paragraph:: *... utilizing the to get the ...* Page 4, last row: missing a dot after the loss equation, before the word *In*. Page 7, Table 1: *BC-random* is called *BC-data* in the text. Aren*t they the same thing? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper proposes a framework (Scoring-Aggregating-Planning (SAP)) for learning task-agnostic priors that allow generalization to new tasks without finetuning. The motivation for this is very clear - humans can perform much better than machines in zero-shot conditions because humans have learned priors about objects, semantics, physics, etc. This is achieved by learning a scoring function based on the final reward and a self-supervised learned dynamics model. Overall, the paper is very clear and easy to follow. The presented task is realistic and important, and the paper seems to address it in a reasonable approach. However, the evaluation seems lacking to me - the evaluation convinced me that SAP works, but I am not convinced that it works better than existing approaches (see below), and especially did not convince me that it is better in the zero-shot test environment. The (anonymized) website contains nice videos that support the submission. Questions for the authors: 1. Page 3, 3rd paragraph of Section 3: the paper says that *The proposed formulation requires much less information and thus more realistic and feasible* - I agree that this is more realistic, but is it really more feasible? The requirement of much less information makes the proposed formulation much more sparse. 2. A basic assumption in the SAP framework is that a local region score is a sum of all the sub-regions. As phrased in the paper: *in the physical world, there is usually some level of rotational or transnational invariance*. I*m not sure that this assumption makes sense neither in the Mario case or in other tasks, e.g., robotics. Doesn*t it matter if you have a *turtle* right in front of you (which means that the turtle is going to hit you), or below you (which means that you are going hit the turtle)? 3. A question about the planning phase - page 5 says: *We select the action sequence that gives us the best-aggregated score and execute the first action*. Do you select the entire sequence of actions in the new environment in advance? Can the agent observe the new state after every action, and decide on the next action based on the actual step that the action has reached, rather than on the state that was approximated in advance? In other words - what happens if the first action in the new test environment yields an unexpected state, that was not predicted well by the dynamics model; does the agent continue on the initial planned trajectory (that ignores the *surprise*), or does it compute its next action based on the unexpected state? 4. Experiments: in Gridworld and Mario - are there any stronger baselines in the literature, or reductions of known baselines to the zero-shot scenario? Are the chosen *Human Priors*, BC-random and BC-SAP just strawmen? Since the main goal of this paper is the zero-shot task, what would convince me is a state-of-the-art model that does possibly *better than SAP on the training level*, but *worse than SAP in generalizing to the new level*. Additionally, are there other baselines that specifically address the zero-shot task in the literature? Minor (did not impact score): Page 2, 1st paragraph: *... we show that how an intelligent agent*... Page 3, 3rd paragraph: *... in model-free RL problem* - missing an *a* or *problem*s**? Page 3, 3rd paragraph: *. Model based method ...* - missing an *a* as well? Page 4, 1st paragraph:: *... utilizing the to get the ...* Page 4, last row: missing a dot after the loss equation, before the word *In*. Page 7, Table 1: *BC-random* is called *BC-data* in the text. Aren*t they the same thing?\n",
      "      - Review:###This paper presents an imitation learning method that deploys previously well-studied techniques such as siamese networks, inverse RL, learning distance functions for IRL and tracking. + the paper studies an important problem of IL using visual data. + I found the ablation studies in the appendix quite useful in understanding the efficiency of the proposed method. -In terms of novelty, the proposed approach is a combination of several past works so the technical novelty is limited. Additionally, it is not clear how impactful the proposed method can be given that it is only tested on a synthetic domain which is the same as the train domain. So, from the current experimental results it is not clear if this approach would be effective to be applied in a real system (e.g. robots) on the practical side. -There are not enough evaluation done to compare with the most updated state-of-the-art baselines. The evaluations are done on just a single synthetic domain with a single character. Therefore, the train and test videos are very similar. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper presents an imitation learning method that deploys previously well-studied techniques such as siamese networks, inverse RL, learning distance functions for IRL and tracking. + the paper studies an important problem of IL using visual data. + I found the ablation studies in the appendix quite useful in understanding the efficiency of the proposed method. -In terms of novelty, the proposed approach is a combination of several past works so the technical novelty is limited. Additionally, it is not clear how impactful the proposed method can be given that it is only tested on a synthetic domain which is the same as the train domain. So, from the current experimental results it is not clear if this approach would be effective to be applied in a real system (e.g. robots) on the practical side. -There are not enough evaluation done to compare with the most updated state-of-the-art baselines. The evaluations are done on just a single synthetic domain with a single character. Therefore, the train and test videos are very similar.\n",
      "    - Issues with Baselines, Ablation Studies, or Fair Comparison (L1, 2)\n",
      "      - Review:###In this paper, the authors propose a method to perform architecture search on the number of channels in convolutional layers. The proposed method, called AutoSlim, is a one-shot approach based on previous work of Slimmable Networks [2,3]. The authors have tested the proposed methods on a variety of architectures on ImageNet dataset. The paper is well-written and easy to follow. I really appreciate the authors for structuring this paper so well. I have the following questions: Q1: In figure 4, the authors find that “Compared with default MobileNet v2, our optimized configuration has fewer channels in shallow layers and more channels in deep ones.” This is interesting. Because in network pruning methods, it is found that usually later stages get pruned more [1] (e.g. VGG), indicating that there is more redundancy for deep layers. However, in this case, actually deep layers get more channels than standard models. Is there any justification for this? Is it that more channels in deep layers benefit the accuracy? Q2: In “Training Optimized Networks”, the authors mentioned that “By default we search for the network FLOPs at approximately 200M, 300M and 500M, and train a slimmable model.” Does this mean that the authors train the final optimized models from scratch as a slimmable network using “sandwich rule” and “in-place distillation” rule? Or are the authors just training the final model with standard training schedule? If it is the first case, can the authors justify why? Q3: In Table 1, “Heavy Models”, what is the difference between “ResNet-50” and “He-ResNet-50”? Also, why the params, memory and CPU Latency of some networks are omitted? Q4: In the last paragraph of section 4, the authors tried the transferability of networks learned from ImageNet to CIFAR-10 dataset. I am not sure how the authors transfer the networks from Imagenet to CIFAR-10? Is it the ratio of the number of channels? Can the authors provide the architecture details of MobileNet v2 on CIFAR-10 dataset? Q5: What is the estimated time for a typical run of AutoSlim? How does it compare to network pruning methods or neural architecture search methods? Q6: Can the methods be used to search for the number of neurons in fully connected layers? Are there any results? [1] Rethinking the Value of Network Pruning. Zhuang et al. ICLR 2019 [2] Slimmable neural networks. Yu et al. ICLR 2019. [3] Universally Slimmable Networks and Improved Training Techniques. Yu et al. Arxiv. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###In this paper, the authors propose a method to perform architecture search on the number of channels in convolutional layers. The proposed method, called AutoSlim, is a one-shot approach based on previous work of Slimmable Networks [2,3]. The authors have tested the proposed methods on a variety of architectures on ImageNet dataset. The paper is well-written and easy to follow. I really appreciate the authors for structuring this paper so well. I have the following questions: Q1: In figure 4, the authors find that “Compared with default MobileNet v2, our optimized configuration has fewer channels in shallow layers and more channels in deep ones.” This is interesting. Because in network pruning methods, it is found that usually later stages get pruned more [1] (e.g. VGG), indicating that there is more redundancy for deep layers. However, in this case, actually deep layers get more channels than standard models. Is there any justification for this? Is it that more channels in deep layers benefit the accuracy? Q2: In “Training Optimized Networks”, the authors mentioned that “By default we search for the network FLOPs at approximately 200M, 300M and 500M, and train a slimmable model.” Does this mean that the authors train the final optimized models from scratch as a slimmable network using “sandwich rule” and “in-place distillation” rule? Or are the authors just training the final model with standard training schedule? If it is the first case, can the authors justify why? Q3: In Table 1, “Heavy Models”, what is the difference between “ResNet-50” and “He-ResNet-50”? Also, why the params, memory and CPU Latency of some networks are omitted? Q4: In the last paragraph of section 4, the authors tried the transferability of networks learned from ImageNet to CIFAR-10 dataset. I am not sure how the authors transfer the networks from Imagenet to CIFAR-10? Is it the ratio of the number of channels? Can the authors provide the architecture details of MobileNet v2 on CIFAR-10 dataset? Q5: What is the estimated time for a typical run of AutoSlim? How does it compare to network pruning methods or neural architecture search methods? Q6: Can the methods be used to search for the number of neurons in fully connected layers? Are there any results? [1] Rethinking the Value of Network Pruning. Zhuang et al. ICLR 2019 [2] Slimmable neural networks. Yu et al. ICLR 2019. [3] Universally Slimmable Networks and Improved Training Techniques. Yu et al. Arxiv.\n",
      "      - Review:###The paper questions the conventional wisdom of using explicit regularization methods (e.g., L2, dropout) in training neural networks. The authors compare data augmentation with explicit regularization on several image classification datasets, architectures and amount of data, concluding using data augmentations is enough to reach a on-par performance with using explicit regularization. I do have several concerns about the paper. 1. My most worrying concerns are about the experiments. (1) The ImageNet experimental setting is not that convincing since it follows a different resolution than the literature. The results obtained (e.g., 17% top-5 error) are too far from state-of-the-art. (2) The weight decay and dropout are used together, but not separately studied. In fact, in state-of-the-art CIFAR and ImageNet models, dropout are often not used. Though the reason is that they already use data augmentations, so dropout is typically no longer helpful (WRN is an exception). I think L2 alone is more worth studying, since it is probably known that dropout doesn*t help upon a conventional augmentation. (3) The hyperparameters used for WD+dropout in the experiments are *as specified in the original papers*. But the original papers assume the conventional data augmentation. If you use new data augmentation schemes (light/heavier), the regularization hyperparameters should be tuned accordingly. I believe if the strengths of weight decay is properly tuned then it should help even with data augmentation, by a noticeable margin. (4) If we only see WRN and DenseNet on CIFAR datasets (All-CNN is probably outdated and performs poorly, and ImageNet is not convincing as said in (1)), we notice that actually WD+dropout do provides a small increase on the augmentation schemes. This does not support the main claim of the paper. (5) More experiments on other domains (e.g., NLP) can be used to strengthen the paper, since the title does not specify a modality. 2. *Explicit regularization techniques ... they blindly reduce the effective capacity of the model, introduce sensitive hyper-parameters and require deeper and wider architectures to compensate for the reduced capacity*. I cannot agree the regularization schemes just *blindly* reduce the capacity. Take L2 weight decay as example, it does not reduce the theoretical representation power of the network, all it does is to encourage simpler solutions. Also, data augmentation schemes involve a lot of hyper-parameters too, and possibly requires deeper/wider architectures to fully exploit its advantage. In my opinion, data augmentation does not solve the possible inconvenience brought by explicit regularizations. 3.The definitions of explicit and implicit regularization in Section 2 a bit vague. Under this two definitions, I can see dropout actually falls in both categories, despite slightly more similar to the explicit one. On one hand it is specifically restricting the model*s capacity by sampling a smaller model in each iteration, and on the other hand it also changes *the learning algorithm* and *characteristics of the network architecture*. Similar thing holds for *Stochastic Depth*. Also, injecting noise in intermediate activations is very similar to dropout since dropout is actually injecting noise by randomly removing a portion of the activations. However I can see under these two definitions injecting noise is implicit while dropout is implicit. I think it helps to list at least 5 or 6 examples for each category right there. In summary, the claims are not well supported by the experiments, and I tend to reject the paper. -------------------------------------------------------------- I appreciate the detailed author response and have some quick comments: I couldn*t agree with the arguments about hyperparameters of DA. They are meaningful but their influence to the network training still needs to be tuned, possibly for different architectures, to best optimize performance. It is also not justified that *increasing resolution* won*t help in this case since your top-5 error is a bit too high compared with recent results. Especially for a strong argument in the title I would expect a more standard setting evaluated. The DenseNet paper only uses dropout in absence of data augmentation, and in my personal experience if DA is used dropout is not helpful anymore, for ResNet as well. In standard ImageNet augmentation scheme, no one uses dropout in popular models of recent years (ResNet, DenseNet, SENet, etc.) but all use weight decay. I*m happy with some other parts of the response, e.g., about the title, update of definitions. Some other points seem more like opinions and I might have a different opinion from the authors, e.g., whether weight decay constrains hypothesis space. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper questions the conventional wisdom of using explicit regularization methods (e.g., L2, dropout) in training neural networks. The authors compare data augmentation with explicit regularization on several image classification datasets, architectures and amount of data, concluding using data augmentations is enough to reach a on-par performance with using explicit regularization. I do have several concerns about the paper. 1. My most worrying concerns are about the experiments. (1) The ImageNet experimental setting is not that convincing since it follows a different resolution than the literature. The results obtained (e.g., 17% top-5 error) are too far from state-of-the-art. (2) The weight decay and dropout are used together, but not separately studied. In fact, in state-of-the-art CIFAR and ImageNet models, dropout are often not used. Though the reason is that they already use data augmentations, so dropout is typically no longer helpful (WRN is an exception). I think L2 alone is more worth studying, since it is probably known that dropout doesn*t help upon a conventional augmentation. (3) The hyperparameters used for WD+dropout in the experiments are *as specified in the original papers*. But the original papers assume the conventional data augmentation. If you use new data augmentation schemes (light/heavier), the regularization hyperparameters should be tuned accordingly. I believe if the strengths of weight decay is properly tuned then it should help even with data augmentation, by a noticeable margin. (4) If we only see WRN and DenseNet on CIFAR datasets (All-CNN is probably outdated and performs poorly, and ImageNet is not convincing as said in (1)), we notice that actually WD+dropout do provides a small increase on the augmentation schemes. This does not support the main claim of the paper. (5) More experiments on other domains (e.g., NLP) can be used to strengthen the paper, since the title does not specify a modality. 2. *Explicit regularization techniques ... they blindly reduce the effective capacity of the model, introduce sensitive hyper-parameters and require deeper and wider architectures to compensate for the reduced capacity*. I cannot agree the regularization schemes just *blindly* reduce the capacity. Take L2 weight decay as example, it does not reduce the theoretical representation power of the network, all it does is to encourage simpler solutions. Also, data augmentation schemes involve a lot of hyper-parameters too, and possibly requires deeper/wider architectures to fully exploit its advantage. In my opinion, data augmentation does not solve the possible inconvenience brought by explicit regularizations. 3.The definitions of explicit and implicit regularization in Section 2 a bit vague. Under this two definitions, I can see dropout actually falls in both categories, despite slightly more similar to the explicit one. On one hand it is specifically restricting the model*s capacity by sampling a smaller model in each iteration, and on the other hand it also changes *the learning algorithm* and *characteristics of the network architecture*. Similar thing holds for *Stochastic Depth*. Also, injecting noise in intermediate activations is very similar to dropout since dropout is actually injecting noise by randomly removing a portion of the activations. However I can see under these two definitions injecting noise is implicit while dropout is implicit. I think it helps to list at least 5 or 6 examples for each category right there. In summary, the claims are not well supported by the experiments, and I tend to reject the paper. -------------------------------------------------------------- I appreciate the detailed author response and have some quick comments: I couldn*t agree with the arguments about hyperparameters of DA. They are meaningful but their influence to the network training still needs to be tuned, possibly for different architectures, to best optimize performance. It is also not justified that *increasing resolution* won*t help in this case since your top-5 error is a bit too high compared with recent results. Especially for a strong argument in the title I would expect a more standard setting evaluated. The DenseNet paper only uses dropout in absence of data augmentation, and in my personal experience if DA is used dropout is not helpful anymore, for ResNet as well. In standard ImageNet augmentation scheme, no one uses dropout in popular models of recent years (ResNet, DenseNet, SENet, etc.) but all use weight decay. I*m happy with some other parts of the response, e.g., about the title, update of definitions. Some other points seem more like opinions and I might have a different opinion from the authors, e.g., whether weight decay constrains hypothesis space.\n",
      "    - Unclear or Problematic Comparison to Baselines (L1, 11)\n",
      "      -  Summary: The authors first analyze and answer the question: What training examples should be focused and how large the emphasis spread should be? Then, they proposed the gradient rescaling framework serving as emphasis regularization. Strengths: 1. The paper is well organized except the reference citation (read difficultly) 2. The proposed method is very simple and effective. 3. Experiments show the improvements over SOTA. Weakness: 1. The experiments lack the recent important baseline *symmetric cross entropy for robust learning with noisy labels, ICCV2019*, which are the current SOTA. Maybe the author should check the above paper and show the results. 2. The experiments are only conducted on symmetric noise. Actually, asymmetric noise is also important. The author should conduct at least some experiments on asymmetric noise. (L0, 1)\n",
      "        * Examples:\n",
      "          -  Summary: The authors first analyze and answer the question: What training examples should be focused and how large the emphasis spread should be? Then, they proposed the gradient rescaling framework serving as emphasis regularization. Strengths: 1. The paper is well organized except the reference citation (read difficultly) 2. The proposed method is very simple and effective. 3. Experiments show the improvements over SOTA. Weakness: 1. The experiments lack the recent important baseline *symmetric cross entropy for robust learning with noisy labels, ICCV2019*, which are the current SOTA. Maybe the author should check the above paper and show the results. 2. The experiments are only conducted on symmetric noise. Actually, asymmetric noise is also important. The author should conduct at least some experiments on asymmetric noise.\n",
      "      -  The paper proposes to study how early stopping in optimization helps find confident examples. Overall, the paper is well-organized and easy to read. Although there is some parallel study regarding the theoretical aspect of how early stopping help finds confident examples (i.e., Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks, which has unfortunately not been cited), the paper focuses on the empirical perspective. A thorough empirical study illustrating how early stop works would interest the label noise community. The authors claim that early stopping is efficient to find a maximal safe set. I think it would be necessary to illustrate the maximal safe set for all the datasets. The authors only did this for one case of CIFAR-100, which is not convincing. The small loss based learning has shown the confident examples extracted. It would be essential to compare the proposed method with those methods. The authors are suggested to compare the proposed method with more baselines. There are lots of algorithms exploiting the transition matrix and with statistically consistent estimators. The authors have ignored all those methods. It is unclear from the paper that if the baselines have used the clean validation sets. For fair comparison, those clean data should be used in the training procedures of the baselines. Clothing1M is a more challenging dataset with real-world label noise. The dataset also includes some clean data for validation use. The authors should verify the effectiveness of the proposed method on this dataset. The authors are suggested to make it clear why noise rates are sometimes available for use. (L0, 1)\n",
      "        * Examples:\n",
      "          -  The paper proposes to study how early stopping in optimization helps find confident examples. Overall, the paper is well-organized and easy to read. Although there is some parallel study regarding the theoretical aspect of how early stopping help finds confident examples (i.e., Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks, which has unfortunately not been cited), the paper focuses on the empirical perspective. A thorough empirical study illustrating how early stop works would interest the label noise community. The authors claim that early stopping is efficient to find a maximal safe set. I think it would be necessary to illustrate the maximal safe set for all the datasets. The authors only did this for one case of CIFAR-100, which is not convincing. The small loss based learning has shown the confident examples extracted. It would be essential to compare the proposed method with those methods. The authors are suggested to compare the proposed method with more baselines. There are lots of algorithms exploiting the transition matrix and with statistically consistent estimators. The authors have ignored all those methods. It is unclear from the paper that if the baselines have used the clean validation sets. For fair comparison, those clean data should be used in the training procedures of the baselines. Clothing1M is a more challenging dataset with real-world label noise. The dataset also includes some clean data for validation use. The authors should verify the effectiveness of the proposed method on this dataset. The authors are suggested to make it clear why noise rates are sometimes available for use.\n",
      "      -  This article proposes a method for object counting which can be trained with weak supervision. Object counting methods are often trained with point annotations, i.e., one click-point per object. In this article, a weaker way of annotation is used: count-based annotation, i.e., the number of objects of each class present in the image are given as annotation but no precise location of the objects. This article is an extension of density-based object counting methods for weakly supervision. This article analyzes the effect of pooling layers on WSL. The findings indicate that the pooling layer constrained the objects* locations to a predefined grid and accordingly they remove it. When removing this effect is alleviated. Moreover, the authors realized that a sift on the predictions also happen. Accordingly, they learn rotated versions of the model for centering the predictions. As a countereffect, several points per object are detected, and then a gini impurity regularization is proposed to reduce the number of detected objects. Then, the authors connect this formulation with VAE. The experiments are conducted on a toy example, i.e., circle finding, and on the well established Mall dataset. Main concerns: The experimental section should make experiments on standard datasets (i.e., USCD, Trancos, Mall, PkLot, Shangai, Penguins) using standard evaluation protocols (MAE, GAME). The current evaluation only shows the qualitative results on one frame of the Mall dataset. The results of the proposed method should be compared with baselines. For instance, Glance method, which is only trained with count-level annotation, and with *C-WSL: Count-guided Weakly Supervised Localization*. It should be also compared with WSL methods based on image-level labels such as *Object Counting and Instance Segmentation with Image-level Supervision* or *Where are the Masks: Instance Segmentation with Image-level Supervision* and then with methods that use point supervision as an upper bound such as density method or Where are the blobs. Moreover, some ablation studies about the effect of each component of the model would be needed. Regarding the model I have some concerns: - Removing the pooling layers does not seem a very good idea. This will remove part of the ability to detect objects at multiple scales and moreover will increase its computational power. Maybe for easy cases as circles and heads of mall people where the resolution changes are small could work. But not for more challenging datasets. - The rotation of the model could be ok for circles or heads. But making the model to learn rotation-invariant features could be challenging in more difficult datasets. - Eq.5 and 8 which are used in the model seem to use point supervision and not just the count based. Which makes the method not a weakly supervised one. Are these equations the ones used or others? (L0, 1)\n",
      "        * Examples:\n",
      "          -  This article proposes a method for object counting which can be trained with weak supervision. Object counting methods are often trained with point annotations, i.e., one click-point per object. In this article, a weaker way of annotation is used: count-based annotation, i.e., the number of objects of each class present in the image are given as annotation but no precise location of the objects. This article is an extension of density-based object counting methods for weakly supervision. This article analyzes the effect of pooling layers on WSL. The findings indicate that the pooling layer constrained the objects* locations to a predefined grid and accordingly they remove it. When removing this effect is alleviated. Moreover, the authors realized that a sift on the predictions also happen. Accordingly, they learn rotated versions of the model for centering the predictions. As a countereffect, several points per object are detected, and then a gini impurity regularization is proposed to reduce the number of detected objects. Then, the authors connect this formulation with VAE. The experiments are conducted on a toy example, i.e., circle finding, and on the well established Mall dataset. Main concerns: The experimental section should make experiments on standard datasets (i.e., USCD, Trancos, Mall, PkLot, Shangai, Penguins) using standard evaluation protocols (MAE, GAME). The current evaluation only shows the qualitative results on one frame of the Mall dataset. The results of the proposed method should be compared with baselines. For instance, Glance method, which is only trained with count-level annotation, and with *C-WSL: Count-guided Weakly Supervised Localization*. It should be also compared with WSL methods based on image-level labels such as *Object Counting and Instance Segmentation with Image-level Supervision* or *Where are the Masks: Instance Segmentation with Image-level Supervision* and then with methods that use point supervision as an upper bound such as density method or Where are the blobs. Moreover, some ablation studies about the effect of each component of the model would be needed. Regarding the model I have some concerns: - Removing the pooling layers does not seem a very good idea. This will remove part of the ability to detect objects at multiple scales and moreover will increase its computational power. Maybe for easy cases as circles and heads of mall people where the resolution changes are small could work. But not for more challenging datasets. - The rotation of the model could be ok for circles or heads. But making the model to learn rotation-invariant features could be challenging in more difficult datasets. - Eq.5 and 8 which are used in the model seem to use point supervision and not just the count based. Which makes the method not a weakly supervised one. Are these equations the ones used or others?\n",
      "      -  This paper introduces an anchor-free object detection framework that aims at simultaneously predicting the object position and the corresponding boundary. To achieve this, the proposed FoveaBox detector predicts category-sensitive semantic maps for the object existing possibility, and produces category-agnostic bounding box for each position that is likely to contain an object. The scales of target boxes are associated with feature pyramid representations. Experiments are performed on MS COCO detection benchmark. Pros: The proposed approach is simple and is shown to avoid most computation and hyper-parameters related to anchor boxes. The paper is well written and easy to follow. Cons: The main issue with the paper is the main idea is similar to [1,2, 3, 4, 5]. For instance, CenterNet also represents each object instance by its features at the center point and achieves similar detection performance compared to the proposed detector. Further, no speed comparison with these approaches is provided in the paper. Without a fair speed comparison and with similar detection performance, it is difficult to fully assess the merits of the proposed approach. Though inference speed comparison is reported with RetineNet. However, a proper and detailed comparison with [1, 2, 3, 4, 5] is missing. 1: Xingyi Zhou, Dequan Wang, Philipp Krähenbühl: Objects as Points. CoRR abs/1904.07850 (2019). 2: Xingyi Zhou, Jiacheng Zhuo, Philipp Krähenbühl: Bottom-Up Object Detection by Grouping Extreme and Center Points. CVPR 2019. 3: Zhi Tian, Chunhua Shen, Hao Chen, Tong He: FCOS: Fully Convolutional One-Stage Object Detection. CoRR abs/1904.01355 (2019). 4: Hei Law and Jia Deng: Cornernet: Detecting objects as paired keypoints. ECCV 2018. 5: Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, Qi Tian: CenterNet: Keypoint Triplets for Object Detection. CoRR abs/1904.08189 (2019). (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper introduces an anchor-free object detection framework that aims at simultaneously predicting the object position and the corresponding boundary. To achieve this, the proposed FoveaBox detector predicts category-sensitive semantic maps for the object existing possibility, and produces category-agnostic bounding box for each position that is likely to contain an object. The scales of target boxes are associated with feature pyramid representations. Experiments are performed on MS COCO detection benchmark. Pros: The proposed approach is simple and is shown to avoid most computation and hyper-parameters related to anchor boxes. The paper is well written and easy to follow. Cons: The main issue with the paper is the main idea is similar to [1,2, 3, 4, 5]. For instance, CenterNet also represents each object instance by its features at the center point and achieves similar detection performance compared to the proposed detector. Further, no speed comparison with these approaches is provided in the paper. Without a fair speed comparison and with similar detection performance, it is difficult to fully assess the merits of the proposed approach. Though inference speed comparison is reported with RetineNet. However, a proper and detailed comparison with [1, 2, 3, 4, 5] is missing. 1: Xingyi Zhou, Dequan Wang, Philipp Krähenbühl: Objects as Points. CoRR abs/1904.07850 (2019). 2: Xingyi Zhou, Jiacheng Zhuo, Philipp Krähenbühl: Bottom-Up Object Detection by Grouping Extreme and Center Points. CVPR 2019. 3: Zhi Tian, Chunhua Shen, Hao Chen, Tong He: FCOS: Fully Convolutional One-Stage Object Detection. CoRR abs/1904.01355 (2019). 4: Hei Law and Jia Deng: Cornernet: Detecting objects as paired keypoints. ECCV 2018. 5: Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, Qi Tian: CenterNet: Keypoint Triplets for Object Detection. CoRR abs/1904.08189 (2019).\n",
      "      -  This paper proposed an interesting idea on how we can leverage the lead bias in summarization datasets to pretrain abstractive news summarization models on large-scale unlabelled corpus in simple and effective way. For pre-training, they collected three years of online news articles data. Then, they take the top 3 sentences of the article as summary and the rest of the article as input document. For better choosing such article-summary pairs, they employ effective data cleaning and filtering process. Overall, they collected 21.4M articles for the pretraining. Overall, the pretrained model does decent on three summarization datasets without any fine-tuning. After fine-tuning the respective datasets, the gains seem significant. Especially on the XSum dataset, the improvements are remarkable. I believe that the idea is interesting but the experiments are incomplete and more investigation is required to make this paper stronger. Therefore I suggest to reject this paper. Arguments: 1) The important experiments that are missing in this paper are evaluating the proposed method on better human written summarization datasets -- DUC. The real world summarizations resemble more like the ones in the DUC dataset and it would be interesting to see if the transfer results of the pretrained model on the DUC datasets. The important question is to understand whether the pretrained model which took advantage of lead-bias could achieve good summaries on real summarization samples. This would also answer whether the pretrained just took advantage of the lead-bias issue of many large summarization datasets or does it really learn good summarization model. 2) This paper has good idea but mainly missing ablation studies. For example, how does the proposed model do compared with GPT-2 in the fine-tuning setting, and how do these two models perform on the DUC datasets. 3) During the dataset filtering/collection, a check on the quality of the filtering process by doing a small human study would have been a great addition. Also, instead of showing the output examples (which can go in the supplementary), human study comparing the quality of the pretrained model with fine-tuning and a baseline (can be from previous work) would have been better. Other minor questions 1) “we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest” -- is there any reason on fixing to these numbers? How did you make this decision ? 2) Even though the performance gains look visibly significant, I would suggest to report the statistical significance scores. (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper proposed an interesting idea on how we can leverage the lead bias in summarization datasets to pretrain abstractive news summarization models on large-scale unlabelled corpus in simple and effective way. For pre-training, they collected three years of online news articles data. Then, they take the top 3 sentences of the article as summary and the rest of the article as input document. For better choosing such article-summary pairs, they employ effective data cleaning and filtering process. Overall, they collected 21.4M articles for the pretraining. Overall, the pretrained model does decent on three summarization datasets without any fine-tuning. After fine-tuning the respective datasets, the gains seem significant. Especially on the XSum dataset, the improvements are remarkable. I believe that the idea is interesting but the experiments are incomplete and more investigation is required to make this paper stronger. Therefore I suggest to reject this paper. Arguments: 1) The important experiments that are missing in this paper are evaluating the proposed method on better human written summarization datasets -- DUC. The real world summarizations resemble more like the ones in the DUC dataset and it would be interesting to see if the transfer results of the pretrained model on the DUC datasets. The important question is to understand whether the pretrained model which took advantage of lead-bias could achieve good summaries on real summarization samples. This would also answer whether the pretrained just took advantage of the lead-bias issue of many large summarization datasets or does it really learn good summarization model. 2) This paper has good idea but mainly missing ablation studies. For example, how does the proposed model do compared with GPT-2 in the fine-tuning setting, and how do these two models perform on the DUC datasets. 3) During the dataset filtering/collection, a check on the quality of the filtering process by doing a small human study would have been a great addition. Also, instead of showing the output examples (which can go in the supplementary), human study comparing the quality of the pretrained model with fine-tuning and a baseline (can be from previous work) would have been better. Other minor questions 1) “we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest” -- is there any reason on fixing to these numbers? How did you make this decision ? 2) Even though the performance gains look visibly significant, I would suggest to report the statistical significance scores.\n",
      "      -  This paper proposes Recency Bias, an adaptive mini batch selection method for training deep neural networks. To select informative minibatches for training, the proposed method maintains a fixed size sliding window of past model predictions for each data sample. At a given iteration, samples which have highly inconsistent predictions within the sliding window are added to the minibatch. The main contribution of this paper is the introduction of sliding window to remember past model predictions, as an improvement over the SOTA approach: Active Bias, which maintains a growing window of model predictions. Empirical studies are performed to show the superiority of Recency Bias over two SOTA approaches. Results are shown on the task of (1) image classification from scratch and (2) image classification by fine-tuning pretrained networks. +ves: + The idea of using a sliding window over a growing window in active batch selection is interesting. + Overall, the paper is well written. In particular, the Related Work section has a nice flow and puts the proposed method into context. Despite the method having limited novelty (sliding window instead of a growing window), the method has been well motivated by pointing out the limitations in SOTA methods. + The results section is well structured. It*s nice to see hyperparameter tuning results; and loss convergence graphs in various learning settings for each dataset. Concerns: - The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. Despite the paper stating that there have been earlier work (Joseph et al, 2019 and Wang et al, 2019) that attempt mini-batch selection, the paper does not compare with them. This is limiting. Further, since the proposed method is not specific to the domain of images, evaluating it on tasks other than image classification, such as text classification for instance, would have helped validate its applicability across domains. - Considering the limited results, a deeper analysis of the proposed method would have been nice. The idea of a sliding window over a growing window is a generic one, and there have been many efforts to theoretically analyze active learning over the last two decades. How does the proposed method fit in there? (For e.g., how does the expected model variance change in this setting?) Some form of theoretical/analytical reasoning behind the effectiveness of recency bias (which is missing) would provide greater insights to the community and facilitate further research in this direction. - The claim of 20.5% reduction in test error mentioned in the abstract has not been clearly addressed and pointed out in the results section of the paper. - On the same note, the results are not conclusively in favor of the proposed method, and only is marginally better than the competitors. Why does online batch perform consistently than the proposed method? There is no discussion of these inferences from the results. - The results would have been more complete if results were shown in a setting where just recency bias is used without the use of the selection pressure parameter. In other words, an ablation study on the effect of the selection pressure parameter would have been very useful. - How important is the warm-up phase to the proposed method? Considering the paper states that this is required to get good estimates of the quantization index of the samples, some ablation studies on reducing/increasing the warm-up phase and showing the results would have been useful to understand this. - Fig 4: Why are there sharp dips periodically in all the graphs? What do these correspond to? - The intuition behind the method is described well, however, the proposed method would have been really solidified if it were analysed in the context of a simple machine learning problem (such as logistic regression). As an example, verifying if the chosen minibatch samples are actually close to the decision boundary of a model (even if the model is very simple) would have helped analyze the proposed method well. Minor comments: * It would have been nice to see the relation between the effect of using recency bias and the difficulty of the task/dataset. * In the 2nd line in Introduction, it should be *deep networks* instead of *deep networks netowrks*. * Since both tasks in the experiments are about image classification, it would be a little misleading to present them as *image classification* and *finetuning*. A more informative way of titling them would be *image classification from scratch* and *image classification by finetuning*. * In Section 3.1, in the LHS of equation 3, it would be appropriate to use P(y_i/x_i; q) instead of P(y/x_i; q) since the former term was used in the paragraph. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Some of my queries were clarified. However, unfortunately, I still think more needs to be done to explain the consistency of the results and to study the generalizability of this work across datasets. I retain my original decision for these reasons. (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper proposes Recency Bias, an adaptive mini batch selection method for training deep neural networks. To select informative minibatches for training, the proposed method maintains a fixed size sliding window of past model predictions for each data sample. At a given iteration, samples which have highly inconsistent predictions within the sliding window are added to the minibatch. The main contribution of this paper is the introduction of sliding window to remember past model predictions, as an improvement over the SOTA approach: Active Bias, which maintains a growing window of model predictions. Empirical studies are performed to show the superiority of Recency Bias over two SOTA approaches. Results are shown on the task of (1) image classification from scratch and (2) image classification by fine-tuning pretrained networks. +ves: + The idea of using a sliding window over a growing window in active batch selection is interesting. + Overall, the paper is well written. In particular, the Related Work section has a nice flow and puts the proposed method into context. Despite the method having limited novelty (sliding window instead of a growing window), the method has been well motivated by pointing out the limitations in SOTA methods. + The results section is well structured. It*s nice to see hyperparameter tuning results; and loss convergence graphs in various learning settings for each dataset. Concerns: - The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. Despite the paper stating that there have been earlier work (Joseph et al, 2019 and Wang et al, 2019) that attempt mini-batch selection, the paper does not compare with them. This is limiting. Further, since the proposed method is not specific to the domain of images, evaluating it on tasks other than image classification, such as text classification for instance, would have helped validate its applicability across domains. - Considering the limited results, a deeper analysis of the proposed method would have been nice. The idea of a sliding window over a growing window is a generic one, and there have been many efforts to theoretically analyze active learning over the last two decades. How does the proposed method fit in there? (For e.g., how does the expected model variance change in this setting?) Some form of theoretical/analytical reasoning behind the effectiveness of recency bias (which is missing) would provide greater insights to the community and facilitate further research in this direction. - The claim of 20.5% reduction in test error mentioned in the abstract has not been clearly addressed and pointed out in the results section of the paper. - On the same note, the results are not conclusively in favor of the proposed method, and only is marginally better than the competitors. Why does online batch perform consistently than the proposed method? There is no discussion of these inferences from the results. - The results would have been more complete if results were shown in a setting where just recency bias is used without the use of the selection pressure parameter. In other words, an ablation study on the effect of the selection pressure parameter would have been very useful. - How important is the warm-up phase to the proposed method? Considering the paper states that this is required to get good estimates of the quantization index of the samples, some ablation studies on reducing/increasing the warm-up phase and showing the results would have been useful to understand this. - Fig 4: Why are there sharp dips periodically in all the graphs? What do these correspond to? - The intuition behind the method is described well, however, the proposed method would have been really solidified if it were analysed in the context of a simple machine learning problem (such as logistic regression). As an example, verifying if the chosen minibatch samples are actually close to the decision boundary of a model (even if the model is very simple) would have helped analyze the proposed method well. Minor comments: * It would have been nice to see the relation between the effect of using recency bias and the difficulty of the task/dataset. * In the 2nd line in Introduction, it should be *deep networks* instead of *deep networks netowrks*. * Since both tasks in the experiments are about image classification, it would be a little misleading to present them as *image classification* and *finetuning*. A more informative way of titling them would be *image classification from scratch* and *image classification by finetuning*. * In Section 3.1, in the LHS of equation 3, it would be appropriate to use P(y_i/x_i; q) instead of P(y/x_i; q) since the former term was used in the paragraph. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Some of my queries were clarified. However, unfortunately, I still think more needs to be done to explain the consistency of the results and to study the generalizability of this work across datasets. I retain my original decision for these reasons.\n",
      "      - Review:#### Summary The paper shows that good old reward weighted regression (RWR) with value-function baseline is still state-of-the-art algorithm. # Decision The paper is well-written and provides many evaluations. The contribution should be articulated more carefully, though, taking into account that most algorithmic ideas are present in prior work (https://openreview.net/forum?id=H1gdF34FvS&noteId=Bkxi11nsdr). Perhaps, the experience replay part is somewhat novel. It seems emphasizing more that the aim is to show that simple methods are competitive rather than focusing on novelty could be a good idea. Provided that the authors incorporate the feedback of the other reviewers and update the paper accordingly, it will make a good contribution. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:#### Summary The paper shows that good old reward weighted regression (RWR) with value-function baseline is still state-of-the-art algorithm. # Decision The paper is well-written and provides many evaluations. The contribution should be articulated more carefully, though, taking into account that most algorithmic ideas are present in prior work (https://openreview.net/forum?id=H1gdF34FvS&noteId=Bkxi11nsdr). Perhaps, the experience replay part is somewhat novel. It seems emphasizing more that the aim is to show that simple methods are competitive rather than focusing on novelty could be a good idea. Provided that the authors incorporate the feedback of the other reviewers and update the paper accordingly, it will make a good contribution.\n",
      "      - Review:###For news article it has been know since long that the LEAD baseline is a tough-to-beat competitor. This paper proposes to use this knowledge as self-supervision for training summarization models. For this the author download and clean 3 years of news articles and use this to (pre-)train a Tranformer model. This alone already provides a competitive baseline, which is greatly improved by fine-tuning it on 3 different data-sets. While the data-set can probably not be released, it would be very helpful to have the model available for reproductivity and benchmarking. The paper is clear and well-written. Section 4 I believe is very redundant for an ICLR audience and could be moved to the appendix, making space for a more detailed analysis. One criticism is that the paper is light: the author show that a simple idea works (this is a compliment), but I would have expected to have used the remaining space for ablation studies or a discussion on where this leads. One important point which I would like to see before recommending acceptance is a comparison to know if what is helping is just more data, or the summarization objective. Using lots of more data beats all those numbers (see BERTSUM paper, Liu & Lapata 2019). The comparison I am missing is training BERT on your crawled data-set, and use that for BERTSUM (the code is available). If that helps as much as the summarization pre-training then it would be disappointing but a nice result in favor of language modeling. If not, then it is a strong support for your idea. Two other points which should at least be discussed, as it gives the impression of cherry-picking results instead: 1/ Table 1 is recall; Table 2&3 F1. Why? 2/ The parameters of fine-tuning of the appendix vary wildly depending on the data-set (in particular, the difference in the width of the beam search is striking). Was this optimized on test-data? What is the sensitivity of the summaries to this? I do not understand the last two sentences of Sect 4 (*A candidate word leading...). Could you explain? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###For news article it has been know since long that the LEAD baseline is a tough-to-beat competitor. This paper proposes to use this knowledge as self-supervision for training summarization models. For this the author download and clean 3 years of news articles and use this to (pre-)train a Tranformer model. This alone already provides a competitive baseline, which is greatly improved by fine-tuning it on 3 different data-sets. While the data-set can probably not be released, it would be very helpful to have the model available for reproductivity and benchmarking. The paper is clear and well-written. Section 4 I believe is very redundant for an ICLR audience and could be moved to the appendix, making space for a more detailed analysis. One criticism is that the paper is light: the author show that a simple idea works (this is a compliment), but I would have expected to have used the remaining space for ablation studies or a discussion on where this leads. One important point which I would like to see before recommending acceptance is a comparison to know if what is helping is just more data, or the summarization objective. Using lots of more data beats all those numbers (see BERTSUM paper, Liu & Lapata 2019). The comparison I am missing is training BERT on your crawled data-set, and use that for BERTSUM (the code is available). If that helps as much as the summarization pre-training then it would be disappointing but a nice result in favor of language modeling. If not, then it is a strong support for your idea. Two other points which should at least be discussed, as it gives the impression of cherry-picking results instead: 1/ Table 1 is recall; Table 2&3 F1. Why? 2/ The parameters of fine-tuning of the appendix vary wildly depending on the data-set (in particular, the difference in the width of the beam search is striking). Was this optimized on test-data? What is the sensitivity of the summaries to this? I do not understand the last two sentences of Sect 4 (*A candidate word leading...). Could you explain?\n",
      "      - Review:###The main contribution of the paper is the extension of techniques for weakly supervised localization, i.e. given ground truth counts of objects in a given image, one can do training to generate hidden layer density maps that allow for feature detection and localization of objects. The main contribution of the paper seems to be the regularization of the density map by incorporation of a Gini impurity penalty and the contrasting of the regularizer against beta-variational autoencoder formulation. The experiments show the utility of the method in a toy example and an example involving pedestrians in video surveillance. While the extensions are interesting, I find that the paper contributions are incremental and do not offer sufficient experimental results. The experiments are qualitative (just illustrations of results on a few images). While I understand the context of the contribution from authors point of view and can appreciate the study of weakly-supervised learning, I find it important to ask how the present framework compares with a traditional method that uses background subtraction (thus gaining appearance invariance to an extent) followed by geometric prior driven indexing of counts and locations (see for example: Lan Dong et al (ICCV 2007 - Fast Crowd Segmentation via Shape Indexing). The inductive bias and interpretability of design is extremely transparent and obvious. Can you please elaborate how your work compares in performance to such a technique and what you mean by feature interpretability in your context? My rating is based on the fact that the results are preliminary and further quantitative experimentation is needed. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The main contribution of the paper is the extension of techniques for weakly supervised localization, i.e. given ground truth counts of objects in a given image, one can do training to generate hidden layer density maps that allow for feature detection and localization of objects. The main contribution of the paper seems to be the regularization of the density map by incorporation of a Gini impurity penalty and the contrasting of the regularizer against beta-variational autoencoder formulation. The experiments show the utility of the method in a toy example and an example involving pedestrians in video surveillance. While the extensions are interesting, I find that the paper contributions are incremental and do not offer sufficient experimental results. The experiments are qualitative (just illustrations of results on a few images). While I understand the context of the contribution from authors point of view and can appreciate the study of weakly-supervised learning, I find it important to ask how the present framework compares with a traditional method that uses background subtraction (thus gaining appearance invariance to an extent) followed by geometric prior driven indexing of counts and locations (see for example: Lan Dong et al (ICCV 2007 - Fast Crowd Segmentation via Shape Indexing). The inductive bias and interpretability of design is extremely transparent and obvious. Can you please elaborate how your work compares in performance to such a technique and what you mean by feature interpretability in your context? My rating is based on the fact that the results are preliminary and further quantitative experimentation is needed.\n",
      "      - Review:###The paper presents a regularization technique for conditional density estimation. The method is simple: adding noise to the data points, and training on the noisy data points. The paper also further gives an interpretation of the method, as a form of smoothing the curvature of the density function. It further proves the consistency of the method. Pros: (i) The paper is well written. (ii) The method is simple to implement. (iii) The authors demonstrate a clear intuition of what the method does, i.e., as a form of smoothing. Cons: (i) The method itself is not novel. Adding noise to for regularization is a quite common technique used in many different applications. (ii) The experiments performed in the paper are all very small scaled. Not very convincing. (iii) Continuing the last point, it is unclear whether such technique can be scaled up. The method in another view is to replace the empirical distribution with a kernel density estimate. We know kernel methods don*t scale. It*s hence questionable whether the method can give any benefits in large scale dataset. (iv) If the use of noise regularization is to smooth the density function, how does it compare to methods that enforces Lipschitz continuity? e.g., gradient penalty, or construct a lipschtz constrained network such as in [1]. [1] Sorting out lipschitz function approximation. Anil et al. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper presents a regularization technique for conditional density estimation. The method is simple: adding noise to the data points, and training on the noisy data points. The paper also further gives an interpretation of the method, as a form of smoothing the curvature of the density function. It further proves the consistency of the method. Pros: (i) The paper is well written. (ii) The method is simple to implement. (iii) The authors demonstrate a clear intuition of what the method does, i.e., as a form of smoothing. Cons: (i) The method itself is not novel. Adding noise to for regularization is a quite common technique used in many different applications. (ii) The experiments performed in the paper are all very small scaled. Not very convincing. (iii) Continuing the last point, it is unclear whether such technique can be scaled up. The method in another view is to replace the empirical distribution with a kernel density estimate. We know kernel methods don*t scale. It*s hence questionable whether the method can give any benefits in large scale dataset. (iv) If the use of noise regularization is to smooth the density function, how does it compare to methods that enforces Lipschitz continuity? e.g., gradient penalty, or construct a lipschtz constrained network such as in [1]. [1] Sorting out lipschitz function approximation. Anil et al.\n",
      "      - Review:###This paper presents a method to train a network for counting/localizing objects in a weakly supervised framework. The network is optimized based on weak supervision about target object counts; training images have only the ground truth of the total number of object counts without their positions in images. Through analyzing the feature maps of the network trained on toy bubble images, the authors propose two regularization techniques for rotation robustness and sparseness of the map in order to improve performance of object localization in the feature map. The experimental results on object counting tasks using the Mall dataset show that the proposed method produces favorable performance. This paper is leaning toward rejection due to the following two reasons. (1) The presented techniques are limited to the specific task of object counting. (2) They are derived in an ad-hoc way based on less theoretical background and thus lack novelty. The detailed comments are as follows. * The technique to embed rotation invariance by Eq.(4) is presented in an ad-hoc way. This seems to be limited to the tasks on toy data such as bubble images shown in the paper. In the real-world tasks/images to count the more complicated objects, the spatial correlation among object parts is an important clue to provide discriminative features for detecting objects such as torso below the head in human figures. Such spatial dependency could be missed by imposing the rotation invariance on the network. * The regularization by Gini impurity is a well-known technique to induce sparsity, lacking novelty. There is a large body of researches for enhancing sparsity, and thus to validate the regularization, the authors should compare the Gini impurity with the other sparsity-inducing regularization such as entropy. For example, it could be possible to argue the regularizations in terms of their derivatives (gradients) to be used for back-propagation. This paper lacks detailed analysis and discussion about the regularization in this counting framework. * It is unclear how the perspective from the (beta-)VAE contributes to the analysis of the proposed method. There seems to be less connection between Eq.(8) and the object counting task/framework. The authors just show the similarity between the presented method and VAE in terms of formulation. Since the proposed method belongs to just a simple optimization with sparsity-inducing regularization, the reviewer cannot find any convincing reason to discuss the connection to VAE. * The method is not fully validated in the experiments. The authors provide only one experimental result on the Mall dataset which is insufficient to validate the effectiveness of the proposed method. Considering that the method is limited to the specific task of object counting, it is necessary to qualitatively and thoroughly evaluate the performance on various datasets that exhibit various environmental conditions regarding such as lighting and occlusion. Minor comments: - Show the index for summation in Eqs.(1,4,6,7,8). - It is confusing to show q_Theta(D|I) in Eq.(3), even though the authors aim to discuss the connection to VAE. Show the mathematical definition of q_Theta(D|I) before that. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper presents a method to train a network for counting/localizing objects in a weakly supervised framework. The network is optimized based on weak supervision about target object counts; training images have only the ground truth of the total number of object counts without their positions in images. Through analyzing the feature maps of the network trained on toy bubble images, the authors propose two regularization techniques for rotation robustness and sparseness of the map in order to improve performance of object localization in the feature map. The experimental results on object counting tasks using the Mall dataset show that the proposed method produces favorable performance. This paper is leaning toward rejection due to the following two reasons. (1) The presented techniques are limited to the specific task of object counting. (2) They are derived in an ad-hoc way based on less theoretical background and thus lack novelty. The detailed comments are as follows. * The technique to embed rotation invariance by Eq.(4) is presented in an ad-hoc way. This seems to be limited to the tasks on toy data such as bubble images shown in the paper. In the real-world tasks/images to count the more complicated objects, the spatial correlation among object parts is an important clue to provide discriminative features for detecting objects such as torso below the head in human figures. Such spatial dependency could be missed by imposing the rotation invariance on the network. * The regularization by Gini impurity is a well-known technique to induce sparsity, lacking novelty. There is a large body of researches for enhancing sparsity, and thus to validate the regularization, the authors should compare the Gini impurity with the other sparsity-inducing regularization such as entropy. For example, it could be possible to argue the regularizations in terms of their derivatives (gradients) to be used for back-propagation. This paper lacks detailed analysis and discussion about the regularization in this counting framework. * It is unclear how the perspective from the (beta-)VAE contributes to the analysis of the proposed method. There seems to be less connection between Eq.(8) and the object counting task/framework. The authors just show the similarity between the presented method and VAE in terms of formulation. Since the proposed method belongs to just a simple optimization with sparsity-inducing regularization, the reviewer cannot find any convincing reason to discuss the connection to VAE. * The method is not fully validated in the experiments. The authors provide only one experimental result on the Mall dataset which is insufficient to validate the effectiveness of the proposed method. Considering that the method is limited to the specific task of object counting, it is necessary to qualitatively and thoroughly evaluate the performance on various datasets that exhibit various environmental conditions regarding such as lighting and occlusion. Minor comments: - Show the index for summation in Eqs.(1,4,6,7,8). - It is confusing to show q_Theta(D|I) in Eq.(3), even though the authors aim to discuss the connection to VAE. Show the mathematical definition of q_Theta(D|I) before that.\n",
      "  - Empirical Validation and Experimental Rigor (L2, 7)\n",
      "    - Insufficient Experimental Validation or Incomplete/Weak Comparison to Baselines (L1, 2)\n",
      "      - Review:###The uncertainty of having a missing value is investigated on the prediction by not assigning a single imputed value but N different values generated via an imputer network (based on GAIN). Unlike old-fashion multiple imputation techniques, one predictor is trained on different samples and it induces the uncertainty. The experiments on number of datasets show the proposed predictor is capable of having a fairly better performance. Overall, this paper raises an interesting point about missing data imputation via generative models, and well-written; however, there are number of concerns: 1- The predictor is trained on different version of imputed samples (imputed via the generator); this equates to making noisy version of the real samples, where noise is applied to the missing variables. A side effect of this is generalization of the predictor; thus, have you been careful that the improved accuracy is not due to generalization? In other words, if we imposed the generalization via adding gaussian noise to the imputed samples by GAIN for example, would we get improved accuracy too? 2- Your method known as GI is a modified version of GAIN. You could also use MisGAN, and I am wondering if the results would have been different if generator in MisGAN was used in GI in Figure 2 (b), as MisGAN works better than GAIN. 3- I am also wondering how the GAIN imputation changes by removing the MSE term? GI discards the MSE term in GAIN, and it changes the distribution of the imputed variables by GAIN. Could you maybe fit a Normal distribution on a chosen imputed variable (N=128) and visualize how different it is from the distribution of imputed variables with GAIN with MSE term. 4- In justification for claim 1, it is said “This is equivalent to training models using noisy labels”. This is not accurate: in noisy label prediction, we have one (noisy) y corresponding to each x, in your case there are multiple ys for one sample. 5- In the implementation details, I cannot fully wrap my head around the part “z vector of size 1/8”; how did you choose this 1/8? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The uncertainty of having a missing value is investigated on the prediction by not assigning a single imputed value but N different values generated via an imputer network (based on GAIN). Unlike old-fashion multiple imputation techniques, one predictor is trained on different samples and it induces the uncertainty. The experiments on number of datasets show the proposed predictor is capable of having a fairly better performance. Overall, this paper raises an interesting point about missing data imputation via generative models, and well-written; however, there are number of concerns: 1- The predictor is trained on different version of imputed samples (imputed via the generator); this equates to making noisy version of the real samples, where noise is applied to the missing variables. A side effect of this is generalization of the predictor; thus, have you been careful that the improved accuracy is not due to generalization? In other words, if we imposed the generalization via adding gaussian noise to the imputed samples by GAIN for example, would we get improved accuracy too? 2- Your method known as GI is a modified version of GAIN. You could also use MisGAN, and I am wondering if the results would have been different if generator in MisGAN was used in GI in Figure 2 (b), as MisGAN works better than GAIN. 3- I am also wondering how the GAIN imputation changes by removing the MSE term? GI discards the MSE term in GAIN, and it changes the distribution of the imputed variables by GAIN. Could you maybe fit a Normal distribution on a chosen imputed variable (N=128) and visualize how different it is from the distribution of imputed variables with GAIN with MSE term. 4- In justification for claim 1, it is said “This is equivalent to training models using noisy labels”. This is not accurate: in noisy label prediction, we have one (noisy) y corresponding to each x, in your case there are multiple ys for one sample. 5- In the implementation details, I cannot fully wrap my head around the part “z vector of size 1/8”; how did you choose this 1/8?\n",
      "      - Review:###This article present an approach to assign an importance value to an observation, that quantifies its usefulness for a specific objective task. This is useful in many contexts, such as domain adaptation, corrupted sample discovery or robust learning. The importance values may also be used to improve the performance of a model for the task. The importance values are learned jointly with that model. A small neural network called by the authors a Data Value Estimator (DVE) is learnt by the authors to estimate sample selection probabilities, which will dictate which instances will be used for the main model that tackles the objective task. While the main model is trained through usual mini-batch gradient descent, the DVE can not be, since the sampling process is not differentiable. It follows that the DVE is trained with a RL signal, that follows the variation of the loss throughout the learning process. The method proposed by the authors is new and show very significant results over existing methods. It is scalable, while many of the presented approaches are not. It is said to have a much lower computational burden than some existing methods, e.g. LOO or Data Shapley. The paper is very well written, and the method is illustrated on several datasets, from different domains. However, it seems that many approaches that did not suffer from the same complexity drawbacks of LOO and Data Shapley were not compared to this work. While some of the presented approaches are recent, e.g. ChoiceNet (2018), others are more established, e.g. domain adversarial networks (DANs, Ganin et al 2016) and are not compared for domain adaptation tasks. Given that the contributions of the authors are solely empirical, it is necessary to compare their approach to other scalable domain adaptation approaches. The approach proposed by the authors also features many hyperparameters, with fixed chosen values, and the architecture of the DVE is not precised, which may impair the reproducibility of the paper. The authors should provide code if not already provided. There is a mistake on the legends of Figure 2 and Figure 3, since accuracy should increase when removing the least important samples. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This article present an approach to assign an importance value to an observation, that quantifies its usefulness for a specific objective task. This is useful in many contexts, such as domain adaptation, corrupted sample discovery or robust learning. The importance values may also be used to improve the performance of a model for the task. The importance values are learned jointly with that model. A small neural network called by the authors a Data Value Estimator (DVE) is learnt by the authors to estimate sample selection probabilities, which will dictate which instances will be used for the main model that tackles the objective task. While the main model is trained through usual mini-batch gradient descent, the DVE can not be, since the sampling process is not differentiable. It follows that the DVE is trained with a RL signal, that follows the variation of the loss throughout the learning process. The method proposed by the authors is new and show very significant results over existing methods. It is scalable, while many of the presented approaches are not. It is said to have a much lower computational burden than some existing methods, e.g. LOO or Data Shapley. The paper is very well written, and the method is illustrated on several datasets, from different domains. However, it seems that many approaches that did not suffer from the same complexity drawbacks of LOO and Data Shapley were not compared to this work. While some of the presented approaches are recent, e.g. ChoiceNet (2018), others are more established, e.g. domain adversarial networks (DANs, Ganin et al 2016) and are not compared for domain adaptation tasks. Given that the contributions of the authors are solely empirical, it is necessary to compare their approach to other scalable domain adaptation approaches. The approach proposed by the authors also features many hyperparameters, with fixed chosen values, and the architecture of the DVE is not precised, which may impair the reproducibility of the paper. The authors should provide code if not already provided. There is a mistake on the legends of Figure 2 and Figure 3, since accuracy should increase when removing the least important samples.\n",
      "    - Lack of Theoretical/Empirical Depth and Statistical Reporting (L1, 2)\n",
      "      -  ------------------------- Update after rebuttal --------------------------- Thank you for addressing my concerns. I feel the rebuttal did improve the paper, e.g., the significance of results can be evaluated better now. I still like the overall idea of the paper as optimizing connectivity patterns in architectures has so far mostly been ignored while it is actually straight-forward to do (as shown in this work). I increased my score accordingly. However, the novelty and significance of this work is still limited in my option and therefore I do not argue heavily in favor of accepting this submission. -------------------------------------------------------------------------------------------------------------------------------------------------- The authors propose a method for learning the connections/ connectivity pattern (dubbed: the topology; meaning which layer is directly connected to which other layer) in neural networks. They do so by weighting connections between layers (e.g., by weighting skip connections) with a real valued parameter. This real-valued parameterization of the connections is then optimized by gradient descent along with the weights of neural networks. The authors also propose L1 regularization on the connectivity parameters to induce sparsity. The proposed method is evaluated by optimizing the topology for ResNets, MobileNetsV2 and their proposed “TopoNets”. Originality and significance. The manuscript addresses an interesting problem: while there has been lots of work on manually designing better architectures as well as automated design (a.k.a. neural architecture search, NAS), there is little work on optimizing the overall topology (meaning the connectivity patterns between layers). Most prior work solely focuses on search for blocks or cells and then stacking these cells in a pre-defined, not-optimized manner. However, there has been some work also including this in architecture search (e.g., [1,2,3]), and especially the work [4] seems very related but is not discussed. The authors of [4] propose, very similar to this submission, a gradient-based optimization of the connections (in a different way though). The proposed method for optimizing the topology is also very similar to DARTS, simply applied to the connectivity pattern rather than on the operations-level. However, here the topology is optimized along with the network’s weights on the training data rather than the bi-level optimization from DARTS, where the architectural parameters are optimized on the validation data instead (which is very reasonable as one usually considers the architecture as a hyperparameter). I wonder if this has also been considered/tested by the authors of this paper as I would consider the topology to be a hyperparameter which should not be optimized on training but rather validation data. Knowing [4] and DARTS, the proposed method seems to be rather incremental and straightforward rather than ground breaking. While the proposed method allows for more flexible topologies, it introduces different “stages” for their TopoNets, which are actually a similar concept as blocks or cells from the NAS literature. This again does not allow connections between arbitrary layers (but rather only between layers in the same stage; to the best of my understanding). Empirical results show rather small improvements and their significance is unclear (see next paragraph). Clarity and quality. The paper is mostly well written, well motivated and easy to follow. The mathematical formalism is a little vague at some points (e.g., in Section 2.1., the notation G is used for defining a graph and later in Equation (2) as a function computing feature maps in networks). While the literature on manual design of architectures is thoroughly reviewed, there is missing related work in the context of neural architecture search, as already discussed above. The quality of the results is questionable as differences in accuracies are in almost all experiment rather small (e.g., tuning MobileNetsV2 on Imagenet: 72.62% (original) Top-1 accuracy vs. 72.84% (optimized) and it seems that the authors do only report results for a single run of experiments. In order to assess if the differences are actually statistically significant, the authors would need to report several runs and would need to state, e.g., means and standard deviations. Overall, the authors address an interesting problem, which seems to have fallen into oblivion in current NAS literature: while researcher optimize cells, which are then stacked to build the final model, not many researcher look into connectivity patterns / topology on the macro level, meaning connections across cells and how cells should be stacked. This paper addresses this problem to some extent. However, the proposed optimization method is, in my opinion, rather incremental (with respect to DARTS and [4]) and therefore of limited novelty and significance. It is currently hard to assess the empirical results due to rather small improvements and the lack of repeated runs of experiments. Mainly for these two reasons, I do not recommend the paper for acceptance. [1] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Quoc V. Le, and Alex Kurakin. Large-scale evolution of image classifiers. ICML, 2017. [2] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Efficient multi-objective neural architecture search via lamarckian evolution. ICLR, 2019. [3] Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. ICML, 2018 [4] Karim Ahmed and Lorenzo Torresani. Maskconnect: Connectivity learning by gradient descent. ECCV, 2018 (L0, 1)\n",
      "        * Examples:\n",
      "          -  ------------------------- Update after rebuttal --------------------------- Thank you for addressing my concerns. I feel the rebuttal did improve the paper, e.g., the significance of results can be evaluated better now. I still like the overall idea of the paper as optimizing connectivity patterns in architectures has so far mostly been ignored while it is actually straight-forward to do (as shown in this work). I increased my score accordingly. However, the novelty and significance of this work is still limited in my option and therefore I do not argue heavily in favor of accepting this submission. -------------------------------------------------------------------------------------------------------------------------------------------------- The authors propose a method for learning the connections/ connectivity pattern (dubbed: the topology; meaning which layer is directly connected to which other layer) in neural networks. They do so by weighting connections between layers (e.g., by weighting skip connections) with a real valued parameter. This real-valued parameterization of the connections is then optimized by gradient descent along with the weights of neural networks. The authors also propose L1 regularization on the connectivity parameters to induce sparsity. The proposed method is evaluated by optimizing the topology for ResNets, MobileNetsV2 and their proposed “TopoNets”. Originality and significance. The manuscript addresses an interesting problem: while there has been lots of work on manually designing better architectures as well as automated design (a.k.a. neural architecture search, NAS), there is little work on optimizing the overall topology (meaning the connectivity patterns between layers). Most prior work solely focuses on search for blocks or cells and then stacking these cells in a pre-defined, not-optimized manner. However, there has been some work also including this in architecture search (e.g., [1,2,3]), and especially the work [4] seems very related but is not discussed. The authors of [4] propose, very similar to this submission, a gradient-based optimization of the connections (in a different way though). The proposed method for optimizing the topology is also very similar to DARTS, simply applied to the connectivity pattern rather than on the operations-level. However, here the topology is optimized along with the network’s weights on the training data rather than the bi-level optimization from DARTS, where the architectural parameters are optimized on the validation data instead (which is very reasonable as one usually considers the architecture as a hyperparameter). I wonder if this has also been considered/tested by the authors of this paper as I would consider the topology to be a hyperparameter which should not be optimized on training but rather validation data. Knowing [4] and DARTS, the proposed method seems to be rather incremental and straightforward rather than ground breaking. While the proposed method allows for more flexible topologies, it introduces different “stages” for their TopoNets, which are actually a similar concept as blocks or cells from the NAS literature. This again does not allow connections between arbitrary layers (but rather only between layers in the same stage; to the best of my understanding). Empirical results show rather small improvements and their significance is unclear (see next paragraph). Clarity and quality. The paper is mostly well written, well motivated and easy to follow. The mathematical formalism is a little vague at some points (e.g., in Section 2.1., the notation G is used for defining a graph and later in Equation (2) as a function computing feature maps in networks). While the literature on manual design of architectures is thoroughly reviewed, there is missing related work in the context of neural architecture search, as already discussed above. The quality of the results is questionable as differences in accuracies are in almost all experiment rather small (e.g., tuning MobileNetsV2 on Imagenet: 72.62% (original) Top-1 accuracy vs. 72.84% (optimized) and it seems that the authors do only report results for a single run of experiments. In order to assess if the differences are actually statistically significant, the authors would need to report several runs and would need to state, e.g., means and standard deviations. Overall, the authors address an interesting problem, which seems to have fallen into oblivion in current NAS literature: while researcher optimize cells, which are then stacked to build the final model, not many researcher look into connectivity patterns / topology on the macro level, meaning connections across cells and how cells should be stacked. This paper addresses this problem to some extent. However, the proposed optimization method is, in my opinion, rather incremental (with respect to DARTS and [4]) and therefore of limited novelty and significance. It is currently hard to assess the empirical results due to rather small improvements and the lack of repeated runs of experiments. Mainly for these two reasons, I do not recommend the paper for acceptance. [1] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Quoc V. Le, and Alex Kurakin. Large-scale evolution of image classifiers. ICML, 2017. [2] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Efficient multi-objective neural architecture search via lamarckian evolution. ICLR, 2019. [3] Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. ICML, 2018 [4] Karim Ahmed and Lorenzo Torresani. Maskconnect: Connectivity learning by gradient descent. ECCV, 2018\n",
      "      - Review:###I. Summary of the paper This paper describes a principled strategy for searching for the most suitable neural network architecture out of a particular class of architectures. Specifically, the problem is framed as an optimisation problem over a set of directed acyclic graphs (DAGs) that correspond to potential network architectures. By optimising the edge weights of this representation, a suitable architecture can be generated. In addition to the aforementioned optimisation scheme, the paper also presents a regularisation that results in *sparse* networks, i.e. networks with a smaller number of edges. Multiple experiments on *tuning* existing architectures on several data sets conclude the paper. II. Summary of the review This paper discusses a highly relevant subject, namely how to select neural network architectures in a principled manner. While the presented work already goes into a good direction, I cannot give it my endorsement for acceptance because of the following reasons: - The paper is lacking clarity: concepts could be explained somewhat better, and the paper is suffering from language/grammar issues that make it harder to understand the contents. - Lack of experimental or theoretical depth: the proposed method is presented as-is; no theoretical analysis of its behaviour is performed; while this is not necessarily a problem, as there are several empirical experiments, the experimental section is not sufficiently detailed: for example, no limitations of the method are being discussed and the presented results are not state-of-the-art accuracies. Nevertheless, I want to point out that this paper has the potential to become an important contribution to the community; it is absolutely clear that more principled approaches are required to select network architectures. In the following, I will comment on the individual aspects in more detail. III. Detailed comments (clarity) - The abstract could be improved in terms of its logical flow. Instead of trying to introduce new terminology (macro/micro etc.) here, the abstract should rather state directly that this paper frames network architecture selection as an optimisation problem over DAGs. - The use of topology is slightly non-standard here. What is the meaning behind the *macro* and *micro* operations? This should be explained somewhat better. - Figure 1 should be extended to show an example of how the depicted graphs are described through the terminology mentioned in the paper. For example, individual edges or nodes could be highlighted and referred to in the text to make the *mapping* clearer. - I do not understand how operations such as *addition* are represented in the DAG. Ideally, this should also be elucidated by a figure. - When discussing *intervals* of residual connections, I am assuming that the paper refers to how many layers are skipped? If so, this should be mentioned and defined explicitly. - The term *searching space* should be replaced by *search space*, as the latter is more standard usage. - I do not understand why the optimisation of the topology can decrease the computational burden, as claimed on p. 2. The optimisation process still has to be performed, just like the training of the network, correct? Am I misunderstanding this? - The caption of Figure 2 could be extended; does a single node type mean that the complete network only consists of nodes of that type? Moreover, accuracies/errors should be shown in addition to the loss curves. - The term *dense connection* is vague; I think the paper should use *densely-connected graph* here. - The sentence *Among these nodes, [...]* refers to the *whole* network, and not to the way the output tensor is processed. Am I understanding this correctly? - I do not understand the initial sentences in Section 2.2; what is the meaning of *cell* in this case? - The term *topological structures* should be renamed to *(sub)graphs* in order to improve clarity. - I do not understand the comment on sparsity in Section 2.4. How are *moderate sparsity* and Figure 2 connected? - In the algorithm, I would use *Sparsity Type* instead of *Sparse Type* to refer to the parameter. - What does *Complete* (without ) mean in Table 3? - The footnote below Table 3 is not referenced anywhere in the text or in the table. - In Figure 3, are the adjacency matrices consistent? What happens if the training process is repeated? It would be highly interesting to show *averaged* matrices over multiple runs. IV. Detailed comments (experiments & theory) - A theoretical analysis of the proposed method would be interesting. Does the optimisation always converge? Are minima unique? What is the computational complexity? At least some of these aspects should be discussed. - The limitations of the proposed method are not explained. For example, what is the meaning of the sentence on p. 2 about *excluding the influence of the mixture of different layers/nodes*? It is my understanding that the proposed method can only change the *connections* between blocks of a network, but not the type of layers. Is this correct? If so, it would be a major limitation that should be mentioned explicitly. - Another limitation that is not discussed is the scaling to very deep networks. How problematic is it to model all potential connections in such a network? Are there limits to the current optimisation scheme? This needs to be assessed in the experimental section. - For all experimental tables, standard deviations and means should be provided. This is necessary in order to assess the stability of the proposed method, because there are multiple sources of stochasticity: one arising from the optimisation procedure, the other one arising from the training of the network itself. - The results reported for the experiments are somewhat behind the state-of-the-art in terms of accuracy values. This should be stated more clearly; I assume that it is caused by limitations of the proposed method, which prohibit an application to very recent architectures. Is this correct? If so, it should at least be mentioned. - The claim that nodes at the start of a topological ordering contribute more to specific stages needs to be (empirically) proven. V. Style issues The paper is not easy to read because of several non-standard phrases or expressions. - The phrase *in topology* is often added to a sentence where it does not entirely make sense. For example, *[The] architecture can be expressed as a directed acyclic graph (DAG) in topology*. I do not see the necessity of adding *in topology* here. There are other places at well from which I would remove this phrase. - *effective networks* --> *effective network architectures* - *largely affects* --> *largely affect* - *Motivated by which* --> *Motivated by this* - *innovative method* --> *method* (or *novel method*) - *as a complete graph, through* --> *as a complete graph, and through* - *auxiliary sparsity constraint* --> *an auxiliary sparsity constraint* - *named as TopoNet* --> *called TopoNet* - *At initial periods* --> *Previously* (I am not sure I understand this correctly) - *red signs* --> *red arrows* - *for its topology* --> *in terms of its topology* (?) - *both combining* --> *both a combination* (?) - *number of interval* --> *number of intervals* - *straight connected* --> *directly connected* - *conduct transformation* --> *performs a transformation* - What is the meaning of the phrase *These may cover the influence [...]*? Is this a reference to limitations of existing networks? - *opted from* --> *chosen from* - *Following two simple design rules* --> *We follow two simple design rules* - *1000-dimension* --> * -dimensional* - *We raise two ways* --> *We describe two ways* - *consited* --> *consisted* / *consists* - *deepen the depth* --> *increase the depth* - *origin* --> *original* - *promotions* --> *improvements* - *can make more profit* --> *can be useful to improve performance* (I am guessing this from the context) - *sparseness on representation* --> *sparsity* - *Adaptive one* --> *The adaptive one* - *At the fore* --> *At the beginning/start* - I do not understand the sentence about the *free lunch*. Does it refer to the fact that some connections can still be removed from the network without decreasing accuracy? - *less computation costs* --> *lower computation costs* - *shortcut offers* --> *shortcuts offer* - *benefits optimization* --> *benefit optimization* - *feasible way to the optimization* --> *feasible way for the optimization* - Some references in the bibliography are not capitalised consistently (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###I. Summary of the paper This paper describes a principled strategy for searching for the most suitable neural network architecture out of a particular class of architectures. Specifically, the problem is framed as an optimisation problem over a set of directed acyclic graphs (DAGs) that correspond to potential network architectures. By optimising the edge weights of this representation, a suitable architecture can be generated. In addition to the aforementioned optimisation scheme, the paper also presents a regularisation that results in *sparse* networks, i.e. networks with a smaller number of edges. Multiple experiments on *tuning* existing architectures on several data sets conclude the paper. II. Summary of the review This paper discusses a highly relevant subject, namely how to select neural network architectures in a principled manner. While the presented work already goes into a good direction, I cannot give it my endorsement for acceptance because of the following reasons: - The paper is lacking clarity: concepts could be explained somewhat better, and the paper is suffering from language/grammar issues that make it harder to understand the contents. - Lack of experimental or theoretical depth: the proposed method is presented as-is; no theoretical analysis of its behaviour is performed; while this is not necessarily a problem, as there are several empirical experiments, the experimental section is not sufficiently detailed: for example, no limitations of the method are being discussed and the presented results are not state-of-the-art accuracies. Nevertheless, I want to point out that this paper has the potential to become an important contribution to the community; it is absolutely clear that more principled approaches are required to select network architectures. In the following, I will comment on the individual aspects in more detail. III. Detailed comments (clarity) - The abstract could be improved in terms of its logical flow. Instead of trying to introduce new terminology (macro/micro etc.) here, the abstract should rather state directly that this paper frames network architecture selection as an optimisation problem over DAGs. - The use of topology is slightly non-standard here. What is the meaning behind the *macro* and *micro* operations? This should be explained somewhat better. - Figure 1 should be extended to show an example of how the depicted graphs are described through the terminology mentioned in the paper. For example, individual edges or nodes could be highlighted and referred to in the text to make the *mapping* clearer. - I do not understand how operations such as *addition* are represented in the DAG. Ideally, this should also be elucidated by a figure. - When discussing *intervals* of residual connections, I am assuming that the paper refers to how many layers are skipped? If so, this should be mentioned and defined explicitly. - The term *searching space* should be replaced by *search space*, as the latter is more standard usage. - I do not understand why the optimisation of the topology can decrease the computational burden, as claimed on p. 2. The optimisation process still has to be performed, just like the training of the network, correct? Am I misunderstanding this? - The caption of Figure 2 could be extended; does a single node type mean that the complete network only consists of nodes of that type? Moreover, accuracies/errors should be shown in addition to the loss curves. - The term *dense connection* is vague; I think the paper should use *densely-connected graph* here. - The sentence *Among these nodes, [...]* refers to the *whole* network, and not to the way the output tensor is processed. Am I understanding this correctly? - I do not understand the initial sentences in Section 2.2; what is the meaning of *cell* in this case? - The term *topological structures* should be renamed to *(sub)graphs* in order to improve clarity. - I do not understand the comment on sparsity in Section 2.4. How are *moderate sparsity* and Figure 2 connected? - In the algorithm, I would use *Sparsity Type* instead of *Sparse Type* to refer to the parameter. - What does *Complete* (without ) mean in Table 3? - The footnote below Table 3 is not referenced anywhere in the text or in the table. - In Figure 3, are the adjacency matrices consistent? What happens if the training process is repeated? It would be highly interesting to show *averaged* matrices over multiple runs. IV. Detailed comments (experiments & theory) - A theoretical analysis of the proposed method would be interesting. Does the optimisation always converge? Are minima unique? What is the computational complexity? At least some of these aspects should be discussed. - The limitations of the proposed method are not explained. For example, what is the meaning of the sentence on p. 2 about *excluding the influence of the mixture of different layers/nodes*? It is my understanding that the proposed method can only change the *connections* between blocks of a network, but not the type of layers. Is this correct? If so, it would be a major limitation that should be mentioned explicitly. - Another limitation that is not discussed is the scaling to very deep networks. How problematic is it to model all potential connections in such a network? Are there limits to the current optimisation scheme? This needs to be assessed in the experimental section. - For all experimental tables, standard deviations and means should be provided. This is necessary in order to assess the stability of the proposed method, because there are multiple sources of stochasticity: one arising from the optimisation procedure, the other one arising from the training of the network itself. - The results reported for the experiments are somewhat behind the state-of-the-art in terms of accuracy values. This should be stated more clearly; I assume that it is caused by limitations of the proposed method, which prohibit an application to very recent architectures. Is this correct? If so, it should at least be mentioned. - The claim that nodes at the start of a topological ordering contribute more to specific stages needs to be (empirically) proven. V. Style issues The paper is not easy to read because of several non-standard phrases or expressions. - The phrase *in topology* is often added to a sentence where it does not entirely make sense. For example, *[The] architecture can be expressed as a directed acyclic graph (DAG) in topology*. I do not see the necessity of adding *in topology* here. There are other places at well from which I would remove this phrase. - *effective networks* --> *effective network architectures* - *largely affects* --> *largely affect* - *Motivated by which* --> *Motivated by this* - *innovative method* --> *method* (or *novel method*) - *as a complete graph, through* --> *as a complete graph, and through* - *auxiliary sparsity constraint* --> *an auxiliary sparsity constraint* - *named as TopoNet* --> *called TopoNet* - *At initial periods* --> *Previously* (I am not sure I understand this correctly) - *red signs* --> *red arrows* - *for its topology* --> *in terms of its topology* (?) - *both combining* --> *both a combination* (?) - *number of interval* --> *number of intervals* - *straight connected* --> *directly connected* - *conduct transformation* --> *performs a transformation* - What is the meaning of the phrase *These may cover the influence [...]*? Is this a reference to limitations of existing networks? - *opted from* --> *chosen from* - *Following two simple design rules* --> *We follow two simple design rules* - *1000-dimension* --> * -dimensional* - *We raise two ways* --> *We describe two ways* - *consited* --> *consisted* / *consists* - *deepen the depth* --> *increase the depth* - *origin* --> *original* - *promotions* --> *improvements* - *can make more profit* --> *can be useful to improve performance* (I am guessing this from the context) - *sparseness on representation* --> *sparsity* - *Adaptive one* --> *The adaptive one* - *At the fore* --> *At the beginning/start* - I do not understand the sentence about the *free lunch*. Does it refer to the fact that some connections can still be removed from the network without decreasing accuracy? - *less computation costs* --> *lower computation costs* - *shortcut offers* --> *shortcuts offer* - *benefits optimization* --> *benefit optimization* - *feasible way to the optimization* --> *feasible way for the optimization* - Some references in the bibliography are not capitalised consistently\n",
      "    - Metrics and Experimental Reporting Issues (L1, 3)\n",
      "      -  This paper proposed an auxiliary loss based on mutual information for graph neural network. Such loss is to maximize the mutual information between edge representation and corresponding edge feature in GNN ‘message passing’ function. By assuming a gaussian distribution of edge feature given edge representation, the training can be done efficiently with tractable density. Experiments on molecule regression and knowledge graph completion show better performance than MPNN. Overall the paper is written in a clear way which is easy to follow. The idea of using mutual information as some kind of regularization is also interesting. However, there are some concerns I have with the paper: Regarding formulation 1. The derivation up to Eq(8) looks fine to me, where the assumptions are reasonable. However from Eq(8) one can see this is reduced to an ‘auto-encoder’ type of regularization, where one can have a trivial solution for reconstruction -- the identity network, when the hidden dimension is larger than input dimension. And in this paper, dimension of W should always be larger than the dimension of e (for example, in molecules e should be low dimension vector with the bond type, distance, etc., while W should have dimension that matches the node embeddings). I think the original loss (i.e., the supervised MSE, cross entropy etc) would help a bit with such degenerated case, but it is possible that the learned f(e) contains both identity mapping (or equivalent) and the representation that contributes to original loss. 2. Actually I’m also not sure if I get the motivation here. If one needs to do this regularization for edges, why don’t we consider this auxiliary loss for node embeddings as well? As in molecules, atoms have more interesting features than bonds, which should account more if the mutual information loss is needed. Regarding experiment 1. In Figure 1, the training loss of EIGNN is better than MPNN. This is a bit counterintuitive to me, as I think the auxiliary is a kind of regularization -- which might help with generalization but not necessarily the training loss. 2. The original paper of MPNN reports the relative MAE. Is it possible to report the results using the same metric as previous paper? It would make the comparison more consistent -- though showing the improvement in current way is not too bad. 3. I think one simple ablation study would to concat the edge feature directly inside the ‘message passing’ procedure, or have some ‘residual’ type of connection for edge features. (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper proposed an auxiliary loss based on mutual information for graph neural network. Such loss is to maximize the mutual information between edge representation and corresponding edge feature in GNN ‘message passing’ function. By assuming a gaussian distribution of edge feature given edge representation, the training can be done efficiently with tractable density. Experiments on molecule regression and knowledge graph completion show better performance than MPNN. Overall the paper is written in a clear way which is easy to follow. The idea of using mutual information as some kind of regularization is also interesting. However, there are some concerns I have with the paper: Regarding formulation 1. The derivation up to Eq(8) looks fine to me, where the assumptions are reasonable. However from Eq(8) one can see this is reduced to an ‘auto-encoder’ type of regularization, where one can have a trivial solution for reconstruction -- the identity network, when the hidden dimension is larger than input dimension. And in this paper, dimension of W should always be larger than the dimension of e (for example, in molecules e should be low dimension vector with the bond type, distance, etc., while W should have dimension that matches the node embeddings). I think the original loss (i.e., the supervised MSE, cross entropy etc) would help a bit with such degenerated case, but it is possible that the learned f(e) contains both identity mapping (or equivalent) and the representation that contributes to original loss. 2. Actually I’m also not sure if I get the motivation here. If one needs to do this regularization for edges, why don’t we consider this auxiliary loss for node embeddings as well? As in molecules, atoms have more interesting features than bonds, which should account more if the mutual information loss is needed. Regarding experiment 1. In Figure 1, the training loss of EIGNN is better than MPNN. This is a bit counterintuitive to me, as I think the auxiliary is a kind of regularization -- which might help with generalization but not necessarily the training loss. 2. The original paper of MPNN reports the relative MAE. Is it possible to report the results using the same metric as previous paper? It would make the comparison more consistent -- though showing the improvement in current way is not too bad. 3. I think one simple ablation study would to concat the edge feature directly inside the ‘message passing’ procedure, or have some ‘residual’ type of connection for edge features.\n",
      "      - Review:###In this paper, the authors propose a new method for semi-supervised node classification by drawing connection between GCN and MF. The authors borrow the idea of convergence of GCN as Laplacian Smoothing. With this observation, the authors propose a joint loss with two components: classification loss and structure loss for the similarity between embedding of neighboring nodes. The authors train the parameters via optimizing the two losses alternatively. Experiments are carried out on seven networks with comparison to baselines. Strength: 1. It is an interesting and innovative idea to draw connection between GCN and MF. 2. The propose method is more suitable for distributed setting. With negative sampling for structure loss, both structure batch and classification batch can be constructed locally with only one-hop information. 3. The authors carry out experiments on seven real-world networks with ablation study for components in the model. Moreover, the authors carry out comparison to baselines in distributed setting. Weakness: 1. The connection of GCN to MF is very indirect. It holds only when the GCN converges to the Laplacian smoothing. It is not clear whether this holds empirically. Moreover, there are too much intermediate steps and approximation between the Laplacian smoothing to the matrix factorization. As far as I am concerned, the connection is closer to node embedding versus matrix factorization. 2. Given that GCN serves as Laplacian smoothing, it would be great if the authors can simply add additional regularization on dis(h_i, h_i) for (v_i, v_j)in E. Moreover, there is no reference and description to the Planetoid* algorithm. 3. The authors use alternative batches between structure and classification loss. It would be interesting to see if joint training the two loss in mini-batch among a node and its neighbors can leads to any difference. 3. The authors report only accuracy as evaluation metrics. It would be better If the authors could report recall@K and F1 score as well. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###In this paper, the authors propose a new method for semi-supervised node classification by drawing connection between GCN and MF. The authors borrow the idea of convergence of GCN as Laplacian Smoothing. With this observation, the authors propose a joint loss with two components: classification loss and structure loss for the similarity between embedding of neighboring nodes. The authors train the parameters via optimizing the two losses alternatively. Experiments are carried out on seven networks with comparison to baselines. Strength: 1. It is an interesting and innovative idea to draw connection between GCN and MF. 2. The propose method is more suitable for distributed setting. With negative sampling for structure loss, both structure batch and classification batch can be constructed locally with only one-hop information. 3. The authors carry out experiments on seven real-world networks with ablation study for components in the model. Moreover, the authors carry out comparison to baselines in distributed setting. Weakness: 1. The connection of GCN to MF is very indirect. It holds only when the GCN converges to the Laplacian smoothing. It is not clear whether this holds empirically. Moreover, there are too much intermediate steps and approximation between the Laplacian smoothing to the matrix factorization. As far as I am concerned, the connection is closer to node embedding versus matrix factorization. 2. Given that GCN serves as Laplacian smoothing, it would be great if the authors can simply add additional regularization on dis(h_i, h_i) for (v_i, v_j)in E. Moreover, there is no reference and description to the Planetoid* algorithm. 3. The authors use alternative batches between structure and classification loss. It would be interesting to see if joint training the two loss in mini-batch among a node and its neighbors can leads to any difference. 3. The authors report only accuracy as evaluation metrics. It would be better If the authors could report recall@K and F1 score as well.\n",
      "      - Review:###This paper introduces a unified model which combines label propagation algorithm (LPA) and graph convolutional networks (GCNs) for node classification. The motivation of this combination is supported by two analysis on the feature/label smoothing and feature/label influence. The proposed GCN-LPA framework utilizeds LPA to adjust the edge weight A* through the label information. Then, this edge weight A* is used to transfer the knowledge from label information to feature information for enhancing the representation learning in GCN. An end-to-end solution is proposed by treating the LPA process as regularization. Overall, the idea of unifying GCNs and LAP in an end-to-end fashion is interesting. One major concern is that from the experiment, it is unclear how much the LPA impacts the node classification. It will be more convincing if the performance comparison under different percentage of labeled samples (during LPA) is provided. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper introduces a unified model which combines label propagation algorithm (LPA) and graph convolutional networks (GCNs) for node classification. The motivation of this combination is supported by two analysis on the feature/label smoothing and feature/label influence. The proposed GCN-LPA framework utilizeds LPA to adjust the edge weight A* through the label information. Then, this edge weight A* is used to transfer the knowledge from label information to feature information for enhancing the representation learning in GCN. An end-to-end solution is proposed by treating the LPA process as regularization. Overall, the idea of unifying GCNs and LAP in an end-to-end fashion is interesting. One major concern is that from the experiment, it is unclear how much the LPA impacts the node classification. It will be more convincing if the performance comparison under different percentage of labeled samples (during LPA) is provided.\n",
      "  - Novel Defense and Attack Training Schemes for Adversarial Robustness: Evaluation and Generalization Concerns (L2, 3)\n",
      "    - Review:###In general, this paper follows the min-max training framework for adversarial robustness. Instead of using a gradient-based attack to solve the inner maximization, the authors use a neural network to learn the attack results. From the experimental results, this method can effectively defend against CW and PGD on CIFAR-10 and CIFAR-100. But the clean accuracy is lower than Madry et al. Also, there are too many works on robustness defense that have been proven ineffective (consider the works by Carlini). Since this is a new way of robust training and there is no certified guarantee, I suggest the authors refer [1] to evaluate the effectiveness of the defense more thoroughly to convince the readers that it really works. Especially, a robustness evaluation under adaptive attacks is necessary. In other words, if the attacker knows the strategy used by the defender (such as the attacker network structure), it may be possible to break the model. I am not convinced by the limiting cycle claim in Figure 1. I do not think this scenario (gradient descent goes along a cycle) is possible. If we take the integral of the gradient along this cycle from x to itself, we will get 0=f(x)-f(x)=-int_{t on cycle}f*(t)dt<0, which means that the function is not continuous at x. I suggest the authors have a surface plot of the function if they think this is possible. [1] Carlini, Nicholas, et al. *On evaluating adversarial robustness.* arXiv preprint arXiv:1902.06705 (2019). (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###In general, this paper follows the min-max training framework for adversarial robustness. Instead of using a gradient-based attack to solve the inner maximization, the authors use a neural network to learn the attack results. From the experimental results, this method can effectively defend against CW and PGD on CIFAR-10 and CIFAR-100. But the clean accuracy is lower than Madry et al. Also, there are too many works on robustness defense that have been proven ineffective (consider the works by Carlini). Since this is a new way of robust training and there is no certified guarantee, I suggest the authors refer [1] to evaluate the effectiveness of the defense more thoroughly to convince the readers that it really works. Especially, a robustness evaluation under adaptive attacks is necessary. In other words, if the attacker knows the strategy used by the defender (such as the attacker network structure), it may be possible to break the model. I am not convinced by the limiting cycle claim in Figure 1. I do not think this scenario (gradient descent goes along a cycle) is possible. If we take the integral of the gradient along this cycle from x to itself, we will get 0=f(x)-f(x)=-int_{t on cycle}f*(t)dt<0, which means that the function is not continuous at x. I suggest the authors have a surface plot of the function if they think this is possible. [1] Carlini, Nicholas, et al. *On evaluating adversarial robustness.* arXiv preprint arXiv:1902.06705 (2019).\n",
      "    - Review:###The authors propose a framework where one component is an attacker network that keeps learning about how to perturb the loss more, and one component is a defense network that robustify learning with respect to the attacker network. The framework is flexible on how the attacker network can be trained, and advances over previous works where the attacker is a human-designed algorithm rather than a learning model. Experiment results show that the framework reaches superior defense performance. The authors also extend the framework to help imitation learning. Overall the paper is a pleasure to read. My questions/suggestions are (1) Given that the framework seems natural in design, a deeper contribution would be talking about how to successfully train the framework in practice. The authors talk about the connection of the framework to GAN, and the latter is not that easy to train. However, we see very little information on how to train the framework in the paper. Was it super easy to train the framework (why?), or did the author encounter any difficulties? Are there important heuristics that help train the framework successfully? (2) While the framework leads to a better defense mechanism (the authors* goal), one could wonder whether it leads to a better attacker as well. Instead of just checking the differences of the attacking examples generated, can we take the inner attacker and see if it is more effective in attacking than PGM and CW? How does the goodness of the attacker improve with time? Do different L2L variants generate attackers with different quality? Do the quality connect with the defense performance? (3) Section 4 looks distracting to me. It is good to know that the framework can be extended to imitation learning, but the section is best put at a longer version or another paper, rather than occupying a significant amount of space in the current paper. I have read the rebuttal and thank the authors for the response. (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###The authors propose a framework where one component is an attacker network that keeps learning about how to perturb the loss more, and one component is a defense network that robustify learning with respect to the attacker network. The framework is flexible on how the attacker network can be trained, and advances over previous works where the attacker is a human-designed algorithm rather than a learning model. Experiment results show that the framework reaches superior defense performance. The authors also extend the framework to help imitation learning. Overall the paper is a pleasure to read. My questions/suggestions are (1) Given that the framework seems natural in design, a deeper contribution would be talking about how to successfully train the framework in practice. The authors talk about the connection of the framework to GAN, and the latter is not that easy to train. However, we see very little information on how to train the framework in the paper. Was it super easy to train the framework (why?), or did the author encounter any difficulties? Are there important heuristics that help train the framework successfully? (2) While the framework leads to a better defense mechanism (the authors* goal), one could wonder whether it leads to a better attacker as well. Instead of just checking the differences of the attacking examples generated, can we take the inner attacker and see if it is more effective in attacking than PGM and CW? How does the goodness of the attacker improve with time? Do different L2L variants generate attackers with different quality? Do the quality connect with the defense performance? (3) Section 4 looks distracting to me. It is good to know that the framework can be extended to imitation learning, but the section is best put at a longer version or another paper, rather than occupying a significant amount of space in the current paper. I have read the rebuttal and thank the authors for the response.\n",
      "    - Review:###The paper proposes a new way of adversarial training by placing another neural network called *attacker* network, and let the attacker to learn how to generate adversarial examples during training. This training scheme is formulated to solving a joint training according to min-max problem. Experimental results show that the method outperforms existing adversarial training in CIFAR-10/100 once the gradient information can be provided into the attacker network. In overall, the paper is well-written with an interesting message: There are certain features useful across all data samples in the inner maximization problem, which can be induced from gradient information. The experimental results are presented clearly, demonstrating its effectiveness and efficiency in running time. Section 4 seems to support the main claim in a novel way as well. The general motivation or justification on the proposed methods, e.g. the *limiting cycle* argument or the visualization part were not that convincing or seems slightly over-claimed to me, nevertheless. - I think adding a discussion on how to generalize the framework into other threat models, e.g. L2 or (even) unrestricted attacks would further strengthen the paper. I feel the current framework may suffers some training difficulties on these other threat models, even such kinds of discussion would also valuable to understand the method. - As the current formulation can generalize the general inner maximization optimization process, comparing or applying the method with a more recent form of adversarial training, e.g. TRADES, would be nice to demonstrate the general applicability of the method. (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###The paper proposes a new way of adversarial training by placing another neural network called *attacker* network, and let the attacker to learn how to generate adversarial examples during training. This training scheme is formulated to solving a joint training according to min-max problem. Experimental results show that the method outperforms existing adversarial training in CIFAR-10/100 once the gradient information can be provided into the attacker network. In overall, the paper is well-written with an interesting message: There are certain features useful across all data samples in the inner maximization problem, which can be induced from gradient information. The experimental results are presented clearly, demonstrating its effectiveness and efficiency in running time. Section 4 seems to support the main claim in a novel way as well. The general motivation or justification on the proposed methods, e.g. the *limiting cycle* argument or the visualization part were not that convincing or seems slightly over-claimed to me, nevertheless. - I think adding a discussion on how to generalize the framework into other threat models, e.g. L2 or (even) unrestricted attacks would further strengthen the paper. I feel the current framework may suffers some training difficulties on these other threat models, even such kinds of discussion would also valuable to understand the method. - As the current formulation can generalize the general inner maximization optimization process, comparing or applying the method with a more recent form of adversarial training, e.g. TRADES, would be nice to demonstrate the general applicability of the method.\n",
      "  - Questions about Ablation Studies and Component Contribution (L2, 2)\n",
      "    - Review:###This paper proposes a novel algorithm NeuralMCS for maximum common subgraph (MCS) identification. The proposed algorithm consists of two components. One is a neural-network model based on Graph Matching Networks (GMN, Li et al, 2019) to learn a node-to-node matching matrix from examples of the ground-truth MCS results. Another is the algorithm called GSE (Guided Subgraph Extraction) to obtain an MCS by making an explicit assignment from the estimated matching matrix by the NN model. Experimental comparisons are made to other NN-based approaches combined with threshold-based assignments by the Hungarian algorithm (w.r.t the accuracy) and to a state-of-the-art exact algorithm MCSplit, and show the effectiveness of NeuralMCS. This paper proposes an interesting method for MCS detection which would have large application interests. Though the basic idea is nice, the reported performance gains would be a bit less convincing due to the following evaluation problem and its weak novelty. The algorithm has two parts, and the first NN part to learn a matching matrix is mostly based on the already existing algorithm of GMN (Li et al, 2019). The novel part would be primarily in post-processing normalization (described in 3.1) for the matching matrix and seems to also be applicable to other NNs (for example, GAT?). The second part GSE to get an explicit subgraph also seem to be applied independently to the first part. We can see that combining these two parts worked, but it is unclear how each component contributes to the performance gain compared to any possible alternatives of each part. I understand that MCS detection from a matching matrix (and node state vectors) is not exact if we just use Hungarian-like linear assignment problem (LAP) solvers for a submatrix obtained by a simple thresholding, but both post-processing normalization and GSE parts (which brings the novelty) can be more carefully evaluated through some *ablation studies* using some simple alternative substitutes. (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposes a novel algorithm NeuralMCS for maximum common subgraph (MCS) identification. The proposed algorithm consists of two components. One is a neural-network model based on Graph Matching Networks (GMN, Li et al, 2019) to learn a node-to-node matching matrix from examples of the ground-truth MCS results. Another is the algorithm called GSE (Guided Subgraph Extraction) to obtain an MCS by making an explicit assignment from the estimated matching matrix by the NN model. Experimental comparisons are made to other NN-based approaches combined with threshold-based assignments by the Hungarian algorithm (w.r.t the accuracy) and to a state-of-the-art exact algorithm MCSplit, and show the effectiveness of NeuralMCS. This paper proposes an interesting method for MCS detection which would have large application interests. Though the basic idea is nice, the reported performance gains would be a bit less convincing due to the following evaluation problem and its weak novelty. The algorithm has two parts, and the first NN part to learn a matching matrix is mostly based on the already existing algorithm of GMN (Li et al, 2019). The novel part would be primarily in post-processing normalization (described in 3.1) for the matching matrix and seems to also be applicable to other NNs (for example, GAT?). The second part GSE to get an explicit subgraph also seem to be applied independently to the first part. We can see that combining these two parts worked, but it is unclear how each component contributes to the performance gain compared to any possible alternatives of each part. I understand that MCS detection from a matching matrix (and node state vectors) is not exact if we just use Hungarian-like linear assignment problem (LAP) solvers for a submatrix obtained by a simple thresholding, but both post-processing normalization and GSE parts (which brings the novelty) can be more carefully evaluated through some *ablation studies* using some simple alternative substitutes.\n",
      "    - Review:###This paper tackles the problem of affective impact prediction from multimodal sequences. The authors achieve state-of-the-art performance by using (i) a two-time-scale temporal feature extractor, (ii) progressive training strategy for multi-modal feature fusion, and (iii) pretraining. They divide a long video sequence evenly into several clips. The idea of applying one LSTM for intra-clips (short-time) temporal feature extraction and another LSTM for inter-clips (long-time) temporal feature extraction, resulting in two-time-scale, looks reasonable choice but not novel. The proposed progressive training strategy is mainly used for the modality-specific LSTMs training, which is used during the intra-clip short-time modeling phrase. It seems working well in practice. However, from the explanation, it*s not clear why this strategy is good for the *complementary* fusion. Each modality LSTM is trained sequentially with features extracted from other fixed modality LSTMs. Also, the authors seem not explaining why they set the training order for LSTMs at this stage to be the descending order of their performance in the previous stage. In the experiments section, two weak baseline models and several previous state-of-the-art models are compared. However, enough ablation studies on the two-time-scale structure and proposed training strategy are not provided to demonstrate that the proposed method does provide complementary multi-modal feature fusion, which is claimed as a contribution. Overall, although it achieves state-of-the-art performance on the task, none of the claimed contributions is novel or significant enough. It is a combination of existing ideas (but giving good performance). The proposed training procedure is also weak to be considered as a scientific contribution. (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###This paper tackles the problem of affective impact prediction from multimodal sequences. The authors achieve state-of-the-art performance by using (i) a two-time-scale temporal feature extractor, (ii) progressive training strategy for multi-modal feature fusion, and (iii) pretraining. They divide a long video sequence evenly into several clips. The idea of applying one LSTM for intra-clips (short-time) temporal feature extraction and another LSTM for inter-clips (long-time) temporal feature extraction, resulting in two-time-scale, looks reasonable choice but not novel. The proposed progressive training strategy is mainly used for the modality-specific LSTMs training, which is used during the intra-clip short-time modeling phrase. It seems working well in practice. However, from the explanation, it*s not clear why this strategy is good for the *complementary* fusion. Each modality LSTM is trained sequentially with features extracted from other fixed modality LSTMs. Also, the authors seem not explaining why they set the training order for LSTMs at this stage to be the descending order of their performance in the previous stage. In the experiments section, two weak baseline models and several previous state-of-the-art models are compared. However, enough ablation studies on the two-time-scale structure and proposed training strategy are not provided to demonstrate that the proposed method does provide complementary multi-modal feature fusion, which is claimed as a contribution. Overall, although it achieves state-of-the-art performance on the task, none of the claimed contributions is novel or significant enough. It is a combination of existing ideas (but giving good performance). The proposed training procedure is also weak to be considered as a scientific contribution.\n",
      "- Experiment and Methodological Explanation (experimental setup, baseline justification, technical parameter explanation) (L3, 2)\n",
      "  - Review:###*UPDATE* I have read the other reviews, authors* comments and the revised version of the manuscript. I have modified my rating to accept. The updated version, with variance across runs reported, comparison to randomly wired networks, and clearer writing, is substantially better. The core idea is simple to understand in retrospect, and could lead to more follow-up work in the vein of DenseNets and DART (with a more constrained search space). The paper proposes a refinement of the idea behind DenseNets -- rather than summing over all previous layers* outputs, sum a weighted combination instead where the weights are learned. This idea can be extended to search through the space of all possible residual connections, which they call TopoNet. This is practically achieved by enforcing a sparsity constraint on the learned weights. There is an additional nuance when enforcing the sparsity constraint: downstream layers have many more incoming residual connections, and may need an appropriately scaled sparsity penalty. It may help to clarify in the text that weights can be positive or negative (the current motivation from the point of view of residuals at different intervals suggests all the weights should be non-negative). Table 3 is baffling. Were the initial edge values (column 2) chosen so as to make the number of params and FLOPS somewhat comparable across different rows? How were these initial edge values set (seems very specific for N_3 to go from 46 to 103 when residual interval goes from 4 to 2, etc.)? The comment that number of params and FLOPS changes are negligible is puzzling; clearly Random, p=0.01 should use much fewer FLOPS than Random, p=0.99. Without additional information behind the numbers for the baselines in Table 3, it is unclear if TopoNets indeed give an improvement over the baselines (Random and Residual). The text will also benefit from a careful elaboration of the differences in the Random baselines in the paper vs. the Xie et al approach of trying random architectures. (L2, 1)\n",
      "    * Examples:\n",
      "      - Review:###*UPDATE* I have read the other reviews, authors* comments and the revised version of the manuscript. I have modified my rating to accept. The updated version, with variance across runs reported, comparison to randomly wired networks, and clearer writing, is substantially better. The core idea is simple to understand in retrospect, and could lead to more follow-up work in the vein of DenseNets and DART (with a more constrained search space). The paper proposes a refinement of the idea behind DenseNets -- rather than summing over all previous layers* outputs, sum a weighted combination instead where the weights are learned. This idea can be extended to search through the space of all possible residual connections, which they call TopoNet. This is practically achieved by enforcing a sparsity constraint on the learned weights. There is an additional nuance when enforcing the sparsity constraint: downstream layers have many more incoming residual connections, and may need an appropriately scaled sparsity penalty. It may help to clarify in the text that weights can be positive or negative (the current motivation from the point of view of residuals at different intervals suggests all the weights should be non-negative). Table 3 is baffling. Were the initial edge values (column 2) chosen so as to make the number of params and FLOPS somewhat comparable across different rows? How were these initial edge values set (seems very specific for N_3 to go from 46 to 103 when residual interval goes from 4 to 2, etc.)? The comment that number of params and FLOPS changes are negligible is puzzling; clearly Random, p=0.01 should use much fewer FLOPS than Random, p=0.99. Without additional information behind the numbers for the baselines in Table 3, it is unclear if TopoNets indeed give an improvement over the baselines (Random and Residual). The text will also benefit from a careful elaboration of the differences in the Random baselines in the paper vs. the Xie et al approach of trying random architectures.\n",
      "  - Review:###The paper proposes a method to learn an explanation of black-box systems from its outputs. The method is based on the information bottleneck as the objective function is designed to measure mutual information between input x, system output y, and narrowed information of input t. t is constructed by filtering x with maintaining interpretability of y, so that it is finally assumed as the explanation of the system extracted by the proposed method. The paper is well motivated and well written. Enough experiments were conducted to assess the advantage of the proposed method in the classification tasks. It looks a good paper. Maybe the paper is focused on only tasks that the predictor does not generate much information, such as classification. It is still unclear how the proposed method work when it is applied to the output-rich models, i.e., the model should keep as much information as inputs. The proposed method automatically selects some important chunks from inputs, but the chunks still rely on some task-specific hand-crafted chunking strategies. The paper also conducted some experiments by changing the strategy, but it is still unclear what is the important criteria. It is also good to show how actually the thickness of the bottleneck (controlled by k) works in actual cases, e.g., showing results for the same example with moving k. Trivial comments: * The example in 3.2 *great, great* and *great, thought provoking* looks still ambiguous to explain what the section want to say. * *x_i \times z_j* in p.iv looks ambiguous. * z_j^* in 3rd eqn. of p.v should take l: z_j^{*(l)} * the max operator over l in 3rd eqn. of p.v looks to hide other values than the highest one (specifically, the L1 norm of z_j^* does not become k by this eqn. as the k-hot vector does). Summation looks intuitively better than max. Could you explain how this eqn. was constructed? * f(.) in the 4th eqn. of p.v may be undefined in the main text. *eta_1 of Adam looks to be set to not a standard value (0.9). Is there any reason? (L2, 1)\n",
      "    * Examples:\n",
      "      - Review:###The paper proposes a method to learn an explanation of black-box systems from its outputs. The method is based on the information bottleneck as the objective function is designed to measure mutual information between input x, system output y, and narrowed information of input t. t is constructed by filtering x with maintaining interpretability of y, so that it is finally assumed as the explanation of the system extracted by the proposed method. The paper is well motivated and well written. Enough experiments were conducted to assess the advantage of the proposed method in the classification tasks. It looks a good paper. Maybe the paper is focused on only tasks that the predictor does not generate much information, such as classification. It is still unclear how the proposed method work when it is applied to the output-rich models, i.e., the model should keep as much information as inputs. The proposed method automatically selects some important chunks from inputs, but the chunks still rely on some task-specific hand-crafted chunking strategies. The paper also conducted some experiments by changing the strategy, but it is still unclear what is the important criteria. It is also good to show how actually the thickness of the bottleneck (controlled by k) works in actual cases, e.g., showing results for the same example with moving k. Trivial comments: * The example in 3.2 *great, great* and *great, thought provoking* looks still ambiguous to explain what the section want to say. * *x_i \times z_j* in p.iv looks ambiguous. * z_j^* in 3rd eqn. of p.v should take l: z_j^{*(l)} * the max operator over l in 3rd eqn. of p.v looks to hide other values than the highest one (specifically, the L1 norm of z_j^* does not become k by this eqn. as the k-hot vector does). Summation looks intuitively better than max. Could you explain how this eqn. was constructed? * f(.) in the 4th eqn. of p.v may be undefined in the main text. *eta_1 of Adam looks to be set to not a standard value (0.9). Is there any reason?\n",
      "- Mutual Information Usage and Edge Feature Integration in Graph Neural Networks (L3, 2)\n",
      "  - Review:###In this paper, the authors proposed a new kind of graph neural network that can use continuous edge features. Specifically, a variational lower bound is proposed for mutual information and integrated into the GNN model so that MI between edge features and the message passing channel is maximized. Experiments on molecule graph datasets and knowledge graph datasets show the effectiveness of the proposed method compared to the state-of-the-art GNN models. Utilizing continuous edge features in GNN is an important and difficult task. The authors proposed an elegant solution. The paper is well written and extensive experiments on large scale datasets are compared with 9 competitors that are all outperformed by EIGNN on nearly all the cases. To conclude, I think the paper will be a good addition to the conference. (L2, 1)\n",
      "    * Examples:\n",
      "      - Review:###In this paper, the authors proposed a new kind of graph neural network that can use continuous edge features. Specifically, a variational lower bound is proposed for mutual information and integrated into the GNN model so that MI between edge features and the message passing channel is maximized. Experiments on molecule graph datasets and knowledge graph datasets show the effectiveness of the proposed method compared to the state-of-the-art GNN models. Utilizing continuous edge features in GNN is an important and difficult task. The authors proposed an elegant solution. The paper is well written and extensive experiments on large scale datasets are compared with 9 competitors that are all outperformed by EIGNN on nearly all the cases. To conclude, I think the paper will be a good addition to the conference.\n",
      "  - Review:###This paper introduces a mutual information term into the training objective of message passing graph neural networks. The additional term favors the preservation on information in a mapping from an input edge feature vector e_{i,j} to a weight matrix f(e_{i,j}) used in computing messages across the edge from node i to node j. A variational lower bound on the mutual information is used in training. Impressive empirical results are given for chemical property prediction and relation prediction in knowledge graphs. I have no real complaints other than I might recommend citing the original work on infomax: *Self Organization in a Perceptual Network*, Ralph Linsker, 1988. Postscript: I have been swayed by the complaints of reviewer 1 and reduced my score to weak reject. (L2, 1)\n",
      "    * Examples:\n",
      "      - Review:###This paper introduces a mutual information term into the training objective of message passing graph neural networks. The additional term favors the preservation on information in a mapping from an input edge feature vector e_{i,j} to a weight matrix f(e_{i,j}) used in computing messages across the edge from node i to node j. A variational lower bound on the mutual information is used in training. Impressive empirical results are given for chemical property prediction and relation prediction in knowledge graphs. I have no real complaints other than I might recommend citing the original work on infomax: *Self Organization in a Perceptual Network*, Ralph Linsker, 1988. Postscript: I have been swayed by the complaints of reviewer 1 and reduced my score to weak reject.\n",
      "- Novelty, Prior Work Distinction, and Contextualization (L3, 35)\n",
      "  - Lack of Context or Incomplete Related Work (L2, 3)\n",
      "    - Review:###In this paper, the authors propose a method for making a neural network equivariant. Their method also can be applied to make each layer equivariant too. Strengths: -- The paper is very well written and easy to follow with clear notation. -- The derivations seem to be correct. Weaknesses: -- The experiment is nice but very limited and does not demonstrate the benefits of having an equivariant network. For example, the authors do not report the accuracy of recovering the original (0) rotation. -- The novelty of the work is questionable. While the development is different, the final example for equivarification of a neural network is very similar to the existing works by Cohen and Welling. -- There are other works on equivarification that are missed by this paper. For example, consider the following paper: Lenssen, J. E., Fey, M., & Libuschewski, P. (2018). Group equivariant capsule networks. In NeurIPS. -- The layer-wise equivariant method does have extra computational overheads. -- The fact that we have to specify the groups that we want to make the network equivariant with respect to is a limitation. The promise of capsule networks, in contrast, is to *ideally* learn the pose (variation) vectors in a data-driven way. Sabour, S., Frosst, N., & Hinton, G. E. (2017). Dynamic routing between capsules. In NeurIPS. -- The following statements need more explanation: * *However, these may require extra training overhead as the augmented data increase.* (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###In this paper, the authors propose a method for making a neural network equivariant. Their method also can be applied to make each layer equivariant too. Strengths: -- The paper is very well written and easy to follow with clear notation. -- The derivations seem to be correct. Weaknesses: -- The experiment is nice but very limited and does not demonstrate the benefits of having an equivariant network. For example, the authors do not report the accuracy of recovering the original (0) rotation. -- The novelty of the work is questionable. While the development is different, the final example for equivarification of a neural network is very similar to the existing works by Cohen and Welling. -- There are other works on equivarification that are missed by this paper. For example, consider the following paper: Lenssen, J. E., Fey, M., & Libuschewski, P. (2018). Group equivariant capsule networks. In NeurIPS. -- The layer-wise equivariant method does have extra computational overheads. -- The fact that we have to specify the groups that we want to make the network equivariant with respect to is a limitation. The promise of capsule networks, in contrast, is to *ideally* learn the pose (variation) vectors in a data-driven way. Sabour, S., Frosst, N., & Hinton, G. E. (2017). Dynamic routing between capsules. In NeurIPS. -- The following statements need more explanation: * *However, these may require extra training overhead as the augmented data increase.*\n",
      "    - Review:###In this work, the authors employ concepts from group theory to turn an arbitrary feed forward neural network into an equivariant one, i.e. a network whose output transforms in a way that is consistent with the transformation of the input. To this end, the authors first introduce the basic concepts of group theory required to follow their work and provide a comprehensive definition of equivariance. They then explain how to equivarify (w.r.t. a finite group G) a given neural network, and present experimental results on rotated MNIST digits to support their approach. I find that the proposed approach is very elegant and addresses a highly important question, namely how to devise or modify an architecture such that it preserves symmetries. In my opinion, the current paper makes an interesting theoretical contribution towards achieving this goal, but it also has several shortcomings that are detailed below. Based on these shortcomings, I recommend rejecting the paper but I would be willing to increase the score if these points were addressed in sufficient detail. Major comments: 1) Scaling The authors mention in the abstract that ‘although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one in terms of the number of the parameters.’ Based on Eq. (3.0.2) and Fig. 3, it is my understanding that n evaluations of each data point (input to a layer) are required for a cyclic group of order n. If the outputs of a layer are then concatenated, the input dimension of the subsequent layer grows by a factor of n. I would therefore argue that out-the-box application of the proposed approach does increase the number of variables dramatically and that the abstract is misleading in this respect. The authors briefly comment on this point with one sentence in the third paragraph of the introduction. However, I would appreciate if this point was addressed in more detail, for example in a dedicated paragraph after the theory is introduced. Please also address the question of whether variable sharing is essential from an equivariance point of view, or whether it’s simply a necessity to prevent an explosion of the number of parameters. Furthermore, convolutions encode translational symmetry which may be beneficial for the current application but may not be desirable for other datasets. A few comments for clarification would be very helpful. Finally, the equivarified network seems to increase the required number of computations significantly compared to the original one, which I find worth a comment . 2) Experiment The authors only consider a convolutional architecture and say that ‘in order to illustrate flexibility we choose not to simply equivarify the original network’. However, to me one of the main advantages of the paper seems to be that you can take this approach to equivarify any FFN. It would therefore be interesting to see this approach be applied to different networks starting with a simpler one, e.g. a 2-layer MLP. The authors could then compare the original network to the equivarified one with and without variable sharing. That would not only help the reader understand the approach better but also be much more in line with the main motivation of the paper. Then adding a second experiment, e.g. a convolutional architecture, to demonstrate flexibility would be very interesting. With regard to Fig. 4, I think there may be better ways of summarising the results than dumping 160 numbers of which only 4 seem to be of interest. The message seems to be that the network yields identical probabilities irrespective of the degree of rotation. What I find surprising is that all numbers are actually identical (shifted by 10). Is this by construction? 3) Limitations As indicated in the second paragraph of Sec. 4, this approach is limited to finite groups and the authors only consider image rotations w.r.t. the cyclic group of degree 4. Although I appreciate that this is meant to serve as a toy problem to illustrate that the approach works, I do not think that rotations by a constant angle are very interesting. What would be really interesting is equivariance w.r.t continuous rotations (Lie Groups), e.g. the SO(2) in this particular case. I doubt that an extension to the SO(2) is straightforward within the current theoretical framework. However, even if that is the case, I would appreciate if the authors could comment on this in a paragraph. Minor comments: i) There are many typos and grammar mistakes in the paper: ‘any feedforward networks’ -> ‘any feedforward network’. ‘enables to design’ -> ‘enables us to design’ ‘our proposed equivarification method use’ -> ‘our proposed equivarification method uses’ ‘traditional sense multiplication’ -> ‘traditional sense of multiplication’ ‘a group G acts’ -> ‘a group G that acts’ ‘neural network that defined on the’ -> ‘neural network that is defined on the’ ‘which achieves promising performance’ -> ‘which achieve promising performance’ ‘supplymentary material’ -> ‘supplementary material’ Etc. ii) I think there may be a mistake in the 3rd point of Definition 3.0.3: For consistency with the previous definitions and with Fig. 1, shouldn’t F map from X to Z and hat F from X to hat Z? iii) Last paragraph of Sec 3: ‘then after the identification hat F becomes a map from Z to...*. Should it be ‘a map from X to ..’? iv) In Definition 3.0.3 you define the tuple (hat Z, T, p) to be a G-equivarification, but in the paragraph below you call the G-product itself a G-equivarification (without including T and p). v) Footnote 2: You could correct for that and present the theory shifting by g instead of g^-1 to make it easier for the reader to follow. Or, at least, give a reference to the footnote earlier on in Example 4.0.1 to avoid confusion. vi) Unless there is a special reason for this, I would suggest changing the enumeration of definitions, lemmas, examples and equations, i.e. (3.0.1) -> (3.1), etc... ********************************************************* I increased my rating based on the authors addressing many of my comments. ********************************************************* (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###In this work, the authors employ concepts from group theory to turn an arbitrary feed forward neural network into an equivariant one, i.e. a network whose output transforms in a way that is consistent with the transformation of the input. To this end, the authors first introduce the basic concepts of group theory required to follow their work and provide a comprehensive definition of equivariance. They then explain how to equivarify (w.r.t. a finite group G) a given neural network, and present experimental results on rotated MNIST digits to support their approach. I find that the proposed approach is very elegant and addresses a highly important question, namely how to devise or modify an architecture such that it preserves symmetries. In my opinion, the current paper makes an interesting theoretical contribution towards achieving this goal, but it also has several shortcomings that are detailed below. Based on these shortcomings, I recommend rejecting the paper but I would be willing to increase the score if these points were addressed in sufficient detail. Major comments: 1) Scaling The authors mention in the abstract that ‘although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one in terms of the number of the parameters.’ Based on Eq. (3.0.2) and Fig. 3, it is my understanding that n evaluations of each data point (input to a layer) are required for a cyclic group of order n. If the outputs of a layer are then concatenated, the input dimension of the subsequent layer grows by a factor of n. I would therefore argue that out-the-box application of the proposed approach does increase the number of variables dramatically and that the abstract is misleading in this respect. The authors briefly comment on this point with one sentence in the third paragraph of the introduction. However, I would appreciate if this point was addressed in more detail, for example in a dedicated paragraph after the theory is introduced. Please also address the question of whether variable sharing is essential from an equivariance point of view, or whether it’s simply a necessity to prevent an explosion of the number of parameters. Furthermore, convolutions encode translational symmetry which may be beneficial for the current application but may not be desirable for other datasets. A few comments for clarification would be very helpful. Finally, the equivarified network seems to increase the required number of computations significantly compared to the original one, which I find worth a comment . 2) Experiment The authors only consider a convolutional architecture and say that ‘in order to illustrate flexibility we choose not to simply equivarify the original network’. However, to me one of the main advantages of the paper seems to be that you can take this approach to equivarify any FFN. It would therefore be interesting to see this approach be applied to different networks starting with a simpler one, e.g. a 2-layer MLP. The authors could then compare the original network to the equivarified one with and without variable sharing. That would not only help the reader understand the approach better but also be much more in line with the main motivation of the paper. Then adding a second experiment, e.g. a convolutional architecture, to demonstrate flexibility would be very interesting. With regard to Fig. 4, I think there may be better ways of summarising the results than dumping 160 numbers of which only 4 seem to be of interest. The message seems to be that the network yields identical probabilities irrespective of the degree of rotation. What I find surprising is that all numbers are actually identical (shifted by 10). Is this by construction? 3) Limitations As indicated in the second paragraph of Sec. 4, this approach is limited to finite groups and the authors only consider image rotations w.r.t. the cyclic group of degree 4. Although I appreciate that this is meant to serve as a toy problem to illustrate that the approach works, I do not think that rotations by a constant angle are very interesting. What would be really interesting is equivariance w.r.t continuous rotations (Lie Groups), e.g. the SO(2) in this particular case. I doubt that an extension to the SO(2) is straightforward within the current theoretical framework. However, even if that is the case, I would appreciate if the authors could comment on this in a paragraph. Minor comments: i) There are many typos and grammar mistakes in the paper: ‘any feedforward networks’ -> ‘any feedforward network’. ‘enables to design’ -> ‘enables us to design’ ‘our proposed equivarification method use’ -> ‘our proposed equivarification method uses’ ‘traditional sense multiplication’ -> ‘traditional sense of multiplication’ ‘a group G acts’ -> ‘a group G that acts’ ‘neural network that defined on the’ -> ‘neural network that is defined on the’ ‘which achieves promising performance’ -> ‘which achieve promising performance’ ‘supplymentary material’ -> ‘supplementary material’ Etc. ii) I think there may be a mistake in the 3rd point of Definition 3.0.3: For consistency with the previous definitions and with Fig. 1, shouldn’t F map from X to Z and hat F from X to hat Z? iii) Last paragraph of Sec 3: ‘then after the identification hat F becomes a map from Z to...*. Should it be ‘a map from X to ..’? iv) In Definition 3.0.3 you define the tuple (hat Z, T, p) to be a G-equivarification, but in the paragraph below you call the G-product itself a G-equivarification (without including T and p). v) Footnote 2: You could correct for that and present the theory shifting by g instead of g^-1 to make it easier for the reader to follow. Or, at least, give a reference to the footnote earlier on in Example 4.0.1 to avoid confusion. vi) Unless there is a special reason for this, I would suggest changing the enumeration of definitions, lemmas, examples and equations, i.e. (3.0.1) -> (3.1), etc... ********************************************************* I increased my rating based on the authors addressing many of my comments. *********************************************************\n",
      "    - Review:###This paper proposes a method, known as DrGCN, for reweighting the different dimensions of the node representations in graph convolutional networks (GCN). Specifically, the representation of every node is element-wise multiplied with a weight vector, which is parameterized as a function of the average input node representation, where the function is a two-layer neural network. At a conceptual level, this is similar to various existing normalization schemes, such as batch normalization and weight normalization. While it is claimed in Section 4.3 that the difference is that “[batch normalization] reduces variance between samples, while DrGCN reduces variance between dimensions”, I am not sure if this characterization is accurate. Batch normalization actually makes each dimension have unit (sample) variance and so does not make the variance of each dimension small. What it does is to make the sample variance of each dimension equal, which is also what DrGCN tries to do, i.e.: reduce variance across dimensions (since samples in the case of GCNs are the representations of different nodes). DrGCN is also similar to weight normalization because like weight normalization, DrGCN learns a transformation on top of the vanilla representation (in the case of weight normalization, the vanilla representation is the normalized weight vector; in the case of DrGCN, the vanilla representation is the node representation before reweighting). The conceptual contribution therefore seems incremental. Incremental conceptual contributions would be fine if (1) they result in a surprising theoretical result, or if (2) they result in a surprising improvement in empirical performance. Unfortunately, neither seems to be demonstrated in this paper. In the theoretical analysis, there are various occurrences of unjustified leaps of logic; as a result, what is claimed to be shown by the analysis is different from what is actually shown, and it is unclear what is actually shown is substantially related to the proposed method. For example, in Section 3.1, the paper says that “GCNs are different from fully-connected networks only in \tilde{A}, and degrade to fully-connected networks when \tilde{A} = I. So, our analysis can be somehow be generalized to DrGCNs.” The first sentence is true; in other words, it says that fully-connected networks are a special case of GCNs when \tilde{A} = I. However, the second sentence does not follow - just showing a special case (which is what the subsequent analysis does) does not say much about the general case. Relatedly, the architecture that is analyzed is of the form H^l = W^l phi(S^l H^{l-1}) + b^l for l = 1, …, k, whereas the architecture that is proposed is H^l = phi(W^l S^l H^{l-1} \tilde{A}^{l} + b^{l}) for l = 1, …, k. The latter cannot be cast as the former unless \tilde{A} is diagonal. Also, the caveat of the mean field approximation are not stated - whatever result that is shown is only valid at the infinite width limit, which is different from what is claimed in the abstract, which says that *We prove that DrGCNs can reduce the variance of the node representations by connecting our problem to the theory of the mean field.” Additionally, the analysis is done in the case where S is directly parameterized, whereas the proposed method parameterizes S as the output of a two-layer neural network. I presume the reason why latter was done in practice because it worked better empirically. Because this is not explained by the analysis, the theory is incomplete, and so this caveat should be clearly stated in the abstract. In the experimental results, the performance improvement of DrGCN over layer normalization is not statistically significant. Also, DrGCN is only compared to other normalization schemes (batch normalization and layer normalization) on one dataset, and so there is insufficient evidence to conclude that DrGCN generally works better than existing methods empirically, Additionally, in section 2.1, it is claimed that “sampling-based GCNs still lie within the framework of equation (2), as we can set all unsampled edges to 0 in \tilde{A}”. This does not seem to be accurate, because in sample-based GCNs, different edges are sampled in every minibatch, and so there is no fixed choice of \tilde{A} that makes these GCNs equivalent to the formulation in eq. (2). Also, if the goal is to reduce variance across dimensions (as the paper claims in Section 4.3), why was the average node representation fed into the two-layer neural network rather than its reciprocal? (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###This paper proposes a method, known as DrGCN, for reweighting the different dimensions of the node representations in graph convolutional networks (GCN). Specifically, the representation of every node is element-wise multiplied with a weight vector, which is parameterized as a function of the average input node representation, where the function is a two-layer neural network. At a conceptual level, this is similar to various existing normalization schemes, such as batch normalization and weight normalization. While it is claimed in Section 4.3 that the difference is that “[batch normalization] reduces variance between samples, while DrGCN reduces variance between dimensions”, I am not sure if this characterization is accurate. Batch normalization actually makes each dimension have unit (sample) variance and so does not make the variance of each dimension small. What it does is to make the sample variance of each dimension equal, which is also what DrGCN tries to do, i.e.: reduce variance across dimensions (since samples in the case of GCNs are the representations of different nodes). DrGCN is also similar to weight normalization because like weight normalization, DrGCN learns a transformation on top of the vanilla representation (in the case of weight normalization, the vanilla representation is the normalized weight vector; in the case of DrGCN, the vanilla representation is the node representation before reweighting). The conceptual contribution therefore seems incremental. Incremental conceptual contributions would be fine if (1) they result in a surprising theoretical result, or if (2) they result in a surprising improvement in empirical performance. Unfortunately, neither seems to be demonstrated in this paper. In the theoretical analysis, there are various occurrences of unjustified leaps of logic; as a result, what is claimed to be shown by the analysis is different from what is actually shown, and it is unclear what is actually shown is substantially related to the proposed method. For example, in Section 3.1, the paper says that “GCNs are different from fully-connected networks only in \tilde{A}, and degrade to fully-connected networks when \tilde{A} = I. So, our analysis can be somehow be generalized to DrGCNs.” The first sentence is true; in other words, it says that fully-connected networks are a special case of GCNs when \tilde{A} = I. However, the second sentence does not follow - just showing a special case (which is what the subsequent analysis does) does not say much about the general case. Relatedly, the architecture that is analyzed is of the form H^l = W^l phi(S^l H^{l-1}) + b^l for l = 1, …, k, whereas the architecture that is proposed is H^l = phi(W^l S^l H^{l-1} \tilde{A}^{l} + b^{l}) for l = 1, …, k. The latter cannot be cast as the former unless \tilde{A} is diagonal. Also, the caveat of the mean field approximation are not stated - whatever result that is shown is only valid at the infinite width limit, which is different from what is claimed in the abstract, which says that *We prove that DrGCNs can reduce the variance of the node representations by connecting our problem to the theory of the mean field.” Additionally, the analysis is done in the case where S is directly parameterized, whereas the proposed method parameterizes S as the output of a two-layer neural network. I presume the reason why latter was done in practice because it worked better empirically. Because this is not explained by the analysis, the theory is incomplete, and so this caveat should be clearly stated in the abstract. In the experimental results, the performance improvement of DrGCN over layer normalization is not statistically significant. Also, DrGCN is only compared to other normalization schemes (batch normalization and layer normalization) on one dataset, and so there is insufficient evidence to conclude that DrGCN generally works better than existing methods empirically, Additionally, in section 2.1, it is claimed that “sampling-based GCNs still lie within the framework of equation (2), as we can set all unsampled edges to 0 in \tilde{A}”. This does not seem to be accurate, because in sample-based GCNs, different edges are sampled in every minibatch, and so there is no fixed choice of \tilde{A} that makes these GCNs equivalent to the formulation in eq. (2). Also, if the goal is to reduce variance across dimensions (as the paper claims in Section 4.3), why was the average node representation fed into the two-layer neural network rather than its reciprocal?\n",
      "  - Novelty, Distinction from Prior Work, and Citation Issues (L2, 32)\n",
      "    - Comparison to Prior or Related Work and Novelty Justification (L1, 2)\n",
      "      - Review:###Review of “Improving Evolutionary Strategies with Generative Neural Networks” Typically in ES, the distribution of solution candidates come from a hand-engineered distribution (i.e. multivariate Gaussian, or other parametrized distributions). In place of hand-engineered distribution choices, they introduce the use of GANs as a tool for Evolution Strategies. I liked the novelty of combining the use of GANs in new directions (namely ES/GA). The core idea is to model the density function using Generative Neural Networks (GNN, MacKay 1995), and find the parameters of this GNN using tools from the normalizing flows literature for their NICE invertible properties (okay pun intended :) along with GAN-style training using historical data from the ES process. They demonstrate their method on traditional blackbox optimization toy tasks (such as Rosenbrok and Rastrigin functions), and also on a few continuous control RL benchmark tasks, to demonstrate improved performance over a strong representative ES algorithm (XNES). Overall, I liked the work as it provides a fresh way of using GANs with another subfield (ES/GA). If they want to improve the work, I would suggest demonstrating that their approach can solve certain difficult tasks that traditional ES methods (or RL methods) cannot solve. Although the experiments chosen are not difficult ones, I believe they were chosen for clarity to showcase the method, so I think that is fine (in case there are complaints that they experiments are too simple). (For the record, I was looking to give a score of 7, but the ICLR system made me choose between 6 and 8, and I chose 8.) (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Review of “Improving Evolutionary Strategies with Generative Neural Networks” Typically in ES, the distribution of solution candidates come from a hand-engineered distribution (i.e. multivariate Gaussian, or other parametrized distributions). In place of hand-engineered distribution choices, they introduce the use of GANs as a tool for Evolution Strategies. I liked the novelty of combining the use of GANs in new directions (namely ES/GA). The core idea is to model the density function using Generative Neural Networks (GNN, MacKay 1995), and find the parameters of this GNN using tools from the normalizing flows literature for their NICE invertible properties (okay pun intended :) along with GAN-style training using historical data from the ES process. They demonstrate their method on traditional blackbox optimization toy tasks (such as Rosenbrok and Rastrigin functions), and also on a few continuous control RL benchmark tasks, to demonstrate improved performance over a strong representative ES algorithm (XNES). Overall, I liked the work as it provides a fresh way of using GANs with another subfield (ES/GA). If they want to improve the work, I would suggest demonstrating that their approach can solve certain difficult tasks that traditional ES methods (or RL methods) cannot solve. Although the experiments chosen are not difficult ones, I believe they were chosen for clarity to showcase the method, so I think that is fine (in case there are complaints that they experiments are too simple). (For the record, I was looking to give a score of 7, but the ICLR system made me choose between 6 and 8, and I chose 8.)\n",
      "      - Review:###[Note: I wrote this review after John Schulman*s first comment, before any reply, and before Gehrard Neumann*s comment] The authors propose an actor-critic algorithm based mostly on regression. Being off-policy, the algorithm can learn from multiple policies. It can also be applied to continuous as well as to discrete actions, and it can be trained in a batch RL setting. They compare it to a set of state-of-the-art algorithms in standard openAI gym continuous action benchmarks and show competitive performance despite a much simpler implementation of the algorithm. I basically subscribe to John Schulman*s comment below, both about empirical results and about citing Self-Imitation Learning, but I have a stronger point against the paper, which is insufficient positionning with respect to the relevant literature. The paper does not cite or discuss the one below, though it looks VERY close: @inproceedings{neumann2009fitted, title={Fitted Q-iteration by advantage weighted regression}, author={Neumann, Gerhard and Peters, Jan R}, booktitle={Advances in neural information processing systems}, pages={1177--1184}, year={2009} } This paper also starts from RWR and performs weighted regression based on the advantage rather than on the return. So to me it is exactly the same idea, and it is mandatory that the authors clearly establish what is the novelty of their work with respect to this previous paper. Less importantly, the authors may also want to have a look at : @article{zimmer2019exploiting, title={Exploiting the sign of the advantage function to learn deterministic policies in continuous domains}, author={Zimmer, Matthieu and Weng, Paul}, journal={arXiv preprint arXiv:1906.04556}, year={2019} } which also uses ideas along the same line. To me, a good way to improve the novelty of this work would be to perform a detailed empirical study of the inner mechanisms of the algorithm on very simple benchmarks where the value function and policy could be visualized. In particular, how stable is the estimation of the value function? This is known to be an issue, as most algorithms avoid approximating it and prefer estimating the Q-function. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###[Note: I wrote this review after John Schulman*s first comment, before any reply, and before Gehrard Neumann*s comment] The authors propose an actor-critic algorithm based mostly on regression. Being off-policy, the algorithm can learn from multiple policies. It can also be applied to continuous as well as to discrete actions, and it can be trained in a batch RL setting. They compare it to a set of state-of-the-art algorithms in standard openAI gym continuous action benchmarks and show competitive performance despite a much simpler implementation of the algorithm. I basically subscribe to John Schulman*s comment below, both about empirical results and about citing Self-Imitation Learning, but I have a stronger point against the paper, which is insufficient positionning with respect to the relevant literature. The paper does not cite or discuss the one below, though it looks VERY close: @inproceedings{neumann2009fitted, title={Fitted Q-iteration by advantage weighted regression}, author={Neumann, Gerhard and Peters, Jan R}, booktitle={Advances in neural information processing systems}, pages={1177--1184}, year={2009} } This paper also starts from RWR and performs weighted regression based on the advantage rather than on the return. So to me it is exactly the same idea, and it is mandatory that the authors clearly establish what is the novelty of their work with respect to this previous paper. Less importantly, the authors may also want to have a look at : @article{zimmer2019exploiting, title={Exploiting the sign of the advantage function to learn deterministic policies in continuous domains}, author={Zimmer, Matthieu and Weng, Paul}, journal={arXiv preprint arXiv:1906.04556}, year={2019} } which also uses ideas along the same line. To me, a good way to improve the novelty of this work would be to perform a detailed empirical study of the inner mechanisms of the algorithm on very simple benchmarks where the value function and policy could be visualized. In particular, how stable is the estimation of the value function? This is known to be an issue, as most algorithms avoid approximating it and prefer estimating the Q-function.\n",
      "    - Concerns about Novelty Versus Prior Work/Similar Methods (L1, 8)\n",
      "      -  The authors propose a search for neural architectures with different resource requirements by training a single model only. Furthermore, models found at the end of the search require no additional post-processing and are ready for deployment. A weight-sharing technique is applied to make this happen. The authors argue that there are multiple adaptions required to make it work. This includes the child model sampling strategy, use of model distillation, weight initialization, learning rate schedule, regularization and batch norm calibration. The work seems to be an incremental extension of Yu & Huang (2019b) and phrased as a NAS algorithm. Many techniques considered vital for the proposed method make use of techniques proposed by Yu & Huang (2019b) (sandwich rule, inplace distillation, batch norm calibration and the way how weights are shared). Other required techniques are either proposed by others (initialization) or very simple (learning rate schedule and regularization). The authors claim that they extend the work by Yu &Huang (2019b) *to handle a much larger space where the architectural dimensions, including kernel sizes, channel numbers, input resolutions, network depths are jointly search*. They never clarify why this is a non-trivial step and they might want to point this out in their rebuttal. Besides this, the authors did a very good job. The paper is well-written, references are given wherever needed and all the closest related work is covered sufficiently. The experimental part conducts several ablation studies which supports their various decisions. Unfortunately, all results reported use heavy data augmentation which makes a comparison to other methods (besides EfficientNet) impossible. 600M is considered the upper bound for mobile architectures by the NAS community. Unfortunately, no such model is considered making it even harder to compare to existing NAS methods. The EfficientNet numbers reported don*t match the ones reported in the original paper as far as I see. A red dot in Figure 3 could be added for the BigNASModel with 660M parameters. (L0, 1)\n",
      "        * Examples:\n",
      "          -  The authors propose a search for neural architectures with different resource requirements by training a single model only. Furthermore, models found at the end of the search require no additional post-processing and are ready for deployment. A weight-sharing technique is applied to make this happen. The authors argue that there are multiple adaptions required to make it work. This includes the child model sampling strategy, use of model distillation, weight initialization, learning rate schedule, regularization and batch norm calibration. The work seems to be an incremental extension of Yu & Huang (2019b) and phrased as a NAS algorithm. Many techniques considered vital for the proposed method make use of techniques proposed by Yu & Huang (2019b) (sandwich rule, inplace distillation, batch norm calibration and the way how weights are shared). Other required techniques are either proposed by others (initialization) or very simple (learning rate schedule and regularization). The authors claim that they extend the work by Yu &Huang (2019b) *to handle a much larger space where the architectural dimensions, including kernel sizes, channel numbers, input resolutions, network depths are jointly search*. They never clarify why this is a non-trivial step and they might want to point this out in their rebuttal. Besides this, the authors did a very good job. The paper is well-written, references are given wherever needed and all the closest related work is covered sufficiently. The experimental part conducts several ablation studies which supports their various decisions. Unfortunately, all results reported use heavy data augmentation which makes a comparison to other methods (besides EfficientNet) impossible. 600M is considered the upper bound for mobile architectures by the NAS community. Unfortunately, no such model is considered making it even harder to compare to existing NAS methods. The EfficientNet numbers reported don*t match the ones reported in the original paper as far as I see. A red dot in Figure 3 could be added for the BigNASModel with 660M parameters.\n",
      "      -  The paper presents the empirical observation that one can freeze (stop updating) a significant fraction of neural network parameters after only training for a short amount of time, without hurting final performance too much. The technical contribution made by this paper is an algorithm for determining which weights to freeze, called partial backpropagation, and an empirical validation of the algorithm on various models for image recognition. The observation that weights can be frozen is somewhat interesting, although similar findings have been reported before. It*s not clear the proposed algorithm is useful. The authors mention that fully parameterized models are expensive to run, but they don*t demonstrate any speed-ups using their approach. Such speed-up would also not be expected since the forward pass of the algorithm cannot get faster by freezing weights, and the impact on the backward pass is limited. I*d be willing to raise my rating if the authors can convince me of the usefulness of their algorithm. (L0, 1)\n",
      "        * Examples:\n",
      "          -  The paper presents the empirical observation that one can freeze (stop updating) a significant fraction of neural network parameters after only training for a short amount of time, without hurting final performance too much. The technical contribution made by this paper is an algorithm for determining which weights to freeze, called partial backpropagation, and an empirical validation of the algorithm on various models for image recognition. The observation that weights can be frozen is somewhat interesting, although similar findings have been reported before. It*s not clear the proposed algorithm is useful. The authors mention that fully parameterized models are expensive to run, but they don*t demonstrate any speed-ups using their approach. Such speed-up would also not be expected since the forward pass of the algorithm cannot get faster by freezing weights, and the impact on the backward pass is limited. I*d be willing to raise my rating if the authors can convince me of the usefulness of their algorithm.\n",
      "      -  The paper targets on learning the number of channels across all layers, under computation/model size/memory constraints. The method is simple and the results seems promising. However, the following issues need to be resolved: 1. The main method is based on published *slimmable networks,* such that the novelty is limited; 2. The method is very simpler to DropPath in [1], which uses DropPath to learn important branches while this paper uses it to learn channels. They are similar. 3. Better ablation studies are required in Table 1. This table should be simplified. As the method cannot learn architectures but channel numbers, the only useful pairs of comparisons are those having the same architecture, such as a pair of MobileNet vs AutoSlim-MobileNet. 4. an important detail is missing: where does the AutoSlim start from? Does it start from a larger model than the baseline? In the set of *500M FLOPs* experiments, I see the size of *AutoSlim-MobileNet v1* (4.6M) is larger than *MobileNet v1 1.0x* (4.2M), this implies that AutoSlim start from a *MobileNet v1 Nx* and N > 1.0. What is exactly N? 5. If AutoSlim starts from a larger baseline model with N times (N > 1.0) width, then the pruning baseline methods (AMC and ThiNet) should also start from the same larger models for fair comparison. In general, starting from a larger model and pruning it down can achieve a better accuracy vs. size trade-off. 6. *300 epochs with linearly decaying learning rate for mobile networks, 100 epochs with step learning rate schedule for ResNet-50 based models*, are baselines trained in the same way? Minor: 1. missing captions in a couple of figures, e.g., Figure 5. 2. *the importance of trained weights* vs *the importance of channel numbers* is trivial [1] Bender, Gabriel, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. *Understanding and simplifying one-shot architecture search.* In International Conference on Machine Learning, pp. 549-558. 2018. (L0, 1)\n",
      "        * Examples:\n",
      "          -  The paper targets on learning the number of channels across all layers, under computation/model size/memory constraints. The method is simple and the results seems promising. However, the following issues need to be resolved: 1. The main method is based on published *slimmable networks,* such that the novelty is limited; 2. The method is very simpler to DropPath in [1], which uses DropPath to learn important branches while this paper uses it to learn channels. They are similar. 3. Better ablation studies are required in Table 1. This table should be simplified. As the method cannot learn architectures but channel numbers, the only useful pairs of comparisons are those having the same architecture, such as a pair of MobileNet vs AutoSlim-MobileNet. 4. an important detail is missing: where does the AutoSlim start from? Does it start from a larger model than the baseline? In the set of *500M FLOPs* experiments, I see the size of *AutoSlim-MobileNet v1* (4.6M) is larger than *MobileNet v1 1.0x* (4.2M), this implies that AutoSlim start from a *MobileNet v1 Nx* and N > 1.0. What is exactly N? 5. If AutoSlim starts from a larger baseline model with N times (N > 1.0) width, then the pruning baseline methods (AMC and ThiNet) should also start from the same larger models for fair comparison. In general, starting from a larger model and pruning it down can achieve a better accuracy vs. size trade-off. 6. *300 epochs with linearly decaying learning rate for mobile networks, 100 epochs with step learning rate schedule for ResNet-50 based models*, are baselines trained in the same way? Minor: 1. missing captions in a couple of figures, e.g., Figure 5. 2. *the importance of trained weights* vs *the importance of channel numbers* is trivial [1] Bender, Gabriel, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. *Understanding and simplifying one-shot architecture search.* In International Conference on Machine Learning, pp. 549-558. 2018.\n",
      "      - Review:###The authors proposed a method to optimize the topology of neural networks in a soft fashion. The main idea is to formulate the network as a complete graph (or a sequence of complete subgraphs), and to optimize the relative importance of each edge using gradient descent. The overall approach is similar to differentiable architecture search, except that (1) the continuous architecture is optimized wrt the training set (instead of the validation set), and (2) the learned architecture is never discretized at the end of training. The paper is well-organized and easy to follow. The authors have also conducted controlled experiments to convincingly show that the method is leading to improvement. I*m a bit concerned about the technical novelty, however, as the approach can be viewed as an application of (a simplified version of) differentiable NAS to a search space analogous to the one used in [1]. In fact, the notion of soft topology has already been introduced in this prior work (Figure 2 in [1]: *The aggregation is done by weighted sum with learnable positive weights w0, w1, w2*), which was also optimized using gradient descent. A difference between this work and [1] is that whether the underlying graph is complete or randomly generated, but such a distinction is minor (we can always get densely connected random graphs by adjusting the hyperparameters of the graph generator). In addition, I*m not sure whether the learned continuous alpha can be conveniently referred to as *topology*. Note the mathematical definition of topology is discrete by nature. I believe the authors would need to either revise this terminology (e.g., by referring to it as “soft topology”, as a generalized definition of hard topology), or provide a way to induce discrete subgraphs from the continuous architecture. Sparsity regularization alone may not be sufficient as the non-zero alpha*s are still real-valued. Minor issue: I like Figure 1 a lot. However, it seems the equivalence between the 3rd and the 4th sub-figures in Figure 1 can only be established for ResNet-V2 blocks, where there is no ReLU after each addition. It is not immediately obvious how this analysis can generalize to ResNet-V1 blocks (which still offers reasonably good empirical performance). [1] Xie, Saining, et al. *Exploring randomly wired neural networks for image recognition.* arXiv preprint arXiv:1904.01569 (2019). (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The authors proposed a method to optimize the topology of neural networks in a soft fashion. The main idea is to formulate the network as a complete graph (or a sequence of complete subgraphs), and to optimize the relative importance of each edge using gradient descent. The overall approach is similar to differentiable architecture search, except that (1) the continuous architecture is optimized wrt the training set (instead of the validation set), and (2) the learned architecture is never discretized at the end of training. The paper is well-organized and easy to follow. The authors have also conducted controlled experiments to convincingly show that the method is leading to improvement. I*m a bit concerned about the technical novelty, however, as the approach can be viewed as an application of (a simplified version of) differentiable NAS to a search space analogous to the one used in [1]. In fact, the notion of soft topology has already been introduced in this prior work (Figure 2 in [1]: *The aggregation is done by weighted sum with learnable positive weights w0, w1, w2*), which was also optimized using gradient descent. A difference between this work and [1] is that whether the underlying graph is complete or randomly generated, but such a distinction is minor (we can always get densely connected random graphs by adjusting the hyperparameters of the graph generator). In addition, I*m not sure whether the learned continuous alpha can be conveniently referred to as *topology*. Note the mathematical definition of topology is discrete by nature. I believe the authors would need to either revise this terminology (e.g., by referring to it as “soft topology”, as a generalized definition of hard topology), or provide a way to induce discrete subgraphs from the continuous architecture. Sparsity regularization alone may not be sufficient as the non-zero alpha*s are still real-valued. Minor issue: I like Figure 1 a lot. However, it seems the equivalence between the 3rd and the 4th sub-figures in Figure 1 can only be established for ResNet-V2 blocks, where there is no ReLU after each addition. It is not immediately obvious how this analysis can generalize to ResNet-V1 blocks (which still offers reasonably good empirical performance). [1] Xie, Saining, et al. *Exploring randomly wired neural networks for image recognition.* arXiv preprint arXiv:1904.01569 (2019).\n",
      "      - Review:###This is a nice piece of incremental work on top of previously published GAN imputation methods. It seems to work well in the limited evaluation and is at least claimed to be easier to use for practitioners. This paper could benefit tremendously from both better evaluation and discussion. The paper would be much clearer if GI contextualized itself relative to GAIN on the one hand (which is the most similar GAN method) and multiple imputation on the other hand (of which this is almost, but not quite, an instance of). Suggestions for improving the introduction & discussion: * The purpose of this paper is to model uncertainties about missing values — you really should say more about probabilistic methods than *A few exceptions exist such as Bayesian models”. At least give some motivation for why certain imputations problems couldn’t be feasibly solved by modeling the missing values in a probabilistic programming framework. * Other GAN methods for imputation (GAIN and MisGAN) are dismissed as *often very complicated to be applied in practical setups by practitioners”. Given that the described method resembles GAIN, is it really much simpler? If so, can you be more specific when characterizing related work? * *This is different from approaches such as multiple imputation where several predictors are trained on different imputed versions of a dataset.” — the main difference between this approach and MI is that you’re interleaving imputation and training a downstream model. Emphasize this earlier on, since it will make the whole technique easier to understand. Suggestions for improving the evaluation: * You’re imputing missing rectangles from an image dataset — please show us the resulting images. I would greatly prefer this to Section 4.5 — which is a very low sample size, low dimensionality example and it’s really unclear how well it generalizes to real data. * *We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution.” — I think this is a mistake, likely motivated by the proposed method doing worse under the RMSE metric. Show us several relevant metrics and then discuss their tradeoffs afterward. * *We run each experiment multiple times (at least 4)” — please report how often each experiment was run, even better if you standardize this number. * For Table 2, please provide accuracy without missing values as a baseline. * Add MICE or some other “standard” imputation method as a baseline. Suggestions for improving readability: * Many sentences start with “in this” (e.g. “in this case”, “in this setting”, &c). Sometimes these sentences even co-occur within the same paragraph. Try to switch up the phrasing and move away from repetition. * Not a complete sentence: *For instance, jointly training multiple generator/discriminator networks, tuning objective functions with multiple hyper-parameters, etc.* Update: I think the latest draft of the paper is a big improvement, the inclusion of a *classical* baseline, improved language and additional appendices are all welcome. I*m leaving the rating as a *weak accept* since the paper still feels rough and could use additional editing/streamlining. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This is a nice piece of incremental work on top of previously published GAN imputation methods. It seems to work well in the limited evaluation and is at least claimed to be easier to use for practitioners. This paper could benefit tremendously from both better evaluation and discussion. The paper would be much clearer if GI contextualized itself relative to GAIN on the one hand (which is the most similar GAN method) and multiple imputation on the other hand (of which this is almost, but not quite, an instance of). Suggestions for improving the introduction & discussion: * The purpose of this paper is to model uncertainties about missing values — you really should say more about probabilistic methods than *A few exceptions exist such as Bayesian models”. At least give some motivation for why certain imputations problems couldn’t be feasibly solved by modeling the missing values in a probabilistic programming framework. * Other GAN methods for imputation (GAIN and MisGAN) are dismissed as *often very complicated to be applied in practical setups by practitioners”. Given that the described method resembles GAIN, is it really much simpler? If so, can you be more specific when characterizing related work? * *This is different from approaches such as multiple imputation where several predictors are trained on different imputed versions of a dataset.” — the main difference between this approach and MI is that you’re interleaving imputation and training a downstream model. Emphasize this earlier on, since it will make the whole technique easier to understand. Suggestions for improving the evaluation: * You’re imputing missing rectangles from an image dataset — please show us the resulting images. I would greatly prefer this to Section 4.5 — which is a very low sample size, low dimensionality example and it’s really unclear how well it generalizes to real data. * *We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution.” — I think this is a mistake, likely motivated by the proposed method doing worse under the RMSE metric. Show us several relevant metrics and then discuss their tradeoffs afterward. * *We run each experiment multiple times (at least 4)” — please report how often each experiment was run, even better if you standardize this number. * For Table 2, please provide accuracy without missing values as a baseline. * Add MICE or some other “standard” imputation method as a baseline. Suggestions for improving readability: * Many sentences start with “in this” (e.g. “in this case”, “in this setting”, &c). Sometimes these sentences even co-occur within the same paragraph. Try to switch up the phrasing and move away from repetition. * Not a complete sentence: *For instance, jointly training multiple generator/discriminator networks, tuning objective functions with multiple hyper-parameters, etc.* Update: I think the latest draft of the paper is a big improvement, the inclusion of a *classical* baseline, improved language and additional appendices are all welcome. I*m leaving the rating as a *weak accept* since the paper still feels rough and could use additional editing/streamlining.\n",
      "      - Review:###This paper introduces a deep reasoning networks for de-mixing overlapping patterns with some logic constraints. There are two applications considered in the paper: de-mixing overlapping hand-written digits and inferring crystal structures of materials from X-ray diffraction data. The experiments indicate the proposed method work pretty well on these tasks. I like the general idea of this paper, since it has the flavor of combining deep learning with logic rules, although I feel weird to view the generative decoder as thinking fast and the reasoning modules as thinking slow. The notion of thinking fast and slow in the model does not well match the intuition given in the first paragraph of the introduction. The so-called reasoning module is essentially some contraints (i.e., regualrization losses) and a training data sampler. It is far away from the concept of (symbolic or logic) reasoning. There is not too much reasoning happening here. The way the paper relaxes the discrete logic constraints to continuous and differentiable objective that can be jointly optimized by SGD is interesting, which is similar to [Harnessing deep neural networks with logic rules, ACL 2016]. The carefully designed training data sampler that samples data according to a constraint graph also resembles GraphRNN, as the authors have mentioned in the paper. I feel the combination of these techniques is definitely interesting but also somehow incremental. I am not a big fan of some big claims in the paper. The reasoning modules are not what I expect. For the experiments, I think the authors do a good job presenting these experimental details and evaluations. These experiments are interesting and also show some advantages of the propose method. However, some baselines are also doing pretty well, indicating that the task is not difficult in general. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper introduces a deep reasoning networks for de-mixing overlapping patterns with some logic constraints. There are two applications considered in the paper: de-mixing overlapping hand-written digits and inferring crystal structures of materials from X-ray diffraction data. The experiments indicate the proposed method work pretty well on these tasks. I like the general idea of this paper, since it has the flavor of combining deep learning with logic rules, although I feel weird to view the generative decoder as thinking fast and the reasoning modules as thinking slow. The notion of thinking fast and slow in the model does not well match the intuition given in the first paragraph of the introduction. The so-called reasoning module is essentially some contraints (i.e., regualrization losses) and a training data sampler. It is far away from the concept of (symbolic or logic) reasoning. There is not too much reasoning happening here. The way the paper relaxes the discrete logic constraints to continuous and differentiable objective that can be jointly optimized by SGD is interesting, which is similar to [Harnessing deep neural networks with logic rules, ACL 2016]. The carefully designed training data sampler that samples data according to a constraint graph also resembles GraphRNN, as the authors have mentioned in the paper. I feel the combination of these techniques is definitely interesting but also somehow incremental. I am not a big fan of some big claims in the paper. The reasoning modules are not what I expect. For the experiments, I think the authors do a good job presenting these experimental details and evaluations. These experiments are interesting and also show some advantages of the propose method. However, some baselines are also doing pretty well, indicating that the task is not difficult in general.\n",
      "      - Review:###This paper presents an approach called a neural computer, which has a Differential Neural Computer (DNC) at its core that is optimised with an evolutionary strategy. In addition to the typical DNC architecture, the system proposed in this paper has different modules that transfer different domain representations into the same representation, which allows the system to generalise to different and unseen tasks. The paper is interesting and well written but I found that the contributions of this paper could be made more clear. First, the idea of evolving a Neural Turing machine was first proposed in Greve et al. 2016, which the authors cite, but only in passing in the conclusion. Greve et al. paper introduced the idea of hard attention mechanisms in an NTM through evolution and the benefits of having a memory structure that does not have to be differentiable. However, if the reader of this paper is not careful, they would miss this fact. I, therefore, suggest featuring Greve’s paper more prominently and highlighting the differences/similarities to the current paper earlier in the introduction. Second, the idea of learned modules to allow the approach to work across different domains is interesting, but I’m wondering how novel it really is? Isn’t this basically just like feature engineering and changing the underlying representation, something that we have been doing for a long time? Also, the domains that this approach can be applied to seem potentially limited in that the two problems have to already be very similar; In fact, I probably wouldn’t call them different tasks but the same task with a different visual representation. I also had a question about the DNC training. Is the DNC version also trained with NES? It would be good to know how much of the difference between the proposed approach and the DNC is because of the training method (NES vs SGD) or other factors. Once the points raised above and the specific contributions of this paper are made more clear, I would suggest accepting it. ####After rebuttal#### The authors* response and the revised paper address most of my concerns now. Since I do believe the approach could be more novel, I*m keeping my weak accept, but do think it is very interesting work. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper presents an approach called a neural computer, which has a Differential Neural Computer (DNC) at its core that is optimised with an evolutionary strategy. In addition to the typical DNC architecture, the system proposed in this paper has different modules that transfer different domain representations into the same representation, which allows the system to generalise to different and unseen tasks. The paper is interesting and well written but I found that the contributions of this paper could be made more clear. First, the idea of evolving a Neural Turing machine was first proposed in Greve et al. 2016, which the authors cite, but only in passing in the conclusion. Greve et al. paper introduced the idea of hard attention mechanisms in an NTM through evolution and the benefits of having a memory structure that does not have to be differentiable. However, if the reader of this paper is not careful, they would miss this fact. I, therefore, suggest featuring Greve’s paper more prominently and highlighting the differences/similarities to the current paper earlier in the introduction. Second, the idea of learned modules to allow the approach to work across different domains is interesting, but I’m wondering how novel it really is? Isn’t this basically just like feature engineering and changing the underlying representation, something that we have been doing for a long time? Also, the domains that this approach can be applied to seem potentially limited in that the two problems have to already be very similar; In fact, I probably wouldn’t call them different tasks but the same task with a different visual representation. I also had a question about the DNC training. Is the DNC version also trained with NES? It would be good to know how much of the difference between the proposed approach and the DNC is because of the training method (NES vs SGD) or other factors. Once the points raised above and the specific contributions of this paper are made more clear, I would suggest accepting it. ####After rebuttal#### The authors* response and the revised paper address most of my concerns now. Since I do believe the approach could be more novel, I*m keeping my weak accept, but do think it is very interesting work.\n",
      "      - Review:###[Summary] This paper studies the impact of initialization noise on the theories of wide neural networks in the Neural Tangent Kernels (NTK) regime. The paper proves that the difference between the trained neural net and the kernel interpolator (with the NTK) can be bounded by O(sigma^L + 1/sqrt{m}), where sigma^2 is the initializing variance of each individual weight entry. Relationships between the generalization error of these two functions are derived from the above bound. [Pros] The general message that this paper conveys is interesting -- the initial network f_{\theta_0}(x), which is typically omitted (or made small by making sigma small) in NTK analyses, can deviate the converged NN from the kernel interpolator in terms of generalization error. [Cons] There are fundamental mistakes in the statements/proofs of Theorem 2, 3, 4: -- Theorem 2: the statement is “whp over W, the bound … holds uniformly for x”. The proof relies on Lemma 3, whose statement is also uniform over x, but the proof applies the Markov inequality *for a single x* and is thus valid only for a single x. (As it’s Markov, it seems not sensible to apply the union bound upon it.) -- Theorem 3: the difference between L^NN_test and L^int_test should be on the order of (sigma^L + 1/sqrt{m}) rather than it squared. To bound the difference in squared loss we have a^2 - b^2 <= O(1) * |a-b| (if a, b are bounded by O(1)). We don’t have a^2 - b^2 <= |a - b|^2. -- Theorem 4: J(X_test) as defined is a vector whose dimension grows with the number of test data points, where the theorem requires it to be a scalar. Indeed the treatment of test data as a fixed matrix (rather than samples from a distribution) is already a bit atypical. *** I have read the authors* rebuttal and the other reviews, and I*m glad to see the issues with Theorem 3 and 4 pointed out above are fixed in the revision. However, I also agree with the other reviewers that the paper in the present stage has not yet demonstrated sufficient technical contributions, and thus I am keeping my original evaluation. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###[Summary] This paper studies the impact of initialization noise on the theories of wide neural networks in the Neural Tangent Kernels (NTK) regime. The paper proves that the difference between the trained neural net and the kernel interpolator (with the NTK) can be bounded by O(sigma^L + 1/sqrt{m}), where sigma^2 is the initializing variance of each individual weight entry. Relationships between the generalization error of these two functions are derived from the above bound. [Pros] The general message that this paper conveys is interesting -- the initial network f_{\theta_0}(x), which is typically omitted (or made small by making sigma small) in NTK analyses, can deviate the converged NN from the kernel interpolator in terms of generalization error. [Cons] There are fundamental mistakes in the statements/proofs of Theorem 2, 3, 4: -- Theorem 2: the statement is “whp over W, the bound … holds uniformly for x”. The proof relies on Lemma 3, whose statement is also uniform over x, but the proof applies the Markov inequality *for a single x* and is thus valid only for a single x. (As it’s Markov, it seems not sensible to apply the union bound upon it.) -- Theorem 3: the difference between L^NN_test and L^int_test should be on the order of (sigma^L + 1/sqrt{m}) rather than it squared. To bound the difference in squared loss we have a^2 - b^2 <= O(1) * |a-b| (if a, b are bounded by O(1)). We don’t have a^2 - b^2 <= |a - b|^2. -- Theorem 4: J(X_test) as defined is a vector whose dimension grows with the number of test data points, where the theorem requires it to be a scalar. Indeed the treatment of test data as a fixed matrix (rather than samples from a distribution) is already a bit atypical. *** I have read the authors* rebuttal and the other reviews, and I*m glad to see the issues with Theorem 3 and 4 pointed out above are fixed in the revision. However, I also agree with the other reviewers that the paper in the present stage has not yet demonstrated sufficient technical contributions, and thus I am keeping my original evaluation.\n",
      "    - Incremental or weak technical novelty/contribution (L1, 1)\n",
      "      - Review:###The paper proposes SSE-PT for sequential recommendation, which is an extension of previous work SASRec by adding user embedding with SSE regularization [Wu et al. 2019] . They further extend SSE-PT to SSE-PT++ to handle longer sequence. Experiments on five datasets show that the SSE-PT and SSE-PT++ outperform several baseline approaches. Detailed comments: 1) The technical contribution seems to be scattered: user embedding is introduced, effect of different types of regularization is studied and sampling based approach is added to address long sequence. It could be better if the author could make clear what the major contribution of this paper is. Also, SSE [Wu et al. 2019] is existing technique and simply applying it to sequential recommendation is a bit incremental. 2) In addition to SASRec, there are some other transformer based model (e.g., [1]) for sequential recommendation and the paper discuss how the proposed method differ from them. 3) In SSE-PT++, would sampling start index v based on the recency (e.g., with exponential decay) make more sense than uniform probability? 4? Overall, experiments look comprehensive: The baseline methods include both non-deep-learning methods and recent deep learning based methods for sequential recommendation; ablation study is conducted; case study is performed on MovieLens to show how the attention weights differ from SASRec; running time is compared against baselines and sensitivity analysis on hyper-parameters are also provided. To summarize, the paper is a bit incremental/scattered in terms of technical contribution but the execution of this paper looks solid. I would give a “weak accept” to this paper given the reasons listed above. [1] F. Sun et. al. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper proposes SSE-PT for sequential recommendation, which is an extension of previous work SASRec by adding user embedding with SSE regularization [Wu et al. 2019] . They further extend SSE-PT to SSE-PT++ to handle longer sequence. Experiments on five datasets show that the SSE-PT and SSE-PT++ outperform several baseline approaches. Detailed comments: 1) The technical contribution seems to be scattered: user embedding is introduced, effect of different types of regularization is studied and sampling based approach is added to address long sequence. It could be better if the author could make clear what the major contribution of this paper is. Also, SSE [Wu et al. 2019] is existing technique and simply applying it to sequential recommendation is a bit incremental. 2) In addition to SASRec, there are some other transformer based model (e.g., [1]) for sequential recommendation and the paper discuss how the proposed method differ from them. 3) In SSE-PT++, would sampling start index v based on the recency (e.g., with exponential decay) make more sense than uniform probability? 4? Overall, experiments look comprehensive: The baseline methods include both non-deep-learning methods and recent deep learning based methods for sequential recommendation; ablation study is conducted; case study is performed on MovieLens to show how the attention weights differ from SASRec; running time is compared against baselines and sensitivity analysis on hyper-parameters are also provided. To summarize, the paper is a bit incremental/scattered in terms of technical contribution but the execution of this paper looks solid. I would give a “weak accept” to this paper given the reasons listed above. [1] F. Sun et. al. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer\n",
      "    - Insufficient Novelty and Lack of Distinction from Prior Work (L1, 1)\n",
      "      - Review:###The paper introduces foveabox, a method that performs *keypoint* like object detection -- instead of *anchor* based detection (to be discussed later). The idea is simple: predict class labels for pixels that fall within (a reduced version) the GT boxes of the instance; and predict bounding box offsets for those positive pixels. The idea is built on top of the FPN backbone, where a set of feature maps (each representing a specific scale) are used to detect object boxes in multiple scales. The method is mainly compared against RetinaNet (which is *anchor* driven), and also compared against other more recent methods (ExtremeNet, CenterNet, FCOS) etc. + The paper is quite well written and structured, the illustrations are also clear; + I have also read its previous version, and the new version has added a significant amount of work improving it -- e.g. added feature alignment and group norm; + I haven*t fully checked the results section of other concurrent papers for full comparison, but the current results are among the state-of-the-art for one-stage detectors. - I don*t think the paper breaks away from the notion of *anchors*. The current approach is in fact implicitly defining a *single* anchor for each feature map, and changing the assignment rule from IoU based to distance/scale based. I firmly believe that such modification can lead to concrete improvements -- for example recent work has shown that changing assignment rule can improve AP even with fewer anchors (TensorMask); but to say that the paper breaks away from the usage of anchors, it is a bit far for me. I think I will be way more convinced if the experiments are done on a single-scale feature map (anchor also works with single-scale but I am not sure without defining different anchors it can work just on C4 for example). - Also because such assignment rule has been there before (e.g. DeepMask has used x, y, scale for assignment), and with other recent works (CenterNet, Objects as Keypoints), the contribution of this work is fairly limited Therefore, I vote for reject. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper introduces foveabox, a method that performs *keypoint* like object detection -- instead of *anchor* based detection (to be discussed later). The idea is simple: predict class labels for pixels that fall within (a reduced version) the GT boxes of the instance; and predict bounding box offsets for those positive pixels. The idea is built on top of the FPN backbone, where a set of feature maps (each representing a specific scale) are used to detect object boxes in multiple scales. The method is mainly compared against RetinaNet (which is *anchor* driven), and also compared against other more recent methods (ExtremeNet, CenterNet, FCOS) etc. + The paper is quite well written and structured, the illustrations are also clear; + I have also read its previous version, and the new version has added a significant amount of work improving it -- e.g. added feature alignment and group norm; + I haven*t fully checked the results section of other concurrent papers for full comparison, but the current results are among the state-of-the-art for one-stage detectors. - I don*t think the paper breaks away from the notion of *anchors*. The current approach is in fact implicitly defining a *single* anchor for each feature map, and changing the assignment rule from IoU based to distance/scale based. I firmly believe that such modification can lead to concrete improvements -- for example recent work has shown that changing assignment rule can improve AP even with fewer anchors (TensorMask); but to say that the paper breaks away from the usage of anchors, it is a bit far for me. I think I will be way more convinced if the experiments are done on a single-scale feature map (anchor also works with single-scale but I am not sure without defining different anchors it can work just on C4 for example). - Also because such assignment rule has been there before (e.g. DeepMask has used x, y, scale for assignment), and with other recent works (CenterNet, Objects as Keypoints), the contribution of this work is fairly limited Therefore, I vote for reject.\n",
      "    - Lack of Novelty or Incremental Technical Contribution (L1, 3)\n",
      "      -  This paper proposes two ways to aggregate existing style transfer methods and shows improvements on quality and flexibility. However, the proposed method does not solve the limitations of any previous methods. Instead, it is as simple as an easy combination: the proposed SST is a sequential combination and the proposed PST has no difference with running each single method separately. To me this is more like an engineering effort rather than a research work. (1) For SST, it just connects N existing methods, using the output of method 1 as the input of method 2. The quality of results might be improved but there is little novelty. I agree combing methods can be a contribution only when there are principle designs and in-depth analysis. (2) For PST, I do not see its difference with running single method separately. Putting all previous methods altogether cannot be called being more flexible. As said in the paper, when for photorealistic transfer, the proposed PST set the loss weight of other methods except Luan et al. as 0. Then is it the same with running the single method of Luan et al.? In general, I do not encourage such a way of exploring research. Authors should focus more on the unsolved issues in style transfer, e.g., how to do geometric style transfer (shape). (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper proposes two ways to aggregate existing style transfer methods and shows improvements on quality and flexibility. However, the proposed method does not solve the limitations of any previous methods. Instead, it is as simple as an easy combination: the proposed SST is a sequential combination and the proposed PST has no difference with running each single method separately. To me this is more like an engineering effort rather than a research work. (1) For SST, it just connects N existing methods, using the output of method 1 as the input of method 2. The quality of results might be improved but there is little novelty. I agree combing methods can be a contribution only when there are principle designs and in-depth analysis. (2) For PST, I do not see its difference with running single method separately. Putting all previous methods altogether cannot be called being more flexible. As said in the paper, when for photorealistic transfer, the proposed PST set the loss weight of other methods except Luan et al. as 0. Then is it the same with running the single method of Luan et al.? In general, I do not encourage such a way of exploring research. Authors should focus more on the unsolved issues in style transfer, e.g., how to do geometric style transfer (shape).\n",
      "      - Review:###Summary & Pros - This paper proposes a few-shot learning method that uses self-supervision as an auxiliary label and trains primary and auxiliary labels via multi-task learning. - This paper provides extensive experiments for analyzing the effect of self-supervision on various few-shot learning settings: (1) self-supervision can improve various few-shot learning algorithms, ProtoNet & MAML; (2) self-supervision with similar samples can provide more improvements. Concerns #1: Novelty of the proposed method - This paper uses a multi-task learning approach with self-supervision. But this approach is already used in various tasks, e.g., domain adaptation, semi-supervised learning, training GANs. Thus, the proposed method (in Section 3) using a multi-task learning objective with self-supervised losses seems to be incremental. Concerns #2: Somewhat unsurprising experimental results - This paper shows various experimental results, but some experiments seem to be trivial. For example, the performance gap is typically increased when learning harder tasks, e.g., when the number of training samples is decreasing, the performance gap between methods is typically increasing in a fully-supervised setting. Thus I think results in the paragraph *Gains are larger for harder tasks* might be predictable. Other examples are Figure 4a and 4b in Section 4.2 because one can easily expect that using training more in-domain samples can provide more performance gain. - In the case of Figure 4d in Section 4.2, the authors claimed that the effectiveness of SSL decreases as the distance from the supervised domain increases. However, I think Figure 4d is not matched to the claim. For example, in the case of Dogs, better performance is achieved when using a more dissimilar domain for self-supervision except for D_s=D_ss. So I wonder how to draw the lines in Figure 4d. Some experimental results provide meaningful messages, e.g., single self-supervision can improve performance significantly while joint self-supervision does marginally. However, the contribution of the methodology is limited and some experimental results seem to incremental. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Summary & Pros - This paper proposes a few-shot learning method that uses self-supervision as an auxiliary label and trains primary and auxiliary labels via multi-task learning. - This paper provides extensive experiments for analyzing the effect of self-supervision on various few-shot learning settings: (1) self-supervision can improve various few-shot learning algorithms, ProtoNet & MAML; (2) self-supervision with similar samples can provide more improvements. Concerns #1: Novelty of the proposed method - This paper uses a multi-task learning approach with self-supervision. But this approach is already used in various tasks, e.g., domain adaptation, semi-supervised learning, training GANs. Thus, the proposed method (in Section 3) using a multi-task learning objective with self-supervised losses seems to be incremental. Concerns #2: Somewhat unsurprising experimental results - This paper shows various experimental results, but some experiments seem to be trivial. For example, the performance gap is typically increased when learning harder tasks, e.g., when the number of training samples is decreasing, the performance gap between methods is typically increasing in a fully-supervised setting. Thus I think results in the paragraph *Gains are larger for harder tasks* might be predictable. Other examples are Figure 4a and 4b in Section 4.2 because one can easily expect that using training more in-domain samples can provide more performance gain. - In the case of Figure 4d in Section 4.2, the authors claimed that the effectiveness of SSL decreases as the distance from the supervised domain increases. However, I think Figure 4d is not matched to the claim. For example, in the case of Dogs, better performance is achieved when using a more dissimilar domain for self-supervision except for D_s=D_ss. So I wonder how to draw the lines in Figure 4d. Some experimental results provide meaningful messages, e.g., single self-supervision can improve performance significantly while joint self-supervision does marginally. However, the contribution of the methodology is limited and some experimental results seem to incremental.\n",
      "      - Review:###The study analyzes NFL data to mitigate injuries of NFL players. Methods such as K-NN, XGBoost, and SVM are used for the analysis and predictive modeling. Although the research is important for NFL and sports industries, there are several problems as an ICLR paper: First, it is not formatted correctly. I guess using single column, the content may be overlength. Second, the methods used in the paper are conventional and there is no novelty from the algorithmic perspective. Third, the findings is useful within the sports industry, but does not conveys much insight for the ICLR community. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The study analyzes NFL data to mitigate injuries of NFL players. Methods such as K-NN, XGBoost, and SVM are used for the analysis and predictive modeling. Although the research is important for NFL and sports industries, there are several problems as an ICLR paper: First, it is not formatted correctly. I guess using single column, the content may be overlength. Second, the methods used in the paper are conventional and there is no novelty from the algorithmic perspective. Third, the findings is useful within the sports industry, but does not conveys much insight for the ICLR community.\n",
      "    - Lack of Technical Novelty or Significant Contribution (L1, 2)\n",
      "      -  The paper talks about creating a Tech Knowledge Graph of 260 million tripes, with 52 million entities coming from 38 research domains. The authors claim this is the first of it*s technology specific Chinese KG, so they claim scale, specificity and list out various aspects that makes this distinct to existing KG*s. A few things that the paper is missing or not written well: 1. Related work is pretty much just listing existing KG*s but no limitations and how those limitations are addressed by this work other than the fact that tech KG is technology specific. By being technology specific how does it help, where is KG being used and what metrics are current KG*s fall short etc. 2. There is no novelty in the way KG is built, so there is no technical contribution to this paper making it a very weak submission for ICLR standards. 3. There is a huge appendix section with some results and lot of information. It would have been great to distill how KG*s are being used in existing benchmarks and either show the value of a tech KG in the metrics/benchmarks or introduce a new metric or benchmark to measure the value of a specific domain KG like the tech KG. If the authors can address these issues, the value of the work can increase significantly. (L0, 1)\n",
      "        * Examples:\n",
      "          -  The paper talks about creating a Tech Knowledge Graph of 260 million tripes, with 52 million entities coming from 38 research domains. The authors claim this is the first of it*s technology specific Chinese KG, so they claim scale, specificity and list out various aspects that makes this distinct to existing KG*s. A few things that the paper is missing or not written well: 1. Related work is pretty much just listing existing KG*s but no limitations and how those limitations are addressed by this work other than the fact that tech KG is technology specific. By being technology specific how does it help, where is KG being used and what metrics are current KG*s fall short etc. 2. There is no novelty in the way KG is built, so there is no technical contribution to this paper making it a very weak submission for ICLR standards. 3. There is a huge appendix section with some results and lot of information. It would have been great to distill how KG*s are being used in existing benchmarks and either show the value of a tech KG in the metrics/benchmarks or introduce a new metric or benchmark to measure the value of a specific domain KG like the tech KG. If the authors can address these issues, the value of the work can increase significantly.\n",
      "      - Review:###I have read the author response, thank you for responding. Original review: This paper presents the extraction of a bibliographic database of Chinese technical papers. This database could potentially be a valuable resource for the community. However, the paper is mis-targeted to the ICLR conference, as it does not discuss learning representations or using deep learning (the 36-page appendix does include some material on knowledge graph embedding, but this is not covered in the main body of the paper). Also, for important tasks like name deduplication, the paper does not discuss or compare against techniques from previous work, and instead proposes a small set of heuristics, which is a sensible approach for building a resource but will not be of interest to the ICLR audience. The paper refers to their resource as a knowledge graph, but I would say it is more accurate to call it a bibliographic database (it consists primarily of paper titles, authors, and keywords). This is very different from the broader KBs and KGs discussed in the related work. YAGO, Freebase, Cyc, and so on capture a much wider variety of semantic relationships and entity types. I think re-positioning this submission as being aimed at building a bibliographic database, rather than a Knowledge Graph, would help ensure it reaches the right audience. Also, targeting a different venue like a digital libraries conference and making a strong case that this set of bibliographic data offers advantages in coverage or accuracy over other comparable resources, if any, would help. Finally, one of the most valuable features of a bibliographic database is the citation graph, it would be exciting if TechKG could be extended to include citation information. Minor: Section 2, most of these citations should not be shortcites but should be regular full cites. For example in the first paragraph, Miller (1995) should be (Miller, 1995). (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###I have read the author response, thank you for responding. Original review: This paper presents the extraction of a bibliographic database of Chinese technical papers. This database could potentially be a valuable resource for the community. However, the paper is mis-targeted to the ICLR conference, as it does not discuss learning representations or using deep learning (the 36-page appendix does include some material on knowledge graph embedding, but this is not covered in the main body of the paper). Also, for important tasks like name deduplication, the paper does not discuss or compare against techniques from previous work, and instead proposes a small set of heuristics, which is a sensible approach for building a resource but will not be of interest to the ICLR audience. The paper refers to their resource as a knowledge graph, but I would say it is more accurate to call it a bibliographic database (it consists primarily of paper titles, authors, and keywords). This is very different from the broader KBs and KGs discussed in the related work. YAGO, Freebase, Cyc, and so on capture a much wider variety of semantic relationships and entity types. I think re-positioning this submission as being aimed at building a bibliographic database, rather than a Knowledge Graph, would help ensure it reaches the right audience. Also, targeting a different venue like a digital libraries conference and making a strong case that this set of bibliographic data offers advantages in coverage or accuracy over other comparable resources, if any, would help. Finally, one of the most valuable features of a bibliographic database is the citation graph, it would be exciting if TechKG could be extended to include citation information. Minor: Section 2, most of these citations should not be shortcites but should be regular full cites. For example in the first paragraph, Miller (1995) should be (Miller, 1995).\n",
      "    - Lack of Thorough or Correctly Positioned Related Work / Missing References (L1, 6)\n",
      "      -  In the paper, the authors consider composition problems and use the stochastically controlled stochastic gradient method (SCSG) to approximate the gradient G(x) and \n",
      "abla f(x). The authors also provide convergence analysis of the proposed method for strongly convex problems and non-convex problems. Authors then conduct experiments on the mean-variance optimization in portfolio management task and the nonlinear embedding problem, results show that the proposed method is faster. The following are my concerns: 1) There are several important related works missing in the paper, e.g., [1][2]. 2) The convergence results of the proposed method in the paper are not state-of-the-art. For a strongly convex case, the result in the paper is O( n+ k^2 min(n, 1/u^2) log1/e), it is not necessarily better than O(n+k^3 log 1/e) in [1] or O(n+kn^{2/3} log 1/e). For a non-convex case, the result in the paper is O(min{1/e^{9/5}, n^{4/5} / e}), it is not necessarily better than O(n^{2/3}/e) in [1] or [2]. 3) More compared results should be conducted in the experiments, e.g. [1][2]. [1]Huo, Zhouyuan, et al. *Accelerated method for stochastic composition optimization with nonsmooth regularization.* Thirty-Second AAAI Conference on Artificial Intelligence. 2018. [2] Zhang, Junyu, and Lin Xiao. *A Composite Randomized Incremental Gradient Method.* International Conference on Machine Learning. 2019. (L0, 1)\n",
      "        * Examples:\n",
      "          -  In the paper, the authors consider composition problems and use the stochastically controlled stochastic gradient method (SCSG) to approximate the gradient G(x) and \n",
      "abla f(x). The authors also provide convergence analysis of the proposed method for strongly convex problems and non-convex problems. Authors then conduct experiments on the mean-variance optimization in portfolio management task and the nonlinear embedding problem, results show that the proposed method is faster. The following are my concerns: 1) There are several important related works missing in the paper, e.g., [1][2]. 2) The convergence results of the proposed method in the paper are not state-of-the-art. For a strongly convex case, the result in the paper is O( n+ k^2 min(n, 1/u^2) log1/e), it is not necessarily better than O(n+k^3 log 1/e) in [1] or O(n+kn^{2/3} log 1/e). For a non-convex case, the result in the paper is O(min{1/e^{9/5}, n^{4/5} / e}), it is not necessarily better than O(n^{2/3}/e) in [1] or [2]. 3) More compared results should be conducted in the experiments, e.g. [1][2]. [1]Huo, Zhouyuan, et al. *Accelerated method for stochastic composition optimization with nonsmooth regularization.* Thirty-Second AAAI Conference on Artificial Intelligence. 2018. [2] Zhang, Junyu, and Lin Xiao. *A Composite Randomized Incremental Gradient Method.* International Conference on Machine Learning. 2019.\n",
      "      -  The paper proposes a method for learning options that transfer across multiple learning tasks. The method takes a number of demonstration trajectories as input and attempts to create a set of options that can recreate the trajectories with minimal terminations. I currently recommend rejection. The evaluation of the proposed method is rather weak and does not clearly demonstrate that the author’s goals have been achieved. The paper could also do a better job of situating the approach with regard to existing option learning approaches. Detailed comments: - The paper strongly emphasizes reusability of learnt options over multiple tasks as a key goal. This aspect is largely absent from the practical part of the paper, however. The proposed algorithm largely ignores the multi-task aspect beyond requiring demonstrations from different tasks - also see the next remark. In the experiments, the multi-task transfer is not emphasized. In the 4rooms domain options are learnt on training tasks and evaluated on test tasks, but the effect of task distribution or task diversity on generalisation is not investigated. Moreover, the larger scale ATARI experiments do not seem to include any multi-task aspects at all, with options immediately being learnt on the target task. - The objective in (1) omits any explicit mention of different tasks. It would be good to indicate explicitly how it depends on the distribution of tasks and what the expectation is taken over. - The authors indicate that their learning objective needs the transition function P for the MDP. This is never further discussed. Do the experiments assume known transition functions? If not, how are these functions estimated? If a model is known, does it still make sense to learn option policies from samples or would it be better to use planning based options (see e.g.[1])? - While the paper cites a number of option learning approaches, it could do a better job of situating the research within the literature. There are a number of option induction approaches that explicitly focus on reusability of options - see e.g. [1], [4]. There have also been a large number of approaches that focus on hierarchical learning to represent a distribution over demonstration trajectories: see eg. [3],[5], [6], [7]. Some of these approaches might also be better baselines than OptionCritic which doesn’t explicitly take into account learning from demonstrations or multi-task transfer. [1] Mann, T. A., Mannor, S., & Precup, D. (2015). Approximate value iteration with temporally extended actions. JAIR, 53, 375-438. [2] Konidaris, G., & Barto, A. G. (2007). Building Portable Options: Skill Transfer in Reinforcement Learning. In IJCAI, [3] Konidaris, G., Kuindersma, S., Grupen, R., & Barto, A. (2012). Robot learning from demonstration by constructing skill trees. IJRR, 31(3), 360-375. [4] Andreas, J., Klein, D., & Levine, S. (2017). Modular multitask reinforcement learning with policy sketches. ICML [5] Henderson, P., Chang, W. D., Bacon, P. L., Meger, D., Pineau, J., & Precup, D. (2018). Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement learning. AAAI. [6] Co-Reyes, J. D., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., & Levine, S. (2018). Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings.ICML [7] Daniel, C., Van Hoof, H., Peters, J., & Neumann, G. (2016). Probabilistic inference for determining options in reinforcement learning. Machine Learning, 104(2-3), 337-357. Minor comments: - The results on ATARI seem to have been ended before reaching final learning performance - I couldn’t find details for how the transition function in 4-rooms is changed - Does the optionCritic comparison include the deliberation cost? Since this paper aims to minimise option terminations that seems to be the most logical comparison. - Why don’t the ATARI results compare against other approaches? - The influence of the KL penalty isn’t really examined in results beyond looking at performance. How does it influence the trade-off between representing trajectories and diversity? (L0, 1)\n",
      "        * Examples:\n",
      "          -  The paper proposes a method for learning options that transfer across multiple learning tasks. The method takes a number of demonstration trajectories as input and attempts to create a set of options that can recreate the trajectories with minimal terminations. I currently recommend rejection. The evaluation of the proposed method is rather weak and does not clearly demonstrate that the author’s goals have been achieved. The paper could also do a better job of situating the approach with regard to existing option learning approaches. Detailed comments: - The paper strongly emphasizes reusability of learnt options over multiple tasks as a key goal. This aspect is largely absent from the practical part of the paper, however. The proposed algorithm largely ignores the multi-task aspect beyond requiring demonstrations from different tasks - also see the next remark. In the experiments, the multi-task transfer is not emphasized. In the 4rooms domain options are learnt on training tasks and evaluated on test tasks, but the effect of task distribution or task diversity on generalisation is not investigated. Moreover, the larger scale ATARI experiments do not seem to include any multi-task aspects at all, with options immediately being learnt on the target task. - The objective in (1) omits any explicit mention of different tasks. It would be good to indicate explicitly how it depends on the distribution of tasks and what the expectation is taken over. - The authors indicate that their learning objective needs the transition function P for the MDP. This is never further discussed. Do the experiments assume known transition functions? If not, how are these functions estimated? If a model is known, does it still make sense to learn option policies from samples or would it be better to use planning based options (see e.g.[1])? - While the paper cites a number of option learning approaches, it could do a better job of situating the research within the literature. There are a number of option induction approaches that explicitly focus on reusability of options - see e.g. [1], [4]. There have also been a large number of approaches that focus on hierarchical learning to represent a distribution over demonstration trajectories: see eg. [3],[5], [6], [7]. Some of these approaches might also be better baselines than OptionCritic which doesn’t explicitly take into account learning from demonstrations or multi-task transfer. [1] Mann, T. A., Mannor, S., & Precup, D. (2015). Approximate value iteration with temporally extended actions. JAIR, 53, 375-438. [2] Konidaris, G., & Barto, A. G. (2007). Building Portable Options: Skill Transfer in Reinforcement Learning. In IJCAI, [3] Konidaris, G., Kuindersma, S., Grupen, R., & Barto, A. (2012). Robot learning from demonstration by constructing skill trees. IJRR, 31(3), 360-375. [4] Andreas, J., Klein, D., & Levine, S. (2017). Modular multitask reinforcement learning with policy sketches. ICML [5] Henderson, P., Chang, W. D., Bacon, P. L., Meger, D., Pineau, J., & Precup, D. (2018). Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement learning. AAAI. [6] Co-Reyes, J. D., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., & Levine, S. (2018). Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings.ICML [7] Daniel, C., Van Hoof, H., Peters, J., & Neumann, G. (2016). Probabilistic inference for determining options in reinforcement learning. Machine Learning, 104(2-3), 337-357. Minor comments: - The results on ATARI seem to have been ended before reaching final learning performance - I couldn’t find details for how the transition function in 4-rooms is changed - Does the optionCritic comparison include the deliberation cost? Since this paper aims to minimise option terminations that seems to be the most logical comparison. - Why don’t the ATARI results compare against other approaches? - The influence of the KL penalty isn’t really examined in results beyond looking at performance. How does it influence the trade-off between representing trajectories and diversity?\n",
      "      - Review:###Summary: There are three main contributions of the paper: i) The authors present an empirical study of different self-supervised learning (SSL) methods in the context of self-supervised learning. ii) They point out how SSL helps more when the dataset is harder. iii) They point out how domain matters while using SSL for training and present a method to choose samples from an unlabeled dataset. Strengths: 1) They confirm the results of [7] and provide additional evidence of the benefit of the self-supervised learning in the few-shot setting. They also showcase an interesting new result that self-supervised learning helps more in case of harder problems. 2) The authors have a done a commendable job of coming up with a meaningful set of experiments by varying base-models, self-supervised methods, datasets, and few-shot learning methods in Section 4.1. This is quite a comprehensive study. 3) The paper is well-written and well-motivated. Weaknesses: 1) The main weakness of the paper is Section 4.2*s experimental setup. i) The definition of domain distance in not quite meaningful. Since the chosen datasets have very different classes (airplanes vs dogs etc), the average embeddings for different datasets/classes will be far from each other. In Figure 4d, it is misleading to show a trendline that includes the same domain as that will always be 0. If that datapoint is removed the trend line is mostly flat. The authors want to present a quantifiable way to show how domain distribution affect performance on self-supervised learning methods. But this definition of domain distance is more meaningful in the domain adaptation setting (like Amazon-Office dataset used for domain adaptation (https://people.eecs.berkeley.edu/~jhoffman/domainadapt/)) as in that case the requirement is to get the embeddings close to each other for the same class but from different domains. ii) The authors go on to create an *unlabeled pool* by combining images from many domains. Then they train a domain classifier by labeling in-domain images as positive and labeling the images in pool as negative. Considering all images in the pool as negative is not correct as there can be images of same class in unlabeled pool. iii) Then they choose the samples which the classifier predicts comes from the same domain. This probably succeeds as it is done on top of ResNet-101 features (that has seen all of ImageNet). This technique probably works possibly due to the ResNet being pre-trained with so many labeled classes. One baseline might have just been to choose the k-nearest neighbors (from unlabled pool) to the average embedding of all the images of the chosen dataset. Since, it is quite easy for classifiers to detect from which dataset an image came from [6] it might be easy to choose an image from similar domain by using nearest neighbor in the embedding space. I am not sure how well their heuristic will work for an unlabeled pool that the classifier has never seen. Additionally, a portion of the dataset has been created by combining existing datasets. Since statistics of different datasets vary a lot artificially, the creation of an unlabeled pool by combining different datasets might work in the favor of the proposed heuristic of looking at *domain distance* to choose the samples. 2) Effect of domain shift in SSL (Figure 4b) is studying an extreme case where the domain shift results in almost no common classes in the SSL training phase. It would make more sense to include datasets where at least some of the classes are shared so that self-supervised learning methods get to see some relevant classes. In practice, a self-supervised learning method would be applied on a large unlabeled pool of images. Hopefully with increasing diversity and number of images, there might be some images on which ding SSL helps the downstream few-shot task. Hence comparison with such a dataset like ImageNet/iNatrualist is important here. Decision: The paper presents an well thought-out empirical study on self-supervised learning for few-shot learning. But there are major concerns with the empirical setup and methods presented in the section where they propose a method to choose samples for SSL from an unlabeled pool of images. Minor Comments: 1) “In contrast, we humans can quickly learn new concepts from limited training data” - Remove we. 2) “Despite recent advances, these techniques have only been applied to a few domains (e.g., entry-level classes on internet imagery), and under the assumption that large amounts of unlabeled images are available.” This is not true. Self-supervised learning methods have been used for continuous control in reinforcement learning[1, 2], cross-modal learning[3], navigation [5], action recognition[4] etc. Later on in related work the authors list many papers that use self-supervised learning in different contexts. This line should be modified to reflect how common self-supervised learning methods are in other fields as well and with less data. 3) “making the rotation task too hard or too trivial to benefit main task” - add respectively. 4) *With random sampling, the extra unlabeled data often hurts the performance, while those sampled using the “domain weighs” improves performance on most datasets.* replace weighs with weights 5) [8] points out how self-supervised learning on a large dataset but with a domain shift (YFCC100M) is not as effective for pre-training as it is doing self-supervised learning on the downstream task*s dataset (ImageNet). While it is a different setting it is inline with one of the main conclusions of the paper. References: [1] “PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations” Rico Jonschkowski, Roland Hafner, Jonathan Scholz, and Martin Riedmiller. [2] “Learning Actionable Representations from Visual Observations” Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, Pierre Sermanet [3] “Look, Listen and Learn” Relja Arandjelovi?, Andrew Zisserman [4] “Self-supervised Spatiotemporal Learning via Video Clip Order Prediction” Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, Yueting Zhuang. [5] “Scaling and Benchmarking Self-Supervised Visual Representation Learning” Priya Goyal, Dhruv Mahajan, Abhinav Gupta, Ishan Misra [6] *Unbiased Look at Dataset Bias* Antonio Torralba and Alexei A. Efros. [7] *Boosting ´few-shot visual learning with self-supervision.* Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Perez, and Matthieu Cord. [8] *Deep clustering for unsupervised learning of visual features* Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Summary: There are three main contributions of the paper: i) The authors present an empirical study of different self-supervised learning (SSL) methods in the context of self-supervised learning. ii) They point out how SSL helps more when the dataset is harder. iii) They point out how domain matters while using SSL for training and present a method to choose samples from an unlabeled dataset. Strengths: 1) They confirm the results of [7] and provide additional evidence of the benefit of the self-supervised learning in the few-shot setting. They also showcase an interesting new result that self-supervised learning helps more in case of harder problems. 2) The authors have a done a commendable job of coming up with a meaningful set of experiments by varying base-models, self-supervised methods, datasets, and few-shot learning methods in Section 4.1. This is quite a comprehensive study. 3) The paper is well-written and well-motivated. Weaknesses: 1) The main weakness of the paper is Section 4.2*s experimental setup. i) The definition of domain distance in not quite meaningful. Since the chosen datasets have very different classes (airplanes vs dogs etc), the average embeddings for different datasets/classes will be far from each other. In Figure 4d, it is misleading to show a trendline that includes the same domain as that will always be 0. If that datapoint is removed the trend line is mostly flat. The authors want to present a quantifiable way to show how domain distribution affect performance on self-supervised learning methods. But this definition of domain distance is more meaningful in the domain adaptation setting (like Amazon-Office dataset used for domain adaptation (https://people.eecs.berkeley.edu/~jhoffman/domainadapt/)) as in that case the requirement is to get the embeddings close to each other for the same class but from different domains. ii) The authors go on to create an *unlabeled pool* by combining images from many domains. Then they train a domain classifier by labeling in-domain images as positive and labeling the images in pool as negative. Considering all images in the pool as negative is not correct as there can be images of same class in unlabeled pool. iii) Then they choose the samples which the classifier predicts comes from the same domain. This probably succeeds as it is done on top of ResNet-101 features (that has seen all of ImageNet). This technique probably works possibly due to the ResNet being pre-trained with so many labeled classes. One baseline might have just been to choose the k-nearest neighbors (from unlabled pool) to the average embedding of all the images of the chosen dataset. Since, it is quite easy for classifiers to detect from which dataset an image came from [6] it might be easy to choose an image from similar domain by using nearest neighbor in the embedding space. I am not sure how well their heuristic will work for an unlabeled pool that the classifier has never seen. Additionally, a portion of the dataset has been created by combining existing datasets. Since statistics of different datasets vary a lot artificially, the creation of an unlabeled pool by combining different datasets might work in the favor of the proposed heuristic of looking at *domain distance* to choose the samples. 2) Effect of domain shift in SSL (Figure 4b) is studying an extreme case where the domain shift results in almost no common classes in the SSL training phase. It would make more sense to include datasets where at least some of the classes are shared so that self-supervised learning methods get to see some relevant classes. In practice, a self-supervised learning method would be applied on a large unlabeled pool of images. Hopefully with increasing diversity and number of images, there might be some images on which ding SSL helps the downstream few-shot task. Hence comparison with such a dataset like ImageNet/iNatrualist is important here. Decision: The paper presents an well thought-out empirical study on self-supervised learning for few-shot learning. But there are major concerns with the empirical setup and methods presented in the section where they propose a method to choose samples for SSL from an unlabeled pool of images. Minor Comments: 1) “In contrast, we humans can quickly learn new concepts from limited training data” - Remove we. 2) “Despite recent advances, these techniques have only been applied to a few domains (e.g., entry-level classes on internet imagery), and under the assumption that large amounts of unlabeled images are available.” This is not true. Self-supervised learning methods have been used for continuous control in reinforcement learning[1, 2], cross-modal learning[3], navigation [5], action recognition[4] etc. Later on in related work the authors list many papers that use self-supervised learning in different contexts. This line should be modified to reflect how common self-supervised learning methods are in other fields as well and with less data. 3) “making the rotation task too hard or too trivial to benefit main task” - add respectively. 4) *With random sampling, the extra unlabeled data often hurts the performance, while those sampled using the “domain weighs” improves performance on most datasets.* replace weighs with weights 5) [8] points out how self-supervised learning on a large dataset but with a domain shift (YFCC100M) is not as effective for pre-training as it is doing self-supervised learning on the downstream task*s dataset (ImageNet). While it is a different setting it is inline with one of the main conclusions of the paper. References: [1] “PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations” Rico Jonschkowski, Roland Hafner, Jonathan Scholz, and Martin Riedmiller. [2] “Learning Actionable Representations from Visual Observations” Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, Pierre Sermanet [3] “Look, Listen and Learn” Relja Arandjelovi?, Andrew Zisserman [4] “Self-supervised Spatiotemporal Learning via Video Clip Order Prediction” Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, Yueting Zhuang. [5] “Scaling and Benchmarking Self-Supervised Visual Representation Learning” Priya Goyal, Dhruv Mahajan, Abhinav Gupta, Ishan Misra [6] *Unbiased Look at Dataset Bias* Antonio Torralba and Alexei A. Efros. [7] *Boosting ´few-shot visual learning with self-supervision.* Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Perez, and Matthieu Cord. [8] *Deep clustering for unsupervised learning of visual features* Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze\n",
      "      - Review:###The authors proposed to mix together multiple styles by proposing two frameworks: 1) serial style transfer (SST), which combines style transfer methods in series; and 2) parallel style transfer (PST), which combines style transfer methods in parallel. The paper is clearly presented. It is interesting to see work on mixing up different styles, since it is not extensive studied so far. Though not much studied, this topic is not new [ref 1], [ref 2]. The authors didn*t provide a thorough literature review on mixing multiple styles in the related work or anywhere else in the submission. In terms of the methodology, the novelty is quite limited. The proposed SST and PST are simple frameworks to mix different styles, which, by the way, are fully based on existing style transfer methods. At some point, PST is similar to [ref 2], and the difference is minor. PST linearly combines the losses for different styles to construct the final loss, while [ref 2] linearly combines together the features for different styles. About experiments, it is not clear about how the predefined parameters (e.g. alpha, w_1, w_2, etc.) are determined. They were just empirically set and mentioned in the experiment section. It is appreciated to see more results in the Appendix, as well as the user study. However, due to lack of novelty, I think this submission may not be qualified for acceptance at this moment. Minor: I think the authors should give their proposed framework another name instead of using *cascade,* which has a similar meaning of *series.* [ref 1] Google*s arty filters one-up Prisma by mixing various styles. https://www.engadget.com/2016/10/27/google-style-transfer-tech/ [ref 2] Pegios, et al. Style Decomposition for Improved Neural Style Transfer. ArXiv, 2018. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The authors proposed to mix together multiple styles by proposing two frameworks: 1) serial style transfer (SST), which combines style transfer methods in series; and 2) parallel style transfer (PST), which combines style transfer methods in parallel. The paper is clearly presented. It is interesting to see work on mixing up different styles, since it is not extensive studied so far. Though not much studied, this topic is not new [ref 1], [ref 2]. The authors didn*t provide a thorough literature review on mixing multiple styles in the related work or anywhere else in the submission. In terms of the methodology, the novelty is quite limited. The proposed SST and PST are simple frameworks to mix different styles, which, by the way, are fully based on existing style transfer methods. At some point, PST is similar to [ref 2], and the difference is minor. PST linearly combines the losses for different styles to construct the final loss, while [ref 2] linearly combines together the features for different styles. About experiments, it is not clear about how the predefined parameters (e.g. alpha, w_1, w_2, etc.) are determined. They were just empirically set and mentioned in the experiment section. It is appreciated to see more results in the Appendix, as well as the user study. However, due to lack of novelty, I think this submission may not be qualified for acceptance at this moment. Minor: I think the authors should give their proposed framework another name instead of using *cascade,* which has a similar meaning of *series.* [ref 1] Google*s arty filters one-up Prisma by mixing various styles. https://www.engadget.com/2016/10/27/google-style-transfer-tech/ [ref 2] Pegios, et al. Style Decomposition for Improved Neural Style Transfer. ArXiv, 2018.\n",
      "      - Review:###This paper presents a training approach on label noise datasets and outperforms state-of-art methods. It defines the samples whose average probability on assigned label in recent q iterations is largest among all labels as memorized samples, in the sense of the network memorize these samples. Then authors proposed two stage method which firstly early-stops at minimum validation error (or memorized rate), and then trains on maximal safe set that gathers memorized samples. The experiments compared several state-of-art approaches and showed that the proposed method benefits from early-stopping and safe set. Authors also showed that the prestopping idea can also be used to improve other approaches. Pros: The proposed method achieves better performance than state-of-art methods. Authors have good experiments which evaluate on multiple datasets and algorithms. Authors also investigate the relation between model complexity and performance of co-teaching+ Cons: Many recent papers indicate the “error-prone period”, authors should include related works about early-stopping on label noise training. https://arxiv.org/pdf/1901.09960.pdf fig1 https://arxiv.org/pdf/1903.11680.pdf fig5 https://arxiv.org/pdf/1906.05392.pdf fig3 Although the method achieves good performance, since the idea is a bit straightforward especially after exploring above papers, I am slightly worried about novelty of the ideas. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper presents a training approach on label noise datasets and outperforms state-of-art methods. It defines the samples whose average probability on assigned label in recent q iterations is largest among all labels as memorized samples, in the sense of the network memorize these samples. Then authors proposed two stage method which firstly early-stops at minimum validation error (or memorized rate), and then trains on maximal safe set that gathers memorized samples. The experiments compared several state-of-art approaches and showed that the proposed method benefits from early-stopping and safe set. Authors also showed that the prestopping idea can also be used to improve other approaches. Pros: The proposed method achieves better performance than state-of-art methods. Authors have good experiments which evaluate on multiple datasets and algorithms. Authors also investigate the relation between model complexity and performance of co-teaching+ Cons: Many recent papers indicate the “error-prone period”, authors should include related works about early-stopping on label noise training. https://arxiv.org/pdf/1901.09960.pdf fig1 https://arxiv.org/pdf/1903.11680.pdf fig5 https://arxiv.org/pdf/1906.05392.pdf fig3 Although the method achieves good performance, since the idea is a bit straightforward especially after exploring above papers, I am slightly worried about novelty of the ideas.\n",
      "      - Review:###This paper studies the solution of neural network training in the NTK regime. The trained network can be written as the sum of two terms --- the first is the minimum RKHS norm interpolating solution, and the second term depends on the initialization. When the initialization scale is small, the second term almost vanishes, but when the initialization scale is large, it*s likely that the second term becomes very large, leading to worse generalization. The technical contribution of this paper is pretty low. The most important formula is (14), which only appears in the second half of the paper (the first half of the paper is almost all known results). The bounds in later part of the paper are also straightforward. Moreover, another paper https://arxiv.org/abs/1905.07777 already studied the same question and showed that non-zero output can increase the generalization error. ----------- update: I have read the authors* response. My assessment stays the same since I still think that the technical contribution of this paper is quite limited. Also there is a negative effect of using small init, which the authors might have overlooked: when the init is smaller, you*d need a larger width for the NN to be in the NTK regime. See e.g. *Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks. Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruosong Wang. ICML 2019*. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper studies the solution of neural network training in the NTK regime. The trained network can be written as the sum of two terms --- the first is the minimum RKHS norm interpolating solution, and the second term depends on the initialization. When the initialization scale is small, the second term almost vanishes, but when the initialization scale is large, it*s likely that the second term becomes very large, leading to worse generalization. The technical contribution of this paper is pretty low. The most important formula is (14), which only appears in the second half of the paper (the first half of the paper is almost all known results). The bounds in later part of the paper are also straightforward. Moreover, another paper https://arxiv.org/abs/1905.07777 already studied the same question and showed that non-zero output can increase the generalization error. ----------- update: I have read the authors* response. My assessment stays the same since I still think that the technical contribution of this paper is quite limited. Also there is a negative effect of using small init, which the authors might have overlooked: when the init is smaller, you*d need a larger width for the NN to be in the NTK regime. See e.g. *Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks. Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruosong Wang. ICML 2019*.\n",
      "    - Missing or Inadequate Discussion of Related Work and Citations (L1, 2)\n",
      "      - Review:###This paper studies overparameterized fully-connected neural networks trained with squared loss. The authors show that the resulting network can be decomposed as a sum of the solution of a certain interpolating kernel regression and a term that only depends on initialization. Based on this, the authors also derive a generalization bound of deep neural networks by transferring it to a kernel method. My major concern about this paper is the novelty and significance of its results: In terms of connection to NTK, It seems that the connection between neural networks trained with squared loss and the result of NTK-based kernel regression has already been well-studied by Arora, Sanjeev, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. *On exact computation with an infinitely wide neural net.* arXiv preprint arXiv:1904.11955 (2019). which is a missed citation. Without a clear explanation on the difference between the submission and this paper above, I don’t think this paper is ready for publication. In terms of generalization, it is also very difficult to judge whether this paper*s result is novel. In fact this paper misses almost all citations on generalization bounds for neural networks. Moreover, the generalization bound given in this paper does not seem to be very complete and significant, since the authors do not show when can L_{test}^{int} be small. To demonstrate the novelty and significance of the result, the authors should at least compare their generalization result with the following generalization bounds for over-parameterized neural networks in Section 4: Allen-Zhu, Zeyuan, Yuanzhi Li, and Yingyu Liang. *Learning and generalization in overparameterized neural networks, going beyond two layers.* arXiv preprint arXiv:1811.04918 (2018). Cao, Yuan, and Quanquan Gu. *A generalization theory of gradient descent for learning over-parameterized deep relu networks.* arXiv preprint arXiv:1902.01384 (2019). Arora, Sanjeev, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. *Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.* arXiv preprint arXiv:1901.08584 (2019). Cao, Yuan, and Quanquan Gu. *Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks.* arXiv preprint arXiv:1905.13210 (2019). Overall, I suggest that the authors should make a clear discussion on the relation of this paper to many existing works mentioned above. As long as the authors can give a convincing demonstration of the novelty and significance of their results, I will be happy to increase my score. A minor comment: how can the bound in Theorem 3 be derived based on Theorem 2? Should there be a constant factor in the bound? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper studies overparameterized fully-connected neural networks trained with squared loss. The authors show that the resulting network can be decomposed as a sum of the solution of a certain interpolating kernel regression and a term that only depends on initialization. Based on this, the authors also derive a generalization bound of deep neural networks by transferring it to a kernel method. My major concern about this paper is the novelty and significance of its results: In terms of connection to NTK, It seems that the connection between neural networks trained with squared loss and the result of NTK-based kernel regression has already been well-studied by Arora, Sanjeev, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. *On exact computation with an infinitely wide neural net.* arXiv preprint arXiv:1904.11955 (2019). which is a missed citation. Without a clear explanation on the difference between the submission and this paper above, I don’t think this paper is ready for publication. In terms of generalization, it is also very difficult to judge whether this paper*s result is novel. In fact this paper misses almost all citations on generalization bounds for neural networks. Moreover, the generalization bound given in this paper does not seem to be very complete and significant, since the authors do not show when can L_{test}^{int} be small. To demonstrate the novelty and significance of the result, the authors should at least compare their generalization result with the following generalization bounds for over-parameterized neural networks in Section 4: Allen-Zhu, Zeyuan, Yuanzhi Li, and Yingyu Liang. *Learning and generalization in overparameterized neural networks, going beyond two layers.* arXiv preprint arXiv:1811.04918 (2018). Cao, Yuan, and Quanquan Gu. *A generalization theory of gradient descent for learning over-parameterized deep relu networks.* arXiv preprint arXiv:1902.01384 (2019). Arora, Sanjeev, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. *Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.* arXiv preprint arXiv:1901.08584 (2019). Cao, Yuan, and Quanquan Gu. *Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks.* arXiv preprint arXiv:1905.13210 (2019). Overall, I suggest that the authors should make a clear discussion on the relation of this paper to many existing works mentioned above. As long as the authors can give a convincing demonstration of the novelty and significance of their results, I will be happy to increase my score. A minor comment: how can the bound in Theorem 3 be derived based on Theorem 2? Should there be a constant factor in the bound?\n",
      "      - Review:###This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives. Pros: - The writing is good - Satisfactory empirical results Cons: - The proposed method is very similar to certain methods in the literature Detail comments: (1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited. (2) Experiments - How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available. - What does the * in DRCN* mean in Table 1? - ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin. (3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition. Typos: - In Eq.(1), there is a missing D in the first term. D_s should be mathcal{D}_s to match previous notation. - In Eq.(2), the *0,* is not meaningful given the definition of []^+. Refs - Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR*06) (Vol. 2, pp. 1735-1742). IEEE. - Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham. - Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236. # Update after rebuttal Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are *totally different* can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives. Pros: - The writing is good - Satisfactory empirical results Cons: - The proposed method is very similar to certain methods in the literature Detail comments: (1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited. (2) Experiments - How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available. - What does the * in DRCN* mean in Table 1? - ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin. (3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition. Typos: - In Eq.(1), there is a missing D in the first term. D_s should be mathcal{D}_s to match previous notation. - In Eq.(2), the *0,* is not meaningful given the definition of []^+. Refs - Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR*06) (Vol. 2, pp. 1735-1742). IEEE. - Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham. - Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236. # Update after rebuttal Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are *totally different* can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.\n",
      "    - Novelty/connection to prior work and missing citations (L1, 2)\n",
      "      - Review:###The paper studies the training process of NNs through the lens of Fourier analysis. The authors argue that during the training process, NNs will first learn low frequencies part of the function first and then the high frequency part. To verify this claim empirically, the author propose two methods: 1. examine the convergence of different frequencies in a pre-selected direction in the frequency space during training; 2. examine the convergence rate of the 2-norm of low v.s. high frequencies during training. Through the experimental results of these two methods, the authors conclude that NNs learn the low frequency components before the high frequency components. The authors also discuss a potential application of this observation to solving high dimensional PDEs: coupling DNNs training (good at learning low frequency components) with the Jacobi method (good at learning high frequency components). Finally, the authors also provide some theoretical intuition (Thm 1., 2.) why low frequency components are learned faster and an explanation why NNs could generalize well on images but perform poorly on tasks like learning parity functions. Other comments: 1. It seems the filtering method is a better (might be a sufficient) way to justify the F-Principle than the projection method, given the projection method examines only one direction (also appointed out in the paper). 2. When talking about Fourier transform, would you specify what is the domain of the functions and how the functions are defined (section 3.1) The notation there is somewhat confusing (which makes the rest of the paper difficult to follow) since you are mentioning the Fourier transform of the set {(x_i, y_i)}. It will be helpful to define the function before defining its Fourier transform. Please also mention what is the domain of the function, {x_i}_i or R^d? 3. According to equation (4), it seems the domain of the functions is {x_i}_i, otherwise equation (4) should be a function of xin R^d, not x_i. 4. Could you elaborate why (4) is a good approximation of the low frequency energy rather than the L2 norm (over xin R^d) of (4) with x_i replaced by xin R^d. 5. It might be useful to refine the related work section. It is not clear what are the previous contributions prior to this paper, and it seems [1] shares some similar results/observation with the this paper. Overall, I lean to a weak rejection. The key findings (and similar results, e.g. NNs learn simple functions first), i.e. F-Principle seems to have already appeared in previous works, e.g. [1] and the theoretical results of this paper are limited to an idealized setting (results of more general setting appear in another work, mentioned in the paper.) [1]Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred A Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of deep neural networks. arXiv preprint arXiv:1806.08734, 2018. 1, 8, A (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper studies the training process of NNs through the lens of Fourier analysis. The authors argue that during the training process, NNs will first learn low frequencies part of the function first and then the high frequency part. To verify this claim empirically, the author propose two methods: 1. examine the convergence of different frequencies in a pre-selected direction in the frequency space during training; 2. examine the convergence rate of the 2-norm of low v.s. high frequencies during training. Through the experimental results of these two methods, the authors conclude that NNs learn the low frequency components before the high frequency components. The authors also discuss a potential application of this observation to solving high dimensional PDEs: coupling DNNs training (good at learning low frequency components) with the Jacobi method (good at learning high frequency components). Finally, the authors also provide some theoretical intuition (Thm 1., 2.) why low frequency components are learned faster and an explanation why NNs could generalize well on images but perform poorly on tasks like learning parity functions. Other comments: 1. It seems the filtering method is a better (might be a sufficient) way to justify the F-Principle than the projection method, given the projection method examines only one direction (also appointed out in the paper). 2. When talking about Fourier transform, would you specify what is the domain of the functions and how the functions are defined (section 3.1) The notation there is somewhat confusing (which makes the rest of the paper difficult to follow) since you are mentioning the Fourier transform of the set {(x_i, y_i)}. It will be helpful to define the function before defining its Fourier transform. Please also mention what is the domain of the function, {x_i}_i or R^d? 3. According to equation (4), it seems the domain of the functions is {x_i}_i, otherwise equation (4) should be a function of xin R^d, not x_i. 4. Could you elaborate why (4) is a good approximation of the low frequency energy rather than the L2 norm (over xin R^d) of (4) with x_i replaced by xin R^d. 5. It might be useful to refine the related work section. It is not clear what are the previous contributions prior to this paper, and it seems [1] shares some similar results/observation with the this paper. Overall, I lean to a weak rejection. The key findings (and similar results, e.g. NNs learn simple functions first), i.e. F-Principle seems to have already appeared in previous works, e.g. [1] and the theoretical results of this paper are limited to an idealized setting (results of more general setting appear in another work, mentioned in the paper.) [1]Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred A Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of deep neural networks. arXiv preprint arXiv:1806.08734, 2018. 1, 8, A\n",
      "      - Review:###This paper proposes a two-phase training method for learning with label noise. On the positive side, this paper focuses on the idea of prestopping and proposes several relevant definitions to formalize their idea and come up with a heuristic algorithm. However, I believe the paper has missed several very relevant papers that provides very similar ideas. Both [2] & [3] provide theoretical analysis to why early stopping matters in learning with label noise for DNNs. Before these two papers, [1] also observed that the learning trajectories for clean and noisy samples are different in label noise problem, and they used early stopping in their experiments to address this issue. Given these existing literatures, the contribution of this paper should be considered more properly. [1] Learning with Bad Training Data via Iterative Trimmed Loss Minimization, Yanyao Shen, Sujay Sanghavi, ICML 2019. [2] Hu, Wei, Zhiyuan Li, and Dingli Yu. *Understanding Generalization of Deep Neural Networks Trained with Noisy Labels.* arXiv preprint arXiv:1905.11368 (2019). [3] Li, Mingchen, Mahdi Soltanolkotabi, and Samet Oymak. *Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks.* arXiv preprint arXiv:1903.11680 (2019). (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a two-phase training method for learning with label noise. On the positive side, this paper focuses on the idea of prestopping and proposes several relevant definitions to formalize their idea and come up with a heuristic algorithm. However, I believe the paper has missed several very relevant papers that provides very similar ideas. Both [2] & [3] provide theoretical analysis to why early stopping matters in learning with label noise for DNNs. Before these two papers, [1] also observed that the learning trajectories for clean and noisy samples are different in label noise problem, and they used early stopping in their experiments to address this issue. Given these existing literatures, the contribution of this paper should be considered more properly. [1] Learning with Bad Training Data via Iterative Trimmed Loss Minimization, Yanyao Shen, Sujay Sanghavi, ICML 2019. [2] Hu, Wei, Zhiyuan Li, and Dingli Yu. *Understanding Generalization of Deep Neural Networks Trained with Noisy Labels.* arXiv preprint arXiv:1905.11368 (2019). [3] Li, Mingchen, Mahdi Soltanolkotabi, and Samet Oymak. *Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks.* arXiv preprint arXiv:1903.11680 (2019).\n",
      "    - Overclaiming Novelty or Insufficient Distinction from Prior Work (L1, 5)\n",
      "      -  The paper adds an interesting new perspective to equivariant neural nets. However, the actual construction looks equivalent to steerable neural nets to me (see the papers by Cohen and Welling). The generalization of steerable nets has been published under the name *gauge equivariant neural nets*, it would be very interesting to chart out the exact connections between these concepts. The authors mention that Z^{\times G} is not the only possible lifting space. I believe that the general case would be Z^V where V is a representation of G. Many of the earlier papers on equivariant nets were written in the language of representation theory. It is interesting that similar nets can be constructed by purely group theoretic methods, but I really think that ultimately they are same thing. Consequently, I would expect all the experimental results to be identical. What would make this paper really valuable for didactic purposes is if these connections were carefully mapped out and presented with intuitive diagrams and examples. (L0, 1)\n",
      "        * Examples:\n",
      "          -  The paper adds an interesting new perspective to equivariant neural nets. However, the actual construction looks equivalent to steerable neural nets to me (see the papers by Cohen and Welling). The generalization of steerable nets has been published under the name *gauge equivariant neural nets*, it would be very interesting to chart out the exact connections between these concepts. The authors mention that Z^{\times G} is not the only possible lifting space. I believe that the general case would be Z^V where V is a representation of G. Many of the earlier papers on equivariant nets were written in the language of representation theory. It is interesting that similar nets can be constructed by purely group theoretic methods, but I really think that ultimately they are same thing. Consequently, I would expect all the experimental results to be identical. What would make this paper really valuable for didactic purposes is if these connections were carefully mapped out and presented with intuitive diagrams and examples.\n",
      "      - Review:###The paper proposes a variance reduction based algorithm to solve compositional problems. The idea comes from the stochastically controlled stochastic gradient (SCSG) methods. The paper applies the idea from SCSG to estimating the inner function G(x) and the gradient \n",
      "abla f_k to solve compositional problems. The paper provides a theoretical analysis of the query complexity of the algorithm in both convex and non-convex setting. The experiments show the performance of the proposed algorithm is better than other recent methods. The paper seems to be the first attempt to extending stochastically controlled functions to the compositional problems. However, I vote for rejecting this submission for the following concerns. (1) Since SCSG is a member of the SVRG family of algorithms, the difference between this paper and [Xiangru Lian, Mengdi Wang, and Ji Liu, 2017] is not significant enough, especially in the algorithm design and the proof of the theoretical theorem. (2) The formulation of the compositional problems comes from reinforcement learning, risk-averse learning, nonlinear embedding, etc. However, the experiments are only performed on nonlinear-embedding problems. I think performing the experiments on different kinds of problems will be helpful to justify the significance. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper proposes a variance reduction based algorithm to solve compositional problems. The idea comes from the stochastically controlled stochastic gradient (SCSG) methods. The paper applies the idea from SCSG to estimating the inner function G(x) and the gradient \n",
      "abla f_k to solve compositional problems. The paper provides a theoretical analysis of the query complexity of the algorithm in both convex and non-convex setting. The experiments show the performance of the proposed algorithm is better than other recent methods. The paper seems to be the first attempt to extending stochastically controlled functions to the compositional problems. However, I vote for rejecting this submission for the following concerns. (1) Since SCSG is a member of the SVRG family of algorithms, the difference between this paper and [Xiangru Lian, Mengdi Wang, and Ji Liu, 2017] is not significant enough, especially in the algorithm design and the proof of the theoretical theorem. (2) The formulation of the compositional problems comes from reinforcement learning, risk-averse learning, nonlinear embedding, etc. However, the experiments are only performed on nonlinear-embedding problems. I think performing the experiments on different kinds of problems will be helpful to justify the significance.\n",
      "      - Review:###This paper proposes a framework to predict valence and arousal tasks in videos. The framework mainly employs LSTM in a two-time-scale-structure to take multimodal inputs. In general, the proposed framework groups well-studied techniques to solve a well-known task of multimodal learning. DNN based Multimodal learning has been heavily investigated for a long time. Numerous frameworks have been proposed with various success. Although the proposed framework is technically sound, the proposed *residual-based training strategy* and *long temporal fusion* are kind of trivial or lackluster. I can hardly identify any significant contributions that support a publication in top machine learning conferences such as ICLR. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a framework to predict valence and arousal tasks in videos. The framework mainly employs LSTM in a two-time-scale-structure to take multimodal inputs. In general, the proposed framework groups well-studied techniques to solve a well-known task of multimodal learning. DNN based Multimodal learning has been heavily investigated for a long time. Numerous frameworks have been proposed with various success. Although the proposed framework is technically sound, the proposed *residual-based training strategy* and *long temporal fusion* are kind of trivial or lackluster. I can hardly identify any significant contributions that support a publication in top machine learning conferences such as ICLR.\n",
      "      - Review:###This paper studies the importance of a neural networks weights and to which extend do they need to be updated. Particularly, the authors show that freezing weights which have small gradient in the very beginning of the training only results in a very slight drop in the final accuracy. This paper should be rejected because (1) the paper only provides some empirical results on freezing network network weights, I don*t think there are much insights and useful information; (2) To my knowledge, the phenomenon that only a few parameters are important has been observed before by many papers. Given that, I vote for a rejection. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper studies the importance of a neural networks weights and to which extend do they need to be updated. Particularly, the authors show that freezing weights which have small gradient in the very beginning of the training only results in a very slight drop in the final accuracy. This paper should be rejected because (1) the paper only provides some empirical results on freezing network network weights, I don*t think there are much insights and useful information; (2) To my knowledge, the phenomenon that only a few parameters are important has been observed before by many papers. Given that, I vote for a rejection.\n",
      "      - Review:###This work proposes a framework for solving de-mixing problems. The hard constraints from human inputs about a specific problem are relaxed into continuous constraints (the *slow* reasoning part), and a reconstruction loss measures the fitness of the inferred labels with the observations (the *fast* pattern recognition part). Due to the relaxation inference becomes an optimization problem, and on a Sudoku task and a crystal-structure-phase-mapping recovery task (both de-mixing tasks), the proposed method gets very good performance (100% for all Sudoku tasks including one in the appendix). Pros: 1. The method works well for the two demixing tasks. 2. It *led to the discovery of a new material that is important for solar fuels technology* Cons: 1. The generative decoder seems to be pretrained on both tasks instead of learned (correct me if I misunderstood), and I*m not sure if this approach can work in cases where we don*t have access to such a generative decoder, so branding the approach *deep reasoning network* might be an overclaim. 2. No reasonable baselines are used: The supervised baseline in Sudoku does not use those handcrafted constraints at all. Given pretrained decoders, a reasonable baseline would be randomized optimization methods such as simulated annealing, which might also solve the two tasks listed here. 3. This paper proposes a deep reasoning framework with relaxation and continuous optimization, but it is unclear whether this can solve general reasoning problems such as multi-hop QA or some NP-hard integer programming problems. Questions: 1. In algorithm 1, how are the penalty weights and thresholds adjusted? 2. How to determine whether a run needs to restart? Overall this work points an interesting direction of combining reasoning and pattern recognition in the same network and the proposal works well on two de-mixing problems. However, I am not convinced that the proposed solution can generalize to tasks other than the tasks proposed here, and the usage of pretrained generative decoders undermines the significance of this work. Therefore, I am inclined to reject this paper. ---updates after reading authors* rebuttal---- Thanks for revising the paper and addressing my concerns! However, my concern Con #2 has not been fully addressed. I think a reasonable baseline (at least for Sudoku) is simulated annealing, such as in https://www.researchgate.net/publication/220704743_Sudoku_Using_Parallel_Simulated_Annealing. I believe that with restarts those baselines would also solve the Sudoku problem. Another concern I still have is the claim of *reasoning*, and I*d suggest to narrow down the claim to be only on pattern de-mixing, since the reasoning part seems to be writing down continuous constraints from the discrete constraints (same as the concern in review #3). Although the proposed approach can solve some NP-C integer programming problems, it is unclear based on the experiments here whether it can work for general reasoning tasks (e.g., DROP https://allennlp.org/drop or listops https://arxiv.org/pdf/1804.06028.pdf) without writing new rules manually. Besides, after reading Reviewer 3*s comments, I also feel it unsuitable to train DRNet (generalization) on test set for 25 epochs even though you made it explicit in the revised paper. I*d recommend removing that experiment since it doesn*t change this work that much. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This work proposes a framework for solving de-mixing problems. The hard constraints from human inputs about a specific problem are relaxed into continuous constraints (the *slow* reasoning part), and a reconstruction loss measures the fitness of the inferred labels with the observations (the *fast* pattern recognition part). Due to the relaxation inference becomes an optimization problem, and on a Sudoku task and a crystal-structure-phase-mapping recovery task (both de-mixing tasks), the proposed method gets very good performance (100% for all Sudoku tasks including one in the appendix). Pros: 1. The method works well for the two demixing tasks. 2. It *led to the discovery of a new material that is important for solar fuels technology* Cons: 1. The generative decoder seems to be pretrained on both tasks instead of learned (correct me if I misunderstood), and I*m not sure if this approach can work in cases where we don*t have access to such a generative decoder, so branding the approach *deep reasoning network* might be an overclaim. 2. No reasonable baselines are used: The supervised baseline in Sudoku does not use those handcrafted constraints at all. Given pretrained decoders, a reasonable baseline would be randomized optimization methods such as simulated annealing, which might also solve the two tasks listed here. 3. This paper proposes a deep reasoning framework with relaxation and continuous optimization, but it is unclear whether this can solve general reasoning problems such as multi-hop QA or some NP-hard integer programming problems. Questions: 1. In algorithm 1, how are the penalty weights and thresholds adjusted? 2. How to determine whether a run needs to restart? Overall this work points an interesting direction of combining reasoning and pattern recognition in the same network and the proposal works well on two de-mixing problems. However, I am not convinced that the proposed solution can generalize to tasks other than the tasks proposed here, and the usage of pretrained generative decoders undermines the significance of this work. Therefore, I am inclined to reject this paper. ---updates after reading authors* rebuttal---- Thanks for revising the paper and addressing my concerns! However, my concern Con #2 has not been fully addressed. I think a reasonable baseline (at least for Sudoku) is simulated annealing, such as in https://www.researchgate.net/publication/220704743_Sudoku_Using_Parallel_Simulated_Annealing. I believe that with restarts those baselines would also solve the Sudoku problem. Another concern I still have is the claim of *reasoning*, and I*d suggest to narrow down the claim to be only on pattern de-mixing, since the reasoning part seems to be writing down continuous constraints from the discrete constraints (same as the concern in review #3). Although the proposed approach can solve some NP-C integer programming problems, it is unclear based on the experiments here whether it can work for general reasoning tasks (e.g., DROP https://allennlp.org/drop or listops https://arxiv.org/pdf/1804.06028.pdf) without writing new rules manually. Besides, after reading Reviewer 3*s comments, I also feel it unsuitable to train DRNet (generalization) on test set for 25 epochs even though you made it explicit in the revised paper. I*d recommend removing that experiment since it doesn*t change this work that much.\n",
      "- Reward Formulation and Evaluation in Imitation Learning (L3, 3)\n",
      "  - Review:###**Summary of the paper: The paper proposes an IL method named support-guided adversarial IL (SAIL), which is based on generative adversarial IL (GAIL) (Ho and Ermon, 2016) and random expert distillation (RED) (Wang et al., 2019). The key idea of SAIL is to construct a reward function by multiplying reward functions learned by GAIL and RED. This multiplication yields two benefits; 1) it handles the issue of biased reward in GAIL, since state-action pairs outside the expert’s support are assigned low reward values. 2) SAIL’s reward is more reliable than RED’s reward for state-action pairs inside the expert’s support. The authors show that SAIL is at least as fast as than GAIL in terms of the sample complexity. Experiments on continuous control benchmarks show that SAIL is overall more stable than GAIL. **Rating: The paper proposes a simple but effective combination of existing methods. The proposed method is well motivated and performs well on benchmarks. Still, the paper has some issues regarding justification, clarity, and evaluation, which should be addressed (see below). I vote for weak acceptance. **Major comments/questions: - No guarantee of the optimality of the learned policy. Can it be guaranteed that SAIL learns the expert policy? (assuming the expert policy is realizable). Propositions 1 and 2 show the convergence of the support estimation, but these results are not related to the optimality of a policy learned with the reward function. This is an important point for justifying SAIL, since SAIL does not perform distribution matching to learn the expert policy, and it also does not perform IRL to learn the reward function. Therefore, SAIL lacks the optimality guarantee from both distribution matching and IRL perspectives. Please address and clarify this point. - Clarity in the theoretical analysis. In the theoretical analysis, the paper assumes a rate of GAIL for support estimation. This is quite confusing, since GAIL performs distribution matching and does not estimate the support. Also, given that r_gail = -log D(s,a), the reward’s upper-bound (R_gail) is infinity and the bound in Eq. (9) is not informative. - The reward r_red is constant at the optimal. Eq. (2) and Eq. (3) imply that, for state-action pairs from the expert’s state-action distribution, r_red is constant at the optimal. Specifically, the optimal solution of Eq. (2) is hat{\theta} = \theta, which yields to a constant value of r_red(s,a) in Eq. (3). In this scenario, SAIL is equivalent to GAIL for the expert state-action distribution. This means that Eq. (2) should not be optimized until optimal, and some early stopping criteria are required. Does this scenario (constant value of r_red) occur in the experiments? - IRL baseline methods. The paper should compare SAIL to methods which aim to handle the bias in reward function, e.g., DAC (Kostrikov et al. 2019). While DAC requires the time limit, this time limit is known in the benchmark tasks. Also, IRL methods such as AIRL (Fu et al., 2018) should be compared, since IRL methods are better than GAIL at handling bias in reward function (Kostrikov et al. 2019). **Minor comments/questions: - Typos: *offline RL algorithms* should be *off-policy RL algorithms*. Line 5 of Algorithm 1 should perform gradient ascent instead of gradient descent. An expectation over state-action distribution of expert is missing from Eq. (2). - What are the bold numbers in table 1 and 2 indicating? Why does the Hopper task have two bold numbers, but the other tasks have only one? --After author response-- I have read the author response and other reviews. I thank the authors for including additional experiments. However, the authors* arguments regarding optimality do not fully address my comments (see below). I will keep the vote of weak acceptance. The authors argue that *In this asymptotic case, SAIL is equivalent to performing distribution matching via GAIL with the additional *constraint* that candidate distributions need to have the same support of the expert distribution*. However, the support of the expert distribution may coincide with the entire state-action space, which makes the additional constraint uninformative in the asymptotic case. Specifically, the expert distribution coincides with the state-action space when the expert policy has an infinite support (e.g., the expert policy is Gaussian). Assuming the asymptotic case, the support estimation in RED will give an indicator function over an entire state-action space, and the support constraint in SAIL is always satisfied. In other words, SAIL is exactly equivalent to GAIL in this case. For these reasons, the authors* arguments regarding optimality do not fully address my comments. I think additional assumptions are required, e.g., the expert policy needs to have a finite support or be deterministic. (L2, 1)\n",
      "    * Examples:\n",
      "      - Review:###**Summary of the paper: The paper proposes an IL method named support-guided adversarial IL (SAIL), which is based on generative adversarial IL (GAIL) (Ho and Ermon, 2016) and random expert distillation (RED) (Wang et al., 2019). The key idea of SAIL is to construct a reward function by multiplying reward functions learned by GAIL and RED. This multiplication yields two benefits; 1) it handles the issue of biased reward in GAIL, since state-action pairs outside the expert’s support are assigned low reward values. 2) SAIL’s reward is more reliable than RED’s reward for state-action pairs inside the expert’s support. The authors show that SAIL is at least as fast as than GAIL in terms of the sample complexity. Experiments on continuous control benchmarks show that SAIL is overall more stable than GAIL. **Rating: The paper proposes a simple but effective combination of existing methods. The proposed method is well motivated and performs well on benchmarks. Still, the paper has some issues regarding justification, clarity, and evaluation, which should be addressed (see below). I vote for weak acceptance. **Major comments/questions: - No guarantee of the optimality of the learned policy. Can it be guaranteed that SAIL learns the expert policy? (assuming the expert policy is realizable). Propositions 1 and 2 show the convergence of the support estimation, but these results are not related to the optimality of a policy learned with the reward function. This is an important point for justifying SAIL, since SAIL does not perform distribution matching to learn the expert policy, and it also does not perform IRL to learn the reward function. Therefore, SAIL lacks the optimality guarantee from both distribution matching and IRL perspectives. Please address and clarify this point. - Clarity in the theoretical analysis. In the theoretical analysis, the paper assumes a rate of GAIL for support estimation. This is quite confusing, since GAIL performs distribution matching and does not estimate the support. Also, given that r_gail = -log D(s,a), the reward’s upper-bound (R_gail) is infinity and the bound in Eq. (9) is not informative. - The reward r_red is constant at the optimal. Eq. (2) and Eq. (3) imply that, for state-action pairs from the expert’s state-action distribution, r_red is constant at the optimal. Specifically, the optimal solution of Eq. (2) is hat{\theta} = \theta, which yields to a constant value of r_red(s,a) in Eq. (3). In this scenario, SAIL is equivalent to GAIL for the expert state-action distribution. This means that Eq. (2) should not be optimized until optimal, and some early stopping criteria are required. Does this scenario (constant value of r_red) occur in the experiments? - IRL baseline methods. The paper should compare SAIL to methods which aim to handle the bias in reward function, e.g., DAC (Kostrikov et al. 2019). While DAC requires the time limit, this time limit is known in the benchmark tasks. Also, IRL methods such as AIRL (Fu et al., 2018) should be compared, since IRL methods are better than GAIL at handling bias in reward function (Kostrikov et al. 2019). **Minor comments/questions: - Typos: *offline RL algorithms* should be *off-policy RL algorithms*. Line 5 of Algorithm 1 should perform gradient ascent instead of gradient descent. An expectation over state-action distribution of expert is missing from Eq. (2). - What are the bold numbers in table 1 and 2 indicating? Why does the Hopper task have two bold numbers, but the other tasks have only one? --After author response-- I have read the author response and other reviews. I thank the authors for including additional experiments. However, the authors* arguments regarding optimality do not fully address my comments (see below). I will keep the vote of weak acceptance. The authors argue that *In this asymptotic case, SAIL is equivalent to performing distribution matching via GAIL with the additional *constraint* that candidate distributions need to have the same support of the expert distribution*. However, the support of the expert distribution may coincide with the entire state-action space, which makes the additional constraint uninformative in the asymptotic case. Specifically, the expert distribution coincides with the state-action space when the expert policy has an infinite support (e.g., the expert policy is Gaussian). Assuming the asymptotic case, the support estimation in RED will give an indicator function over an entire state-action space, and the support constraint in SAIL is always satisfied. In other words, SAIL is exactly equivalent to GAIL in this case. For these reasons, the authors* arguments regarding optimality do not fully address my comments. I think additional assumptions are required, e.g., the expert policy needs to have a finite support or be deterministic.\n",
      "  - Review:###The paper proposes an imitation learning algorithm that combines support estimation with adversarial training. The key idea is simple: multiply the reward from Random Expert Distillation (RED) with the reward from Generative Adversarial Imitation Learning (GAIL). The new reward combines the best of both methods. Like the GAIL reward, the new reward encourages exploration and can be estimated from a small number of demonstrations. Like the RED reward, the new reward avoids survival bias and is more stable than the adversarial reward. I have a concern regarding the Lunar Lander experiment. Were the demonstrations generated in the modified environment? If they were generated in the original environment (with early termination), this may have unintentionally created a state distribution mismatch between the demonstration environment and training environment that unfairly hurts the GAIL baseline*s performance. If the demonstrations were instead generated in the modified environment (without early termination) where the agent is actually trained, the demonstrations would contain many self-loop transitions at the goal state, and GAIL would likely not exhibit survival bias. I am also a bit concerned about the MuJoCo results. The stochasticity of the demonstrations and the evaluation trajectories may have a significant effect on the standard deviation of rewards. Was a stochastic policy or a deterministic policy used to generate the demonstrations? Were the evaluation trajectories generated by rolling out the stochastic imitation policy, or by rolling out a deterministic version of the imitation agent? Also, could the authors provide the mean and standard deviation of rewards in the demonstrations in Tables 1-2 and Figure 4? It would be nice to establish a rough upper bound on the performance of the imitation methods. Update: After reading the author response, I have increased my score from 3 to 6. (L2, 1)\n",
      "    * Examples:\n",
      "      - Review:###The paper proposes an imitation learning algorithm that combines support estimation with adversarial training. The key idea is simple: multiply the reward from Random Expert Distillation (RED) with the reward from Generative Adversarial Imitation Learning (GAIL). The new reward combines the best of both methods. Like the GAIL reward, the new reward encourages exploration and can be estimated from a small number of demonstrations. Like the RED reward, the new reward avoids survival bias and is more stable than the adversarial reward. I have a concern regarding the Lunar Lander experiment. Were the demonstrations generated in the modified environment? If they were generated in the original environment (with early termination), this may have unintentionally created a state distribution mismatch between the demonstration environment and training environment that unfairly hurts the GAIL baseline*s performance. If the demonstrations were instead generated in the modified environment (without early termination) where the agent is actually trained, the demonstrations would contain many self-loop transitions at the goal state, and GAIL would likely not exhibit survival bias. I am also a bit concerned about the MuJoCo results. The stochasticity of the demonstrations and the evaluation trajectories may have a significant effect on the standard deviation of rewards. Was a stochastic policy or a deterministic policy used to generate the demonstrations? Were the evaluation trajectories generated by rolling out the stochastic imitation policy, or by rolling out a deterministic version of the imitation agent? Also, could the authors provide the mean and standard deviation of rewards in the demonstrations in Tables 1-2 and Figure 4? It would be nice to establish a rough upper bound on the performance of the imitation methods. Update: After reading the author response, I have increased my score from 3 to 6.\n",
      "  - Review:###This paper proposes an approach for improving adversarial imitation learning, by combining it with support-estimation-based imitation learning. In particular, the paper explores a combination of GAIL (Ho and Ermon, 2016) and RED (Wang et. al., 2019), where the reward for the policy-gradient is a product of the rewards obtained from them separately. The motivation is that, while AIL methods are sample-efficient (in terms of expert data) and implicitly promote useful exploration, they could be unreliable outside the support of the expert policy. Therefore, augmenting them by constraining the imitator to the support of the expert policy (with a method such as RED) could result in an overall better imitation learning algorithm. While the motivation and intuition are clear to me, I have reservations about the claims made in the abstract and the experimental sections: 1. SAIL is an effective method for solving the reward bias in AIL. The reward in SAIL is “always” non-negative (product of 2 non-negative terms), making the method a very ad-hoc way of getting around the reward bias problem, especially when compared to other methods such as those which estimate the value function of the absorbing state (Kostrikov et. al. 2019). Consider a simple chain MDP with 3 states A, B and a terminal state T. The actions are left/right from each state. Let the expert trajectory be A->B->T. Also, for SAIL, consider perfect support estimation with an optimal RED-network. When at B, the agent can terminate with a right-action and collect some reward. But taking left and collecting 0 reward (due to perfect support estimation) makes it land in A, from where it can now achieve a positive reward for the A->B transition, and repeat the process. Hence, one could always create MDPs where the Q value of B->A is higher than B->T. The Lunar-Lander environment (with certain parameters) in Section 4.1 appears to present a scenario where SAIL get arounds the reward bias, but this doesn’t remove my doubts over the generalization of this approach. Also, in Table 1, why does GAIL not hover above the landing spot even in the default case? If the reward bias is strong there, with sufficient exploration, the agent should converge to the same policy as in the modified case. Figure 3 is concerning for the same reason as above. It shows the immediate reward at the goal state, and points that SAIL has large reward for no-op action. The issue is that RL optimizes for actions that have the maximum Q value, not the action with the maximum immediate reward. 2. I would recommend that the authors refer to the original GAIL algorithm as “GAIL” in the experiments section, and their practical stabilization trick as “GAIL-bounded” (or something to that effect). Referring to original algorithm as GAIL-log, and the modification as GAIL could be misleading to readers. 3. The authors claim that SAIL has better training stability, leading to more robust policies. If this is due to the algorithmic contribution of combining AIL and Support-Estimation-IL, then GAIL-log and SAIL-log in Table 2. should show this in the standard deviation numbers. This doesn’t appear to be the case. Also, Figure 4 (Half-Cheetah) has unusually large variance for SAIL-log. 4. Figure 4 and Table 2 numbers are very different. Take Humanoid for instance. From Figure 4, it seems that SAIL is way better than GAIL. But if you look at Table 2, they both achieve mean-score in excess of 10k. What’s the difference between Table 2. and final performance in Figure 4? (L2, 1)\n",
      "    * Examples:\n",
      "      - Review:###This paper proposes an approach for improving adversarial imitation learning, by combining it with support-estimation-based imitation learning. In particular, the paper explores a combination of GAIL (Ho and Ermon, 2016) and RED (Wang et. al., 2019), where the reward for the policy-gradient is a product of the rewards obtained from them separately. The motivation is that, while AIL methods are sample-efficient (in terms of expert data) and implicitly promote useful exploration, they could be unreliable outside the support of the expert policy. Therefore, augmenting them by constraining the imitator to the support of the expert policy (with a method such as RED) could result in an overall better imitation learning algorithm. While the motivation and intuition are clear to me, I have reservations about the claims made in the abstract and the experimental sections: 1. SAIL is an effective method for solving the reward bias in AIL. The reward in SAIL is “always” non-negative (product of 2 non-negative terms), making the method a very ad-hoc way of getting around the reward bias problem, especially when compared to other methods such as those which estimate the value function of the absorbing state (Kostrikov et. al. 2019). Consider a simple chain MDP with 3 states A, B and a terminal state T. The actions are left/right from each state. Let the expert trajectory be A->B->T. Also, for SAIL, consider perfect support estimation with an optimal RED-network. When at B, the agent can terminate with a right-action and collect some reward. But taking left and collecting 0 reward (due to perfect support estimation) makes it land in A, from where it can now achieve a positive reward for the A->B transition, and repeat the process. Hence, one could always create MDPs where the Q value of B->A is higher than B->T. The Lunar-Lander environment (with certain parameters) in Section 4.1 appears to present a scenario where SAIL get arounds the reward bias, but this doesn’t remove my doubts over the generalization of this approach. Also, in Table 1, why does GAIL not hover above the landing spot even in the default case? If the reward bias is strong there, with sufficient exploration, the agent should converge to the same policy as in the modified case. Figure 3 is concerning for the same reason as above. It shows the immediate reward at the goal state, and points that SAIL has large reward for no-op action. The issue is that RL optimizes for actions that have the maximum Q value, not the action with the maximum immediate reward. 2. I would recommend that the authors refer to the original GAIL algorithm as “GAIL” in the experiments section, and their practical stabilization trick as “GAIL-bounded” (or something to that effect). Referring to original algorithm as GAIL-log, and the modification as GAIL could be misleading to readers. 3. The authors claim that SAIL has better training stability, leading to more robust policies. If this is due to the algorithmic contribution of combining AIL and Support-Estimation-IL, then GAIL-log and SAIL-log in Table 2. should show this in the standard deviation numbers. This doesn’t appear to be the case. Also, Figure 4 (Half-Cheetah) has unusually large variance for SAIL-log. 4. Figure 4 and Table 2 numbers are very different. Take Humanoid for instance. From Figure 4, it seems that SAIL is way better than GAIL. But if you look at Table 2, they both achieve mean-score in excess of 10k. What’s the difference between Table 2. and final performance in Figure 4?\n",
      "- Theoretical Foundation, Analysis, and Justification (L3, 22)\n",
      "  - Justification and Clarity of Probabilistic and Regularization Assumptions in Neural Network Models (L2, 2)\n",
      "    - Review:###Main contribution of the paper - The paper argues that the base assumption, the i.i.d. of the activated elements (activations) in the hidden layers, the existing methods (lee.et.al 2018) hold is not convincible. - Instead, the author proposes a new way to probabilistically model the hidden layers, activations, and layer/layer connections. - Based on the probabilistic model, the paper proposes a new regularizer. Methods - The author argues that the activation is not iid by empirically showing that the trained MLP (in most cases) does not un-correlated. - The author proposes a new probabilistic model for MLP, and CNN assuming the Gibbs distribution to each activation and also assuming the product of expert (poE) model to explain the layer/layer relationship. - And according to their model, CNN will be explained by the MRF model. - The author proposes a regularization term regarding layer/layer connection. - They argue that the SGD training can be seen as a first-order approximation of the inference of the hidden activations in MLP. Questions - See the Concerns Strongpoints - The probabilistic explanation of the MLP and the CNN seems novel and was interesting to the reviewer - The proposed explanation assumes a weaker condition compared to the existing methods. Concerns - The main concern is that the reviewer cannot fully convince that i.i.d. assumption is wrong. Even though the trained MLP does not support the i.i.d. condition, one can suppose that the reason would be the typical training method (SGD), just finding the local minima in a deterministic way. Maybe the proof in Appendix.G. supports the argument of the author, but the reviewer failed to clearly agree with the argument. A clear explanation regarding the issue would be required. - As far as the author understands, the paper proposes a probabilistic (Bayesian) model for explaining MLP, but it seems that they just used SGD for training the model. In that case, the reviewer is little suspicious of the role of the proposed regularization in that the regularization comes from Bayesian formulation, but the model was trained in a deterministic way. The reviewer wants to ask the author that (1) is it possible to infer the model in a Bayesian manner such as sampling? (2) Is there any justification for using SGD when conducting the experiments regarding the regularization? If it is related to Appendix.G, clearer explanation would be appreciated. - As far as the reviewer understands, the regularization deals with the practical part of the paper. It would be better to see the effect of the regularization of widely used networks such as small-layered ResNet or others. If the proposed formulation has other practical strongpoints, it would be nice to clarify them. - The explanation using Gibbs distribution and PoE looks similar to RBM. The reviewer strongly wants a clear explanation of the difference and the strongpoints compared to RBM. Conclusion - The author proposed a new probabilistic explanation of the neural network, which seems novel and worth reporting. - However, the reviewer failed to fully agree on some steps in the process of the paper. Therefore, the reviewer temporary rates the paper as weak-reject, but this can be adjusted after seeing the answers of the author. Inquiries - See the concerns parts. (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###Main contribution of the paper - The paper argues that the base assumption, the i.i.d. of the activated elements (activations) in the hidden layers, the existing methods (lee.et.al 2018) hold is not convincible. - Instead, the author proposes a new way to probabilistically model the hidden layers, activations, and layer/layer connections. - Based on the probabilistic model, the paper proposes a new regularizer. Methods - The author argues that the activation is not iid by empirically showing that the trained MLP (in most cases) does not un-correlated. - The author proposes a new probabilistic model for MLP, and CNN assuming the Gibbs distribution to each activation and also assuming the product of expert (poE) model to explain the layer/layer relationship. - And according to their model, CNN will be explained by the MRF model. - The author proposes a regularization term regarding layer/layer connection. - They argue that the SGD training can be seen as a first-order approximation of the inference of the hidden activations in MLP. Questions - See the Concerns Strongpoints - The probabilistic explanation of the MLP and the CNN seems novel and was interesting to the reviewer - The proposed explanation assumes a weaker condition compared to the existing methods. Concerns - The main concern is that the reviewer cannot fully convince that i.i.d. assumption is wrong. Even though the trained MLP does not support the i.i.d. condition, one can suppose that the reason would be the typical training method (SGD), just finding the local minima in a deterministic way. Maybe the proof in Appendix.G. supports the argument of the author, but the reviewer failed to clearly agree with the argument. A clear explanation regarding the issue would be required. - As far as the author understands, the paper proposes a probabilistic (Bayesian) model for explaining MLP, but it seems that they just used SGD for training the model. In that case, the reviewer is little suspicious of the role of the proposed regularization in that the regularization comes from Bayesian formulation, but the model was trained in a deterministic way. The reviewer wants to ask the author that (1) is it possible to infer the model in a Bayesian manner such as sampling? (2) Is there any justification for using SGD when conducting the experiments regarding the regularization? If it is related to Appendix.G, clearer explanation would be appreciated. - As far as the reviewer understands, the regularization deals with the practical part of the paper. It would be better to see the effect of the regularization of widely used networks such as small-layered ResNet or others. If the proposed formulation has other practical strongpoints, it would be nice to clarify them. - The explanation using Gibbs distribution and PoE looks similar to RBM. The reviewer strongly wants a clear explanation of the difference and the strongpoints compared to RBM. Conclusion - The author proposed a new probabilistic explanation of the neural network, which seems novel and worth reporting. - However, the reviewer failed to fully agree on some steps in the process of the paper. Therefore, the reviewer temporary rates the paper as weak-reject, but this can be adjusted after seeing the answers of the author. Inquiries - See the concerns parts.\n",
      "    - Review:###The authors show that parameters of a DNN do not satisfy the i.i.d. prior assumption and that neural layer activations considered as i.i.d. are not valid assumptions for all hidden layers of the network. One can therefore not rightfully use GPs to describe the network’s hidden layers. The authors suggest formulating the neurons per layer as energy functions thereby rendering a hidden layer as a Gibbs distribution and the connection between adjacent hidden layers as a PoE model. The paper is well written and well postulated. > Fig 4: What is the information presented by each neuron? How would this have looked with the i.i.d. prior in place. > There are places in the paper where one must refer to the supplementary, for example sections H and J with the simulations. Do consider moving these crucial sections to the main paper. > One recurring thought I had when the authors bring up Bayesian Hierarchical model, is that most of the BHMs rely on i.i.d assumptions both in the prior space and with the observations. How would you stand by your claim of explaining a DNN*s layers to be modelled as a BHM? (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###The authors show that parameters of a DNN do not satisfy the i.i.d. prior assumption and that neural layer activations considered as i.i.d. are not valid assumptions for all hidden layers of the network. One can therefore not rightfully use GPs to describe the network’s hidden layers. The authors suggest formulating the neurons per layer as energy functions thereby rendering a hidden layer as a Gibbs distribution and the connection between adjacent hidden layers as a PoE model. The paper is well written and well postulated. > Fig 4: What is the information presented by each neuron? How would this have looked with the i.i.d. prior in place. > There are places in the paper where one must refer to the supplementary, for example sections H and J with the simulations. Do consider moving these crucial sections to the main paper. > One recurring thought I had when the authors bring up Bayesian Hierarchical model, is that most of the BHMs rely on i.i.d assumptions both in the prior space and with the observations. How would you stand by your claim of explaining a DNN*s layers to be modelled as a BHM?\n",
      "  - Theoretical Analysis and Conceptual Clarity Issues (L2, 5)\n",
      "    - Insufficient theoretical justification, definitions, or conceptual grounding (L1, 2)\n",
      "      - Review:###This paper mainly focuses on experimental results on real data to verify the so-called Frequency Principle: DNNs often fit target functions from low to high frequencies during the training process. Some theoretical analyses are also provided to backup the empirical observation. This paper is very well-written. The methods are explained very clearly, and the logic is easy to follow. However I think this paper also has some weak points, as listed below: (1) The frequency principle lacks a rigorous definition. In Section 2, the authors provide very inspiring explanations and examples, however no rigorous definitions are given. Is there a way to directly quantitatively define the response frequency? (2) In Section 3.1, it is not explained why the frequencies are calculated based on the samples. Probably I have missed something, but based on the description, can’t the frequencies be directly calculated on a grid along the 1-dimensional subspace defined by the mean and first principle component of the data? In other words, even after training the predictor function with real data samples for several steps, the frequency of a predictor function should still be its own property and should be independent of the distribution of data inputs. In fact, are the vectors for different even orthonormal vectors? I think unless follows certain specific distributions, these vectors are not even close to orthonormal. Therefore using them to calculate the frequencies is very weird. (3) In fact, are Sections 3,4 and 6 studying the same kind of frequency? It is not very clear due to the vague definitions. Because of these concerns, I think this paper is on the borderline. For now I tend to recommend a weak reject. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper mainly focuses on experimental results on real data to verify the so-called Frequency Principle: DNNs often fit target functions from low to high frequencies during the training process. Some theoretical analyses are also provided to backup the empirical observation. This paper is very well-written. The methods are explained very clearly, and the logic is easy to follow. However I think this paper also has some weak points, as listed below: (1) The frequency principle lacks a rigorous definition. In Section 2, the authors provide very inspiring explanations and examples, however no rigorous definitions are given. Is there a way to directly quantitatively define the response frequency? (2) In Section 3.1, it is not explained why the frequencies are calculated based on the samples. Probably I have missed something, but based on the description, can’t the frequencies be directly calculated on a grid along the 1-dimensional subspace defined by the mean and first principle component of the data? In other words, even after training the predictor function with real data samples for several steps, the frequency of a predictor function should still be its own property and should be independent of the distribution of data inputs. In fact, are the vectors for different even orthonormal vectors? I think unless follows certain specific distributions, these vectors are not even close to orthonormal. Therefore using them to calculate the frequencies is very weird. (3) In fact, are Sections 3,4 and 6 studying the same kind of frequency? It is not very clear due to the vague definitions. Because of these concerns, I think this paper is on the borderline. For now I tend to recommend a weak reject.\n",
      "      - Review:###This paper proposes to analyze the loss of neural networks in the Fourier domain. Since this is computationally expensive for larger-dimensional datasets, the analysis instead first projects the data onto the principal component of the data, and then using a Gaussian kernel estimation (which has nice properties in the Fourier domain). The analysis finds that DNNs tend to learn low-frequency components before high-frequency ones. Overall I quite like the analysis of this paper. I think it could be clearer and contain more experiments but it is otherwise rather convincing proof that DNNs learn low-frequency patterns first. - More experiments: in particular, analyzing this phenomenon over more than a single principal component or through non-linear transformations of the data. - It*s not always clear how is calculated or where it comes from, whether it is implicit through the Gaussian metric or chosen randomly. The paper would benefit from always making this clear in the text and figure captions. - It*s well known that different optimizers seem to learn differently both in terms of speed and features that end up being learned (thus generalization), repeating this analysis for Adam, RMSprop & friends would be great. The paper is mostly easy to read, but there are a few mistakes here and there that slow down reading. Here are a few: - *variation problems* you mean *variational*? - *This difference implicates* -> *implies* - *by the Parseval*s theorem* -> *by Parseval*s theorem* (occurs multiple times) - *difference of* -> *difference between* - *in previous section* -> *in the previous section* - *to verify F-Principle* -> *to verify the F-Principle* (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes to analyze the loss of neural networks in the Fourier domain. Since this is computationally expensive for larger-dimensional datasets, the analysis instead first projects the data onto the principal component of the data, and then using a Gaussian kernel estimation (which has nice properties in the Fourier domain). The analysis finds that DNNs tend to learn low-frequency components before high-frequency ones. Overall I quite like the analysis of this paper. I think it could be clearer and contain more experiments but it is otherwise rather convincing proof that DNNs learn low-frequency patterns first. - More experiments: in particular, analyzing this phenomenon over more than a single principal component or through non-linear transformations of the data. - It*s not always clear how is calculated or where it comes from, whether it is implicit through the Gaussian metric or chosen randomly. The paper would benefit from always making this clear in the text and figure captions. - It*s well known that different optimizers seem to learn differently both in terms of speed and features that end up being learned (thus generalization), repeating this analysis for Adam, RMSprop & friends would be great. The paper is mostly easy to read, but there are a few mistakes here and there that slow down reading. Here are a few: - *variation problems* you mean *variational*? - *This difference implicates* -> *implies* - *by the Parseval*s theorem* -> *by Parseval*s theorem* (occurs multiple times) - *difference of* -> *difference between* - *in previous section* -> *in the previous section* - *to verify F-Principle* -> *to verify the F-Principle*\n",
      "    - Missing, Incomplete, or Unconvincing Theoretical Analysis (L1, 2)\n",
      "      - Review:###The paper considers the problem of parametric conditional density estimation, i.e. given a set of points {(x_n, y_n)} drawn from a distribution , the task is to estimate the conditional distribution p(x|y). The paper considers parametric estimation where in given a parametrized family of distributions f_{theta} we wish to minimize the likelihood of seeing the given data over theta. The parametric family in a lot of applications consists of highly expressive families like neural networks, which leads to the issue of overfitting in small data regimes. This has been tackled via regularization over the parameter space which might be hard to interpret as the associated inductive bias is not well understood and depends on the parametric family under consideration. On the other hand the paper proposes to add explicit noise in the examples used during training, i.e. irrespective of the optimization procedure (which could be mini-bath sgd) the paper proposes to draw examples from the data set, explicitly add noise onto the examples and create a proxy objective over the augmented data set. The paper establishes two theoretical results. First is a simple taylor approximation based analysis to highlight the effect of the variance of noise. The conclusion is that higher variance penalizes the high curvature areas of the resultant density and hence this kind of noise addition could be seen as making resulting density smoother. The second contribution is to show that this procedure is asymptotically consistent, i.e. as n goes to infinity and the number of augmented data points go to infinity, the resulting density converges to the target density. This of course requires the noise variance to follow a decreasing schedule to 0. The main merit of the idea is in its agnostic nature, as it can be applied to any parametric family and the experiements show that it seems to uniform improvement across models, The basic idea proposed by the error, has existed in the space of deep learning based methods forever. This is the same idea behind image augmentation which forms a crucial part of training supervised models in vision. The authors claim that this idea is novel in the space of parametric density estimation, however I do not know enough about the area to verify the claim. It would surprise that this very natural idea has not been tried before. I have gone through the theoretical derivations in the paper and they look sound to me. However the results are all asymptotic in nature without establishing explicit rates which is a little bit of disappointment. Since I am not completely familiar in nature, but I guess such asymptotic consistency might be achievable using other forms of regularization under suitable assumptions. In that light, the theoretical contributions while being sound did not lend much intuition about why such a method might outperform others. Intuition does arise from the derivation for the effect of noise on the objective which helps understand the nature of the noise, but one wonders if similar intuitions could be derived for other forms of regularization as well. It would be great to see this derivation being extended to some concrete scenarios under well understood parametric families and seeing the effect explicitly. Regarding the experiments - The experiments definitely look promising as the improvments seem uniformly good across the cases considered. I am not an expert however in this setting so it is hard for me to judge the quality and significance of the benchmarks. The experiment methodology nevertheless looks sound. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper considers the problem of parametric conditional density estimation, i.e. given a set of points {(x_n, y_n)} drawn from a distribution , the task is to estimate the conditional distribution p(x|y). The paper considers parametric estimation where in given a parametrized family of distributions f_{theta} we wish to minimize the likelihood of seeing the given data over theta. The parametric family in a lot of applications consists of highly expressive families like neural networks, which leads to the issue of overfitting in small data regimes. This has been tackled via regularization over the parameter space which might be hard to interpret as the associated inductive bias is not well understood and depends on the parametric family under consideration. On the other hand the paper proposes to add explicit noise in the examples used during training, i.e. irrespective of the optimization procedure (which could be mini-bath sgd) the paper proposes to draw examples from the data set, explicitly add noise onto the examples and create a proxy objective over the augmented data set. The paper establishes two theoretical results. First is a simple taylor approximation based analysis to highlight the effect of the variance of noise. The conclusion is that higher variance penalizes the high curvature areas of the resultant density and hence this kind of noise addition could be seen as making resulting density smoother. The second contribution is to show that this procedure is asymptotically consistent, i.e. as n goes to infinity and the number of augmented data points go to infinity, the resulting density converges to the target density. This of course requires the noise variance to follow a decreasing schedule to 0. The main merit of the idea is in its agnostic nature, as it can be applied to any parametric family and the experiements show that it seems to uniform improvement across models, The basic idea proposed by the error, has existed in the space of deep learning based methods forever. This is the same idea behind image augmentation which forms a crucial part of training supervised models in vision. The authors claim that this idea is novel in the space of parametric density estimation, however I do not know enough about the area to verify the claim. It would surprise that this very natural idea has not been tried before. I have gone through the theoretical derivations in the paper and they look sound to me. However the results are all asymptotic in nature without establishing explicit rates which is a little bit of disappointment. Since I am not completely familiar in nature, but I guess such asymptotic consistency might be achievable using other forms of regularization under suitable assumptions. In that light, the theoretical contributions while being sound did not lend much intuition about why such a method might outperform others. Intuition does arise from the derivation for the effect of noise on the objective which helps understand the nature of the noise, but one wonders if similar intuitions could be derived for other forms of regularization as well. It would be great to see this derivation being extended to some concrete scenarios under well understood parametric families and seeing the effect explicitly. Regarding the experiments - The experiments definitely look promising as the improvments seem uniformly good across the cases considered. I am not an expert however in this setting so it is hard for me to judge the quality and significance of the benchmarks. The experiment methodology nevertheless looks sound.\n",
      "      - Review:###The paper derives results for nonnegative-matrix factorization along the lines of recent results on SGD for DNNs, showing that the loss is star-convex towards randomized planted solutions. The star-convexity property is also shown to hold to some degree on real world datasets. The paper argues that these results explain the good performance that usual gradient descent procedures achieve in practice. The paper also puts forward a conjecture that more parameters make the loss function easier to optimize by making it more likely that star convexity holds, and that a similar conclusion could hold for DNNs. The paper is rather well written, although there are many small typos or notation errors (of which I mention a few below). In addition, I have a few issues with the presentation of both the theoretical and experimental results. Although the results relating to star convexity seem compelling and interesting to understand the good practical performance of the usual simple NMF algorithms, I find the conjecture on concentration of measure a bit hand-wavy. In particular, I have some questions regarding the main theorem: - *r grows as and m as *: Big-O means asymptotically bounded, i.e., that r is roughly smaller than n^gamma (which is a tautology for anyway), and that m is roughly smaller than n. Is that really what you mean? Or do you mean instead? Fact 2 uses *recall our assumption that * which seems to imply you mean *r = Omega(n^gamma)*, which can also be informally stated as *r grows as n^gamma*, without Big-O notation. Please clarify this notation. - Also, re , I do not quite see why such a strong assumption is made. Wouldn*t be enough with respect to Fact 2 to get an asymptotically vanishing deviation probability? i.e., wouldn*t the final result work with , which is more general? Generally, this begs the question of whether is realistic. Can this be put in perspective with respect to other work? - What is Lemma 9 mentioned in the proof of Lemma 1? Is that in the paper? I could not find it. In particular, I am unclear on *we can do a polynomial number of union bounds*: polynomial with respect to which variable? I also have the following additional questions on the experimental findings and the proposed conjecture. - Section 4.2 and Figure 5 uses the *relative deviation*, which is the standard deviation normalized by the mean. I am not sure what conclusion to draw from this, however. In particular: - Why normalize by dividing by the mean? - What is the evolution of the mean itself? If the mean is always greater than a few standard deviations, then the fact that curvature is getting more concentrated may not matter. How about showing instead the fraction of trials where the curvature was non-negative? (i.e., where we had convexity along the line) - I struggle a bit to make sense of the conjecture. In particular, *concentration of measure* is related to randomness, but it isn*t quite clear which *measure* we*re talking about here. Table 2 is also not very convincing: although the legend indicates *increased width makes the loss surface increasingly locally convex*, it only seems to hold for the fourth row (epoch=300). In addition, these means are reported without standard deviations, making it hard to judge whether to trust the ordering of these few numbers. Typos/unclear: introduction: *strictly non-negative factors*, what do you mean by strictly? *randomly chosen or the global minimizer* is a bit unclear - what is the *or* over? *Convex along straight paths towards the optima x**: x* -> to be consistent with Definition 1. *similar to Dvoretzky*s*: I fail to see the similarity and how this is related to the present paper. *we will assume that there is a planted optimal solution (U*, V*)*: U*, V* should be in bold to be consistent with further notation. *how r and m depends on n*: depends -> depend *as the size of the problem increaseS* *loss function *of* equation 2* Theorem 1: *at least geq* is redundant Theorem 1: *but with exponent -c r^{1/3}* is unclear: which exponent is this? Write the statement in full instead. *it*s second derivative*: its? *for unobserved Data*: why is *data* capitalized? D3: *clearly, equation 9 holds in this case*: not sure what is meant here - equation (10) does not imply (9) or reciprocally. Did you mean that equation 10 holds instead? D3: *3 evenly spaceD lambda* D6: Definition 2: *iff there *exists* positive constants...* D6: *one can easily verify that independenT Gaussian variables* D6: *i=j, j=l, and so on*: what is *and so on*? all indexes are completely arbitrary? D6: *z = P(X_i, X_j, X_k, X_l)*: clarify notation . In the main text, you*ve used the notation instead. D6: Equation (12) and (13): should the RHS be exp(-X) instead of exp(X)? D6: notation is a bit confusing: the text doesn*t explain what it is, and lack of bold face hints that this is a scalar and not a matrix. D6: *Lemma 9 says that each one can be expressED* E1: End of proof of fact 3: do not include an equal sign if there is no left-hand side. E1: *trace and linearity are linear operators*: linearity is linear? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper derives results for nonnegative-matrix factorization along the lines of recent results on SGD for DNNs, showing that the loss is star-convex towards randomized planted solutions. The star-convexity property is also shown to hold to some degree on real world datasets. The paper argues that these results explain the good performance that usual gradient descent procedures achieve in practice. The paper also puts forward a conjecture that more parameters make the loss function easier to optimize by making it more likely that star convexity holds, and that a similar conclusion could hold for DNNs. The paper is rather well written, although there are many small typos or notation errors (of which I mention a few below). In addition, I have a few issues with the presentation of both the theoretical and experimental results. Although the results relating to star convexity seem compelling and interesting to understand the good practical performance of the usual simple NMF algorithms, I find the conjecture on concentration of measure a bit hand-wavy. In particular, I have some questions regarding the main theorem: - *r grows as and m as *: Big-O means asymptotically bounded, i.e., that r is roughly smaller than n^gamma (which is a tautology for anyway), and that m is roughly smaller than n. Is that really what you mean? Or do you mean instead? Fact 2 uses *recall our assumption that * which seems to imply you mean *r = Omega(n^gamma)*, which can also be informally stated as *r grows as n^gamma*, without Big-O notation. Please clarify this notation. - Also, re , I do not quite see why such a strong assumption is made. Wouldn*t be enough with respect to Fact 2 to get an asymptotically vanishing deviation probability? i.e., wouldn*t the final result work with , which is more general? Generally, this begs the question of whether is realistic. Can this be put in perspective with respect to other work? - What is Lemma 9 mentioned in the proof of Lemma 1? Is that in the paper? I could not find it. In particular, I am unclear on *we can do a polynomial number of union bounds*: polynomial with respect to which variable? I also have the following additional questions on the experimental findings and the proposed conjecture. - Section 4.2 and Figure 5 uses the *relative deviation*, which is the standard deviation normalized by the mean. I am not sure what conclusion to draw from this, however. In particular: - Why normalize by dividing by the mean? - What is the evolution of the mean itself? If the mean is always greater than a few standard deviations, then the fact that curvature is getting more concentrated may not matter. How about showing instead the fraction of trials where the curvature was non-negative? (i.e., where we had convexity along the line) - I struggle a bit to make sense of the conjecture. In particular, *concentration of measure* is related to randomness, but it isn*t quite clear which *measure* we*re talking about here. Table 2 is also not very convincing: although the legend indicates *increased width makes the loss surface increasingly locally convex*, it only seems to hold for the fourth row (epoch=300). In addition, these means are reported without standard deviations, making it hard to judge whether to trust the ordering of these few numbers. Typos/unclear: introduction: *strictly non-negative factors*, what do you mean by strictly? *randomly chosen or the global minimizer* is a bit unclear - what is the *or* over? *Convex along straight paths towards the optima x**: x* -> to be consistent with Definition 1. *similar to Dvoretzky*s*: I fail to see the similarity and how this is related to the present paper. *we will assume that there is a planted optimal solution (U*, V*)*: U*, V* should be in bold to be consistent with further notation. *how r and m depends on n*: depends -> depend *as the size of the problem increaseS* *loss function *of* equation 2* Theorem 1: *at least geq* is redundant Theorem 1: *but with exponent -c r^{1/3}* is unclear: which exponent is this? Write the statement in full instead. *it*s second derivative*: its? *for unobserved Data*: why is *data* capitalized? D3: *clearly, equation 9 holds in this case*: not sure what is meant here - equation (10) does not imply (9) or reciprocally. Did you mean that equation 10 holds instead? D3: *3 evenly spaceD lambda* D6: Definition 2: *iff there *exists* positive constants...* D6: *one can easily verify that independenT Gaussian variables* D6: *i=j, j=l, and so on*: what is *and so on*? all indexes are completely arbitrary? D6: *z = P(X_i, X_j, X_k, X_l)*: clarify notation . In the main text, you*ve used the notation instead. D6: Equation (12) and (13): should the RHS be exp(-X) instead of exp(X)? D6: notation is a bit confusing: the text doesn*t explain what it is, and lack of bold face hints that this is a scalar and not a matrix. D6: *Lemma 9 says that each one can be expressED* E1: End of proof of fact 3: do not include an equal sign if there is no left-hand side. E1: *trace and linearity are linear operators*: linearity is linear?\n",
      "    - Review:###Summary: The authors analyze the bias in the straight-through gradient estimator using the framework of harmonic analysis of boolean functions. Based on this analysis, they propose three methods to reduce the bias of the straight-through estimator, resulting in a less-biased estimator that is the same computational complexity as the original. They evaluate this estimator on a series of generative modeling tasks where they demonstrate improvements over existing methods, including the ability to train a very deep stochastic network. I enjoyed this paper -- the exposition is clear, the ideas are (to my knowledge) novel and make sense, and the experimental evaluation is thorough and convincing. I recommend an accept. I skimmed through the proofs in the appendix so cannot with absolute confidence vouch for their correctness. One small piece of feedback: I found the most confusing part of the paper was the section on the *bernoulli splitting trick*. It might be helpful to pull some of the appendix material into this section to make it a little less sparse. (L1, 1)\n",
      "      * Examples:\n",
      "        - Review:###Summary: The authors analyze the bias in the straight-through gradient estimator using the framework of harmonic analysis of boolean functions. Based on this analysis, they propose three methods to reduce the bias of the straight-through estimator, resulting in a less-biased estimator that is the same computational complexity as the original. They evaluate this estimator on a series of generative modeling tasks where they demonstrate improvements over existing methods, including the ability to train a very deep stochastic network. I enjoyed this paper -- the exposition is clear, the ideas are (to my knowledge) novel and make sense, and the experimental evaluation is thorough and convincing. I recommend an accept. I skimmed through the proofs in the appendix so cannot with absolute confidence vouch for their correctness. One small piece of feedback: I found the most confusing part of the paper was the section on the *bernoulli splitting trick*. It might be helpful to pull some of the appendix material into this section to make it a little less sparse.\n",
      "  - Theoretical Foundation and Justification Issues (L2, 15)\n",
      "    - Lack of or Unclear Theoretical Justification/Analysis (L1, 2)\n",
      "      - Review:###Label noise widely exists in the large-scale data sets. This paper proposes a novel approach that directly cleans labels in order to train a high quality model. The proposed method leverages statistical principles to correct data labels and has a theoretical guarantee of the correctness. In particular, a likelihood ratio test (LRT) to flip the labels of training data is used, and it can prove that the LRT label correction algorithm is guaranteed to flip the label so it is consistent with the true Bayesian optimal classifier with high probability. The experimental results on several benchmark data sets show that the proposed method is promising. Overall, this paper could be a significant algorithmic contribution. But I also have some minor concerns: [1] The theoretical analysis and the experimental results are both well organized in the paper. How about the time complexity of the proposed method. If the authors can show the time cost in the paper, I will much more agree with the paper. [2] In the experimental parts, the convergence curve of the proposed method during the training epochs may be better to prove the theoretical analysis. [3]The details of the compared methods should be given, and it will be better to give the results without any noise labels. In this way, the confidence of the paper will be further improved. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###Label noise widely exists in the large-scale data sets. This paper proposes a novel approach that directly cleans labels in order to train a high quality model. The proposed method leverages statistical principles to correct data labels and has a theoretical guarantee of the correctness. In particular, a likelihood ratio test (LRT) to flip the labels of training data is used, and it can prove that the LRT label correction algorithm is guaranteed to flip the label so it is consistent with the true Bayesian optimal classifier with high probability. The experimental results on several benchmark data sets show that the proposed method is promising. Overall, this paper could be a significant algorithmic contribution. But I also have some minor concerns: [1] The theoretical analysis and the experimental results are both well organized in the paper. How about the time complexity of the proposed method. If the authors can show the time cost in the paper, I will much more agree with the paper. [2] In the experimental parts, the convergence curve of the proposed method during the training epochs may be better to prove the theoretical analysis. [3]The details of the compared methods should be given, and it will be better to give the results without any noise labels. In this way, the confidence of the paper will be further improved.\n",
      "      - Review:###This paper explores a well motivated but very heuristic idea for selecting the next samples to train on for training deep learning models. This method relies on looking at the uncertainty of predictions of in the recent history of statements and preferring those instances that have a predictive uncertainty over the recent predictions. This allows the training method to train on instances that are neither too hard nor too easy and focus on reducing the uncertainty whenever it has the greatest potential gain to do so. There are two extra components that make this method work: - Windowing: only looking at the recent history of the instances which has two effects: firstly, the current state of the model is explored which gives a more recent assessment relative to the current state of the model. Secondly, it makes the algorithm faster by reducing the overhead of analyzing the prediction history of samples. - Annealing the selection bias: as the training goes on the selection becomes more random and less biased. This approach is evaluated in on three simple data-sets: MNIST, CIFAR-10 and CIFAR-100. Although this is a very limited subset of models, the results are consistent and statistically significant, although their effect is not really huge. The paper gives very little theoretical justification or analysis of the results but gives only the presented empirical evidence which seems to support the hypothesis on the efficacy of the approach. Another drawback of the approach is that it introduces new hyperparameters: those governing the annealing schedule for the selection bias. Since the approach seems efficient in a relatively constrained setup, it can be reasonably expected that it might be helpful in more general situations, therefore. On the other hand, since it is only evaluated on three very similar tasks, it limits the conclusiveness of the results. That*s why I would for weak accept. In the presence of more empirical (or even theoretical) evidence, I would vote for strong accept. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper explores a well motivated but very heuristic idea for selecting the next samples to train on for training deep learning models. This method relies on looking at the uncertainty of predictions of in the recent history of statements and preferring those instances that have a predictive uncertainty over the recent predictions. This allows the training method to train on instances that are neither too hard nor too easy and focus on reducing the uncertainty whenever it has the greatest potential gain to do so. There are two extra components that make this method work: - Windowing: only looking at the recent history of the instances which has two effects: firstly, the current state of the model is explored which gives a more recent assessment relative to the current state of the model. Secondly, it makes the algorithm faster by reducing the overhead of analyzing the prediction history of samples. - Annealing the selection bias: as the training goes on the selection becomes more random and less biased. This approach is evaluated in on three simple data-sets: MNIST, CIFAR-10 and CIFAR-100. Although this is a very limited subset of models, the results are consistent and statistically significant, although their effect is not really huge. The paper gives very little theoretical justification or analysis of the results but gives only the presented empirical evidence which seems to support the hypothesis on the efficacy of the approach. Another drawback of the approach is that it introduces new hyperparameters: those governing the annealing schedule for the selection bias. Since the approach seems efficient in a relatively constrained setup, it can be reasonably expected that it might be helpful in more general situations, therefore. On the other hand, since it is only evaluated on three very similar tasks, it limits the conclusiveness of the results. That*s why I would for weak accept. In the presence of more empirical (or even theoretical) evidence, I would vote for strong accept.\n",
      "    - Theoretical Contribution or Justification Unclear/Weak (L1, 6)\n",
      "      -  The work poses an interesting question: Are GCNs (and GNNs) just special types of matrix factorization methods? Unfortunately, the short answer is **no**, which goes against what the authors say. Until recently I thought like the authors, but the concurrent work [1] (On the Equivalence between Node Embeddings and Structural Graph Representations) https://openreview.net/forum?id=SJxzFySKwH changed my mind. The work of (Li et al., 2018) shows that nearby nodes tend to get similar representations. There is mounting experimental evidence of that being the case in real-world graphs (e.g., https://arxiv.org/abs/1908.08572). But [1] shows that GCNs and GNNs are fundamentally different from matrix factorization methods, regardless of the loss function used to learn the embeddings. Consider Figure 1 in [1], and it is easy to see that matrix factorization will give different embeddings to the Lynx and Orca nodes, while GCNs and GNNs must give the same embedding. Even if we connect the graphs through the Spruce and the Zooplankton nodes, their conclusion would not change. Matrix factorization (as broadly understood) will give embeddings that can even be used to cluster nodes. The eigenvectors of the symmetric Laplacian encode the diffusion of a type of random walk and nodes that are far away in the graph must have different embeddings (because through the diffusion operator, they are far away). In GCNs, the convergence of the embeddings is better explained by the mixing of a random walk (Theorem 1 of (Xu et al., 2018)), which, in the special case of a GCN, converges to 1/sqrt(degree of node), as shown by (Li et al., 2018) in their Theorem 1 for the symmetric Laplacian. This is unrelated to what we get in matrix factorization as explained earlier. What is wrong with the math: Equation (11) is equated with matrix factorization, but note that it does not account for nonedges, while matrix factorization accounts for nonedges. This issue is more clear in Equation (14). The problem happens when the paper jumps from Equation (14), which is correct but not MF, to Equation (15) which is MF but unrelated to Equation (14). The argument is that “negative edges sampling is used, for better convergence”… sorry, not for better convergence, it completely changes the optimization objective. Hence, GCNs are not matrix factorization methods. I think the paper is a valiant effort, but unfortunately the core premise is incorrect. The jump from Equation (14) to equation (15) cannot be justified, and I believe showcases a fundamental flaw the argument. I do not see a way to fix the paper. I vote to reject it. [1] On the Equivalence between Node Embeddings and Structural Graph Representations, https://openreview.net/forum?id=SJxzFySKwH Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K.I. and Jegelka, S., 2018. Representation learning on graphs with jumping knowledge networks. ICML 2018. Li, Qimai, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. AAAI, 2018. -------------- Read rebuttal. Will keep my original assessment. (L0, 1)\n",
      "        * Examples:\n",
      "          -  The work poses an interesting question: Are GCNs (and GNNs) just special types of matrix factorization methods? Unfortunately, the short answer is **no**, which goes against what the authors say. Until recently I thought like the authors, but the concurrent work [1] (On the Equivalence between Node Embeddings and Structural Graph Representations) https://openreview.net/forum?id=SJxzFySKwH changed my mind. The work of (Li et al., 2018) shows that nearby nodes tend to get similar representations. There is mounting experimental evidence of that being the case in real-world graphs (e.g., https://arxiv.org/abs/1908.08572). But [1] shows that GCNs and GNNs are fundamentally different from matrix factorization methods, regardless of the loss function used to learn the embeddings. Consider Figure 1 in [1], and it is easy to see that matrix factorization will give different embeddings to the Lynx and Orca nodes, while GCNs and GNNs must give the same embedding. Even if we connect the graphs through the Spruce and the Zooplankton nodes, their conclusion would not change. Matrix factorization (as broadly understood) will give embeddings that can even be used to cluster nodes. The eigenvectors of the symmetric Laplacian encode the diffusion of a type of random walk and nodes that are far away in the graph must have different embeddings (because through the diffusion operator, they are far away). In GCNs, the convergence of the embeddings is better explained by the mixing of a random walk (Theorem 1 of (Xu et al., 2018)), which, in the special case of a GCN, converges to 1/sqrt(degree of node), as shown by (Li et al., 2018) in their Theorem 1 for the symmetric Laplacian. This is unrelated to what we get in matrix factorization as explained earlier. What is wrong with the math: Equation (11) is equated with matrix factorization, but note that it does not account for nonedges, while matrix factorization accounts for nonedges. This issue is more clear in Equation (14). The problem happens when the paper jumps from Equation (14), which is correct but not MF, to Equation (15) which is MF but unrelated to Equation (14). The argument is that “negative edges sampling is used, for better convergence”… sorry, not for better convergence, it completely changes the optimization objective. Hence, GCNs are not matrix factorization methods. I think the paper is a valiant effort, but unfortunately the core premise is incorrect. The jump from Equation (14) to equation (15) cannot be justified, and I believe showcases a fundamental flaw the argument. I do not see a way to fix the paper. I vote to reject it. [1] On the Equivalence between Node Embeddings and Structural Graph Representations, https://openreview.net/forum?id=SJxzFySKwH Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K.I. and Jegelka, S., 2018. Representation learning on graphs with jumping knowledge networks. ICML 2018. Li, Qimai, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. AAAI, 2018. -------------- Read rebuttal. Will keep my original assessment.\n",
      "      -  This paper proposed a new graph neural network to utilize the edge features. In particular, it proposes the Edge Information maximized Graph Neural Network (EIGNN) that maximizes the Mutual Information (MI) between edge features and message passing channels. The MI is reformulated as a differentiable objective via a variational approach. The experimental results have some improvement over existing methods. Overall, the idea is novel and well presented. Pros: 1. The idea of utilizing edge features looks novel. 2. The writing is clear. 3. Extensive experiments are done to verify the performance of the proposed method. Cons: 1. The theoretical analysis is just a regular routine. 2. How does the hyper-parameter affect the performance? In other words, how does the component of the edge features affect the model performance? (L0, 1)\n",
      "        * Examples:\n",
      "          -  This paper proposed a new graph neural network to utilize the edge features. In particular, it proposes the Edge Information maximized Graph Neural Network (EIGNN) that maximizes the Mutual Information (MI) between edge features and message passing channels. The MI is reformulated as a differentiable objective via a variational approach. The experimental results have some improvement over existing methods. Overall, the idea is novel and well presented. Pros: 1. The idea of utilizing edge features looks novel. 2. The writing is clear. 3. Extensive experiments are done to verify the performance of the proposed method. Cons: 1. The theoretical analysis is just a regular routine. 2. How does the hyper-parameter affect the performance? In other words, how does the component of the edge features affect the model performance?\n",
      "      - Review:###The paper proposed an unified model for Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN). It is shown how it is possible to infer the relationship between LPA and GCN in terms of label or feature smoothing (how label/feature does propagate over the neighbors) and label or feature influence over the other nodes. The results are given in terms of two theorems (whose proofs are in an appendix) which essentially state that the total label influence of nodes with a particular label “l” on a specific node is proportional to the probability that node is labelled as “l” by LPA. In practice, LPA acts as a regularizer to learn transformation matrices and edge weights simultaneously in GCN. By means of a simple joint loss (eq.8), the regularized training show that transductive learning with the joint model surpasses GCN/GNN baselines. While the proof of the first theorem is reasonable and seems correct (Taylor+ Schwarz inequality + L-Lip), the second left me a little puzzled, in particular wrt eq 16 and 17. Is it possible to add a graphical explanation or idea of the proof? Why “y” has to be reset at each iteration? Unfortunately, the improvement brought by the framework is marginal, even if it seems general. The other limitation is that we have transductive training, but the authors are well aware about this. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper proposed an unified model for Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN). It is shown how it is possible to infer the relationship between LPA and GCN in terms of label or feature smoothing (how label/feature does propagate over the neighbors) and label or feature influence over the other nodes. The results are given in terms of two theorems (whose proofs are in an appendix) which essentially state that the total label influence of nodes with a particular label “l” on a specific node is proportional to the probability that node is labelled as “l” by LPA. In practice, LPA acts as a regularizer to learn transformation matrices and edge weights simultaneously in GCN. By means of a simple joint loss (eq.8), the regularized training show that transductive learning with the joint model surpasses GCN/GNN baselines. While the proof of the first theorem is reasonable and seems correct (Taylor+ Schwarz inequality + L-Lip), the second left me a little puzzled, in particular wrt eq 16 and 17. Is it possible to add a graphical explanation or idea of the proof? Why “y” has to be reset at each iteration? Unfortunately, the improvement brought by the framework is marginal, even if it seems general. The other limitation is that we have transductive training, but the authors are well aware about this.\n",
      "      - Review:###This paper connects graph convolutional networks with label propagation. There are a numbder of issues that need to be solved before possible publication. The theoretical part (secion 2) is hard to follow. For example, the authors introduce in 2.1 a mapping M from vertices to labels. Then in Theorem 1, this mapping M indeed maps features of the vertices to labels. But then waht is the meaning of a L2 norms between features? At this point, I had a look in the experiment section to see what features are considered in practice. In section 4.1, it is written for the citation networks: *each node has a sparse bag-of-words feature vector* or in the coauthor networks: *Node features represent paper keywords for each author*s paper*. How do you relate these claims to your theoretical analysis? Things get even worse in section 2.3, where derivatives with respect to initial feature vector are taken. How do you take a derivative with respect to a bag of words? Equation (5) is not clear at all, we need to read the end of this section to understand that A^* is constrained to have the same support as the adjacency graph and computed as a function of the node features. The authors shuold also define clearly y_hat in (5). (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper connects graph convolutional networks with label propagation. There are a numbder of issues that need to be solved before possible publication. The theoretical part (secion 2) is hard to follow. For example, the authors introduce in 2.1 a mapping M from vertices to labels. Then in Theorem 1, this mapping M indeed maps features of the vertices to labels. But then waht is the meaning of a L2 norms between features? At this point, I had a look in the experiment section to see what features are considered in practice. In section 4.1, it is written for the citation networks: *each node has a sparse bag-of-words feature vector* or in the coauthor networks: *Node features represent paper keywords for each author*s paper*. How do you relate these claims to your theoretical analysis? Things get even worse in section 2.3, where derivatives with respect to initial feature vector are taken. How do you take a derivative with respect to a bag of words? Equation (5) is not clear at all, we need to read the end of this section to understand that A^* is constrained to have the same support as the adjacency graph and computed as a function of the node features. The authors shuold also define clearly y_hat in (5).\n",
      "      - Review:###This paper derives a matrix-factorization approach for node classification. The approach is closely related to GCN. The authors show that the proposed approach outperforms GCN and Planetoid empirically. Though empirically appealing, this paper has a few pitfalls that need be addressed. 1. The wording *unifying* is a misnomer. The title *unifying graph convolutional networks* hallucinates a framework that unifies several neural network architectures, which is not precise. In reality, the authors propose a learning objective that consists of two loss terms, the classification loss and the structure loss. The classification loss is nothing but the usual GCN. The structure loss is the contribution of the paper. The derivation of this term starts from GCN and a Laplacian smoothing argument, and arrives at a matrix factorization form through a series of modeling modifications. By and large, the title is misleading. 2. The wording *correctness of our theoretical analysis* is dubious. The paper does not present a theoretical analysis. The derivation of the matrix factorization is only a modeling process. In no mathematical sense the factorization is equivalent to GCN. 3. The alternating training is questionable. The authors propose alternately optimizing the structure loss and the classification loss. Since taking the gradient of the whole loss function is straightforward in all graph neural network approaches, it is unclear why the authors prefer the alternating optimization approach. Supplementing a convergence plot and comparing the two approaches may help, if the alternating approach is indeed better. 4. The *distributed computing* component needs more substantiation. It is unclear whether this phrase actually means the concept familiar by the parallel computing community. Therein, computation is done by using several machines communicated through networked protocols. Machine setting, parallel implementation details, and speedup are the primary interests in distributed computing. All information should be reported. Questions: 1. First sentence of section 5. What does *all-round* mean? 2. Stability Analysis. What is b? The reader does not find a definition elsewhere. A probably related concept is alpha (see eqn (8)). 3. Figure 3(b) shows that larger b leads to poorer performance. The authors state that a larger b means a stronger emphasis on the structure loss. Consequently, it appears that putting more emphasis on the structure term leads to poorer performance. Then, does it mean that the structure term is a useless contribution? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper derives a matrix-factorization approach for node classification. The approach is closely related to GCN. The authors show that the proposed approach outperforms GCN and Planetoid empirically. Though empirically appealing, this paper has a few pitfalls that need be addressed. 1. The wording *unifying* is a misnomer. The title *unifying graph convolutional networks* hallucinates a framework that unifies several neural network architectures, which is not precise. In reality, the authors propose a learning objective that consists of two loss terms, the classification loss and the structure loss. The classification loss is nothing but the usual GCN. The structure loss is the contribution of the paper. The derivation of this term starts from GCN and a Laplacian smoothing argument, and arrives at a matrix factorization form through a series of modeling modifications. By and large, the title is misleading. 2. The wording *correctness of our theoretical analysis* is dubious. The paper does not present a theoretical analysis. The derivation of the matrix factorization is only a modeling process. In no mathematical sense the factorization is equivalent to GCN. 3. The alternating training is questionable. The authors propose alternately optimizing the structure loss and the classification loss. Since taking the gradient of the whole loss function is straightforward in all graph neural network approaches, it is unclear why the authors prefer the alternating optimization approach. Supplementing a convergence plot and comparing the two approaches may help, if the alternating approach is indeed better. 4. The *distributed computing* component needs more substantiation. It is unclear whether this phrase actually means the concept familiar by the parallel computing community. Therein, computation is done by using several machines communicated through networked protocols. Machine setting, parallel implementation details, and speedup are the primary interests in distributed computing. All information should be reported. Questions: 1. First sentence of section 5. What does *all-round* mean? 2. Stability Analysis. What is b? The reader does not find a definition elsewhere. A probably related concept is alpha (see eqn (8)). 3. Figure 3(b) shows that larger b leads to poorer performance. The authors state that a larger b means a stronger emphasis on the structure loss. Consequently, it appears that putting more emphasis on the structure term leads to poorer performance. Then, does it mean that the structure term is a useless contribution?\n",
      "      - Review:###This paper studies loss landscape of Non-negative matrix factorization (NMF) when the matrix is very large. It shows that with high probability, the landscape is quasi-convex under some conditions. This suggests that the optimization problem would become easier as the size of the matrix becomes very large. Implications on deep networks are also discussed. The NMF problem is known to be NP-hard. In case that the matrix X to factorize is large, the author(s) uses concentration property of random matrix to show that along any random positive matrix U,V and U’,V’, the MSE loss of NMF is convex with high probability. The extra assumption is that the rank of U and V should also be large enough. Section 3 is devoted to prove this. It seems to me there are some typos which are quite serious and make the equation (3) incorrect. However, the main result (Theorem 1) still seems to hold. The equation (3) should replace W_2 with 2 W_2. The reason is in the appendix D.2, the definition of W_2 has missed this constant 2, which is the hat X’’(lambda) at lambda=0. Therefore, all the constants in the equation (4) need to be modified accordingly. In D.2, to derive the equation (9), it seems to me the McLaurin series should give 2 l’’’’(0) l’’(0) >= (l’’’(0))^2, isn’t it? The whole proof is quite long to check. In Fact 1, is the mu the mean of z? In Lemma 1, what is lemma 9? Fact 10 has a constant 2 which seems to be forgotten. Therefore significant modification is needed to correct all the errors. Regarding experiments, some data-set in Table 1 does not seem to me relevant to the paper (Assumption 1), in particular those with r < 10. Figure 6 shows that the gradient flow is close to a straightly line, suggesting that the gradient descent algorithm follows a convex landscape. The Figure 6(b) seems to me have not converged yet, as at step 10,000, the cosine is not as flat as the others. This means that maybe the gradient flow does not converge to the local minima (U^*,V^*). Further explanation about this is needed in the paper. Regarding optimization efficiency, it is not that convincing since even in the over-parameterized regime: the landscape become more convex, but there can be a lot of local minima which are not as good as the global minima. Therefore from an optimization perspective, finding global minima still remain challenging. I think it would be better to mention this somewhere in the paper. Minor typo includes: equation (5), write ||W_2||_F^2. Equation (12), hat 1 should be 1 (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper studies loss landscape of Non-negative matrix factorization (NMF) when the matrix is very large. It shows that with high probability, the landscape is quasi-convex under some conditions. This suggests that the optimization problem would become easier as the size of the matrix becomes very large. Implications on deep networks are also discussed. The NMF problem is known to be NP-hard. In case that the matrix X to factorize is large, the author(s) uses concentration property of random matrix to show that along any random positive matrix U,V and U’,V’, the MSE loss of NMF is convex with high probability. The extra assumption is that the rank of U and V should also be large enough. Section 3 is devoted to prove this. It seems to me there are some typos which are quite serious and make the equation (3) incorrect. However, the main result (Theorem 1) still seems to hold. The equation (3) should replace W_2 with 2 W_2. The reason is in the appendix D.2, the definition of W_2 has missed this constant 2, which is the hat X’’(lambda) at lambda=0. Therefore, all the constants in the equation (4) need to be modified accordingly. In D.2, to derive the equation (9), it seems to me the McLaurin series should give 2 l’’’’(0) l’’(0) >= (l’’’(0))^2, isn’t it? The whole proof is quite long to check. In Fact 1, is the mu the mean of z? In Lemma 1, what is lemma 9? Fact 10 has a constant 2 which seems to be forgotten. Therefore significant modification is needed to correct all the errors. Regarding experiments, some data-set in Table 1 does not seem to me relevant to the paper (Assumption 1), in particular those with r < 10. Figure 6 shows that the gradient flow is close to a straightly line, suggesting that the gradient descent algorithm follows a convex landscape. The Figure 6(b) seems to me have not converged yet, as at step 10,000, the cosine is not as flat as the others. This means that maybe the gradient flow does not converge to the local minima (U^*,V^*). Further explanation about this is needed in the paper. Regarding optimization efficiency, it is not that convincing since even in the over-parameterized regime: the landscape become more convex, but there can be a lot of local minima which are not as good as the global minima. Therefore from an optimization perspective, finding global minima still remain challenging. I think it would be better to mention this somewhere in the paper. Minor typo includes: equation (5), write ||W_2||_F^2. Equation (12), hat 1 should be 1\n",
      "    - Theoretical Issues and Insufficient Justification/Connection (L1, 7)\n",
      "      -  ** post rebuttal start ** After reading reviews and authors* response, I decided not to change my score. However, I feel that this paper is somewhat under-evaluated initially, so I hope the authors have an opportunity in another venue with their revision. Detailed comments: 1.1. I recommend to add an algorithm box describing the learning scheme. It is not end-to-end learning, so it is hard to catch (and potentially, replicate) the learning part. I am also a bit skeptical about the convergence (with non-zero sigma), as Reviewer 2 has a concern about it. 1.3. *We hypothesized that the value of the uncertainty is different depending on whether the inputs are OOD or in-distribution inputs. The results of the ablation study listed in Table1 demonstrate that this hypothesis is true.* 2. *In order to use the data uncertainty, we used the value of sigma.* -> Table 1 proves that your proposal (playing with sigma) is effective, but it does not mean that sigma is the uncertainty which is only essential component for detecting OOD. I recommend the authors to validate their hypothesis, maybe by conducting more experiments to show that the role of mu and sigma is as expected. At least, if mu is proven to have no effect on OOD detection by some experiment, then it can be a clue. Minor comment: I hope ICLR papers are cited as ICLR papers at least in ICLR submissions, not arXiv preprint.. Alemi*s paper is ICLR*17 paper, for example. ** post rebuttal end ** - Summary: This paper proposes to train an OOD detection model from a portion of modified latent vectors; more specifically, similar to VAE, they assume unimodal Gaussian distributed latent space at each layer and use the collection of standard deviation to train an OOD detector. Experimental results on several OOD benchmarks with different backbone networks show that their method outperforms ODIN (Liang et al., 2017). - Decision and supporting arguments: Weak reject. 1. Though the idea of extracting uncertainty is interesting, but I think the motivation and explanation is not enough, so I couldn*t find a rationale why we should do this. I have several questions that I couldn*t find an answer in the submission, could you answer them? 1.1. Are the classification loss and OOD detection loss optimized jointly? 1.2. Is it reasonable to assume unimodal Gaussian distribution over all latent spaces without a carefully designed learning objective? More specifically, to make it learnable, don*t you need a learning objective other than the conventional cross-entropy loss, e.g., *Bayes by backprop* proposed in the early work (Blundell, 2015)? 1.3. Why only the standard deviation values are useful for the OOD detection performance? If they are really useful, how the standard deviation values are related to the OOD detection performance? Blundell et al. Weight Uncertainty in Neural Networks. In ICML, 2015. 2. More ablation study is required to verify the effectiveness of their method. Again, I am not sure why mu and sigma should be split, and why mu should be discarded for the OOD detection part. 3. The architecture design of CNN in Figure 7 also looks arbitrary. 4. Comparison with more state-of-the-art methods is required. ODIN (Liang et al., 2017) is a powerful method but it is somewhat old and many recent works actually combine their method with ODIN for better performance. Why don*t you compare the proposed method with the Mahalanobis distance-based classifier (Lee et al., 2018)? They also estimate the uncertainty by measuring the Mahalanobis distance on the feature spaces & combine them for better OOD detection. - Comments: 1. I couldn*t find any statement about the classification accuracy, does the proposed model have a good classification performance as well? Since {a half of the model capacity is spent to split mu and sigma} and {it should take account of uncertainty in the forward pass}, I am not sure it maintains a good classification performance, compared to the standard classification model with the same capacity. (L0, 1)\n",
      "        * Examples:\n",
      "          -  ** post rebuttal start ** After reading reviews and authors* response, I decided not to change my score. However, I feel that this paper is somewhat under-evaluated initially, so I hope the authors have an opportunity in another venue with their revision. Detailed comments: 1.1. I recommend to add an algorithm box describing the learning scheme. It is not end-to-end learning, so it is hard to catch (and potentially, replicate) the learning part. I am also a bit skeptical about the convergence (with non-zero sigma), as Reviewer 2 has a concern about it. 1.3. *We hypothesized that the value of the uncertainty is different depending on whether the inputs are OOD or in-distribution inputs. The results of the ablation study listed in Table1 demonstrate that this hypothesis is true.* 2. *In order to use the data uncertainty, we used the value of sigma.* -> Table 1 proves that your proposal (playing with sigma) is effective, but it does not mean that sigma is the uncertainty which is only essential component for detecting OOD. I recommend the authors to validate their hypothesis, maybe by conducting more experiments to show that the role of mu and sigma is as expected. At least, if mu is proven to have no effect on OOD detection by some experiment, then it can be a clue. Minor comment: I hope ICLR papers are cited as ICLR papers at least in ICLR submissions, not arXiv preprint.. Alemi*s paper is ICLR*17 paper, for example. ** post rebuttal end ** - Summary: This paper proposes to train an OOD detection model from a portion of modified latent vectors; more specifically, similar to VAE, they assume unimodal Gaussian distributed latent space at each layer and use the collection of standard deviation to train an OOD detector. Experimental results on several OOD benchmarks with different backbone networks show that their method outperforms ODIN (Liang et al., 2017). - Decision and supporting arguments: Weak reject. 1. Though the idea of extracting uncertainty is interesting, but I think the motivation and explanation is not enough, so I couldn*t find a rationale why we should do this. I have several questions that I couldn*t find an answer in the submission, could you answer them? 1.1. Are the classification loss and OOD detection loss optimized jointly? 1.2. Is it reasonable to assume unimodal Gaussian distribution over all latent spaces without a carefully designed learning objective? More specifically, to make it learnable, don*t you need a learning objective other than the conventional cross-entropy loss, e.g., *Bayes by backprop* proposed in the early work (Blundell, 2015)? 1.3. Why only the standard deviation values are useful for the OOD detection performance? If they are really useful, how the standard deviation values are related to the OOD detection performance? Blundell et al. Weight Uncertainty in Neural Networks. In ICML, 2015. 2. More ablation study is required to verify the effectiveness of their method. Again, I am not sure why mu and sigma should be split, and why mu should be discarded for the OOD detection part. 3. The architecture design of CNN in Figure 7 also looks arbitrary. 4. Comparison with more state-of-the-art methods is required. ODIN (Liang et al., 2017) is a powerful method but it is somewhat old and many recent works actually combine their method with ODIN for better performance. Why don*t you compare the proposed method with the Mahalanobis distance-based classifier (Lee et al., 2018)? They also estimate the uncertainty by measuring the Mahalanobis distance on the feature spaces & combine them for better OOD detection. - Comments: 1. I couldn*t find any statement about the classification accuracy, does the proposed model have a good classification performance as well? Since {a half of the model capacity is spent to split mu and sigma} and {it should take account of uncertainty in the forward pass}, I am not sure it maintains a good classification performance, compared to the standard classification model with the same capacity.\n",
      "      - Review:###***Score updated to weak accept after the rebuttal.*** Straight-Through is a popular, yet not theoretically well-understood, biased gradient estimator for Bernoulli random variables. The low variance of this estimator makes it a highly useful tool for training large-scale models with binary latents. However, the bias of this estimator may cause divergence in training, which is a significant practical issue. The paper develops a Fourier analysis of the Straight-Through estimator and provides an expression for the bias of the estimator in terms of the Fourier coefficients of the considered function. Motivated by this expression, the paper proposes two modifications of Straight-Through which may reduce the bias of the estimator, at the cost of the variance. The experimental results show advantage of this improved estimator over Gumbel-Softmax and DARN estimator. While I really like the premise of the paper, I feel that it needs a significant amount of additional work. The text is currently fairly hard to read. The theoretical part of the paper does not quantify the variance of the estimator. The experiments are a bit unfinished and do not include ablations of the proposed modifications of Straight-Through. Most importantly, I think that in the current form the theoretical and the empirical parts of the papers are not well-connected. Because of this, I believe that the paper should currently be rejected, but I encourage the authors to continue this line of work. Pros: 1. Theoretical analysis and empirical improvement of the Straight-Through estimator is an important avenue of work. 2. The paper makes a solid contribution of deriving the Fourier expansion of the Straight-Through estimator bias. 3. Based on this expansion, the paper proposes an algorithm with reduced bias. The algorithm is simple to implement, practical and appears to work slightly better than DARN. Cons: 1. The key weakness of the theoretical part of the paper is that it focuses on the bias of the estimator, but does not quantify the variance, especially after the modifications. If reducing the bias was the only goal, one could use unbiased (but high-variance) estimators such as REINFORCE or VIMCO. 2. The final algorithm appears to be the DARN estimator combined with relaxation by uniform noise (“Bernoulli splitting uniform”) and scaling. The paper does not have an ablation showing how the uniform noise and scaling perform on their own. 3. There are a few incorrect statements that I’ve noticed. * “As a side contribution, we show that the gradient estimator employed with DARN (Gregor et al., 2013), originally proposed for autoregressive models, is a strong baseline for gradient estimation.” - MuProp paper compared to this estimator under the name 1/2-estimator * In Lemma 1 the “REINFORCE gradient” is just the exact gradient of the expectation, not a stochastic REINFORCE gradient. * “ To the best of our knowledge, FouST is the first gradient estimate algorithm that can train very deep stochastic neural networks with Boolean latent variables.” This paper uses up to 11 latent variable layers, while [1] has trained models with >20 latent variable layers (although their “layers” have just one unit). 4. The derivation of “Bernoulli splitting uniform” trick is confusing and contains a lot of typos. For instance, the text before eqn. (14) implies that the distribution of u_i is U[-1, 1], which cannot be right and does not correspond to Algorithm 1. The statement that this trick does not lead to a relaxation is odd, since the function is being evaluated at non-discrete points. 5. There are generally many typos and some poor formatting in the math. For example, in eqn. (6) the coefficients are off by one: it should be c0 + c1 z1 + c2 z2^2 + … . The equations (10) and (11) are poorly formatted. The notation partial_z1 f(u_1, u_2) in eqn. (14) is strange. In many places p^{i->½} is denoted as p^{1->½}. 5. I don’t think I understood the idea of representation scaling (Section 4.4). The eqn. (16) would suggest that the scaling should optimally be set to zero, which is just saying that the gradient is unbiased when the model does not use the latents. There is no other practical guidance on choosing this coefficient. Furthermore, one can always absorb the global scaling factor into the succeeding weights layer of the model, so this trick can probably be replaced by a modification of the weights initialization. 6. The experiments are missing a comparison to the Straight-Through Gumbel-Softmax estimator, introduced in the original Gumbel-Softmax paper. This is a popular biased estimator for Bernoulli latents, e.g. used in [1] [2]. Another interesting comparison would be [3] which proposes a lower-bias version of Gumbel-Softmax. 7. Figure 2 is missing the line for REBAR, even though this line is referred to on Page 8. Figure 2 and Figure 4 are both labeled as training ELBOs, despite the plots being different. [1] Andreas Veit, Serge Belongie “Convolutional Networks with Adaptive Inference Graphs” ECCV 2018 [2] Patrick Chen, Si Si, Sanjiv Kumar, Yang Li, Cho-Jui Hsieh “Learning to Screen for Fast Softmax Inference on Large Vocabulary Neural Networks” ICLR 2019 https://openreview.net/forum?id=ByeMB3Act7 [3] Evgeny Andriyash, Arash Vahdat, Bill Macready “Improved Gradient-Based Optimization Over Discrete Distributions” https://arxiv.org/abs/1810.00116 (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###***Score updated to weak accept after the rebuttal.*** Straight-Through is a popular, yet not theoretically well-understood, biased gradient estimator for Bernoulli random variables. The low variance of this estimator makes it a highly useful tool for training large-scale models with binary latents. However, the bias of this estimator may cause divergence in training, which is a significant practical issue. The paper develops a Fourier analysis of the Straight-Through estimator and provides an expression for the bias of the estimator in terms of the Fourier coefficients of the considered function. Motivated by this expression, the paper proposes two modifications of Straight-Through which may reduce the bias of the estimator, at the cost of the variance. The experimental results show advantage of this improved estimator over Gumbel-Softmax and DARN estimator. While I really like the premise of the paper, I feel that it needs a significant amount of additional work. The text is currently fairly hard to read. The theoretical part of the paper does not quantify the variance of the estimator. The experiments are a bit unfinished and do not include ablations of the proposed modifications of Straight-Through. Most importantly, I think that in the current form the theoretical and the empirical parts of the papers are not well-connected. Because of this, I believe that the paper should currently be rejected, but I encourage the authors to continue this line of work. Pros: 1. Theoretical analysis and empirical improvement of the Straight-Through estimator is an important avenue of work. 2. The paper makes a solid contribution of deriving the Fourier expansion of the Straight-Through estimator bias. 3. Based on this expansion, the paper proposes an algorithm with reduced bias. The algorithm is simple to implement, practical and appears to work slightly better than DARN. Cons: 1. The key weakness of the theoretical part of the paper is that it focuses on the bias of the estimator, but does not quantify the variance, especially after the modifications. If reducing the bias was the only goal, one could use unbiased (but high-variance) estimators such as REINFORCE or VIMCO. 2. The final algorithm appears to be the DARN estimator combined with relaxation by uniform noise (“Bernoulli splitting uniform”) and scaling. The paper does not have an ablation showing how the uniform noise and scaling perform on their own. 3. There are a few incorrect statements that I’ve noticed. * “As a side contribution, we show that the gradient estimator employed with DARN (Gregor et al., 2013), originally proposed for autoregressive models, is a strong baseline for gradient estimation.” - MuProp paper compared to this estimator under the name 1/2-estimator * In Lemma 1 the “REINFORCE gradient” is just the exact gradient of the expectation, not a stochastic REINFORCE gradient. * “ To the best of our knowledge, FouST is the first gradient estimate algorithm that can train very deep stochastic neural networks with Boolean latent variables.” This paper uses up to 11 latent variable layers, while [1] has trained models with >20 latent variable layers (although their “layers” have just one unit). 4. The derivation of “Bernoulli splitting uniform” trick is confusing and contains a lot of typos. For instance, the text before eqn. (14) implies that the distribution of u_i is U[-1, 1], which cannot be right and does not correspond to Algorithm 1. The statement that this trick does not lead to a relaxation is odd, since the function is being evaluated at non-discrete points. 5. There are generally many typos and some poor formatting in the math. For example, in eqn. (6) the coefficients are off by one: it should be c0 + c1 z1 + c2 z2^2 + … . The equations (10) and (11) are poorly formatted. The notation partial_z1 f(u_1, u_2) in eqn. (14) is strange. In many places p^{i->½} is denoted as p^{1->½}. 5. I don’t think I understood the idea of representation scaling (Section 4.4). The eqn. (16) would suggest that the scaling should optimally be set to zero, which is just saying that the gradient is unbiased when the model does not use the latents. There is no other practical guidance on choosing this coefficient. Furthermore, one can always absorb the global scaling factor into the succeeding weights layer of the model, so this trick can probably be replaced by a modification of the weights initialization. 6. The experiments are missing a comparison to the Straight-Through Gumbel-Softmax estimator, introduced in the original Gumbel-Softmax paper. This is a popular biased estimator for Bernoulli latents, e.g. used in [1] [2]. Another interesting comparison would be [3] which proposes a lower-bias version of Gumbel-Softmax. 7. Figure 2 is missing the line for REBAR, even though this line is referred to on Page 8. Figure 2 and Figure 4 are both labeled as training ELBOs, despite the plots being different. [1] Andreas Veit, Serge Belongie “Convolutional Networks with Adaptive Inference Graphs” ECCV 2018 [2] Patrick Chen, Si Si, Sanjiv Kumar, Yang Li, Cho-Jui Hsieh “Learning to Screen for Fast Softmax Inference on Large Vocabulary Neural Networks” ICLR 2019 https://openreview.net/forum?id=ByeMB3Act7 [3] Evgeny Andriyash, Arash Vahdat, Bill Macready “Improved Gradient-Based Optimization Over Discrete Distributions” https://arxiv.org/abs/1810.00116\n",
      "      - Review:###--- response after the author*s rebuttal --- Thank the authors to provide their response and let me clearly understand their contribution. However, after considering those, I will not change my rating. The paper identifies the inaccurate gradient computation in the original DARTS and propose a new estimation and achieves constant improvement. While interesting, the experiments show this approach is just as good as some simple human tricks like early stopping. In their response, they also didn*t intend to generalize their approach to other baselines, and it will significantly limit the impact of this work. Authors insist that involving human prior is not the primary goal of AutoML. This is a subjective opinion, and my subjective opinion is it is wrong. If you take a closer look at all search spaces currently used in the NAS field, human priors are why NAS works up to now. For example, to use a convolutional operation, people use Conv-Bn-ReLU instead let NAS also search the activation functions. To search a cell rather than entire network is also designed by a human. If applied correctly, it is not wrong to include human priors. As stated in guideline, we should apply a higher standard to a 10-page paper, and unfortunately, in my opinion, this paper does not meet such standard with the current version, and heavily revision will be necessary. I encourage the authors to shorten their paper by putting some of the derivations to the appendix and to better show their contribution. If not planning to generalize their method, show more ablation and toy example of gradient trap and how the approach solves that. --- original review--- Summary The paper presents a novel way to refactor second-order gradient term in Differentiable Architecture Search (DARTS) into the inverse of Hessian matrix (H^{-1}) corresponding to the optimal weight w^*, by leveraging a mathematical based on a property that gradient of loss w.r.t. w^* is 0. It provides an estimate relying on H instead of H^{-1} and showing mathematically that the proposed estimate has an angle (w.r.t ground-truth gradient vector) < 90 degrees, while the original estimate in DARTS is not bounded. The paper claims phenomenon (namely, gradient trap in DARTS) is the reason why DARTS and its successors constantly converge into a poor architecture while the proposed amendment will not. It shows the experiments in both original DARTS search space with changes in the search phase, as well as a larger space with fewer operations. The observation that DARTS converging to a fixed point is interesting and the motivation to find the reason and solution is well justified. The technical novelty to compute the actual and estimate term for second-order DARTS is okay, and the identified *gradient trap* seems reasonable. However, I have some doubts regarding both theoretical and empirical aspects of this paper. The evidences in the current version is not sufficient to support all the claims. Especially considering the 10-page length, I hope the author could clarify these concerns during the rebuttal period. Major concerns About the derivation - Based on the derivation in Section 3.3, the proposed gradient amendment relies on Hessian Matrix regarding the optimum w^*(alpha), while in reality, it is rarely satisfied. Will this brings another gradient trap? Could you comment on the theoretical bound between the proposed amendment and the original DARTS estimation? In addition, as stated in DARTS paper (Liu et al.) section 2.3, if w is already a local optimum, the \n",
      "abla L_train (w, alpha) = 0, where this will also make g2 or g2* to 0. However, to obtain a good estimate of the Hessian matrix, one will need to have a good estimate of w^*. Does this contradict the proposed approach? - Computation of the Hessian matrix is missing, and the motivation to use g2*. The proposed estimate depends on the Hessian matrix, however, it is not mentioned in the paper how to compute such matrix, especially *... is computationally intractable due to the large dimensionality of H, ... usually exceeds one million ...* as in Section 3.3. Also, authors proposed to use g2* on top of the Hessian matrix instead its inverse, could author add experiments to show the difference between g2 and g2*? I came across to a concurrent paper submission [1], the derivation of second-order gradient estimate is the same as yours, i.e., one could use negative Hessian to improve the DARTS (see Appendix A.2 in [1]). However, the numerical computation of this Hessian is hard is also mentioned in [1]. Could the author also comment on that? - Why the paper introduces \n",
      "abla_{alpha} w^*(alpha) with regarding L_train while the equation 2,3 is about validation loss? In DARTS, both training and validation dataset is used in training for better generalization. About experiments - Could the author provide an additional experiment, to show in reality, even for a toy example, this gradient trap make DARTS converge to an architecture full of skip-connection, i.e., showing the gradient estimate (i.e., g1+g2) w.r.t. alpha, obtained by original DARTS, proposed approach, if possible, ground-truth estimate, after a certain epoch number (e.g. 50 or any number that DARTS start turning to skip-connection)? I think this will empirically reveals if the gradient trap is causing the DARTS problem and better demonstrate the paper*s method effectiveness. - Additional experiments for a fair comparison with baseline DARTS. Experiment settings for both S1 and S2 seem to make the comparison to DARTS unfair. As shown in some previous work [2], different search space has different characteristic and is usually non-trivial in NAS domain. In addition, [3,4] shows that, weight sharing NAS is usually sensitive to random initialization, and results across runs can be quite different. To have a fair comparison, the paper should include additional experiments, running original DARTS and proposed one on the search space, S1 and S2, **with the same hyper-parameter setting**, for 3 runs with different random initialization. If the new results are still statistically significant, it will be strong evidence that the proposed algorithm indeed improves original DARTS. Minor comments. - Would the author kindly clarify the following points I found confusing during the reading? 1. Introduction third last paragraph: *Our final solution involves using the amended second term of [eq] brings alpha meanwhile keeping the first term unchanged*. What*s does *brings alpha?* means? 2. Section 3.3 line 7, is *fundamentals in mathematics* referring math foundations? 3. Equation 2, 3 are repetitive for no good reason? 4. In table 1,2 Random search results for S2 are missing. Could the author also listed for better comparison? 5. The second line after equation (3), *estimating \n",
      "abla ... ^2* why is this a square while in equation (3) it*s not? - generalization comparison with DARTS+, XNAS, etc. Since the proposed gradient amendment is a general approach, adding this on top of other approaches seems natural. Could the author extend their method to other modified DARTS algorithms. It should obtain even better results with extra human-expertise after fixing the gradient trap. Adding this will further strengthen the paper. - The writing is somehow informal and could be polished, e.g. *This is to say* in paragraph 1 of page 2, misuse of bold style, and some typos: 1. Introduction page 2, *In all experiments*, *I* should not be in bold. 2. line after equation 5, *throughout the remainder of this paper* remainder -> remaining. --- Reference --- [1] Anonymous, UNDERSTANDING AND ROBUSTIFYING DIFFERENTIABLE ARCHITECTURE SEARCH, link: https://openreview.net/forum?id=H1gDNyrKDS. [2] Radosavovic et al., On Network Design Spaces for Visual Recognition, ICCV*19. [3] Li and Talwalker, Random Search and Reproducibility for Neural Architecture Search, UAI*19. [4] Scuito et al., Evaluating the search phase of neural architecture search, arxiv*19. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###--- response after the author*s rebuttal --- Thank the authors to provide their response and let me clearly understand their contribution. However, after considering those, I will not change my rating. The paper identifies the inaccurate gradient computation in the original DARTS and propose a new estimation and achieves constant improvement. While interesting, the experiments show this approach is just as good as some simple human tricks like early stopping. In their response, they also didn*t intend to generalize their approach to other baselines, and it will significantly limit the impact of this work. Authors insist that involving human prior is not the primary goal of AutoML. This is a subjective opinion, and my subjective opinion is it is wrong. If you take a closer look at all search spaces currently used in the NAS field, human priors are why NAS works up to now. For example, to use a convolutional operation, people use Conv-Bn-ReLU instead let NAS also search the activation functions. To search a cell rather than entire network is also designed by a human. If applied correctly, it is not wrong to include human priors. As stated in guideline, we should apply a higher standard to a 10-page paper, and unfortunately, in my opinion, this paper does not meet such standard with the current version, and heavily revision will be necessary. I encourage the authors to shorten their paper by putting some of the derivations to the appendix and to better show their contribution. If not planning to generalize their method, show more ablation and toy example of gradient trap and how the approach solves that. --- original review--- Summary The paper presents a novel way to refactor second-order gradient term in Differentiable Architecture Search (DARTS) into the inverse of Hessian matrix (H^{-1}) corresponding to the optimal weight w^*, by leveraging a mathematical based on a property that gradient of loss w.r.t. w^* is 0. It provides an estimate relying on H instead of H^{-1} and showing mathematically that the proposed estimate has an angle (w.r.t ground-truth gradient vector) < 90 degrees, while the original estimate in DARTS is not bounded. The paper claims phenomenon (namely, gradient trap in DARTS) is the reason why DARTS and its successors constantly converge into a poor architecture while the proposed amendment will not. It shows the experiments in both original DARTS search space with changes in the search phase, as well as a larger space with fewer operations. The observation that DARTS converging to a fixed point is interesting and the motivation to find the reason and solution is well justified. The technical novelty to compute the actual and estimate term for second-order DARTS is okay, and the identified *gradient trap* seems reasonable. However, I have some doubts regarding both theoretical and empirical aspects of this paper. The evidences in the current version is not sufficient to support all the claims. Especially considering the 10-page length, I hope the author could clarify these concerns during the rebuttal period. Major concerns About the derivation - Based on the derivation in Section 3.3, the proposed gradient amendment relies on Hessian Matrix regarding the optimum w^*(alpha), while in reality, it is rarely satisfied. Will this brings another gradient trap? Could you comment on the theoretical bound between the proposed amendment and the original DARTS estimation? In addition, as stated in DARTS paper (Liu et al.) section 2.3, if w is already a local optimum, the \n",
      "abla L_train (w, alpha) = 0, where this will also make g2 or g2* to 0. However, to obtain a good estimate of the Hessian matrix, one will need to have a good estimate of w^*. Does this contradict the proposed approach? - Computation of the Hessian matrix is missing, and the motivation to use g2*. The proposed estimate depends on the Hessian matrix, however, it is not mentioned in the paper how to compute such matrix, especially *... is computationally intractable due to the large dimensionality of H, ... usually exceeds one million ...* as in Section 3.3. Also, authors proposed to use g2* on top of the Hessian matrix instead its inverse, could author add experiments to show the difference between g2 and g2*? I came across to a concurrent paper submission [1], the derivation of second-order gradient estimate is the same as yours, i.e., one could use negative Hessian to improve the DARTS (see Appendix A.2 in [1]). However, the numerical computation of this Hessian is hard is also mentioned in [1]. Could the author also comment on that? - Why the paper introduces \n",
      "abla_{alpha} w^*(alpha) with regarding L_train while the equation 2,3 is about validation loss? In DARTS, both training and validation dataset is used in training for better generalization. About experiments - Could the author provide an additional experiment, to show in reality, even for a toy example, this gradient trap make DARTS converge to an architecture full of skip-connection, i.e., showing the gradient estimate (i.e., g1+g2) w.r.t. alpha, obtained by original DARTS, proposed approach, if possible, ground-truth estimate, after a certain epoch number (e.g. 50 or any number that DARTS start turning to skip-connection)? I think this will empirically reveals if the gradient trap is causing the DARTS problem and better demonstrate the paper*s method effectiveness. - Additional experiments for a fair comparison with baseline DARTS. Experiment settings for both S1 and S2 seem to make the comparison to DARTS unfair. As shown in some previous work [2], different search space has different characteristic and is usually non-trivial in NAS domain. In addition, [3,4] shows that, weight sharing NAS is usually sensitive to random initialization, and results across runs can be quite different. To have a fair comparison, the paper should include additional experiments, running original DARTS and proposed one on the search space, S1 and S2, **with the same hyper-parameter setting**, for 3 runs with different random initialization. If the new results are still statistically significant, it will be strong evidence that the proposed algorithm indeed improves original DARTS. Minor comments. - Would the author kindly clarify the following points I found confusing during the reading? 1. Introduction third last paragraph: *Our final solution involves using the amended second term of [eq] brings alpha meanwhile keeping the first term unchanged*. What*s does *brings alpha?* means? 2. Section 3.3 line 7, is *fundamentals in mathematics* referring math foundations? 3. Equation 2, 3 are repetitive for no good reason? 4. In table 1,2 Random search results for S2 are missing. Could the author also listed for better comparison? 5. The second line after equation (3), *estimating \n",
      "abla ... ^2* why is this a square while in equation (3) it*s not? - generalization comparison with DARTS+, XNAS, etc. Since the proposed gradient amendment is a general approach, adding this on top of other approaches seems natural. Could the author extend their method to other modified DARTS algorithms. It should obtain even better results with extra human-expertise after fixing the gradient trap. Adding this will further strengthen the paper. - The writing is somehow informal and could be polished, e.g. *This is to say* in paragraph 1 of page 2, misuse of bold style, and some typos: 1. Introduction page 2, *In all experiments*, *I* should not be in bold. 2. line after equation 5, *throughout the remainder of this paper* remainder -> remaining. --- Reference --- [1] Anonymous, UNDERSTANDING AND ROBUSTIFYING DIFFERENTIABLE ARCHITECTURE SEARCH, link: https://openreview.net/forum?id=H1gDNyrKDS. [2] Radosavovic et al., On Network Design Spaces for Visual Recognition, ICCV*19. [3] Li and Talwalker, Random Search and Reproducibility for Neural Architecture Search, UAI*19. [4] Scuito et al., Evaluating the search phase of neural architecture search, arxiv*19.\n",
      "      - Review:###The paper considers the problem of out-of-distribution (OOD) sample detection while solving a classification task. The authors tackle the problem of OOD detection with exploiting uncertainty while passing a test sample through the neural network. They treat outputs of (some) layers in a NN as random Gaussian-distributed variables and measure uncertainty as variance of these Gaussians. Then when uncertainty is high, OOD is detected. The overall idea behind the paper could be interesting, but its realisation in the current form is questionable. The paper seems totally misusing the reparametrisation trick and stochastic outputs of layers in NNs. Eq. (2) is not the objective of variational inference that seems to be required for stochastic outputs and the reparametrisation trick as presented before the equation. The objective misses the KL-divergence term! Without it what would stop a neural net to set sigmas to 0 and forget about the stochasticity altogether? Not to mention that the current objective is not mathematically justified. If there is no mix and error in eq. (2) and the networks were trained using this loss (and based on provided code they were using this loss), my wild guess of explaining why this may give best results in the experiments is that the models were trained for surprisingly small number of epochs. Therefore, a hypothesis would be that this small number of epochs did not allow the networks to switch sigmas to 0. Until the authors can clarify and justify the objective, I will vote for rejection only based on this ground. However, there are other issues in the paper as well. First of all, its clarity. It seems that the paper requires a lot of polishing. The first paragraph of this review is based on my assumptions from the paper since I am not completely sure I understand it correctly. More about the clarity issues below For strong evaluation, comparison with Malinin & Gales (2018) work seems to be important since it was the only work also using uncertainties for OOD detection in related work. Also related work section does not look like an exhaustive overview. Some of the detailed comments: 1. “In other words, in-distribution samples possess more features that convolutional filters react to than OOD samples” – first of all, this sentence is not easy to parse. Secondly, it is unclear, why this should be true. If OOD samples are still natural images, they would contain edges just the same as in-distribution samples. That is the power of deep learning enabling transfer learning, that low-level features are the transferable across different data and tasks. Therefore, the claim that “Therefore, the uncertainties of the features will be larger when the inputs are in-distribution samples” requires more elaboration and arguments 2. The arguments of the next paragraph regarding uncertainty of deeper layers should be larger for OOD samples are not very convincing either. It is either requires a definition what the authors mean here as uncertainty, or it is not necessarily true that absence of fixed regions for embeddings leads to higher uncertainty. 3. 3rd and 4th paragraphs in Introduction have too many repetitions of phrases between each other. Compare, e.g. the first sentences of the paragraphs or the last sentences. 4. “One cause of the abovementioned problem is that their approaches” and similarly the next paragraph: “their approaches” stylistically sound wrong. It is appropriate in the previous paragraph since there is a link to “previous studies”. It seems that “these approaches” or “the existing approaches” would be a better choice for this and the next paragraph. 5. “Each uncertainty is easily estimated after training the discriminative model by computing the mean and the variance of their features using a reparameterization trick” – conventional discriminative models do not estimate mean and variances of the features. The issue of estimating uncertainty is addressed by several special methods such as Bayesian variational methods used in the referred papers. Therefore, in order to use a reparametrisation trick one need to firstly choose a special class of models, which is not obvious from the text. 6. “Moreover, UFEL is robust to hyperparameters such as the number of in-distribution classes and the validation dataset.” – the size of the validation dataset? In any case neither size of the validation dataset nor the validation dataset itself are not hyperparameters (should not be hyperparameters for out-of-distribution detection). The number of classes can hardly be called a hyperparameter also. 7. “depends on the difference in the Dirichlet distribution of the categorical parameter <…> In our work, the distribution of the logit of the categorical parameters” – what is/are this/these categorical parameter(s)? 8. “Further, they estimate the parameter of the Dirichlet distribution using a DNN and train the model with in-distribution and OOD datasets” – this sentence may mislead to impression that the proposed method does not need OOD dataset for training, which does not seem to be the case, since lambda and \theta are trained based on OOD samples 9. “because they will not be relevant to the classification accuracy” – who are they? 10. “and epsilon is the Gaussian noise” –> the standard Gaussian noise 11. “where z^0 = x” – it seems this should be placed somewhere earlier when z^l is introduced since z^0 is not used in eq.(2) after which this text is placed 12. It is unclear how lambda^l and CNN \theta are learnt 13. It is unclear how the values of features d(x) are used to detect OOD samples 14. “comparison methods, and models” – not clear what models mean here 15. Missing references to datasets in the main text. At least reference to Appendix A.2 is required 16. “We used 5,000 validation images split from each training dataset and chose the parameter that can obtain” – which parameter? 17. “All the hyperparameters of ODIN” – a reader does not know yet that ODIN is used for comparison 18. “which consists of 100 OOD images from the test dataset and 1,000 images from the in-distribution validation set” – it is a bit confusing to call OOD dataset as a test dataset in this context 19. “We tuned the parameters of the CNN in Equation 4 using 50 validation training images taken from the 100 validation images. The best parameters were chosen by validating the performance using the rest of 50 validation images.” – this is confusing. What parameters do the authors talk about in the second sentence if not the parameters of the CNN? 20. “We used TNR at 95% TPR, AUROC, AUPR, and accuracy (ACC),” – Some elaboration is required, at least the reference to Appendix A.1. What is the changing threshold for AUROC and AUPR? Why AUPR-In and AUPR-Out are considered and only a single AUROC is considered. What is the positive class for AUROC? 23. “For LeNet5, we increased the number of channels of the original LeNet5 to improve accuracy” – do the authors mean that they allowed RGB images as input rather than greyscale? If yes, this explicit explanation would be preferable 24. “We inserted the reparameterization trick” – not the best word choice. Reparametrisation trick is a computational/implementation trick/method and it is hard to say that it can be inserted into a network. I believe what the authors mean is that they inserted mean/std outputs instead of point outputs. Conceptually, this means that the output of the corresponding layers is considered to be stochastic rather than deterministic. Also, it is unclear when the authors say they insert it to the softmax layer. According to Section 3 the softmax layer is never considered to output means and stds. 25. The numbers of epochs for training NNs are very small for LeNet and WideResNet in the experiments. Did the models manage to converge during this short training? Minor: 1. “These data were also used” -> “this data” (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper considers the problem of out-of-distribution (OOD) sample detection while solving a classification task. The authors tackle the problem of OOD detection with exploiting uncertainty while passing a test sample through the neural network. They treat outputs of (some) layers in a NN as random Gaussian-distributed variables and measure uncertainty as variance of these Gaussians. Then when uncertainty is high, OOD is detected. The overall idea behind the paper could be interesting, but its realisation in the current form is questionable. The paper seems totally misusing the reparametrisation trick and stochastic outputs of layers in NNs. Eq. (2) is not the objective of variational inference that seems to be required for stochastic outputs and the reparametrisation trick as presented before the equation. The objective misses the KL-divergence term! Without it what would stop a neural net to set sigmas to 0 and forget about the stochasticity altogether? Not to mention that the current objective is not mathematically justified. If there is no mix and error in eq. (2) and the networks were trained using this loss (and based on provided code they were using this loss), my wild guess of explaining why this may give best results in the experiments is that the models were trained for surprisingly small number of epochs. Therefore, a hypothesis would be that this small number of epochs did not allow the networks to switch sigmas to 0. Until the authors can clarify and justify the objective, I will vote for rejection only based on this ground. However, there are other issues in the paper as well. First of all, its clarity. It seems that the paper requires a lot of polishing. The first paragraph of this review is based on my assumptions from the paper since I am not completely sure I understand it correctly. More about the clarity issues below For strong evaluation, comparison with Malinin & Gales (2018) work seems to be important since it was the only work also using uncertainties for OOD detection in related work. Also related work section does not look like an exhaustive overview. Some of the detailed comments: 1. “In other words, in-distribution samples possess more features that convolutional filters react to than OOD samples” – first of all, this sentence is not easy to parse. Secondly, it is unclear, why this should be true. If OOD samples are still natural images, they would contain edges just the same as in-distribution samples. That is the power of deep learning enabling transfer learning, that low-level features are the transferable across different data and tasks. Therefore, the claim that “Therefore, the uncertainties of the features will be larger when the inputs are in-distribution samples” requires more elaboration and arguments 2. The arguments of the next paragraph regarding uncertainty of deeper layers should be larger for OOD samples are not very convincing either. It is either requires a definition what the authors mean here as uncertainty, or it is not necessarily true that absence of fixed regions for embeddings leads to higher uncertainty. 3. 3rd and 4th paragraphs in Introduction have too many repetitions of phrases between each other. Compare, e.g. the first sentences of the paragraphs or the last sentences. 4. “One cause of the abovementioned problem is that their approaches” and similarly the next paragraph: “their approaches” stylistically sound wrong. It is appropriate in the previous paragraph since there is a link to “previous studies”. It seems that “these approaches” or “the existing approaches” would be a better choice for this and the next paragraph. 5. “Each uncertainty is easily estimated after training the discriminative model by computing the mean and the variance of their features using a reparameterization trick” – conventional discriminative models do not estimate mean and variances of the features. The issue of estimating uncertainty is addressed by several special methods such as Bayesian variational methods used in the referred papers. Therefore, in order to use a reparametrisation trick one need to firstly choose a special class of models, which is not obvious from the text. 6. “Moreover, UFEL is robust to hyperparameters such as the number of in-distribution classes and the validation dataset.” – the size of the validation dataset? In any case neither size of the validation dataset nor the validation dataset itself are not hyperparameters (should not be hyperparameters for out-of-distribution detection). The number of classes can hardly be called a hyperparameter also. 7. “depends on the difference in the Dirichlet distribution of the categorical parameter <…> In our work, the distribution of the logit of the categorical parameters” – what is/are this/these categorical parameter(s)? 8. “Further, they estimate the parameter of the Dirichlet distribution using a DNN and train the model with in-distribution and OOD datasets” – this sentence may mislead to impression that the proposed method does not need OOD dataset for training, which does not seem to be the case, since lambda and \theta are trained based on OOD samples 9. “because they will not be relevant to the classification accuracy” – who are they? 10. “and epsilon is the Gaussian noise” –> the standard Gaussian noise 11. “where z^0 = x” – it seems this should be placed somewhere earlier when z^l is introduced since z^0 is not used in eq.(2) after which this text is placed 12. It is unclear how lambda^l and CNN \theta are learnt 13. It is unclear how the values of features d(x) are used to detect OOD samples 14. “comparison methods, and models” – not clear what models mean here 15. Missing references to datasets in the main text. At least reference to Appendix A.2 is required 16. “We used 5,000 validation images split from each training dataset and chose the parameter that can obtain” – which parameter? 17. “All the hyperparameters of ODIN” – a reader does not know yet that ODIN is used for comparison 18. “which consists of 100 OOD images from the test dataset and 1,000 images from the in-distribution validation set” – it is a bit confusing to call OOD dataset as a test dataset in this context 19. “We tuned the parameters of the CNN in Equation 4 using 50 validation training images taken from the 100 validation images. The best parameters were chosen by validating the performance using the rest of 50 validation images.” – this is confusing. What parameters do the authors talk about in the second sentence if not the parameters of the CNN? 20. “We used TNR at 95% TPR, AUROC, AUPR, and accuracy (ACC),” – Some elaboration is required, at least the reference to Appendix A.1. What is the changing threshold for AUROC and AUPR? Why AUPR-In and AUPR-Out are considered and only a single AUROC is considered. What is the positive class for AUROC? 23. “For LeNet5, we increased the number of channels of the original LeNet5 to improve accuracy” – do the authors mean that they allowed RGB images as input rather than greyscale? If yes, this explicit explanation would be preferable 24. “We inserted the reparameterization trick” – not the best word choice. Reparametrisation trick is a computational/implementation trick/method and it is hard to say that it can be inserted into a network. I believe what the authors mean is that they inserted mean/std outputs instead of point outputs. Conceptually, this means that the output of the corresponding layers is considered to be stochastic rather than deterministic. Also, it is unclear when the authors say they insert it to the softmax layer. According to Section 3 the softmax layer is never considered to output means and stds. 25. The numbers of epochs for training NNs are very small for LeNet and WideResNet in the experiments. Did the models manage to converge during this short training? Minor: 1. “These data were also used” -> “this data”\n",
      "      - Review:###The paper proposed to amend the 2nd order formulation of DARTS for improved stability. The main idea is to leverage the stationarity condition at a local optimum w* (that the derivative of the training loss equals zero). This is an interesting method, though the technique itself is not new outside the sub-field of architecture search. For instance, the same trick has been used in [1] for gradient-based hyperparameter optimization. Note the continuous architecture alpha is mathematically equivalent to a high-dimensional hyperparameter. Some major concerns: * The authors correctly pointed out that the original 2nd-order approximation in DARTS is insufficient to make an accurate gradient direction. On the other hand, the proposed approximation in equation (8) seems aggressive enough to lead to outrageous approximation error. Specifically, the only guarantee is that the gradients before and after approximation form a non-negative angle (that their inner product is non-negative), whereas the angle alone can be insufficient to ensure the quality of individual elements, especially for high-dimension vectors. * According to the authors, an advantage of the proposed approach is that one does not have to rely on early-stopping rules which require human expertise and “violates the fundamental ideology of AutoML” (Section 3.2). However, I am not fully convinced that the proposed method has an edge on this in practice, given the additional hyperparameter eta (one may argue that those handcrafted early stopping rules are robust enough, just like the empirical statement in the paper that eta = 0.1 worked well across the experiments) and potential approximation errors due to the Hessian. * I*m also a bit concerned about the similar empirical performance but longer search time when comparing with other DARTS variants in Table 1 (using search space S1). Minor issue: In the introduction, the authors argue “convergence in search often leads to bad performance in re-training”, saying that a high validation accuracy during search is not a good indicator for the final performance. On the other hand, the goal of the proposed method is exactly to maximize the former rather than the latter. I believe this reasoning needs to be revised. Question: * Since each architecture gradient step is of comparable to the cost of 2nd order DARTS (which took 4 GPU days with 4 search repeats), it is not immediately clear why the proposed Amended-DARTS took only 1.1 GPU days (Table 1 & 2). Can you explain where did the speedup come from? * Is there a particular reason to fix the edges in S1 and make it smaller than the original DARTS space? The question is relevant here because ideally we want an apple-to-apple comparison of those methods in the same space. [1] Pedregosa, Fabian. *Hyperparameter optimization with approximate gradient.*, ICML 2016 ==== post-rebuttal comments ==== I would like to thank the authors for addressing some of my questions in the rebuttal. I decide to keep my score unchanged (weak reject). The reasons are as follows: (1) Trying to improve DARTS from the optimization perspective is certainly interesting. However, the proposed approximation technique is rather a heuristic and only addresses the issue at a superficial level. (2) Evaluating the proposed algorithm in the original DARTS space (in addition to the restricted space in the current manuscript) will substantially strengthen the paper. (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###The paper proposed to amend the 2nd order formulation of DARTS for improved stability. The main idea is to leverage the stationarity condition at a local optimum w* (that the derivative of the training loss equals zero). This is an interesting method, though the technique itself is not new outside the sub-field of architecture search. For instance, the same trick has been used in [1] for gradient-based hyperparameter optimization. Note the continuous architecture alpha is mathematically equivalent to a high-dimensional hyperparameter. Some major concerns: * The authors correctly pointed out that the original 2nd-order approximation in DARTS is insufficient to make an accurate gradient direction. On the other hand, the proposed approximation in equation (8) seems aggressive enough to lead to outrageous approximation error. Specifically, the only guarantee is that the gradients before and after approximation form a non-negative angle (that their inner product is non-negative), whereas the angle alone can be insufficient to ensure the quality of individual elements, especially for high-dimension vectors. * According to the authors, an advantage of the proposed approach is that one does not have to rely on early-stopping rules which require human expertise and “violates the fundamental ideology of AutoML” (Section 3.2). However, I am not fully convinced that the proposed method has an edge on this in practice, given the additional hyperparameter eta (one may argue that those handcrafted early stopping rules are robust enough, just like the empirical statement in the paper that eta = 0.1 worked well across the experiments) and potential approximation errors due to the Hessian. * I*m also a bit concerned about the similar empirical performance but longer search time when comparing with other DARTS variants in Table 1 (using search space S1). Minor issue: In the introduction, the authors argue “convergence in search often leads to bad performance in re-training”, saying that a high validation accuracy during search is not a good indicator for the final performance. On the other hand, the goal of the proposed method is exactly to maximize the former rather than the latter. I believe this reasoning needs to be revised. Question: * Since each architecture gradient step is of comparable to the cost of 2nd order DARTS (which took 4 GPU days with 4 search repeats), it is not immediately clear why the proposed Amended-DARTS took only 1.1 GPU days (Table 1 & 2). Can you explain where did the speedup come from? * Is there a particular reason to fix the edges in S1 and make it smaller than the original DARTS space? The question is relevant here because ideally we want an apple-to-apple comparison of those methods in the same space. [1] Pedregosa, Fabian. *Hyperparameter optimization with approximate gradient.*, ICML 2016 ==== post-rebuttal comments ==== I would like to thank the authors for addressing some of my questions in the rebuttal. I decide to keep my score unchanged (weak reject). The reasons are as follows: (1) Trying to improve DARTS from the optimization perspective is certainly interesting. However, the proposed approximation technique is rather a heuristic and only addresses the issue at a superficial level. (2) Evaluating the proposed algorithm in the original DARTS space (in addition to the restricted space in the current manuscript) will substantially strengthen the paper.\n",
      "      - Review:###This paper proposes a method called *Dimensional Reweighting* for graph convolutional networks. The method involves a reparametrization of GCNs (by adding an extra reweighting block in each layer), which the authors show theoretically can reduce variance. The authors supplement this with extensive empirical experiments showing slightly improved performance by adding their method to existing methods. I would vote to weakly reject this paper for two key reasons - first, I think the writing can be improved and explanations can be clarified, especially for people less familiar with the field like myself. Second, I am not certain how significant the final experimental improvements are (other than on the Reddit dataset), as most of the numbers are not that far apart, and it seems that different methods in the literature already produce fairly different results. Overall, I think the structure of the paper is fairly good. I feel that a few things should be modified for clarity. - You claim a 40% improvement in error rate in the intro, which sounds enormous. I would say *relative error rate* to avoid overclaiming, because 40% improvement sounds like you are reducing (absolute) error from 60% to 20%, while in reality you are reducing error from 3.6% to 2.1%. - In section 2.1, did not know if the projection matrix W was learned or predefined. - I was not sure why you used sigma_g and sigma_s as opposed to just sigma in equation (5). Do you use different activation functions? Also, I did not find what activation function the authors end up using in their experiments. - I may have misunderstood something, but the theory does not seem to match the proposed method exactly. The mean-field theory analysis has the activation function after the reweighting by S but before multiplying by W, while the framework in Section 2.2 has the activation after the reweighting by S and after the multiplying by W. I am not sure how much this difference makes, or if it is significant, but I think it should be explained by the authors. - I also did not understand exactly what the *variance* the authors are reducing is. The authors talks about *reducing the learning variance brought by perturbations on the input,* but when is the input ever perturbed for GCNs? Explaining this more clearly would improve the motivation for this work. - I would appreciate a better intuitive explanation of the measure *K.* I gather that it is related to the *variance* being reduced, but it is different from that. The experimental results are good overall, as the proposed method tends to give the best results (by a small margin) across the board. I especially appreciate that the authors performed many experiments over many different datasets and repeated runs 20 times to try to get confident estimates of how well each method performs. I also appreciate that the authors cleaned up the Citeseer and Cora datasets, and I hope the cleaned datasets will be useful for the research community. With that said, I do not know how significant the improvement is. I think something that would be helpful would be to measure the *variance* that the method is supposed to be reducing (since it sounds like it is not exactly the same thing as K), and showing that in a table as well. This would show experimentally that the method achieves its intended goal. Minor comments - I would recommend that the authors proofread for English grammar and style in updated versions of the paper. For example, in the first paragraph of the introduction, the authors use *is proposed* instead of *were proposed* and typo *broad* as *board.* - Just curious, why did you choose a 2 layer network with 2 activation functions for the Dr block? Why not just have 1 hidden layer? (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper proposes a method called *Dimensional Reweighting* for graph convolutional networks. The method involves a reparametrization of GCNs (by adding an extra reweighting block in each layer), which the authors show theoretically can reduce variance. The authors supplement this with extensive empirical experiments showing slightly improved performance by adding their method to existing methods. I would vote to weakly reject this paper for two key reasons - first, I think the writing can be improved and explanations can be clarified, especially for people less familiar with the field like myself. Second, I am not certain how significant the final experimental improvements are (other than on the Reddit dataset), as most of the numbers are not that far apart, and it seems that different methods in the literature already produce fairly different results. Overall, I think the structure of the paper is fairly good. I feel that a few things should be modified for clarity. - You claim a 40% improvement in error rate in the intro, which sounds enormous. I would say *relative error rate* to avoid overclaiming, because 40% improvement sounds like you are reducing (absolute) error from 60% to 20%, while in reality you are reducing error from 3.6% to 2.1%. - In section 2.1, did not know if the projection matrix W was learned or predefined. - I was not sure why you used sigma_g and sigma_s as opposed to just sigma in equation (5). Do you use different activation functions? Also, I did not find what activation function the authors end up using in their experiments. - I may have misunderstood something, but the theory does not seem to match the proposed method exactly. The mean-field theory analysis has the activation function after the reweighting by S but before multiplying by W, while the framework in Section 2.2 has the activation after the reweighting by S and after the multiplying by W. I am not sure how much this difference makes, or if it is significant, but I think it should be explained by the authors. - I also did not understand exactly what the *variance* the authors are reducing is. The authors talks about *reducing the learning variance brought by perturbations on the input,* but when is the input ever perturbed for GCNs? Explaining this more clearly would improve the motivation for this work. - I would appreciate a better intuitive explanation of the measure *K.* I gather that it is related to the *variance* being reduced, but it is different from that. The experimental results are good overall, as the proposed method tends to give the best results (by a small margin) across the board. I especially appreciate that the authors performed many experiments over many different datasets and repeated runs 20 times to try to get confident estimates of how well each method performs. I also appreciate that the authors cleaned up the Citeseer and Cora datasets, and I hope the cleaned datasets will be useful for the research community. With that said, I do not know how significant the improvement is. I think something that would be helpful would be to measure the *variance* that the method is supposed to be reducing (since it sounds like it is not exactly the same thing as K), and showing that in a table as well. This would show experimentally that the method achieves its intended goal. Minor comments - I would recommend that the authors proofread for English grammar and style in updated versions of the paper. For example, in the first paragraph of the introduction, the authors use *is proposed* instead of *were proposed* and typo *broad* as *board.* - Just curious, why did you choose a 2 layer network with 2 activation functions for the Dr block? Why not just have 1 hidden layer?\n",
      "      - Review:###This paper tackles out-of-distribution samples detection via training VAE-like networks. The key idea is to inject learnable Gaussian noise to each layer across the network in the hope that the variance of the noise correlates well with the uncertainty of the input features. The network is trained to minimize the empirical loss subject to noise perturbation. The paper is well written, and the background is introduced clearly. As I understand it, the goal of *out-of-distribution sample detection* is to train a deep network that simultaneously generalizes well and also be discriminative to outliers. However, it’s not clear to me why the proposed method server this purpose; empirical results are not convincing either. My major concerns are as follows: First of all, from my intuition, it would be much easier to train deterministic networks than their counterparts with randomness. Empirically, researchers also often observe near-zero training loss for large deterministic networks such as Dense-BC trained on simple CIFAR/SVHN datasets. Especially, in this case, the training goal is simply to map higher-dimensional inputs to lower-dimensional classification categories. That being said, one would expect the variances go to zero at convergence to achieve lower empirical loss in the case of no additional diversity (or uncertainty) promotion terms. It is not clear to me how to avoid degenerate solutions at convergence while maintaining good testing performance with the proposed training strategy. From the empirical results, it also appears that all models reported might not be fully optimized? The baseline results are significantly worse than those reported in previous work. Specifically, in table 1, the testing accuracy of Dense-BC trained on CIFAR-100 is only 71.6. In table 2, the reported testing accuracy on CIFAR-10 using Dense-BC is 92.4. However, the results of DenseNet-BC (k=12, L=100, table 2) reported in the original paper are: CIFAR10 94.0 (also leave 5K examples as validation set) CIFAR100 75.9 Meanwhile, the reported accuracy of WRN-40-4 trained on CIFAR-10 and CIFAR-100 are 89.6 and 66.0, respectively. However, the corresponding baseline numbers in the original WRN paper are much higher, CIFAR-10 95.03 CIFAR-100 77.11 Could the authors comment on that? References: Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger. Densely Connected Convolutional Networks https://arxiv.org/abs/1608.06993 Sergey Zagoruyko, Nikos Komodakis. Wide Residual Networks. https://arxiv.org/pdf/1605.07146.pdf (L0, 1)\n",
      "        * Examples:\n",
      "          - Review:###This paper tackles out-of-distribution samples detection via training VAE-like networks. The key idea is to inject learnable Gaussian noise to each layer across the network in the hope that the variance of the noise correlates well with the uncertainty of the input features. The network is trained to minimize the empirical loss subject to noise perturbation. The paper is well written, and the background is introduced clearly. As I understand it, the goal of *out-of-distribution sample detection* is to train a deep network that simultaneously generalizes well and also be discriminative to outliers. However, it’s not clear to me why the proposed method server this purpose; empirical results are not convincing either. My major concerns are as follows: First of all, from my intuition, it would be much easier to train deterministic networks than their counterparts with randomness. Empirically, researchers also often observe near-zero training loss for large deterministic networks such as Dense-BC trained on simple CIFAR/SVHN datasets. Especially, in this case, the training goal is simply to map higher-dimensional inputs to lower-dimensional classification categories. That being said, one would expect the variances go to zero at convergence to achieve lower empirical loss in the case of no additional diversity (or uncertainty) promotion terms. It is not clear to me how to avoid degenerate solutions at convergence while maintaining good testing performance with the proposed training strategy. From the empirical results, it also appears that all models reported might not be fully optimized? The baseline results are significantly worse than those reported in previous work. Specifically, in table 1, the testing accuracy of Dense-BC trained on CIFAR-100 is only 71.6. In table 2, the reported testing accuracy on CIFAR-10 using Dense-BC is 92.4. However, the results of DenseNet-BC (k=12, L=100, table 2) reported in the original paper are: CIFAR10 94.0 (also leave 5K examples as validation set) CIFAR100 75.9 Meanwhile, the reported accuracy of WRN-40-4 trained on CIFAR-10 and CIFAR-100 are 89.6 and 66.0, respectively. However, the corresponding baseline numbers in the original WRN paper are much higher, CIFAR-10 95.03 CIFAR-100 77.11 Could the authors comment on that? References: Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger. Densely Connected Convolutional Networks https://arxiv.org/abs/1608.06993 Sergey Zagoruyko, Nikos Komodakis. Wide Residual Networks. https://arxiv.org/pdf/1605.07146.pdf\n",
      "- Violation of Double Blind or Review Guidelines (L3, 2)\n",
      "  - Review:###I support desk rejection since violating double blind rule, wrong format and insufficient length. (L2, 1)\n",
      "    * Examples:\n",
      "      - Review:###I support desk rejection since violating double blind rule, wrong format and insufficient length.\n",
      "  - Review:###This paper unfortunately violates the blind-review policy: its acknowledgement exposes the authors. I thus support desk rejection. (L2, 1)\n",
      "    * Examples:\n",
      "      - Review:###This paper unfortunately violates the blind-review policy: its acknowledgement exposes the authors. I thus support desk rejection.\n",
      "Calculated maximum depth for CSV columns: 4\n",
      "\n",
      "Saving indented hierarchy data to hierarchy_cs_nomap.csv...\n",
      "Save complete.\n",
      "NOTE: View 'hierarchy_cs_nomap.csv' in spreadsheet software for indentation effect.\n",
      "\n",
      "Processing Complete.\n"
     ]
    }
   ],
   "source": [
    "print_clusters(dfr_cs_frombase_nomap, 'review', 'hierarchy_cs_nomap.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706f3ba",
   "metadata": {},
   "source": [
    "## map-reduce\n",
    "this is the same as the map reduce case above, as indeed we map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a8c44b",
   "metadata": {},
   "source": [
    "# Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6dcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_level_0</th>\n",
       "      <th>_level_1</th>\n",
       "      <th>_level_2</th>\n",
       "      <th>_level_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This paper proposes Recency Bias, an adaptive...</td>\n",
       "      <td>Unclear or Problematic Comparison to Baselines</td>\n",
       "      <td>Empirical Validation and Baseline Comparison D...</td>\n",
       "      <td>Empirical Evaluation, Validation, and Experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review:###This paper proposes an interesting h...</td>\n",
       "      <td>Specific Technical or Algorithmic Concerns</td>\n",
       "      <td>Clarity and Reproducibility of Technical and M...</td>\n",
       "      <td>Clarity, Reproducibility, and Presentation of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review:###This paper explores a well motivated...</td>\n",
       "      <td>Lack of or Unclear Theoretical Justification/A...</td>\n",
       "      <td>Theoretical Foundation and Justification Issues</td>\n",
       "      <td>Theoretical Foundation, Analysis, and Justific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Review:###This paper proposes a training strat...</td>\n",
       "      <td>Lack of Comprehensive Experiments/Empirical Ev...</td>\n",
       "      <td>Empirical Evaluation and Experimental Comparis...</td>\n",
       "      <td>Empirical Evaluation, Validation, and Experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review:###This paper presents a training appro...</td>\n",
       "      <td>Lack of Thorough or Correctly Positioned Relat...</td>\n",
       "      <td>Novelty, Distinction from Prior Work, and Cita...</td>\n",
       "      <td>Novelty, Prior Work Distinction, and Contextua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Review:###The paper paper proposes a mutual in...</td>\n",
       "      <td>Specific Technical or Algorithmic Concerns</td>\n",
       "      <td>Clarity and Reproducibility of Technical and M...</td>\n",
       "      <td>Clarity, Reproducibility, and Presentation of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Review:###This paper proposes a self-supervise...</td>\n",
       "      <td>Hyperparameter Selection and Implementation/Re...</td>\n",
       "      <td>Clarity and Reproducibility of Technical and M...</td>\n",
       "      <td>Clarity, Reproducibility, and Presentation of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>I take issue with the usage of the phrase *sk...</td>\n",
       "      <td>Insufficient or Unconvincing Experimental Eval...</td>\n",
       "      <td>Empirical Evaluation and Experimental Comparis...</td>\n",
       "      <td>Empirical Evaluation, Validation, and Experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Review:###Although I assume somebody well-vers...</td>\n",
       "      <td>Confusing or incomplete modeling/notation</td>\n",
       "      <td>Clarity and Reproducibility of Technical and M...</td>\n",
       "      <td>Clarity, Reproducibility, and Presentation of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Review:###The paper proposes to learn a *virtu...</td>\n",
       "      <td>Confusing or incomplete modeling/notation</td>\n",
       "      <td>Clarity and Reproducibility of Technical and M...</td>\n",
       "      <td>Clarity, Reproducibility, and Presentation of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              _level_0  \\\n",
       "0     This paper proposes Recency Bias, an adaptive...   \n",
       "1    Review:###This paper proposes an interesting h...   \n",
       "2    Review:###This paper explores a well motivated...   \n",
       "3    Review:###This paper proposes a training strat...   \n",
       "4    Review:###This paper presents a training appro...   \n",
       "..                                                 ...   \n",
       "195  Review:###The paper paper proposes a mutual in...   \n",
       "196  Review:###This paper proposes a self-supervise...   \n",
       "197   I take issue with the usage of the phrase *sk...   \n",
       "198  Review:###Although I assume somebody well-vers...   \n",
       "199  Review:###The paper proposes to learn a *virtu...   \n",
       "\n",
       "                                              _level_1  \\\n",
       "0       Unclear or Problematic Comparison to Baselines   \n",
       "1           Specific Technical or Algorithmic Concerns   \n",
       "2    Lack of or Unclear Theoretical Justification/A...   \n",
       "3    Lack of Comprehensive Experiments/Empirical Ev...   \n",
       "4    Lack of Thorough or Correctly Positioned Relat...   \n",
       "..                                                 ...   \n",
       "195         Specific Technical or Algorithmic Concerns   \n",
       "196  Hyperparameter Selection and Implementation/Re...   \n",
       "197  Insufficient or Unconvincing Experimental Eval...   \n",
       "198          Confusing or incomplete modeling/notation   \n",
       "199          Confusing or incomplete modeling/notation   \n",
       "\n",
       "                                              _level_2  \\\n",
       "0    Empirical Validation and Baseline Comparison D...   \n",
       "1    Clarity and Reproducibility of Technical and M...   \n",
       "2      Theoretical Foundation and Justification Issues   \n",
       "3    Empirical Evaluation and Experimental Comparis...   \n",
       "4    Novelty, Distinction from Prior Work, and Cita...   \n",
       "..                                                 ...   \n",
       "195  Clarity and Reproducibility of Technical and M...   \n",
       "196  Clarity and Reproducibility of Technical and M...   \n",
       "197  Empirical Evaluation and Experimental Comparis...   \n",
       "198  Clarity and Reproducibility of Technical and M...   \n",
       "199  Clarity and Reproducibility of Technical and M...   \n",
       "\n",
       "                                              _level_3  \n",
       "0    Empirical Evaluation, Validation, and Experime...  \n",
       "1    Clarity, Reproducibility, and Presentation of ...  \n",
       "2    Theoretical Foundation, Analysis, and Justific...  \n",
       "3    Empirical Evaluation, Validation, and Experime...  \n",
       "4    Novelty, Prior Work Distinction, and Contextua...  \n",
       "..                                                 ...  \n",
       "195  Clarity, Reproducibility, and Presentation of ...  \n",
       "196  Clarity, Reproducibility, and Presentation of ...  \n",
       "197  Empirical Evaluation, Validation, and Experime...  \n",
       "198  Clarity, Reproducibility, and Presentation of ...  \n",
       "199  Clarity, Reproducibility, and Presentation of ...  \n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183cfe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 01:41:02,359 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Response:\n",
      "Certainly! Below is a **refined cluster hierarchy** based on the supplied data, with improved naming, removal of redundancy, and sensible merging of similar nodes/themes, all while preserving the hierarchical organization:\n",
      "\n",
      "---\n",
      "\n",
      "# Refined Cluster Hierarchy\n",
      "\n",
      "## _Level 3: Top-Level Themes_\n",
      "\n",
      "1. **Empirical Evaluation, Experimental Rigor, and Comparison**\n",
      "2. **Clarity, Reproducibility, and Presentation**\n",
      "3. **Novelty, Positioning, and Contextualization**\n",
      "4. **Theoretical Foundation, Analysis, and Justification**\n",
      "5. **Algorithmic/Technical Details, Problem Formulation, and Methodology**\n",
      "6. **Review/Conference Policy and Paper Quality Issues**\n",
      "7. **Specific Topical/Miscellaneous Issues**\n",
      "\n",
      "---\n",
      "\n",
      "## _Level 2: Merged and Renamed Subclusters_\n",
      "\n",
      "### 1. Empirical Evaluation, Experimental Rigor, and Comparison\n",
      "- Empirical Evaluation and Experimental Comparison\n",
      "- Empirical Validation and Baseline Comparison Deficiencies\n",
      "- Empirical Validation and Experimental Rigor\n",
      "- Experimental Sufficiency and Baseline Adequacy\n",
      "- Metrics and Experimental Reporting Issues\n",
      "- Hyperparameter, Tuning, and Methodology Concerns\n",
      "- Ablation Studies and Component Contribution\n",
      "- Generalizability, Scalability, or Efficiency Concerns\n",
      "- Insufficient or Unconvincing Experimental Evidence or Analysis\n",
      "- Issues with Baselines, Fair Comparisons, or Ablation Studies\n",
      "- Insufficient Statistical Reporting or Unconvincing Quantitative Support\n",
      "\n",
      "### 2. Clarity, Reproducibility, and Presentation\n",
      "- Clarity, Presentation, and Exposition Issues\n",
      "- Clarity, Writing, and Organization Problems\n",
      "- Missing, Incomplete, or Vague Explanation/Notation\n",
      "- Unclear Problem Formalization or Ambiguity in Explanation\n",
      "- Clarity, Reproducibility, and Presentation of Methods\n",
      "- Reproducibility and Implementation Details\n",
      "\n",
      "### 3. Novelty, Positioning, and Contextualization\n",
      "- Novelty, Distinction from Prior Work, and Citation Issues\n",
      "- Lack of Novelty or Incremental Technical Contribution\n",
      "- Overclaiming Novelty or Insufficient Distinction from Prior Work\n",
      "- Lack of Context, Related Work, or Missing References\n",
      "- Comparison to Prior or Related Work and Novelty Justification\n",
      "- Citation, Positioning, and Surveying of Literature\n",
      "- Dataset/Task/Protocol Novelty and Relevance\n",
      "\n",
      "### 4. Theoretical Foundation, Analysis, and Justification\n",
      "- Theoretical Foundation and Justification Issues\n",
      "- Theoretical Analysis and Conceptual Clarity\n",
      "- Unclear or Missing Theoretical Guarantees/Contribution\n",
      "- Theoretical Issues and Insufficient Justification/Connection\n",
      "- Misapplied Concepts or Flawed Assumptions\n",
      "\n",
      "### 5. Algorithmic/Technical Details, Problem Formulation, and Methodology\n",
      "- Clarity and Reproducibility of Technical and Methodological Details\n",
      "- Specific Technical or Algorithmic Concerns\n",
      "- Confusing or incomplete modeling/notation\n",
      "- Empirical Evaluation, Sample Efficiency, and Comparisons in Meta-learning/Data Valuation\n",
      "- Reward Formulation and Evaluation (for RL/IL papers)\n",
      "- Problem Formalization and Methodological Justification or Conceptual Errors\n",
      "- Presentation and Clarity of Algorithms and Experiments\n",
      "- Implementation/Hyperparameter/Training Details\n",
      "\n",
      "### 6. Review/Conference Policy and Paper Quality Issues\n",
      "- Violation of Double Blind or Submission Guidelines\n",
      "- Underlength, Poor Formatting, Non-anonymity\n",
      "- Out-of-scope for Conference, Misfit Domain\n",
      "\n",
      "### 7. Specific Topical/Miscellaneous Issues\n",
      "- Mutual Information Usage and Edge Feature Integration in Graph Neural Networks\n",
      "- Defense and Attack Training Schemes for Adversarial Robustness\n",
      "- Discussion of Specific Models/Techniques (e.g., Recommender feedback formulations, Skill Discovery framing)\n",
      "- Domain-specific or Task-specific Comments (as specialized sub-nodes as relevant)\n",
      "\n",
      "---\n",
      "\n",
      "## _Level 1: Further-Refined, Condensed Labels for Node Clarity_\n",
      "\n",
      "Below, at Level 1, clusters are named for the reviewer focus and condensed where possible:\n",
      "\n",
      "### For Level 2 Cluster: \"Empirical Evaluation, Experimental Rigor, and Comparison\"\n",
      "- Experimental Evaluation & Comparative Analysis\n",
      "- Validation/Ablation & Component Contribution\n",
      "- Generalizability and Scalability of Approach\n",
      "\n",
      "### For Level 2 Cluster: \"Clarity, Reproducibility, and Presentation\"\n",
      "- Explanation and Notation Quality\n",
      "- Writing Clarity & Organization\n",
      "- Reproducibility, Method Specification & Implementation\n",
      "\n",
      "### For Level 2 Cluster: \"Novelty, Positioning, and Contextualization\"\n",
      "- Novelty & Distinction from Prior Art\n",
      "- Prior Work Comparison & Contextual References\n",
      "- Dataset/Task Selection & Justification\n",
      "\n",
      "### For Level 2 Cluster: \"Theoretical Foundation, Analysis, and Justification\"\n",
      "- Theoretical Assumptions and Guarantees\n",
      "- Analysis, Proofs, and Conceptual Soundness\n",
      "- Model/Conceptual Misapplication\n",
      "\n",
      "### For Level 2 Cluster: \"Algorithmic/Technical Details, Problem Formulation, and Methodology\"\n",
      "- Problem/Formalization and Technical Design\n",
      "- Algorithm Specification & Methodological Details\n",
      "- Training, Hyperparameter & Implementation Issues\n",
      "\n",
      "### For Level 2 Cluster: \"Review/Conference Policy and Paper Quality Issues\"\n",
      "- Submission and Review Policy Violations\n",
      "- Paper Formatting, Length, and Scope Fit\n",
      "\n",
      "### For Level 2 Cluster: \"Specific Topical/Miscellaneous Issues\"\n",
      "- Topical Concerns: e.g., Mutual Information GNNs, Adversarial Defense Strategy, Skill Discovery, etc.\n",
      "- Task/Domain-Specific Comments\n",
      "\n",
      "---\n",
      "\n",
      "## Example Pathways Through the Hierarchy\n",
      "\n",
      "#### Example 1 (Empirical concerns for a label noise paper)\n",
      "- Level 3: Empirical Evaluation, Experimental Rigor, and Comparison\n",
      "  - Level 2: Empirical Validation and Experimental Comparison Issues\n",
      "    - Level 1: Experimental Evaluation & Comparative Analysis\n",
      "\n",
      "#### Example 2 (Clarity issues for a new skill discovery method)\n",
      "- Level 3: Clarity, Reproducibility, and Presentation\n",
      "  - Level 2: Clarity, Presentation, and Exposition Issues\n",
      "    - Level 1: Writing Clarity & Organization\n",
      "\n",
      "#### Example 3 (Novelty concerns for a new object detection approach)\n",
      "- Level 3: Novelty, Positioning, and Contextualization\n",
      "  - Level 2: Novelty, Distinction from Prior Work, and Citation Issues\n",
      "    - Level 1: Novelty & Distinction from Prior Art\n",
      "\n",
      "#### Example 4 (Theoretical analysis issues for a GNN paper)\n",
      "- Level 3: Theoretical Foundation, Analysis, and Justification\n",
      "  - Level 2: Theoretical Foundation and Justification Issues\n",
      "    - Level 1: Theoretical Assumptions and Guarantees\n",
      "\n",
      "---\n",
      "\n",
      "## Commentary on Merge/Refinement\n",
      "\n",
      "- \"Empirical Evaluation, Validation, and Experimental Comparison\" absorbs any issues about experiments, baselines, ablation, or comparative setup.\n",
      "- \"Clarity, Reproducibility, and Presentation\" collects all concerns about how the paper is explained, written, or the completeness of method explanations.\n",
      "- \"Novelty, Prior Work Distinction, and Contextualization\" merges lack of novelty/discussion/issues with missing related work/citations/contextual placement.\n",
      "- \"Theoretical Foundation, Analysis, and Justification\" merges theoretical justification, conceptual issues, and lack of guarantees.\n",
      "- \"Algorithmic/Technical Details, Problem Formulation, and Methodology\" incorporates concerns about technical correctness, formalization, or experiment/method details.\n",
      "- \"Review/Conference Policy and Paper Quality Issues\" absorbs all aspects of submission or review guideline violation and paper format/scope fit.\n",
      "- \"Specific Topical/Miscellaneous Issues\" is reserved for domain/task-specific issues or clusters not easily merged above.\n",
      "\n",
      "---\n",
      "\n",
      "**This refined structure should be both more readable and more helpful for downstream analysis or review.**\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from jinja2 import Template\n",
    "\n",
    "# Sample data representing the cluster hierarchy tree.\n",
    "\n",
    "# Define a jinja2 template to create the prompt.\n",
    "prompt_template = \"\"\"\n",
    "You are given a cluster hierarchy tree where each row represents a series of cluster names from the bottom level (leaf, _level_0) to higher levels (_level_1, _level_2, etc.). The current hierarchy is as follows:\n",
    "\n",
    "{% for index, row in df.iterrows() %}\n",
    "Row {{ index + 1 }}:\n",
    "{% for level in levels %}\n",
    "  {{ level }}: \"{{ row[level] }}\"\n",
    "{% endfor %}\n",
    "{% endfor %}\n",
    "\n",
    "Please refine this cluster hierarchy. In your refined hierarchy, modify names if necessary and merge similar clusters (i.e. merge nodes in the tree that represent similar themes) while preserving the hierarchical structure.\n",
    "\n",
    "Return the refined hierarchy in a clear, structured format.\n",
    "\"\"\"\n",
    "dfr_cs_frombase_nomap[[c for c in dfr_cs_frombase_nomap.columns if 'level_' in c]].drop_duplicates()\n",
    "df_remap = dfr_cs_frombase_nomap[[c for c in dfr_cs_frombase_nomap.columns if 'level_' in c]] \n",
    "# List the hierarchy levels in order (assuming the DataFrame columns are ordered as the hierarchy).\n",
    "levels = list(df_remap.columns)\n",
    "\n",
    "# Render the prompt using the jinja2 template.\n",
    "template = Template(prompt_template)\n",
    "prompt = template.render(df=df_remap, levels=levels)\n",
    "\n",
    "# # Log the prompt (for debugging purposes) before sending it to the LLM.\n",
    "# print(\"Prompt sent to the LLM:\")\n",
    "# print(prompt)\n",
    "\n",
    "# Call the LLM with the prompt.\n",
    "# Assume that 'llm' is an instance of your LLM class with _call implemented.\n",
    "refined_hierarchy = llm._call(prompt)\n",
    "\n",
    "print(\"\\nLLM Response:\")\n",
    "print(refined_hierarchy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098d949",
   "metadata": {},
   "source": [
    "2025-04-16 01:41:02,359 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
    "\n",
    "LLM Response:\n",
    "Certainly! Below is a **refined cluster hierarchy** based on the supplied data, with improved naming, removal of redundancy, and sensible merging of similar nodes/themes, all while preserving the hierarchical organization:\n",
    "\n",
    "---\n",
    "\n",
    "# Refined Cluster Hierarchy\n",
    "\n",
    "## _Level 3: Top-Level Themes_\n",
    "\n",
    "1. **Empirical Evaluation, Experimental Rigor, and Comparison**\n",
    "2. **Clarity, Reproducibility, and Presentation**\n",
    "3. **Novelty, Positioning, and Contextualization**\n",
    "4. **Theoretical Foundation, Analysis, and Justification**\n",
    "5. **Algorithmic/Technical Details, Problem Formulation, and Methodology**\n",
    "6. **Review/Conference Policy and Paper Quality Issues**\n",
    "7. **Specific Topical/Miscellaneous Issues**\n",
    "\n",
    "---\n",
    "\n",
    "## _Level 2: Merged and Renamed Subclusters_\n",
    "\n",
    "### 1. Empirical Evaluation, Experimental Rigor, and Comparison\n",
    "- Empirical Evaluation and Experimental Comparison\n",
    "- Empirical Validation and Baseline Comparison Deficiencies\n",
    "- Empirical Validation and Experimental Rigor\n",
    "- Experimental Sufficiency and Baseline Adequacy\n",
    "- Metrics and Experimental Reporting Issues\n",
    "- Hyperparameter, Tuning, and Methodology Concerns\n",
    "- Ablation Studies and Component Contribution\n",
    "- Generalizability, Scalability, or Efficiency Concerns\n",
    "- Insufficient or Unconvincing Experimental Evidence or Analysis\n",
    "- Issues with Baselines, Fair Comparisons, or Ablation Studies\n",
    "- Insufficient Statistical Reporting or Unconvincing Quantitative Support\n",
    "\n",
    "### 2. Clarity, Reproducibility, and Presentation\n",
    "- Clarity, Presentation, and Exposition Issues\n",
    "- Clarity, Writing, and Organization Problems\n",
    "- Missing, Incomplete, or Vague Explanation/Notation\n",
    "- Unclear Problem Formalization or Ambiguity in Explanation\n",
    "- Clarity, Reproducibility, and Presentation of Methods\n",
    "- Reproducibility and Implementation Details\n",
    "\n",
    "### 3. Novelty, Positioning, and Contextualization\n",
    "- Novelty, Distinction from Prior Work, and Citation Issues\n",
    "- Lack of Novelty or Incremental Technical Contribution\n",
    "- Overclaiming Novelty or Insufficient Distinction from Prior Work\n",
    "- Lack of Context, Related Work, or Missing References\n",
    "- Comparison to Prior or Related Work and Novelty Justification\n",
    "- Citation, Positioning, and Surveying of Literature\n",
    "- Dataset/Task/Protocol Novelty and Relevance\n",
    "\n",
    "### 4. Theoretical Foundation, Analysis, and Justification\n",
    "- Theoretical Foundation and Justification Issues\n",
    "- Theoretical Analysis and Conceptual Clarity\n",
    "- Unclear or Missing Theoretical Guarantees/Contribution\n",
    "- Theoretical Issues and Insufficient Justification/Connection\n",
    "- Misapplied Concepts or Flawed Assumptions\n",
    "\n",
    "### 5. Algorithmic/Technical Details, Problem Formulation, and Methodology\n",
    "- Clarity and Reproducibility of Technical and Methodological Details\n",
    "- Specific Technical or Algorithmic Concerns\n",
    "- Confusing or incomplete modeling/notation\n",
    "- Empirical Evaluation, Sample Efficiency, and Comparisons in Meta-learning/Data Valuation\n",
    "- Reward Formulation and Evaluation (for RL/IL papers)\n",
    "- Problem Formalization and Methodological Justification or Conceptual Errors\n",
    "- Presentation and Clarity of Algorithms and Experiments\n",
    "- Implementation/Hyperparameter/Training Details\n",
    "\n",
    "### 6. Review/Conference Policy and Paper Quality Issues\n",
    "- Violation of Double Blind or Submission Guidelines\n",
    "- Underlength, Poor Formatting, Non-anonymity\n",
    "- Out-of-scope for Conference, Misfit Domain\n",
    "\n",
    "### 7. Specific Topical/Miscellaneous Issues\n",
    "- Mutual Information Usage and Edge Feature Integration in Graph Neural Networks\n",
    "- Defense and Attack Training Schemes for Adversarial Robustness\n",
    "- Discussion of Specific Models/Techniques (e.g., Recommender feedback formulations, Skill Discovery framing)\n",
    "- Domain-specific or Task-specific Comments (as specialized sub-nodes as relevant)\n",
    "\n",
    "---\n",
    "\n",
    "## _Level 1: Further-Refined, Condensed Labels for Node Clarity_\n",
    "\n",
    "Below, at Level 1, clusters are named for the reviewer focus and condensed where possible:\n",
    "\n",
    "### For Level 2 Cluster: \"Empirical Evaluation, Experimental Rigor, and Comparison\"\n",
    "- Experimental Evaluation & Comparative Analysis\n",
    "- Validation/Ablation & Component Contribution\n",
    "- Generalizability and Scalability of Approach\n",
    "\n",
    "### For Level 2 Cluster: \"Clarity, Reproducibility, and Presentation\"\n",
    "- Explanation and Notation Quality\n",
    "- Writing Clarity & Organization\n",
    "- Reproducibility, Method Specification & Implementation\n",
    "\n",
    "### For Level 2 Cluster: \"Novelty, Positioning, and Contextualization\"\n",
    "- Novelty & Distinction from Prior Art\n",
    "- Prior Work Comparison & Contextual References\n",
    "- Dataset/Task Selection & Justification\n",
    "\n",
    "### For Level 2 Cluster: \"Theoretical Foundation, Analysis, and Justification\"\n",
    "- Theoretical Assumptions and Guarantees\n",
    "- Analysis, Proofs, and Conceptual Soundness\n",
    "- Model/Conceptual Misapplication\n",
    "\n",
    "### For Level 2 Cluster: \"Algorithmic/Technical Details, Problem Formulation, and Methodology\"\n",
    "- Problem/Formalization and Technical Design\n",
    "- Algorithm Specification & Methodological Details\n",
    "- Training, Hyperparameter & Implementation Issues\n",
    "\n",
    "### For Level 2 Cluster: \"Review/Conference Policy and Paper Quality Issues\"\n",
    "- Submission and Review Policy Violations\n",
    "- Paper Formatting, Length, and Scope Fit\n",
    "\n",
    "### For Level 2 Cluster: \"Specific Topical/Miscellaneous Issues\"\n",
    "- Topical Concerns: e.g., Mutual Information GNNs, Adversarial Defense Strategy, Skill Discovery, etc.\n",
    "- Task/Domain-Specific Comments\n",
    "\n",
    "---\n",
    "\n",
    "## Example Pathways Through the Hierarchy\n",
    "\n",
    "#### Example 1 (Empirical concerns for a label noise paper)\n",
    "- Level 3: Empirical Evaluation, Experimental Rigor, and Comparison\n",
    "  - Level 2: Empirical Validation and Experimental Comparison Issues\n",
    "    - Level 1: Experimental Evaluation & Comparative Analysis\n",
    "\n",
    "#### Example 2 (Clarity issues for a new skill discovery method)\n",
    "- Level 3: Clarity, Reproducibility, and Presentation\n",
    "  - Level 2: Clarity, Presentation, and Exposition Issues\n",
    "    - Level 1: Writing Clarity & Organization\n",
    "\n",
    "#### Example 3 (Novelty concerns for a new object detection approach)\n",
    "- Level 3: Novelty, Positioning, and Contextualization\n",
    "  - Level 2: Novelty, Distinction from Prior Work, and Citation Issues\n",
    "    - Level 1: Novelty & Distinction from Prior Art\n",
    "\n",
    "#### Example 4 (Theoretical analysis issues for a GNN paper)\n",
    "- Level 3: Theoretical Foundation, Analysis, and Justification\n",
    "  - Level 2: Theoretical Foundation and Justification Issues\n",
    "    - Level 1: Theoretical Assumptions and Guarantees\n",
    "\n",
    "---\n",
    "\n",
    "## Commentary on Merge/Refinement\n",
    "\n",
    "- \"Empirical Evaluation, Validation, and Experimental Comparison\" absorbs any issues about experiments, baselines, ablation, or comparative setup.\n",
    "- \"Clarity, Reproducibility, and Presentation\" collects all concerns about how the paper is explained, written, or the completeness of method explanations.\n",
    "- \"Novelty, Prior Work Distinction, and Contextualization\" merges lack of novelty/discussion/issues with missing related work/citations/contextual placement.\n",
    "- \"Theoretical Foundation, Analysis, and Justification\" merges theoretical justification, conceptual issues, and lack of guarantees.\n",
    "- \"Algorithmic/Technical Details, Problem Formulation, and Methodology\" incorporates concerns about technical correctness, formalization, or experiment/method details.\n",
    "- \"Review/Conference Policy and Paper Quality Issues\" absorbs all aspects of submission or review guideline violation and paper format/scope fit.\n",
    "- \"Specific Topical/Miscellaneous Issues\" is reserved for domain/task-specific issues or clusters not easily merged above.\n",
    "\n",
    "---\n",
    "\n",
    "**This refined structure should be both more readable and more helpful for downstream analysis or review.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assume this variable holds the refined hierarchy produced by the LLM previously.\n",
    "# Replace the triple-quoted string below with the actual refined hierarchy text.\n",
    "\n",
    "\n",
    "# Create a jinja2 prompt template for the follow-up refinement.\n",
    "prompt_template = \"\"\"\n",
    "The following is a refined cluster hierarchy based on an earlier task:\n",
    "\n",
    "{{ refined_hierarchy }}\n",
    "\n",
    "I would like to further refine this cluster hierarchy.\n",
    "In particular, please modify the hierarchy by **separating out the \"Ablation Studies\" aspects as their own distinct category**.\n",
    "That is, instead of having ablation studies merged with other experimental issues, please create a separate node (or nodes) for ablation studies.\n",
    "Ensure that the overall hierarchical structure remains clear and consistent.\n",
    "\n",
    "Return the updated refined cluster hierarchy in a clear, structured format.\n",
    "\"\"\"\n",
    "\n",
    "# Render the prompt using the jinja2 template\n",
    "template = Template(prompt_template)\n",
    "prompt = template.render(refined_hierarchy=refined_hierarchy)\n",
    "\n",
    "print(\"Prompt sent to the LLM:\")\n",
    "print(prompt)\n",
    "\n",
    "# Send the prompt to the LLM using its _call method.\n",
    "# Assume 'llm' is an instance of your LLM class with _call implemented.\n",
    "response = llm._call(prompt)\n",
    "\n",
    "print(\"\\nLLM Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3db88",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
